[{"id":"2008-09-08-digital-therapy-luncheon","title":"\"Digital Therapy\" luncheon","author":"admin","date":"2008-09-08 13:36:19 -0400","categories":["Announcements"],"url":"digital-therapy-luncheon","content":"Please join us in the Scholars' Lab at noon on Tuesday, September 9th, as we introduce our new [Graduate Fellows in Digital Humanities](http://www.lib.virginia.edu/scholarslab/about/fellowship.html) as part of our first \"Digital Therapy\" Faculty and Grad Luncheon of the semester.\n\nWith projects in social networking, geospatial analysis, and cultural mapping, these three doctoral candidates -- Jean Bauer of the History Department, Pierre Dairon of the French Department, and Abigail Holeman of the Anthropology Department -- are applying exciting new methods to the study of early American history, French literature, and Mesoamerican cosmology. Each will speak briefly about his or her research, and a free lunch will be served.\n"},{"id":"2008-09-08-hello-world-2","title":"Hello, world!","author":"bethany-nowviskie","date":"2008-09-08 13:28:54 -0400","categories":["Announcements"],"url":"hello-world-2","content":"I'm here to cut the ribbon on the [Scholars' Lab](http://www.lib.virginia.edu/scholarslab/index.html) blog.\n\nThe Scholars' Lab was established two short years ago at [UVA Library](http://lib.virginia.edu) as a site for innovation in the humanities and social sciences.  The idea was to combine the resources and expertise of the Library’s successful **Electronic Text** (Etext) and **Geospatial and Statistical Data** (GeoStat) centers with that of UVA's **Research Computing Support Group** in a physical space that promotes collaboration and experimentation.  Now we’re extending the conversations that happen in our offices and in the SLab to a wider forum.\n\nThe past two years have seen amazing work by our [Graduate Fellows in Digital Humanities](http://www.lib.virginia.edu/scholarslab/about/fellows.html) and by scholars from a variety of disciplines and fields who work in collaboration with our on-site experts.  Over the coming months, we’ll be inviting our Fellows, grad student consultants, [Scholars’ Lab faculty and staff](http://www.lib.virginia.edu/scholarslab/consultation/index.html), visiting scholars, and UVA collaborators to share this blog and make it their own.  The substantive (and [CC-licensed](http://creativecommons.org/licenses/by-sa/3.0/)) posts you'll find here will be vetted to ensure that they represent sound scholarship and are squarely on-topic.  We'll also use this space to share announcements about SLab [events](http://www.lib.virginia.edu/scholarslab/about/events.html) and links to interesting and evocative uses of technology in the humanities and social sciences.\n\nWe invite you to comment and engage with us here!\n"},{"id":"2008-09-09-how-to-measure-text","title":"How to Measure Text?","author":"chris-forster","date":"2008-09-08 20:03:13 -0400","categories":["Digital Humanities","Visualization and Data Mining"],"url":"how-to-measure-text","content":"<blockquote>...the words we join have been joined before, and continue to be joined daily. So writing is largely quotation, quotation newly energized, as a cyclotron augments the energies of common particles circulating.\n\n- Hugh Kenner, _The Pound Era_</blockquote>\n\n\nThis month marks the beginning of the complicated process of starting up the [Large Hadron Collider](http://lhc.web.cern.ch/lhc/), the world's largest particle accelerator (Kenner would haved called it a \"cyclotron\"), buried beneath the Franco-Swiss border. Near the top of the LHC's agenda is having a peek into the fabric of space-time to see about the Higgs-Boson, the theorized source of mass.\n\nBut to do so they'll need data--lots of data. According to [CERN](http://gridcafe.web.cern.ch/gridcafe/animations/LHCdata/LHCdata.html), the event summary data extracted from the collider's sensors will produce around 10 terabytes daily. That is something like, to use the cliché, the equivalent of a Library of Congress's worth of data every day (the raw data is much much greater).\n\n\nThe physics involved is obviously too complicated for a mere humanities major to discuss in any intelligent way. The interesting thing is the disparity between the sheer amount of data with which the LHC deals, as compared with the scale of the (textual) data of the humanities. How can the LHC, in a single day, focussed on a highly specific set of questions, produce as much information as the literary output of humans represented by the Library of Congress? Why, in short, is the textual data of the humanities so much smaller than the data produced by the LHC?\n\nIt is, of course, in some ways a silly, completely naive question. But the differences, in size alone, of these two datasets are nevertheless instructive and worthy of consideration. We might oversimplify the matter, and say that the LHC's data, collected from its sensors and culled by its arrays of servers, is fundamentally information-poor data. The challenge faced by the LHC project is sorting through the complexities of the data to find the relevant information that will allow physicists to answer the questions they have. Language, by contrast, is information rich--so rich that our challenge is not how to separate the wheat from the chaff, but how to deal with the sheer flood of information compressed in text.\n\nIt is this fact that explains the disparity in size between the LHC's data and the textual record of the humanities. The textual data of the humanities comes \"preorganized\" by language. While our digital texts encode only strings, language fills texts with syntactic and semantic information of which our systems of markup are completely oblivious.\n\nMartin Wattenberg at IBM's Watson Research Center puts it well in his [interview with _Wired_](http://www.wired.com/science/discoveries/magazine/16-07/pb_visualizing) when he describes language's ability to compress information:\n\n\n<blockquote>Language is one of the best data-compression mechanisms we have. The information contained in literature or email, encodes our identity as human beings. The entire literary canon may be smaller than what comes out of particle accelerators or models of the human brain, but the meaning coded into words can't be measured in bytes. It's deeply compressed. Twelve words from Voltaire can hold a lifetime of experience.</blockquote>\n\n\nWhat happens if we take this understanding of language seriously? How would it change the way we deal with textual data?\n\nRight now we have plenty of digital texts available, but in order to get the information out of the textual data we have to _read _it. Right now, only by reading do we attend to the specifically linguistic nature of textual data. Existing text analysis technologies and techniques remain largely quantitative, relying on machine learning techniques to classify texts that are represented by vectors of frequency counts. Key sources of linguistic information, however, like syntax, remain fundamentally unexploited. We are still, in effect, discarding some of the most basic sources of textual information--such as the _order_ in which the words occur (seriously).\n\nOne avenue, though admittedly crude, is to use a technique like part-of-speech tagging to supplement raw text with part-of-speech tags which provide a fuller, more information-rich digital representation of the linguistic data. By analysing such part-of-speech tags, taking them in pairs, or looking at where in a sentence they occur, we get some sense of how a writer uses language. We step, in short, over the threshold from a purely quantitative view of language use (e.g. how many times does \"of\" occur per thousand words? what are the most frequently occurring terms?), to a mode of analysis that is able to extract the sort of information that we, humans, are able to when we read. Such techniques are admittedly crude; but they begin to recapture the fundamentally linguistic nature of textual data which is too easily discarded in representations of natural languages. To truly capitalize on the information contained in textual data requires finding more ways to digitally attend to the specifically linguistic nature of textual data.\n\nWe are trying to read the finely wrought braille of language through the burlap sack that current digital tools offer. With the combination of natural language processing tools (such as POS taggers, parsers, etc) and ever-more sophisticated machine learning techniques, we may be able to get closer. Humanities data is not, necessarily, smaller--it is just more compressed.\n"},{"id":"2008-09-12-all-good-press-is-local","title":"all good press is local","author":"admin","date":"2008-09-12 08:24:46 -0400","categories":["Announcements"],"url":"all-good-press-is-local","content":"Today's edition of [UVA Today](http://www.virginia.edu/uvatoday/index.php) covers our Grad Fellows program and our first luncheon of the semester. [Read all about it!](http://www.virginia.edu/uvatoday/newsRelease.php?id=6418)\n\n[![image from UVA Today online edition](http://farm4.static.flickr.com/3131/2851294584_993036d123.jpg)](http://www.virginia.edu/uvatoday/newsRelease.php?id=6418)\n"},{"id":"2008-09-16-normality-for-or-against","title":"Normality: For or Against?","author":"jean-bauer","date":"2008-09-16 12:07:59 -0400","categories":["Digital Humanities"],"url":"normality-for-or-against","content":"I’m a historian who is currently designing and/or building four databases.  As I work through the complexities of each project, I’m struck by two thoughts.\n\nFirst: I’m overworked.\n\nSecond: I like the way relational algebra makes me think.\n\nGood database design involves breaking a data set into the smallest viable components and then linking those components back together to facilitate complex analysis.  This process, known as normalization, helps keep the data set free of duplicates and protects the data from being unintentionally deleted or unevenly updated.\n\nAs I research merchants in the eighteenth century and how they connected people and empires with far-flung locations and transfered goods and ideas across oceans, I find it helpful to break those multivalent connections into discrete units.  Who wrote to whom?  Who worked for whom?  Who became a diplomat or consul for the United States?  Who recommended him for that position?  And so on.  Each question has become a relationship in my design for the Early American Foreign Service Database (EAFSD), and by linking all this (and more) information together, the EAFSD will track how the U.S. Foreign Service developed over fifty years.  But there is a catch.\n\nWhen the database is done, I plan on publishing it online so that other researchers can have access to its data.  However, I cannot deny that the EAFSD was designed to answer questions specific to my dissertation.  Other researchers looking at information gathered from the papers of diplomats, consuls, and merchants will (hopefully) want to ask other questions which my database may or not be able to answer.  For example, I only focus on merchants who had a clear connection to the U.S. government (_i.e._, received positions in the Foreign Service), which means that a large segment of the merchant community will not appear in the database.\n\nAlong with the completed database I plan on releasing the source code (both for the database itself and the web application that permits the data migrations and the basic query structure) under an open source license, hopefully making it easier for other scholars to create their own relational databases to track social networks and institutional development.  Once those databases are published similar issues will arise.\n\nWhen a scholar decides to use a relational database in her research, she is making a decision about methodology -- not theory.  A relational database does not dictate what scholars will find in a given data set, but rather shapes their search in ways that need to remain in the forefront of all our minds, even if the methodological discussions get relegated to footnotes or appendices.  If an astronomer has to state the specifications of the telescope along with the data received, a digital humanist should be clear about the choices she made (and why) in designing a database to facilitate her analysis and the analytical limits of the final design.\n\nI became a historian because I see the world as a complex and contingent place that doesn’t respond well to being forced into a constraining model.   While having the EAFSD is a necessary condition of my dissertation it is not a sufficient one.\n\nThere are real world ambiguities and unpredictable turns in my subject matter which _should not_ be modeled in a relational data structure.  High on this list are the many mistakes made by early American diplomats: John Adams picking a fight with the French Foreign Minister in the middle of the Revolutionary War (subject of my Master’s Thesis), James Monroe being recalled by a furious George Washington after denouncing (accurate) rumors regarding a new treaty with Great Britain, Thomas Jefferson breaking the Law of Nations to help Lafayette write the Rights of Man and Citizen, the list goes on and on.  On the other hand, while the database also fails to capture the sheer brilliance of Benjamin Franklin it does hint at John Quincy Adams’ compulsive attention to detail.  None of these stories or personalities map into the database, but they are all crucial to understanding how the newly United States interacted with the larger Atlantic World.\n\nDesigning the EAFSD has sharpened my historical analysis but narrative prose blurs the edges back into the delightfully abnormal lives of the people I seek to understand. \n"},{"id":"2008-09-17-feeds-coins-and-maps-oh-my","title":"Feeds, Coins, and Maps (oh, my)","author":"admin","date":"2008-09-17 09:36:47 -0400","categories":["Announcements","Digital Humanities"],"url":"feeds-coins-and-maps-oh-my","content":"Staff from across the Library are offering learning opportunities through the Scholars' Lab this week!\n\nFirst, Keith Weimer and Chris Ruotolo will give a workshop on using [syndication](http://en.wikipedia.org/wiki/Web_feed) to stay on top of news sources and scholarly journals.  Then, Chris Gist and Kelly Johnston will host the first meeting of an ongoing faculty/grad discussion group on geospatial technology for the humanities.  Finally, Ethan Gruber will present an innovative interface he has created to the [UVA Art Museum](http://www.virginia.edu/artmuseum/)'s collection of Greek and Roman coins.\n\n[Check our calendar](http://www2.lib.virginia.edu/scholarslab/about/events.html#/?p=251) for dates and times!\n"},{"id":"2008-10-09-bamboo-grows-quickly","title":"Bamboo Grows Quickly","author":"jean-bauer","date":"2008-10-09 08:55:18 -0400","categories":["Digital Humanities"],"url":"bamboo-grows-quickly","content":"In July I attended the [fourth Bamboo Planning Workshop](http://projectbamboo.org/1d-princeton-workshop-agenda), held at Princeton University. For those of you unfamiliar with [Project Bamboo](http://projectbamboo.org/) (as distinct from the feeding of pandas), Bamboo is a series of workshops on the future of digital humanities designed by UC Berkeley and my alma mater, the University of Chicago.  The workshops are bringing together humanities scholars, content providers, administrators, and central IT personnel from universities to design an organization that will serve the needs of the digital humanities community.\n\nTypically, only high ranking faculty and administrators get to go, but after juggling the summer schedules of a small staff, my boss at [Documents Compass](http://documentscompass.org/DCAbout.html), Holly Shulman, was kind enough to take me with her.\n\nIn the first general session it quickly dawned on me that I was close to the only non-conference-staff graduate student in the room.  So, as they were passing around the cordless mic, I took a deep breath and raised my hand.  I thanked everyone for all the help they had already given the graduate students at their respective institutions and offered a small plea for continued assistance to those of us who were trying to start careers in a new field.\n\nAfter the session broke up and throughout the rest of the conference, I was amazed by the number of people who came up to me and asked for the “graduate student perspective.”  It was extremely encouraging to meet so many talented and driven professors, researchers, and computer scientists, who felt so protective of the next generation.  I have certainly found that to be true at UVa, and I spent a good deal of my time at Bamboo bragging about the [Scholars’ Lab](http://lib.virginia.edu/scholarslab/) and how it meets many of the needs expressed by the other participants.\n\nNo one knows what Bamboo will become just yet.  The Princeton conference was the end of Phase I, but there are four more phases still to come.  Whatever happens, simply getting all those people together in the same room paid for itself many times over.  I just soaked it all in.\n\nBut, perhaps the best part of the conference was overhearing people talk as they walked out of the hotel.\n\n“Thanks for the tip on mashups, that is just what my project needs.”\n“You know, I don’t even know the name of my central IT contact.  I should look him or her up.”\n“Here’s my email.  Let me know when your project goes into beta, I’d love to check it out.”\n“I only talk with professors when their email is down.  I wonder what they’re working on.  Maybe my team could help.”\n\nUVa has developed its own Bamboo community in preparation for [Phase II](http://projectbamboo.org/workshop-two), beginning next week in San Francisco.  Watch this space to see what happens.\n"},{"id":"2008-10-09-biblical-statistics","title":"Biblical Statistics","author":"matt-munson","date":"2008-10-09 07:39:38 -0400","categories":["Digital Humanities","Visualization and Data Mining"],"url":"biblical-statistics","content":"The first topic that I chose for my dissertation in UVA's [Department of Religious Studies](http://artsandsciences.virginia.edu/religiousstudies/index.html) was the “School of Saint Paul.”  I hoped to show the existence of a group of followers who surrounded Paul and engaged with him in the interpretation of the Old Testament.  In order to do this, I decided to investigate how Paul used scripture in his epistles and how the followers of Paul used the same scripture in their writings.  I anticipated finding certain portions of the Old Testament that either were used exclusively in the Pauline and post-Pauline literature or were used differently in the Pauline and post-Pauline literature than in the rest of the New Testament.\n\nBut I had a problem.   The fund of Pauline and post-Pauline quotations and allusions to the Old Testament numbered more than 1000 cases.  How could I represent such a large set of data in a way that made them easily comprehensible?  A friend of mine suggested that I needed to represent the data graphically.  And a colleague here at the Scholars’ Lab, where I work as a graduate consultant, advised [SPSS](http://en.wikipedia.org/wiki/SPSS) as the best way to accomplish such graphical representation.\n\nI already had a table that I had made in Microsoft Word of every usage of the Old Testament in both the Pauline and post-Pauline literature.  I needed to get this data into SPSS with as little headache as possible.  So, I converted the data into an Excel file, saved this in a format that SPSS could read, and then imported it into SPSS.  At that point, I had accomplished the hard part.  All that was left to do was to analyze and graphically represent this data.  And here is one example of what I produced:\n\n[![Genesis 1-3 in the New Testament](http://farm4.static.flickr.com/3089/2926354745_30d7a3ab10.jpg)](http://farm4.static.flickr.com/3089/2926354745_1271e8325a_o.jpg)\n\nUnfortunately, this analysis of the data made it clear that the evidence was insufficient for my dissertation!  I found no significant chunks of the Old Testament that were used exclusively in the Pauline and post-Pauline literature.  And I discovered that trying to set the Pauline and post-Pauline use of scripture against that of the rest of the New Testament was speculative, at best.  I ended up having to change my dissertation topic.  But it was this statistical analysis and work in information visualization that made it clear to me that the evidence was insufficient.  Without it, it is possible that I would still be chasing the wild goose that was my previous topic.\n"},{"id":"2008-10-21-art-in-the-slab","title":"Art in the SLab","author":"admin","date":"2008-10-21 08:00:04 -0400","categories":["Announcements"],"url":"art-in-the-slab","content":"![Jean Bauer photography](http://farm4.static.flickr.com/3064/2961963222_3fa2a2e441_m.jpg)A bright, sunny, open space like the Scholars' Lab begs to be filled  not only with students and faculty collaborating on digital projects, but also with art!  We're pleased to follow last semester's successful showing of the watercolors of E. F. Chilton with this semester's photography exhibit by our own [Jean Bauer](http://jeanbauer.com/).\n\nJean is a Ph.D. candidate in the History department at UVA and a 2008-2009 Fellow in Digital Humanities at the Scholars' Lab.  Her exhibit, entitled \"Animal, Vegetable, Mineral: Slices of the (Mostly) Natural World\", is on display in the Lab right now.\n"},{"id":"2008-11-04-google-scholar-neglected-corridors-of-the-interwebs","title":"Google Scholar:  Neglected Corridors of the Interwebs","author":"jason-kirby","date":"2008-11-04 07:20:12 -0500","categories":null,"url":"google-scholar-neglected-corridors-of-the-interwebs","content":"Welcome to my first post here on the Scholars’ Lab blog.  My name is Jason Kirby and I’m a third-year Ph.D. student in the Music department at UVa.  I’m in the “Critical and Comparative Studies” track of my program, which means I look at musical sound and musicians through a cultural studies lens.  I’m planning a dissertation on intersections between country and rock music over the past thirty years, and when considering the wide spectrum of academic musicology, I’m squarely a pop music studies guy.  I’ve written about artists ranging from Lucinda Williams to Throbbing Gristle—artists about whom there’s a fair amount of popular-press ink spilled, but not necessarily much scholarly writing (yet).  This brings me to the subject of today’s post.\n\nGoogle Scholar:  I enjoy it, and not for reasons which are necessarily immediately apparent.  As anyone who’s used it with the serious intention of finding real scholarship knows, as a search engine it can be a bit scattershot.  Google Scholar “Bob Dylan,” for instance, and in the top results one gets the mistaken impression that his hit song “Like A Rolling Stone” is a book.  But on the other hand, click through a few more pages of results and you’ll find a an excellent recent article, written by Albin Zak and published in the Journal of the American Musicological Society, which discusses musical, intertextual dialogue between Dylan and Jimi Hendrix.  Zak writes about song structure, and even works in Mikhail Bakhtin—what more could a pop musicologist want?  Plus, if you’re on Grounds, click the link and you’ve got the PDF saved on your desktop; all in less time than it would take to find the same article in AMS’s database (and certainly less time than it takes to page through a hard copy in the stacks).\n\nIf the transition of scholarship online is like the Internet version of a large research library, then Google Scholar for me resembles something like the shelves of a used book store that’s cluttered and random, but stuffed with useful finds (sort of like the Strand in New York City).  Or perhaps a better analogy would be a used music shop where they have the CD cases on display, but the discs themselves are kept behind the counter.  In other words, especially if you’re not associated with a large research institution, you may well not be able to access the PDF file Scholar points you toward—not without subscribing to an online journal or purchasing the article.  This problem underscores a larger issue; as the Internet expands and ostensibly increases public access to knowledge, this free access to information isn’t always “free”.  There’s always a market interest in there somewhere, and Google Scholar reveals this more obviously than other digital research tools.\n\nSpeaking of freedom and openness, one of the recurrent critiques of Google Scholar is that it does not allow users to see how search results are determined.  Though Google recently updated its Scholar search algorithm, as [information standards bloggers have noted](http://www.niso.org/blog/?p=20), we still know frustratingly little about the algorithm itself.  To be fair, other search engines, scholarly and not, have the same problem—it just becomes more high-profile in Google’s case. At this juncture, it's unclear how the recent Google Books settlement (see [Dan Cohen's analysis](http://www.dancohen.org/2008/10/28/first-impressions-of-the-google-books-settlement/)) or [changes in PDF access through Google](http://www.earlham.edu/~peters/fos/2008/11/google-creates-and-searches-ocr.html) will impact the issue.\n\nRegardless, whatever tweaks Google _has _made to its search algorithm, it clearly still needs some work!  As librarians such as Péter Jascó have noted, the number of “hits” one gets through Scholar are often ridiculously overinflated (witness 11,100 for “Bob Dylan”), sorting options for those results are poor, and the search engine often has difficulty distinguishing an author’s name from the rest of an article’s text. [1]\n\nGiven disappointments such as these, it’s not surprising that some librarians are concerned about the changes Google Scholar may bring to the world of academic inquiry.  The current generation of undergraduates is the first to grow up with the Internet a standard part of everyday life, and I think the anxiety some folks feel is that in a world of online-everything, what if speed and ease of use will eventually trump depth and breadth of inquiry?  Certainly, teachers and librarians can and will continue to orient students to the fact that there’s a whole world of research resources out there beyond a one-stop web search.  But it surely doesn’t help matters when Google publishes web ads appealing to the uber-procrastinator:  “Need six authoritative, relevant sources?  Before sunrise?  [Google Scholar](http://photos1.blogger.com/blogger/5153/551/1600/scholar1.gif).”\n\nSo, seeing as I’m a teaching assistant here at the University, would I encourage my students to Google Scholar their entire way through their next paper?  Given what I’ve covered here, no. _ However_, as you may have noticed, this a blog post about why I like Scholar.  So what keeps drawing me back?  I think it all returns to the fact that it’s an outstanding _starting place_ for research inquiries.  What’s more, I find this particularly true for popular music studies inquiries.  Take, for instance, a paper I’m working on this semester about the “Bakersfield Sound” and genre boundaries in 1960s country music.  A preliminary search in RILM of the term “Bakersfield Sound” only got me a 2006 obituary of Buck Owens.  Similarly, a JSTOR search of the same term pulled only five results, a 1970s article on “urban cowboys” the only somewhat relevant one.  But when I Google Scholar’d “Bakersfield Sound,” mixed among some random debris I found an excellent 2005 article on identity and place in California country music by GH Lewis, an article whose bibliography in turn led me to other great sources on the topic.\n\nMy point here?  Google Scholar, just like other search methods, seems to be better suited for some disciplines than others.  Popular music studies, my field, is noted for its hybridity, its tendency to skillfully poach from the methodologies of other disciplines while simultaneously rejecting some of their strictures.  It’s also known as a relatively recent field, with its basic antecedents in 1960s popular-press rock criticism. It’s still an up-and-comer as fields go, still working toward full scholarly respectability.  As such, is it really that surprising that pop music research queries (like those I mentioned above) might fall through the cracks of more traditional scholarly search engines?  From the vantage point of my particular niche, Google Scholar’s biggest advantage is the “wide net” it casts, giving less established fields a better shot.  More traditional discipline-bound search engines might learn something from this, or risk irrelevancy.\n\nJascó, Péter. “Savvy Searching: Google Scholar Revisited.” _Online Information Review_ Vol. 32, No. 1 (2008), pp. 102-114.\n"},{"id":"2008-11-06-iterative-cosmologies","title":"Iterative Cosmologies...","author":"abby-holeman","date":"2008-11-06 09:52:07 -0500","categories":["Digital Humanities","Geospatial and Temporal"],"url":"iterative-cosmologies","content":"“During the Zuni Molawia ceremonial of 1915, when the house-tops were crowded, the roof of one of the houses enlarged that season caved in. The accident occurred, people began to say, because turquoise had not been deposited under the floor of the new chamber.”\n\n\n\n\nElsie Clews Parsons\n\n\n\n\n_Pueblo Indian Religion Vol. 1_, 1939, p.105\n\n\n\n\nThe quote above, read some time ago, was one of the first things I read that spoke to the deeper meaning of many of the “ritual deposits” found by archaeologists. Specifically, how these deposits were connected to built space. I have since encountered innumerable studies from Anthropology, Archaeology, Architecture, Religious Studies, etc., that show how built space and the associated material are microcosms of a larger worldview. These studies demonstrate how space becomes place within a certain cultural logic. For example, in Mesoamerica among the ancient Maya, the quadripartite division of the world organized the gods themselves, ritual calendar (indivisible from the agricultural calendar), the layout of cities, the organization of hierarchy (as seen in four founding lineages noted in the Chilam Balam), the agricultural fields themselves (squares fields with each corner having an altar), the everyday house, the altar within the house, and even individual caches placed in the ground. This is what I like to call an iterative cosmology. This kind of layering can be seen across the globe. The interesting aspect of this link between worldview and space comes at the local level.  It demands interpretation that takes local cultural logic seriously. For example, the above quote suggests that the building fell because of improper offerings, not necessarily _only_ from a lack of structural integrity, or put another way, improper offerings led to a lack of structural integrity and thus the building fell.\n\n\n\n\nWhat on earth does any of this have to do with digital technologies in the humanities? Well, for me the answer came when I tried to ask similar questions about cosmology in a society without surviving myths or writing. All of the studies I have seen come either from areas that do have existing origin myths and writing, or from ethnographic accounts in which the people themselves can explain. The area of northern Mexico (far outside of what is considered Mesoamerica) in which I am interested does not have either extant myths or writing. Yet given the iterative or multi-level, and spatial aspects of many (if not all) Native American belief systems, I believe these systems will leave material patterns behind. That is where GIS comes in.\n\n\n\n\nI turned to GIS as I floundered to come up with a methodology that would allow me to identify _spatial_ patterns. I had an archaeological site in mind and access to enough data about this site that would allow me to answer my questions, but I had little idea about how to get at the spatial aspect of the patterns I had hoped to investigate. GIS technology allows me to take the data I have digitized and not only display it graphically, but conduct a spatial analysis in order to identify spatial structure. Hopefully that spatial structure will help me understand the over-arching cosmological principles that were active in northern Mexico during the 12th and 13th centuries.\n\n\n\n\nThus, I am in the process of georeferencing (recently finished) original excavation maps, and digitizing all of the structures within this site (it’s an intra-site analysis), I will then plot artifact frequencies across the site along with certain cosmologically important architectural features (hearths and posts). All of this data comes from eight published volumes as well as unpublished field notes. None of it was in digital form before I started. From this I can pull actual coordinates of these things and run spatial statistics to ask a series of questions: do certain kind of artifacts group in certain areas of the site? Do the artifact assemblages in rooms that have offerings under the posts look different than assemblages in rooms without these kinds of offerings? If so how are the assemblages different? Where are the elaborated hearths in relation to posts with offerings? Are there discrete boundaries between areas with these elaborated hearths? Do the elaborated hearths co-occur with certain kinds of artifacts? And so on.\n\n\n\n\nSo here I am currently wrangling with large amounts of data to be sure it is internally consistent and get it into ArcMap as the first step. In the back of my mind is how I will disseminate this information. I have heard various ideas, but am currently not sure about some of the best ways to disseminate GIS data….but more on that later!\n\n\n\n"},{"id":"2008-11-13-place-space-maps-and-more-on-gis-day","title":"Place, Space, Maps, and More on GIS Day","author":"admin","date":"2008-11-13 07:09:14 -0500","categories":["Announcements","Geospatial and Temporal"],"url":"place-space-maps-and-more-on-gis-day","content":"![From David Rumsey Map Collection](http://people.virginia.edu/~jfg9x/va-md-de.jpg)Join us next Wednesday, November 19th, as we celebrate all things International GIS Day.  Anyone whose work is grounded in issues of space and place will find something of interest in these cross-disciplinary offerings, centering in cartography and geospatial technologies.\n\nOf special note is a public lecture by [David Rumsey](http://www.davidrumsey.com/), who has worked for a decade to offer open access to his remarkable private map collection through a variety of innovative tools and interfaces. Most recently, he has made historical maps available as layers in Google Earth and on an island in Second Life.  Mr. Rumsey will speak on \"Giving Maps a Second Life with Digital Technologies\" at 4 o'clock in the Harrison-Small auditorium.  This event is co-sponsored by the Center for Emerging Research, Scholarship, and Arts at UVA (CERSA) and the Scholars' Lab, and a reception will follow the talk.\n\n**Schedule of Events:**\n\nCharlottesville Area GIS Users Lunch\nwith a talk by Dr. John Scrivani, Virginia Dept. of Forestry\n12:00 - 1:30 in the Scholars' Lab\n\nGIS Day Cake Cutting\n1:30 in the Scholars' Lab\n\nGIS Day Open House\nwith 10 different GIS user-groups and projects exhibiting!\n1:30 - 2:45 in the Scholars' Lab\n\nTour of \"On the Map: The Seymour I. Schwartz Collection of North American Maps 1500-1800\"\n3:00 - 3:30 in the Small Special Collections Library\n\nSpeaker: David Rumsey\n\"Giving Maps a Second Life with Digital Technologies\"\n4:00 - 5:30 in the Harrison-Small Auditorium\n\nReception\n5:30 - 6:30 in the Small Special Collections Library\n\n\n\n_Image from the David Rumsey Map Collection_\n"},{"id":"2008-11-19-map-vocabularies","title":"Map \"Vocabularies\"","author":"wendy-robertson","date":"2008-11-19 06:09:46 -0500","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"map-vocabularies","content":"For the past year, I have been working on the Scholars' Lab Geospatial Data Portal, the lab’s effort to make our GIS data sets readily available to UVA students, faculty, and staff via the world wide web by using a suite of open source, open standards-based applications. A particular aspect of this project that I have enjoyed exploring is the way in which we display our visual information.\n\n\n\n\n\n\n\nStop to think about the last paper map you used. Minor roads were probably displayed with a line of a certain color and thickness, highways with another. Green spaces were colored differently from open water and buildings etcetera. Cartographers have long toiled to develop visual representations of our environment and make them identifiable for the greater use. People naturally associate certain colors on a map with identifiable features in their environment (eg: the association of green on a map to forests, parks, and open areas). Much like a book, these symbols and representations must create a language which is understandable to the audience; else the information contained on the map will go unutilized.\n\n\n\n\n\n\n\nWhat I have done for the Geospatial Data Portal is to expand our symbolic vocabulary. I create styles; XML based documents which allow us to display visual information through symbols that our patrons will understand and identify with specific attributes. An example: I can map the waterlines for a given city with a solid pink line with a width of 2 pixels. While it is true that the information is mapped and is useful to an extent, I think there is a way to display the same information while making it more visually recognizable as city waterlines and ultimately making the information more useable to our patrons. Instead of a solid pink line of a single width, we can display the information as blue lines with differing widths dependant upon the size of the pipe (ex: a main line feeder pipe with a diameter of 15ft is represented as a blue line with a pixel width of 8, whereas a small pipeline with a diameter of 2ft is represented with a blue line with a 1 pixel width.\n\n\n\n\n\n![](http://people.virginia.edu/~wmr8e/clip_image002.jpg)\n\n\n\n\n\n\n\n\nSo what has this accomplished? People tend to associate size on a map with importance in the real world, so by exaggerating the size difference of the pipe by weighting pixel width we can draw our users’ attention to the important locations on the map. And by using blue, we identify our information of interest as a water feature because most people associate blue on a map with water features in their environment. Now our patrons are able to go from displaying simple lines on a page to creating a map which displays intuitively symbolized information using only their internet browser. I believe this project has the potential to greatly expand the user-base for our GIS data sets and allow for new forms of scholarship because it makes the process of displaying information in an identifiable and comprehensible much more user friendly.\n\n\n\n"},{"id":"2008-11-21-issues-with-translations-walt-whitman-and-jorge-luis-borges","title":"Issues with translations - Walt Whitman and Jorge Luis Borges","author":"gillian-price","date":"2008-11-21 10:38:42 -0500","categories":["Digital Humanities"],"url":"issues-with-translations-walt-whitman-and-jorge-luis-borges","content":"A couple of summers ago, I was desperate for a job so I caved. This was not, in fact, the first time. I remember typing _Leaves of Grass_ for a whole nickel a page before I knew how to type properly, painstakingly pecking out what was then incomprehensible text. During my summer and winter break of 2006, then a proficient typist who had learned a bit of Spanish, I got a pay increase and went to work for my dad, Ken Price, encoding Álvaro Armando Vasseur's 1912 translation of Whitman's poetry for the Walt Whitman Archive. \n\n\n\n\nWhile working on the project, I did not reflect much upon the problems of translations, both within themselves and the issues and concerns in their representation on archives like the Whitman Archive. Frankly, I was too preoccupied about my quickly-approaching study abroad trip to Madrid. My mind was also engaged with memorizing xml code so I wouldn't have to look up everything, and that was sufficiently taxing to keep me from having much time to ponderthe implications of the project I was working on. \n\n\n\n\nEver since I developed an interest in Spanish, translation has been very interesting to me. As many suggest, including one of my personal favorites, Jacques Derrida, translation is not possible, but it is inevitable. That is, one cannot ever hope to achieve a _perfect_ translation of any given text, but people still try to do it anyway. This issue is, coincidentally something we were considering the other week in my literary theory class and one example examined was the difference between the phrase “a dog’s life” in English and the Spanish approximation: “la vida de perro.” The phrase in English, while not entirely positive, also can't be completely separated from our thoughts of the animal we pamper and keep as a pet, who often has a higher standard of living than many humans and doesn't have to worry about how to make ends meet. “La vida de perro” in the Spanish-speaking world, on the other hand, is just the opposite and certainly is not a life you want to have. In Spain the word “perro” carries with it especially negative connotations because of the influence of Islamic culture in which dogs are seen as dirty creatures, so to call someone a dog, or to imply that she or he is one could be quite an insult. \n\n\n\n\nIn his article “Transgenic Deformation: Literary Translation and the Digital Archive” which prefaces Vasseur’s translation on the archive, editor Matt Cohen grapples with several other concerns beyond the nitty gritty of translations including the big question of which texts are translated and why. Cohen cites Lawrence Venuti who asserts that \"asymmetries, inequities, relations of domination and dependence exist in every act of translating.” The reasons we see certain texts translated and not others has everything to do with power relations that dictates which texts are deemed important enough to be translated, (and then represented on archives). Furthermore, the very process by which texts are represented on archives is, in a sense, a translation itself. Not only must one encode the text into XML, but the archivists must make creative decisions that translators make as well, such as _how_ to display the text – whether to remain true to the author’s layout or to explore other options. \n\n\n\n\nIssues of translation like this that come up in working on the Whitman Archive are issues that I suspect will also arise in my collaboration with Jared Lowenstein on his Orbis Quartis project, which is an archive of some early and rare works by Jorge Luis Borges. I think the way Borges’ language and his constant “language slippage” will be a particularly interesting idea to consider when looking at translations of the often multiple versions of his texts.\n"},{"id":"2009-01-05-digital-credibility-in-field-research","title":"“Digital Credibility” in Field Research","author":"wendy-hsu","date":"2009-01-05 09:37:05 -0500","categories":["Digital Humanities"],"url":"digital-credibility-in-field-research","content":"I’m an ethnographer/blogger.\n\nMy dissertation research investigates the social and musical lives of American rock musicians of Asian descent. On the one hand, I follow the conventional methods of participant observation as I travel to ‘field sites’ such as nightclubs, bars, and coffee shops to witness live performances and hang out with musicians. On the other hand, I participate in the indie music scene by blogging (on [yellowbuzz.org](http://yellowbuzz.org)) about my field research experiences. My online participation, however disembodied and virtual, is significant due to the centrality of user-produced or independent media in the indie rock music scenes. For the most part, these research methods take on two distinct lives. Sometimes they intersect and yield interesting results.\n\n+_=-__=-=-=-\n\nEthnographic work on performing arts can sometimes be logistically challenging in our intensely mediated worlds. Typically I carry a number of recording devices including a digital SLR camera, a mini-DV recorder, a handheld digital audio recorder, a laptop computer, and a notebook. This list can be extended or shortened depending on the nature of activities (interviews vs. live performances). Sometimes it is contingent upon whether I expect to make music during my visits.\n\nEarly this fall, I took a series of field research trips to New York City. On one of these trips, I doubled (well, actually tripled) my identity: field researcher, musician, and scholar. I was invited to perform and speak with students at Wheaton College in Norton, Massachusetts. I took the chance to double-dip this visit by scheduling some interviews and making plans to attend shows in New York. So I had a four-bag system: a backpack (my laptop, notebook, show flyers, The Village Voice, other paper products), a carry-on suitcase (audio-visual recording devices and clothes), an electric guitar case, and a guitar pedalboard (assorted guitar effect pedals).\n\nAfter the mini-residency at Wheaton College, I took the Amtrak to New York City. Long story short, my case of guitar effect pedals (worth $1500!) got stolen on the train a few stops north of New York Penn Station. I frantically filed a report with the Amtrak Police. No recovery prevailed. Bummed out as I was, I dragged myself to a midtown bar for an interview with Johnnie Wang of the band [A Black China](http://www.myspace.com/ablackchina). After I told Johnnie about my misfortunes, he offered to buy me a beer. That was the beginning of our friendship. We bonded over being musicians first, then being Americans of Taiwanese/Asian heritage.\n\nMy meeting with Johnnie invigorated me and reminded me of the purpose of my dissertation research. I went to a show the following night in New Jersey and had an interview meeting with Joe Kim of [Kite Operations](http://www.koarecords.com/kiteoperations/) right before my flight back to Charlottesville, with one bag short.\n\n+_=-__=-=-=-\n\nIt took me a while to figure out the educational values and perhaps the theoretical fruitfulness of this experience. This experience can be seen in light of a few issues: methodological approaches to technology, empathy (and relationship) with informants, and researcher’s ‘field identity.’  So, does technology enhance or hinder field research? Frankly, I didn’t end up using most of my recording devices on this trip. During interviews and other exchanges, my informants and I chatted away while I took mental notes. My field-note-taking took place only after the meetings ended.\n\nBut oddly, (the loss of) technology brought me closer to my informants. The story of losing my guitar gear generated a sense of empathy from my informants. I share with them an intimate engagement with music-making technology. They too often travel with gear for both music-making and recording purposes and some have encountered experiences, personally or vicariously, with gear problems. In many ways, it’s not strange at all that I carry so much gear with me. The physical and social attachment to technology is a central part of being and moving around in this media-blasted world. In this case, technological gear adorns me as a tech-media savvy researcher and blogger. This kind of ‘digital credibility’ has helped me earn not only access to, but also empathy and respect from my field informants.\n\nExcess technological devices can weigh down users. But this is not only an academic concern specific to field research methods, as it is a more pervasive issue in the digital age. My responsibility is to figure out the best logistical and theoretical approaches to both online and offline interactions in my field research. I’m still working on it.\n"},{"id":"2009-01-05-teaching-with-artstor","title":"Teaching with ARTStor","author":"fitz-green","date":"2009-01-05 09:36:48 -0500","categories":null,"url":"teaching-with-artstor","content":"I am a teaching assistant for a course on the early history of Christianity. When the professor for the course asked me to lecture for him on early church art and architecture, I was excited. I had recently come upon the new ARTStor online database, and couldn’t wait to find digital images of the churches I wanted to cover in my lecture. But then he said, “I’ll go over to the slide library with you sometime next week and introduce you to the folks there, and they’ll help you pull slides.” Now I had a conflict: Do I do it the old-fashioned way, my professor’s way? Or do I take advantage of what the latest technology has to offer?\n\n\n\n\nI was truly conflicted over this, so rather than making a decision at once I decided to prepare using both, and then decide. More work, yes, but this way I’d get to try both out and see the advantages of both (though, in reality, this was just my way of delaying making a decision). So, I started gathering slides of churches on ARTStor. Santa Sabina and Santa Maria Maggiore in Rome. San Vitale in Ravenna. The great thing about ARTStor is that you can save your groups of images indefinitely. So, if I were to go back and give this lecture again next semester, everything would already be prepared. Also, there are more images to draw from on ARTStor. There are groups of images that a university can purchase rights to, much like a journal subscription, so you have lots more options this way. No more having to use the old red-tinted picture that some art professor took with his point-and-click 35mm while on vacation 30 years ago (I saw my share of these as a student- I could usually tell how old the slide was because the professor’s car always ended up in one of the pictures). In many cases, the ARTStor images were of better quality as a result too.\n\n\n\n\nBut here was the winning point in favor of ARTStor for me. You can save images for a lecture to a folder accessible by the students, so that after the lecture, or while studying for exams, students can go back and review the same images. This is a major bonus. I recall taking art history classes in which the only time you saw a given picture was during the lecture, and then you had to be able to identify it again if it was displayed during the exam. The students who took notes feverishly on every detail of a picture but didn’t look up at the images to study them always did poorly on exams, because when a picture was displayed again during the test they had no idea what it was! This ability to recall the images is not just an advantage for exam taking, but for learning in general. After I gave this lecture, I had one student approach me after class interested in a mysterious unidentifiable woman that appears in the arch mosaics of Santa Maria Maggiore. She thought she had seen a similar depiction in another class, and wondered if the two were related. With ARTStor, I could point her back to the exact image so that she could do more research into the possible connection. More interaction with the material for the students means more of a chance to get them excited about what they are learning, and this is reason enough for me to use ARTStor.\n\n\n\n\nAll of that said, what did I end up using for the lecture? I decided that while I was working for this professor, I wanted to do things his way. I’ll have plenty of opportunity in the future to do things my own way. And, to be honest, the process of going to the slide library, pulling the physical slides, viewing them on light tables with a magnifying glass, dropping them in the carousel with the red dots pointing the right direction, this was all fun. There is something about the physicality of using the slides, like the physicality of a book, that I can understand not wanting to give up. But, for me, the advantages of ARTStor are so numerous that I can’t see not taking advantage of it.\n"},{"id":"2009-01-16-social-media-and-the-inauguration","title":"Social Media and the Inauguration","author":"bess-sadler","date":"2009-01-16 08:46:21 -0500","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"social-media-and-the-inauguration","content":"![Social Media in the SLab](http://farm4.static.flickr.com/3525/3202007970_729b7e0186.jpg) Join us in the Scholars' Lab Monday morning through Wednesday night next week, as we project the social media landscape surrounding next week's historic presidential inauguration.\n\nWe'll be showing real-time [Twitter](http://twitter.com) and [Flickr](http://flickr.com) feeds that record people's responses to the event and their efforts at citizen-journalism. We've also created a home-grown geospatial visualization so that you can follow the worldwide conversation!\n\nVisit the Lab for a little social interaction of your own, or [access the site](http://www2.lib.virginia.edu/scholarslab/inauguration/) (which includes more information and related links) online.\n"},{"id":"2009-02-04-on-asian-american-digital-identity-politics","title":"On “Asian American” Digital Identity Politics","author":"wendy-hsu","date":"2009-02-04 10:04:12 -0500","categories":["Digital Humanities"],"url":"on-asian-american-digital-identity-politics","content":"Everyday, I receive Google Alerts about any websites, blogs, or news feeds containing the keywords “Asian / American / music” in whatever order and combination that Google search engine finds. Most of the Alerts, unsurprisingly, point to stories related to U.S. politics. Interestingly, around the time of the 2008 Presidential Election, my InBox experienced a minor Google Alert “explosion” with news stories and criticisms listing all the color-based social groups, connecting Obama’s racial politics to the now dominant American ideology of multiculturalism. To my disappointment, none of these news stories included anything substantial information with regards to the Asian American (if there is such a thing) perspective on the Obama and Biden duo.  \n\nIs “Asian American” coming to stand in for a keyword, tag (in the speak of blogosphere), or a hip buzzword in our current media environment as digitally informed and constructed? Is there “real content” beyond the textual reference of “Asian” and “American”? If so, how do we assess this content considering the methods of information retrieval, i.e. Google Alerts, and the context of presentation, i.e. hypertextual state of Internet media?\n\nToday, my Google Alerts linked me to a couple of exciting pages of content-worthy materials related to Asian American arts and culture. One of these is a New Yorker article titled [“By the Skin of Our Teeth”](http://www.newyorker.com/arts/critics/theatre/2009/01/26/090126crth_theatre_als) about “The Shipment”, the new play by Young Jean Lee. The reviewer Hilton Als comments on the Lee’s “irreverent take on racial politics.” Commenting on her 2005 play “Songs of the Dragons Flying to Heaven”, featuring the self-violence of an Asian American female character, Lee declares her attitude toward the state of identity politics in the U.S: “For this project, I decided the worst thing I could possibly do was to make an Asian-American identity-politics show, because it can be a very formulaic, very clichéd genre, and very assimilated into white American culture. It’s almost become part of the dominant white power structure to have identity-politics plays about how screwed-over minorities are. It’s such a familiar, soothing pattern. . . . It’s become the status quo.”\n\nWhen I read the passage, I thought to myself, “now, here’s a kernel of wisdom” worth pursuing. What does she mean by “identity-politics show”? What consists of this ‘cliché genre’ of formulaic and assimilationist plays? A good content analyst would seek information about the playwright and this play. Before I jumped into my usual mode of performing a search on Google or Wikipedia search on Young Jean Lee, I slowed down and pondered about the path of information that allowed me to arrive at this intellectually compressed bit of information.\n\nThe New Yorker tags this article with the following keywords: “The Shipment”; Young Jean Lee; Korean-Americans; Douglas Scott Streater; Race Relations; Asian-Americans; “Pullman, WA.” Google search engines must have picked up this article because of the tag “Asian-Americans.” But search engines are not able to make a qualitative distinction between this article [or other substantive articles] from the sources that simply use “Asian American” as a stand-in for cultural multiplicity and diversity. Unfortunately, Asian America still exists, in the digital environment, mostly under a pile of diversity-bound laundry lists at best, or pornography and ads for mail-order brides or other forms of race-related sex industry, at worst.\n\nThe risk of being pigeonholed, tokenized, or even sexualized is no news to individuals of Asian descent in the United States. Playwright Young Jean Lee asserts provocative and vehement critiques for the discursive objectification of Asianness in her 2005 play which opens with a monologue by a woman with the name of “Korean-American”:\n\n\n<blockquote>“Have you ever noticed how most Asian-Americans are slightly brain-damaged from having grown up with Asian parents? It’s like being raised by monkeys—these retarded monkeys who can barely speak English and are too evil to understand anything besides conformity and status. . . . Asian people from Asia are even more brain-damaged, but in a different way, because they are the original monkey. . . . I am so mad about all of the racist things against me in this country, which is America. Like the fact that the reason why so many white men date Asian women is that they can get better-looking Asian women than they can get white women because we . . . have lower self-esteem. It’s like going with an inferior brand so that you can afford more luxury features.”</blockquote>\n\n\nThis is intellectually dense, emotionally heavy stuff. But the fact that it’s available in a point-and-click fashion is astounding. Google Alerts prevent information from fossilization. Without Google Alerts, I would find this article somewhere down the line when I do archival search, plowing through databases for historical artifacts. The newness and immediacy of this information would be lost. Also, it would take many more steps to link this article to other articles related to the subject of “Asian / American / music” published today.\n\nThe other noteworthy piece Google Alerts linked me to is an interview of jazz pianist [Vijay Iyer](http://www.vijay-iyer.com/) by RVAjazz blog entitled [\"Intellect Meets Creativity](http://www.rvajazz.com/2009/01/vijay-iyer-intellect-meets-creativity.html).\" Iyer speaks reflexively about his role as an Indian American musician in the Afro-centric tradition of jazz music: “I'm just fortunate to be able to interact with the music from my perspective, and to reconsider what resonances there might be with my own experience, or with anyone's. The point is to honor that legacy and not commodify it, but also to learn from it. I think that America was invited to reconsider a lot of this in light of the ascent and success of Obama. Those are symptoms of a larger development in our culture - it's about who we are and where we are and what time it is!”\n\nThe juxtaposition between the New Yorker article on Young Jean Lee's play and Vijay Iyer's interview is intellectually curious. Iyer’s perspective on race in America is less dystopic than Lee's. In fact, his alliance with African American culture and struggle speaks to a larger discourse about race in terms of minoritarian politics, quite contrary to the uncritical multiculturalist orientation. Iyer’s interview could tap into the historical and contemporary moments of Afro-Asian connections formed in anti-racist solidarity.\n\nMy research aims to track these moments deliberately and shamelessly, making links and disconnects among them as they occur in real time. Information as such, categorized and recategorized based on similar or dissimilar terms, is generated and circulated at high volume daily on the Internet. Digital technologies allow discourse to flow in disparate, rhizomatic directions. The hypertextual state of Internet media is overwhelming to sort through, but this quality allows information to seep into unexpected cracks and generate surprising juxtapositions. Similar to keywords and tags, identity categories, also reproduce themselves in a semi-irrational, hypertextual fashion in our time. These contradictory patterns as discovered in the digital environment may best represent the schizophrenic style of identity proliferation that would mark our post-identity-politics (or post-Race) age.\n"},{"id":"2009-02-04-peer-review-for-visual-aids","title":"Peer Review for Visual Aids?","author":"wendy-robertson","date":"2009-02-04 10:01:20 -0500","categories":["Visualization and Data Mining"],"url":"peer-review-for-visual-aids","content":"How frustrating is this: You sit down to take in some form of scholarly work (be it a book, an article, or a talk) and you find yourself increasingly confused with a bombardment of information from graphs and figures and maps which don’t make sense because they either have too much or too little information contained within them or the information is poorly labeled (if at all).  Or even worse, you are the person writing the book/article or giving the talk and instead of fielding questions on your scholarly processes, you are repeatedly explaining to the audience what your visual aids _actually _represent.\n\nA picture may be worth a thousand words, but if it is not a language your audience speaks, where have your efforts gotten you?\nTypically, when I read a scholarly article, my first read-through goes as follows: I read the abstract, I look at each one of the figures/maps/tables/graphs and their annotations, and I read the conclusion.  Its not until the second read-through that I examine the bulk of the text.  I think that words sometimes have the unfortunate tendency to obfuscate the true findings of research and, truth be told, I like to find out if I draw the same conclusions from the provided data as the author(s) do.  My process stumbles when I encounter articles with figures/graphs/maps etc. which have either a glut or a dearth of information contained in them, making non-intuitive to the uninitiated reader.  Some highlights:  A map of a state containing rivers, waterbodies, and watershed boundaries (the focus of this particular article) AND all of the major roads and highways (NOT the focus of the article).  All in gray-scale.  Add in the point locations and names of the state’s twelve most populous cities and cram it into a box three inches tall by five inches wide.  The focus of the article was on modeling and delineating the major and minor watersheds of the area in order to develop a best management practice for cooperating water districts.  Needless to say, that point was lost in the shuffle.  Another example which is all too common: a graph depicting change over time of 10 or more constituents using various dotted, dashed, and solid lines of variable thickness.  With that amount of information crammed into a single visual aid, the results are simply lost in the shuffle.\n\nWe have writing clinics and public speaking critique sessions, why don’t we have a peer evaluation system for visual aids?  I think that many people (myself included) fall into a habit of having our material critiqued solely by our close working group.  While this is certainly a necessary step in the writing process--the people most familiar with our work are the ones most likely to pick up on the esoteric flaws--many scholars neglect to obtain peer review from individuals tangential to or completely outside of their small fields.  I would say that one of our main objectives as scholars is to use our work to excite interest from members of the scholarly community inside and outside of our focused area.   In my opinion, an important step towards this goal is to make our visual aids more accessible to the curious non-expert.\n\nI would like to see our scholarly community develop this type of peer-review network where we can utilize the human resources around us to improve our intellectual contribution to all of our respective fields.  We could have minds from a variety of fields of study working collaboratively to improve the accessibility (and therefore the use) of our collective body of knowledge.   I think the concept has amazing potential.\n"},{"id":"2009-02-11-research-applications-for-3d-models-in-art-history","title":"Research Applications for 3D Models in Art History","author":"ethan-gruber","date":"2009-02-11 12:51:23 -0500","categories":["Digital Humanities","Visualization and Data Mining"],"url":"research-applications-for-3d-models-in-art-history","content":"These days, it is difficult to find a television documentary detailing an archaeological site that does not feature a representation in the form of a 3D model.  Computer models make good teaching tools.  A class of students may not have the opportunity to travel to Rome to view the Colosseum first-hand, and even if they did, they would have great difficulty visualizing what the mostly-ruined structure looked like 1,900 years ago.  A model based on the most recent archaeological research, however, can help fill in the gaps left by time and the elements. \n\n\nOne of the more important aspects of a computer model is that it is dynamic.  Using software, a model can be adjusted to reflect newer theories of the site's architectural reconstruction.  This is certainly a stark contrast to artists' sketches and paintings, which, over time, tend to become outdated.  Importantly, like other visualization methods used in the humanities (such as GIS), 3D models can help scholars get a fuller picture of a site and formulate research questions that never would have been considered otherwise.  This is the case in my most recent research.\n\n\nHaving never truly given up on the video game design aspirations of my high school days (I specifically remember my father turning the breaker off to the upstairs when I was up until 4 AM designing a Quake map), I have found a niche within my field of academic interest—Roman archaeology and architectural history.  While many of my Pompeianist classmates take a more traditional approach to graduate research projects, I chose to develop a 3D model of the House of the Faun, one of the largest and most famous houses in the city.  The model was constructed as accurately as possible based on the archaeological plan, a number of artists' reconstructions, and photographs of the house (many gathered from [Flickr](http://www.flickr.com/)).\n\n\n\n\nThe intent of the model was to test art historians' philosophical assertions about Roman atrium houses.  With accurate lighting simulation (i. e., calibrating a simulated sunlight to the latitude and longitude of the house and to any point in time back to antiquity), high resolution images of the model rendered by [Mentalray](http://www.mentalimages.com/) software gave me a glimpse of what the House of the Faun looked like at noon on January 1st, 100 B.C., which is something no artist can replicate.\n\n\n\n\nCoincidentally, lighting simulation may have an impact on how we consider the artwork within the house.  For example, when many art historians point to the colors of a mosaic as being proof of its Greek influence, can that assertion bear the burden of the fact that the mosaic was rarely in sunlight?\n\n\n\n\n![House of the Faun](http://farm4.static.flickr.com/3316/3300953093_53b43154c2.jpg)\n\n\n\n\nMany of us have seen Roman floor mosaics hanging on the walls of American and European museums, but they have been removed from their original context.  Even in Pompeii, one of the best-preserved sites of the ancient world, the roofs collapsed long ago, making it difficult to visualize the natural lighting scenario within the House of the Faun and other structures within the city.  3D models allow us to put artworks back in their original context and consider how the ancients viewed them, which is quite different from how we view them now.  In this case, the computer model is more than just a teaching tool; it is a scholarly research tool.\n"},{"id":"2009-02-22-coins-site","title":"Library Innovation Grant Yields Dividends for Numismatists","author":"admin","date":"2009-02-22 12:25:46 -0500","categories":["Announcements","Digital Humanities","Research and Development"],"url":"coins-site","content":"![Ethan in the SLab](http://farm4.static.flickr.com/3614/3300810539_6e6f755e21.jpg)\n\nA recent post by [Ethan Gruber](/people/ethan-gruber/), a UVA Library staff member who has lately joined the Scholars' Lab team, detailed [his experiments](http://scholarslab.lib.virginia.edu/index.php/digital-humanities/research-applications-for-3d-models-in-art-history/) with 3-dimensional modeling to re-contextualize Roman mosaics -- right down to the interplay of light and shadow in ancient villas.  Now Ethan's work on creating a scholarly interface for the study of Greek and Roman coins has been [profiled in UVA Today](http://www.virginia.edu/uvatoday/newsRelease.php?id=7810).  This project came about through an internal UVA Library Innovation Grant and was undertaken in consultation with Art History professor John Dobbins, a 1994 [IATH](http://iath.virginia.edu) Fellow, whose [Pompeii Forum](http://pompeii.virginia.edu/) project provided an early example for the utility of digital tools for archaeological inquiry.  The rare coins were scanned by Andrew Curley of the Library's [Scholarly Resources](http://lib.virginia.edu/scholarlyresources/) Digitization Services.\n\n\n\n\nPhoto credit: Dan Addison.  Read the full UVA Today press release [here](http://www.virginia.edu/uvatoday/newsRelease.php?id=7810), or jump straight to the [coins collection](http://coins.lib.virginia.edu/).\n"},{"id":"2009-03-11-50","title":"Mapping Regional Language Use","author":"wendy-robertson","date":"2009-03-11 06:15:36 -0400","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"50","content":"So for the thousandth (or so it seems) time I’ve gotten into this discussion with my friends from the East Coast and Midwest (I’m from Texas) about the correct way to refer to a sweet carbonated beverage, and I have finally got to thinking about ways to map locally spoken slang and jargon using GIS.  Starting a database of ‘events’ where a person uses unique language in reference to a common-place item or occurrence (I have a friend from Wisconsin who calls the drinking fountain a “bubbler”) would be an insightful way to examine how jargon or slang starts and spreads geographically.\n\nSo I decided to indulge my curiosity and create a small database consisting of the answers to two quick survey questions; What do you call sweet carbonated beverages?, and what state do you identify yourself as being “from”?.  I solicited friends and colleagues for the answers to these questions and ended up with about 150 useable responses (if you were one of the people who responded with “beer”, I thank you for the interest in the survey, but your answer was not included).  I chose to ask this question (please bear in mind that linguistics is not a focus of my studies) because regardless what you refer to it as, most people have had experience with a coke/soda/soda-pop/pop, which isn’t true for all objects of regional jargon (example: before moving to the East Coast I had never seen nor heard of scrapple) and I wanted to document the geographical extent and overlap of a single object rather than attempt to compare multiple similar objects with this first foray.\n\nApproximately 94% of respondents identified that they referred to sweet carbonated beverages as either “coke”, “soda”, “pop”, or “soda-pop”, so I chose to focus the mapping of this data on those four responses.  I took the responses I received and calculated a ‘count’ by state of each type of response; for example, I received a total of 4 responses from people who identified as being from the state of Missouri.  Three of the respondents refer to sweet carbonated beverages as “coke”, and one refers to it as “soda”.  I took these counts and normalized them to the total number of responses received from the state and used that percentage to map the responses by state broken into ~25%, ~50%, ~75%, and 100%.  For each response (coke, soda, pop, soda-pop) I chose a single color to represent responses on the map, and varied the transparency of the color to represent the percentage of the response (25% response = 75% transparency, 50% response = 50% transparency, 75% response = 25% transparency, and 100% response = 0% transparency).  I mapped all four responses separately first (figure 1).\n\n![Figure 1](http://people.virginia.edu/~jfg9x/clip_image002.jpg)\n\nI chose to vary transparency as opposed to saturation of color (eg: monochromatic choropleth) because I wanted to be able to overlap the response maps to visualize the confluence of the regional terms yet keep the original colors of each response (figure 2).\n\n![](http://people.virginia.edu/~jfg9x/clip_image003.jpg)\n\nThe map above shows the overlap of “coke” responses with “soda” responses, which are displayed by the variation in colors from bright red where 100% responses were “coke” to bright blue where 100% responses were “soda” and various shades of purple and pink in between where there was a mix of responses in that state.  This kind of map can be created using a map with a double ended scale, but that type of visualization is limited to displaying the spectrum between two absolute responses, which would mean that I could only display the confluence of two responses rather than all four (figure 3).\n\n![Figure 3](http://people.virginia.edu/~jfg9x/clip_image004.jpg)\n\nOne interesting thing I noticed when looking at the results of this survey is that I need to meet more people from the Pacific Northwest section of the country.  The other interesting result I noticed which is more pertinent to the questions asked in this study is the confluence of the regional jargon that occurs in the region that includes Kentucky, Indiana, Ohio, and Illinois.  This area represents the confluence of the “soda” and “pop” responses and is also the region with responses of “soda-pop”, a hybridization of “soda” and “pop”.\n\nThis exercise seems to make the argument that assembling databases of ideas such as regional jargon and using tools like GIS to display that information is a thought provoking and possibly worthwhile endeavor.  (I’d like to thank all of my friends and colleagues who participated in this survey that allowed me to assemble and produce this study for digestion by the blogosphere. Thanks, you guys!)\n"},{"id":"2009-03-11-how-digital-humanities-can-improve-my-dissertation-part-1","title":"Mining and Mapping Apocalyptic Texts, Part 1","author":"matt-munson","date":"2009-03-11 06:10:20 -0400","categories":["Digital Humanities","Visualization and Data Mining"],"url":"how-digital-humanities-can-improve-my-dissertation-part-1","content":"I have used computer technology to help my work in biblical interpretation for a while. I learned to do complex digital word searches with the Bibleworks software package early in my graduate career. When I started working at the Scholars’ Lab in the summer of 2006, I was introduced to digital humanities. I found these technologies fascinating. But how, I asked, could they help me interpret ancient religious texts in their original languages? I recently posed this question to some of my colleagues in the Scholars’ Lab and was pleasantly surprised by the answers. In this two-part blog, I will consider these answers in relation to my dissertation, which focuses on several passages in the Apostle Paul that speak of the final fate of humankind. Some of these passages suggest that all people will, in the end, be made right with God. Other passages suggest that some people will be permanently alienated from God. I wish to discover the central kernel of Paul’s thinking about the fate of humankind (called soteriology) that would make both of these statements true. In this first entry, I will focus on how I plan to use text-mining to enhance my ability to compare dozens of Greek, Hebrew, and Latin texts with each other more quickly and more thoroughly than I could manually. The second part will focus on how geographic information systems (GIS) will help me to place Paul’s writings in spatial relationships with other writings of the same time.\n\n\nText-mining involves parsing a digital text, inserting the words along with their linguistic features into a database, searching for patterns within the database, and, finally, evaluating the results. In my case, I will use text-mining methodologies to extract linguistic data from the Pauline texts as well as other early Jewish texts that speak of the fate of humankind. This process will be fairly straightforward for the Pauline texts. There are many versions of the Greek text of Paul that have linguistic data attached to the words. One need simply extract this data from the text and insert it into a database. I will use the text of the Bibleworks 6 package as my source for Paul. For other texts, this process will not be as easy. For instance, the Thesaurus Lingua Graecae has a huge collection of Greek texts for the period that interests me. But they have no linguistic data attached to the words. To attach linguistic data to these words, I will need to write a script, probably in PERL, to query the open source parsing engine from the Perseus Project at Tufts University ([www.perseus.tufts.edu](http://www.perseus.tufts.edu/)). I will then insert the results from these queries into the database for that text.\n\n\n\n\nThe next step will be to design queries that will find appropriate relationships among the texts. Good methodology requires that I test my queries against a set of texts for which I know the results. I will test my queries on several Greek apocalyptic texts which I have already read carefully, noting the sections that relate closely to Paul. Once I have designed a set of useful queries, I will apply these queries to the databases I created earlier.\n\n\n\n\nThe application of these queries should point to numerous texts that I will then manually analyze to determine their meaning and how they relate to the Pauline passages under investigation. If my investigation were purely manual, I would begin by reading the texts in English in order to find promising passages that I would then examine more closely in their original language, whether Greek, Hebrew, or Latin. This digital method, though, will do this first analysis using the original languages. That means that this computerized comparison of the texts in their original languages will find verbal and grammatical similarities that may be obscured or destroyed in translation. In the end, I expect text-mining to return data that would be only partially accessible by manual means.\n\n\n\n\nIn my next post, I will consider how another area of digital humanities, geographic information systems, can help me to explore how the Pauline and apocalyptic texts are related spatially, instead of linguistically, to each other.\n\n\n\n"},{"id":"2009-03-11-rome-reborn","title":"Rome Reborn","author":"fitz-green","date":"2009-03-11 06:14:37 -0400","categories":["Digital Humanities","Geospatial and Temporal"],"url":"rome-reborn","content":"My wife and I frequently engage in a strange kind of “culture war.” She thinks ancient Rome is the more interesting civilization, and I’m partial to ancient Greece. In these debates, I always tell her that I prefer philosophers to politicians. Still, I was excited when I first encountered [Rome Reborn](http://www.romereborn.virginia.edu/), a joint project between UVA’s [Institute for Advanced Technology in the Humanities](http://www.iath.virginia.edu/), a few other schools, and Google (who allows access to the project through [Google Earth](http://earth.google.com/)). The goal of Rome Reborn is to create a 3D digital model of ancient Rome in the year 320. There are plans to extend the project over time, so that you will be able to track the development and growth of the city over time. The buildings have all been reconstructed by computer modeling, and mapped onto Rome’s actual terrain. What a cool project.\n\n\n\n\nI should say, before continuing, that if you want to check out Rome Reborn for yourself, you might have some trouble getting to it. First, you need to download [Google Earth](http://earth.google.com/). Then, you need to turn on the “Ancient Rome 3D” layer, which listed under the “Gallery” layers. Next, get to Rome, zoom into the ancient city and click on a yellow building, which brings a popup window to add the ancient terrain, landmarks, and buildings. Then, you are finally ready to enjoy the model. (But be warned, if you don’t have a good computer with a fast processor and a hefty bit of RAM, you’ll only send yourself into conniptions rather than enjoy the grandeur of this ancient civilization.)\n\n\n\n\nMy first impression, in wandering through the reconstructed forum on Google Earth, was of how chock-a-block the buildings are. You realize how many of the buildings are right on top of one another. You do get this feeling in person, walking around the ruins, but the 3D model captures the hustle and bustle of a true big city that is not conveyed adequately by pictures alone. This project will help scholars puzzle over details of the architecture itself, but having it available to such a wide audience on Google will also help those just learning about Rome. It has the potential to spark students’ interest in learning—for me, this is well worth the effort.\n"},{"id":"2009-03-24-ada-lovelace-day","title":"Ada Lovelace Day","author":"bethany-nowviskie","date":"2009-03-24 14:31:27 -0400","categories":["Digital Humanities"],"url":"ada-lovelace-day","content":"Today has been declared -- quite spontaneously, and to the cheers of a great many people -- [Ada Lovelace Day](http://www.guardian.co.uk/technology/2009/mar/24/ada-lovelace-day), a day on which to honor women working in technology by [writing blog posts](http://findingada.com/) about their often-unsung achievements, and about ways in which they inspire and challenge us.\n\n<!-- more -->\n\nI want to mention two women with whom I have worked closely in my career as a digital humanist.  The first is book artist and media theorist [Johanna Drucker](http://en.wikipedia.org/wiki/Johanna_Drucker), with whom I collaborated on the design of interactive tools for humanities scholarship.  But forget the digital.  I want to thank Johanna for teaching me letterpress printing -- from the minute and retrograde obsession of setting type to the athletic cranking of a [Vandercook press](http://vandercookpress.info/) -- and all the way down to those gentle and girly concerns of a [printer's devil](http://en.wikipedia.org/wiki/Printer%27s_devil) in pink: the best tactics for keeping one's silken tresses out of the rollers, thus avoiding an unladylike scalping, and the preferred soap for scrubbing toxic lead from beneath one's decidedly unmanicured nails.  It's true that we made some [crazy](http://www2.iath.virginia.edu/time/time.html) [things](http://www.speculativecomputing.org/) online, and thought about [some](http://www.digitalhumanities.org/companion/view?docId=blackwell/9781405103213/9781405103213.xml&chunk.id=ss1-4-10&toc.depth=1&toc.id=ss1-4-10&brand=default) [things](http://www.press.uchicago.edu/presssite/metadata.epl?mode=synopsis&bookkey=353566) -- but Johanna and I also got inky, and I don't want a post on women in technology to assume that it's all bits and bytes, when [there are other bites](http://www.youtube.com/watch?v=Iv69kB_e9KY) to care about, as well.\n\nAnd then there's [Bess Sadler](http://www.ibiblio.org/bess/), with whom it's my great pleasure to work at the [University of Virginia Library](http://lib.virginia.edu/scholarslab/).  Ada Lovelace Day comes one day after a little milestone for Bess, the [release](http://virgowww.lib.virginia.edu/) into its native habitat of a [piece of software](http://blacklightopac.org/?page_id=2) that just might make library research a bit more joyful.  Bess has all the things it takes for a woman to succeed in technology: [vision](http://www.ibiblio.org/bess/?p=21), energy, a highly specific strategy for shrugging off the crap, and a deep understanding of the little tweaks it takes to make a system (any system) sing.\n\nBy the way, Bess and Johanna crossed paths [here](http://artistsbooksonline.org/), in the creation of an archive that puts one brand of bite in bytes.\n\n**Update**: Since writing this, I've discovered that several other people have honored Bess today!  See posts by [Amanda](http://householdopera.typepad.com/household_opera/2009/03/happy-ada-lovelace-day.html), [Dorothea](http://cavlec.yarinareth.net/2009/03/24/bess-sadler-library-geek/), and [Randy](http://www.eifl.net/cps/sections/services/eifl-foss/foss-blog/2009_03_24_ada-lovelace-day-bess).\n"},{"id":"2009-03-26-day-of-digital-humanities-2009","title":"Day of Digital Humanities 2009","author":"bethany-nowviskie","date":"2009-03-26 12:52:45 -0400","categories":["Announcements","Digital Humanities"],"url":"day-of-digital-humanities-2009","content":"![](http://tapor.ualberta.ca/taporwiki/images/b/b4/Day-of-dh-sm-crop.png)Ever wonder how folks in the Scholars' Lab spend their day?  Bethany Nowviskie, Director of Digital Research & Scholarship at the UVA Library and Joseph Gilbert, Head of the Scholars' Lab, recently participated in the \"[Day in the Life of the Digital Humanities](http://tapor.ualberta.ca/taporwiki/index.php/Day_in_the_Life_of_the_Digital_Humanities)\" project initiated by our friends at the University of Alberta.  The \"Day of DH\" project encouraged scholars, administrators, students, and others who self-identify as \"digital humanists\" to blog about their day on March 18, 2009.  You can read about [Bethany's day](http://ra.tapor.ualberta.ca/~dayofdh/BethanyNowviskie/) and [Joseph's day](http://ra.tapor.ualberta.ca/~dayofdh/JosephGilbert/), as well as the experiences of a [host of other participants](http://tapor.ualberta.ca/taporwiki/index.php/List_of_Day_of_DH_Participants).\n"},{"id":"2009-03-26-scholarly-publishing-today-and-tomorrow","title":"Scholarly Publishing Today and Tomorrow","author":"ronda-grizzle","date":"2009-03-26 08:56:34 -0400","categories":["Podcasts"],"url":"scholarly-publishing-today-and-tomorrow","content":"Linda Bree from Cambridge University Press talks about \"Scholarly Publishing Today and Tomorrow\"\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.2014989915/enclosure.mp3\"]\n"},{"id":"2009-03-30-a-kindle-for-every-student","title":"A Kindle for Every Student?","author":"fitz-green","date":"2009-03-30 13:09:24 -0400","categories":["Digital Humanities"],"url":"a-kindle-for-every-student","content":"The blogosphere has been abuzz with diverse opinions on the release of Amazon’s new [Kindle 2](http://www.amazon.com/Kindle-Amazons-Wireless-Reading-Generation/dp/B00154JDAI/ref=amb_link_83624371_1?pf_rd_m=ATVPDKIKX0DER&pf_rd_s=center-1&pf_rd_r=0NXGCFS7P9F77XA7ZMSV&pf_rd_t=101&pf_rd_p=469942651&pf_rd_i=507846). So far, most of the news has surrounded the controversial text-to-speech function and whether or not it violates copyright law (more on this [here](http://opinionator.blogs.nytimes.com/2009/02/27/kindle-2-under-fire/) and [here](http://news.cnet.com/8301-13512_3-10184974-23.html)). Regardless of its legality, the speech sounds mechanical, and I don’t see this posing a threat to genuine audio books read with intonation by real people. But my interest is not in this primarily, but in reading via ebook itself. I’ll admit, when it comes to ebooks, I’m still in the undecided camp. On the one hand, I love technology, and can’t resist the latest gadget. On the other hand, I consider myself a “book person.” And the book as physical object matters to me. I want to be able to pick it up, smell it, leaf through the pages. I’m guessing there’s not much to be said for ebook smell.\n\n\n\n\nWhere Kindle does seem to have gotten it right is in the screen. I can’t read books or journal articles on my computer screen, because it’s just not like reading a book. There’s too much glare, it puts too much strain on the eyes, and it’s too distracting. Kindle has solved the glare and eyestrain problems with “electronic ink,” a new technology designed to make letters look more like they do on the written page. From what I can tell, this is a vast improvement over the first generation of ebooks. But what of the distraction factor? Christine Rosen argues that the Kindle is [too distracting](http://www.thenewatlantis.com/publications/people-of-the-screen) to generate productive reading. She tried to read _Nicholas Nickleby_ on a Kindle, but got lured away into Wikipedia searches on Dickens. Alan Jacobs, however, argues almost the [opposite](http://www.thenewatlantis.com/blog/text-patterns/more-kindling). He writes that it is too hard to navigate pages to get to the internet—and that this is a good thing. It keeps you reading, because it’s too hard to leave the book you’re on. If I’m going to shell out the money for an ebook reader, though, I’d want it to do as many things as possible. Ultimately, the discipline required to stave off distraction is not inherent to the print book, but to the act of reading. It is something that is learned by readers, to varying degrees, when they learn to read (on this point, see a recent post, [“In Defense of Readers”](http://alistapart.com/articles/indefenseofreaders)). I can read a novel in a crowded coffee shop, on a busy beach, or just about anywhere without getting distracted. I can’t say the same of my computer. But an ebook needs to be able to do neat techie things that a computer can do in order to be worthwhile. After a point, it’s up to the user to learn how to read well on it.\n\n\n\n\nWhat would I want an ebook reader to do?  Here’s my wish list: I’d like to be able to read a book without interruption. But I’d also want to be able to read journal articles. Could I go to the UVA library website, get an article as .pdf from JSTOR, and read it on a Kindle? This is a copyright nightmare. But it sure would be nice (I did say that this was a wish list). For all of the ancient language work I do, I’d also love to have a Greek dictionary, or maybe even a bible program with ability to switch languages. I have this on my PDA, but a PDA screen is too small and too difficult to read from for any length of time.  The same goes for an ipod/iphone. This sort of basic reference work requires huge volumes of books that can’t be lugged around everywhere, but would always be at hand with an ebook reader. This would be of tremendous value for students. Even though many have praised the fact that the Kindle and its biggest competitor from Sony so far have limited the number of tasks that they can perform, this will ultimately restrict their usage to a niche market: avid readers with money to spend on gadgets. Unfortunately, this is a rather small market. Right now, ebook readers are not made to read all books. They are best suited for reading novels, especially quick page-turners. This leaves out all sorts of books, that are read in all sorts of different ways. Dictionaries aren’t read in the same way as Grisham thrillers, but they’re still books and they’re still “read” in their own way. The difference, I imagine, is that Amazon doesn’t have as much interest in books that are re-read or referenced, because they aren’t “consumed” in the same way and don’t create the need to go buy another book when one is finished.\n\n\n\n\nFor me, the Kindle is still too proprietary, in terms of what can be read on it, and too limited, in terms of its non-reading functionality. Ebooks won’t replace books, unless they can do “e” things. I guess I’m OK with that. I like books.\n\n\n\n"},{"id":"2009-03-30-electronic-text-analysis-and-the-wary-humanist","title":"Electronic Text Analysis and the Wary Humanist","author":"sara-henary","date":"2009-03-30 13:18:34 -0400","categories":["Digital Humanities","Visualization and Data Mining"],"url":"electronic-text-analysis-and-the-wary-humanist","content":"For a long list of complicated reasons, most practitioners of my discipline—political theory—tend to be suspicious of, if not altogether opposed to, the integration of computer technology into their research and teaching. While some scholars cite the superfluity of computer technology to the discipline (excepting, of course, Microsoft Word), others argue that the introduction of certain technologies might somehow actually endanger both thinking and learning (and who wouldn’t find the reduction of Plato to a series of PowerPoint slides, well, a tad reductive?).\n\n\n\nNevertheless, working at the Scholars’ Lab has afforded me the opportunity to sample a range of digital scholarship tools/resources, some of which might appeal to that most skeptical of techno-skeptics, the political theorist. One such resource is [TAPoR](http://portal.tapor.ca/portal/portal), the Text Analysis Portal for Research, a website that provides access to tools used in the analysis of electronic texts.\n\nMany classic texts are now available in electronic form, and “computer-assisted text analysis” (TAPoR website) enables the researcher to explore a text in ways that are difficult, if not impossible, using only conventional tools for text analysis such as the index or a concordance (though much electronic text analysis is modeled conceptually on both of these). This is generally accomplished by allowing the researcher to search a text for specific words or word patterns or to generate a listing of the most frequently used words. In the case of TAPoR’s word/word pattern search, the results are displayed in the context of the surrounding text—the words sought are in bold, while several additional words on either side give the researcher some sense of how the words are being used. One may employ this kind of analysis with many ends in view, though some common goals include: a) testing to see whether—and if so, how often—an author employs either specific language or a certain kind of language and b) exploring certain words or phrases in context in order to gauge the narrowness or expansiveness of the author’s meaning when such language is used.\n\nIn order to explore the features and capacities of TAPoR, I brought to the portal an aspect of a particular research question that I had been thinking about for some time. To write my dissertation, I must provide at least a provisional answer to the following question: Does John Locke articulate a consistent view of the human person throughout his corpus? Although I was very familiar with the way he spoke about the “person” in one text, I was less familiar with his usage in other texts. After sketching a brief definition of the “person” based the first text, I proceeded to investigate whether Locke spoke of the “person” in similar terms in a second text. I pasted the URL of a webpage that contained the second text into TAPoR, which I then asked to search the document for the word “person.” I performed several additional searches, using other key words/phrases from my original definition as well as others that came to mind as I was searching. The results were illuminating. I discovered that although Locke tended to use the word “person” in a more ordinary, less philosophical sense in the second text, all of the basic features of the first text's conception were nevertheless present. While this confirmed the intuition I had about the consistency of Locke’s view of the “person” across his texts (or at least two of them), the specific instances of personhood language that I isolated with the help of TAPoR will allow me to present a much more convincing defense of my position in the dissertation. Additionally, the fact that TAPoR allows the researcher to view all results of a word search simultaneously helped me to formulate more precisely what was going on in the text and to relate it to my more general argument--i.e. a looser, more familiar usage of \"person\" in certain contexts can co-exist with a unified, consistent account of personhood.\n\nTAPoR enabled me to “see” more than I otherwise would have in a text and could be a valuable resource for scholars in any field concerned with the close and careful reading of texts.\n"},{"id":"2009-03-30-how-digital-humanities-can-help-my-dissertation-part-2","title":"Mining and Mapping Apocalyptic Texts, Part 2","author":"matt-munson","date":"2009-03-30 13:18:18 -0400","categories":["Digital Humanities","Geospatial and Temporal"],"url":"how-digital-humanities-can-help-my-dissertation-part-2","content":"As I explained in [my last blog post](http://scholarslab.lib.virginia.edu/index.php/digital-humanities/how-digital-humanities-can-improve-my-dissertation-part-1/), my dissertation will compare several statements about the final fate of humankind in Paul to similar statements in apocalyptic texts. In that post, I described how text-mining could help with the interpretation of the texts which stand at the center of my dissertation. In this post, I will discuss how geographic information systems (GIS) can help to visualize geographic relationships among texts. My ideas here, as in my first blog post, are the result of conversations with other staff members here at the Scholars’ Lab. The question that I pose and answer in this blog post is, What does geography have to do with the analysis of biblical texts? The short answer is, “Much, in every way.” But I can’t just assert that, I need to show it.\n\n\n\n\n\nHistorical-critical study of the Bible has understood for the last two hundred years that the historical circumstances of any person or group profoundly affect the literature that that person or group produces. And scholars understand geographical location to be an integral part of any author’s historical circumstances. I was always dubious about the ability of GIS to help me with my research into the Apostle Paul. After all, scholars have no more evidence for Paul’s geographic location than he gives in his letters, and scholars have already thoroughly discussed this evidence. Another basic tenet of historical-criticism, however, is that we understand an author’s history better when we put it in relationship to the histories of other authors. This goes for geography as well. That means that I should put Paul in geographical relationship with the apocalyptic texts I will study.\n\n\n\n\nBut this process will be more than simply plotting each work’s points of origin on a map. Since GIS is driven by databases, one can query the databases and display the results geographically. For instance, I may find that certain texts assert that the Messiah will descend with angels before the final judgment. If I have geographical data for these texts, I can tell GIS to show the place of origin of all texts that meet these criteria. I could then discover that all of these texts come from a certain area or that they all fall along a certain trade route. I might also discover that they have no apparent geographical similarity. And that is the beauty of GIS. I can follow leads quickly enough that pursuing a red herring no longer requires wasted hours or days. I can check out multiple leads in the time it would take to follow one lead manually.\n\n\n\n\nThe ultimate question, however, is how this technology could help my research. One scenario will make its usefulness apparent. I will consider dozens of apocalyptic texts. If I find that a Paul shares some textual characteristic with only 2 of these texts, I would be hard pressed to show that these three sources by themselves demonstrate an historical pattern. But, if I could show that all three of these texts originated in approximately the same area at approximately the same time, I would show that the texts share more than just textual characteristics. This demonstration would relate the texts more closely to one another and thus strengthen my argument that the textual similarity represents a geographically specific historical pattern. Once such a pattern is recognized, I could interpret these three texts together to reach a fuller understanding of the textual characteristic that is partially represented in each text. And with GIS, one is not limited to analyzing one relationship at a time. One can also assign different symbols to texts depending on which characteristics they have. In this way, one can produce a graphical representation of textual features that may suggest relationships that otherwise would not have been clear. In the end, GIS technologies make it easier to analyze and visualize geographical relationships among texts. As a result, my interpretation of Paul would be based more firmly in Paul’s own historical circumstances.\n\n\n\n"},{"id":"2009-03-31-mapping-then-and-now-are-we-ready-yet-for-academic-social-systems","title":"Mapping Then and Now: Are We Ready (yet) for Academic Social Systems?","author":"ronda-grizzle","date":"2009-03-31 08:51:32 -0400","categories":["Podcasts"],"url":"mapping-then-and-now-are-we-ready-yet-for-academic-social-systems","content":"Ian Johnson, Director of the Archaeological Computing Laboratory at the University of Sydney and TimeMap project leader discusses \"Mapping Then and Now: Are We Ready (yet) for Academic Social Systems?\"\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.2019742791/enclosure.mp3\"]\n"},{"id":"2009-04-03-hide-and-seek-blacklights-smart-search-functionality","title":"Hide and Seek: Blacklight's Smart Search Functionality","author":"fitz-green","date":"2009-04-03 12:26:29 -0400","categories":null,"url":"hide-and-seek-blacklights-smart-search-functionality","content":"Bethany wrote [recently](http://scholarslab.lib.virginia.edu/index.php/digital-humanities/ada-lovelace-day/#more-56) in praise of Bess Sadler’s work on [Blacklight](http://blacklightopac.org/?page_id=2), and its recent [release](http://virgowww.lib.virginia.edu/) (as “VIRGObeta”). I’d like to offer my own (admittedly anecdotal, perhaps insignificant) praise.\n\n\n\n\n<!-- more -->\n\n\n\n\nYesterday I needed to go looking in the library for Jacob Neusner’s translation of the Mishnah into English. I typed into the search box on the UVA library homepage, “Neusner Mishnah.” Seems straightforward enough, right? When I had done the same on Google Books, the book I needed was the very first search result. (Of course, Google limited my viewing of the very page of text I needed to refer to, hence the need to consult the physical book at all!) But when I searched in the legacy UVA catalog system, I received 81 results, _none_ of which were the book I needed. Admittedly, Jacob Neusner has written a lot about the Mishnah, and I was looking for a book he did not author, but translate. Now, I knew how to modify my criteria to get the results I wanted. But should I have to? Shouldn’t the search be smart enough to help me out?  Frustrated, but used to this experience, I next turned to Blacklight to try the same search. The book I needed popped up immediately, on the first page of results. Smart searching. How refreshing!\n"},{"id":"2009-04-03-illuminating-historical-architectur","title":"Illuminating Historical Architecture","author":"ethan-gruber","date":"2009-04-03 12:30:22 -0400","categories":["Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"illuminating-historical-architectur","content":"Following up on my [introduction](http://scholarslab.lib.virginia.edu/index.php/digital-humanities/research-applications-for-3d-models-in-art-history/) to using 3D models to recreate archaeological sites and perform meaningful academic analysis on simulated virtual environments, I will discuss in further detail my current project concerning the recreation of the [House of the Drinking Contest](http://cti.itc.virginia.edu/~jjd5t/ant-pics/10/index.htm) in Seleucia Pieria, the port city of Roman Antioch.\n\n<!-- more -->\n\nThe house in its final phase dates to the third century A.D. and exhibited some of the most complete eastern Roman mosaics, all of which were removed from the site following the 1930's excavations and placed in American museums (including Richmond's very own [Virginia Museum of Fine Arts](http://www.vmfa.state.va.us/)).  What better way to view the mosaics than to recreate the environment in which they existed?  Mosaics in museums are entirely out of their original context.  Many floor mosaics are now hanging on walls.  Even in occasions that museums create elaborate sets to mimick the rooms from which the artwork was taken, it is impossible to recreate the entire structure or accurately recreate the lighting and allow us to view the mosaics as the original owners of the House of the Drinking Contest would have.\n\nIn my previous project of modeling the [House of the Faun](http://en.wikipedia.org/wiki/House_of_the_Faun), one of the largest houses in Pompeii, I had a lot of information to work with.  I had many photographs and artists' reconstructions to consider.  While the ceilings and roofs are gone, the walls are still more or less intact, and so are many of the wall paintings.  The House of the Drinking Contest is much more of a challenge since the walls collapsed and were removed long ago, leaving at most a half a meter of brick and rubble left.  There are clues, however, that let us accurately estimate the height of the walls, and hence a full reconstruction.  The plan indicates that columns were about 0.9 meters in diameter.  From our knowledge of classical orders and the overall dimensions of the house and rooms, we can assume the columns would not have been Corinthian or Ionic since both would have been too out of proportion with respect to the rest of the house.  The reason is that Corinthian and Ionic columns have 10:1 and 9:1 height-to-diameter ratios, respectively.  We can then safely assume an average-height Doric colonnade at a 5.5:1 ratio.  Other clues and experimentation with natural light simulation allow us to predict plausible window locations.\n\n[![House of the Drinking Contest 3D by Ethan Gruber](http://people.virginia.edu/~ewg4x/house_of_the_drinking_contest_thumb.jpg)](http://people.virginia.edu/~ewg4x/house_of_the_drinking_contest.jpg)\n\n(click for larger image)\n\nLighting simulation and computer modeling enable us to take this a step further and create timelapse animations demonstrating how light shifted throughout the hours of the day or days of the year.  We then know when mosaics would have been exposed to direct sunlight or were in the shade.  I found it useful to create an animation of standing in the _triclinium _(dining room) of the house, looking west toward the courtyard, to see if the _triclinium _received direct sunlight at any point of the day.  So far I have found that it does on March 21st of A.D. 200, and probably throughout the spring and autumn.  In fact, the room's mosaics are illuminated quite beautifully right around dinner time.\n\n[[Link to video]](http://people.virginia.edu/~ewg4x/hotdc/caa-hdc-medium.mov).\n\nWhile there is still work to do in the modeling, texturing, and animation of this particular Roman house, the use of accurate modeling techniques and lighting simulation can have a profound impact on archaeology, particularly in cultures that are solar-oriented.  I attended the [Computer Applications and Quantitative Methods in Archaeology](http://caa2009.org) conference last week in Williamsburg, and while there were many demonstrations of 3D models, none of the projects focused on incorporating temporal lighting and analyzing the outcome.  In nearly every case, temporal lighting is not even a consideration.\n\nI did get a chance to informally demonstrate some of my work on the House of the Faun and the House of the Drinking Contest to some other classical archaeologists who are also involved in virtual reconstruction, but this facet of computer modeling has yet to hit the mainstream digital archaeology field, it seems.  Perhaps I will have the opportunity to demonstrate it to a wider audience at CAA next year.\n"},{"id":"2009-04-08-pandora-and-the-genes-of-music-genres","title":"Pandora and the \"genes\" of music genres","author":"jason-kirby","date":"2009-04-08 12:23:39 -0400","categories":["Digital Humanities","Visualization and Data Mining"],"url":"pandora-and-the-genes-of-music-genres","content":"Hello, it’s been a while since I blogged. You may remember me as the music Ph.D. student who was last heard from pondering the uses of Google Scholar. I’m on a new mission this semester, studying for my comprehensive exams.  One of the topics I am researching and preparing an essay on is about genre in popular music. The concept may seem initially so self-evident, you may wonder what there is to write about it, per se. Oh, but there’s lots. This is because the issue of genre always involves the issue of classification, which inherently provokes debate. Take, for instance, a star performer like Beck. His music often includes acoustic guitar, and he’s covered Mississippi John Hurt. So he must be a folkie. Oh wait, but he also apes Prince on some funky jams. So maybe he’s a pop star. But he also headlines a bunch of big rock festivals, and we find his music in the “Rock” section at the record store (wait, what’s a record store?). So I guess we’ll call him a rocker.\n\n\n\n\nMy point being, popular music can be difficult to pin down using genre tags. You’ll find this evidenced in any number of press interviews with musicians who, when pressed by a journalist, pull out that time-worn chesnut that their sound is “unclassifiable”. Genre tags, be it pop, country, rock, hip-hop, salsa, what have you are almost like identifying pornography: I’ll know it when I see it. It’s often somewhat easier to identify what a genre _isn’t_ than what it actually _is_. <!-- more -->Fans and even so-called experts often have difficulty articulating why a particular song or artist fits in a given genre. Based on my readings for this exam topic thus far, I would argue this is because the act of classifying something is essentially making a statement about its _meaning_:  not just semantic/musical meaning, but also meaning that’s intensely cultural, and often political.\n\n\n\n\nThat’s why I like musicologist Robert Walser’s definition of genre in popular music. Updating similar ideas that literary critic Tzvetan Todorov has explored, Walser argues that “Genres…come to function as horizons of expectations for readers (or listeners) and as models of composition for authors (or musicians).”[1] In other words, genres labels are modes of discourse wherein musicians, fans, and music industry workers collaborate to make meaning surrounding the music they love. And perhaps _surrounding_ is the key word, there; one can debate about styles of music with a friend all night long, or a record store employee can create increasingly hyper-specialized bin cards for various sub-genres (Psychobilly, Krautrock, etc.). We circle around the sounds we hear through discourse about them, but ultimately, on a musical-sound level, how do we _know_ that Beck belongs in the “rock” section? As Franco Fabbri has noted, problems with genre are “frontier” problems: “We meet with these whenever we attempt to indicate something which exists at the boundary of two or three zones of meaning.”[2] When considering genre tags, there seems to exist an inescapable gap between our need to accurately label a piece of music and the slippery semantic meanings of recorded sound, which may in any given moment resemble two, three, or even more genres.\n\n\n\n\nWhat then, friends, does this problem have to do with digital humanities? Well, as I research and read in preparation for this exam, I can’t help but frequently think of an Internet service called Pandora that’s richly illustrative of many of these genre issues. For those who may not know, [Pandora](http://www.pandora.com), a free website based out of Oakland,  California since the early ‘00s, is a relatively new and different kind of streaming online radio station. Whereas the playlist at a streaming station such as UVa’s own [WTJU](http://wtju.net/) is determined by in-the-studio DJs, and the “radio” playlist at a website such as last.fm is determined by a process called collaborative filtering (more on this in my next blog), Pandora is notable for its novel method of selecting songs for listeners. Here’s how it works: enter an artist or song into Pandora’s search engine, and the website will issue you a streaming series of songs by artists considered similar to the one you entered. And how is this similarity determined? Through the Music Genome Project, Pandora’s massive undertaking and claim to fame.\n\n\n\n\nThe Music Genome Project, whose work is carried out by roughly fifty analysts at the company’s headquarters in Oakland, is an effort to deconstruct and categorize aspects of pop songs using over 400 different “musical attributes” that the company believes comprise the spectrum of recorded sound. It’s a rigorous close-listening endeavor, specifically intended to focus on aspects like timbre, tempo, harmonic movement, and instrumentation instead of aspects like album art and whether or not the artist has appeared on TRL. This scientific (or pseudo-scientific, depending upon one’s perspective) attention to sonic detail seems—to me at least—an attempt to get beyond the established languages of pop music genres by diving into the nitty-gritty which makes genres what they are. The company builds its credibility as an almost-biologic “genome” of musical characteristics through the depth and breadth of songs it has analyzed: over half a million and counting, according to Pandora’s [website](http://blog.pandora.com/faq/). Pandora says that each of these songs is listened to for 20 to 30 minutes by a trained analyst who tags the song with Pandora-authored characteristics—everything from “meandering melodic phrasing” to “chopped and screwed production”. The company claims that theirs is the most comprehensive effort to systematically categorize music—ever.\n\n\n\n\nFrom the perspective of an academic studying pop music genres, the Music Genome Project presents several fascinating issues. Until my next blog, I’ll briefly set aside an investigation of the company’s claim it can create a taxonomy of the “genes” of popular music. Instead, what I first find most interesting is the story of Pandora and the Music Genome Project’s origins. In a January 2006 [interview](http://twit.tv/itn6) with podcast program “Inside the Net”, Pandora co-founder Tim Westergren told hosts Leo Laporte and Amber MacArthur that he first originated the idea of a music genome while a struggling rock musician himself. He told them his band was “facing the challenge of trying to get known,” and in so doing brainstorming about what aspects of a rock song tend to attract the most commercial attention. Additionally, Westergren shared the intriguing information that he was a film composer at the same time, working for hire to complement a director’s visuals in a given scene. He told Laporte and MacArthur that “In that capacity, one of the things that I had to do was to try and figure out the music taste of a film director.” Westergren said that this challenge was part of what got him thinking about music in terms of distinct, differentiable attributes.\n\n\n\n\nIt just so happens that one of my other exam topics this semester concerns film music soundtracks. Having read much of the academic literature on film music, what Westergren recounts here is fascinating—and doesn’t surprise me. The specific challenges a composer faces when working on a film—How do I balance my need to please the director with my need to express personal creativity?—is a central theme of the literature. Scholars even further back than Irwin Bazelon in 1975 have remarked that the collaboration is by nature difficult, “since the composer spends his entire life _in_ music, working out specific musical relationships, while the director spends his time _out_ of music, involved full-time with films—a visual medium—and only part-time with music, as it affects his film”.[3] Even if the director is a fan of music and has interesting ideas about how she wants it used, if she’s not a musician herself she may have difficulty communicating concepts to the composer which can be actualized musically.\n\n\n\n\nThat challenge has interesting aspects as regards musical genre. On the one hand, bridging the communication gap between composer and director can often force each out of their comfort zone, resulting in new timbres, new melodies—innovation. Many pieces composed for film are quite short, maybe less than a minute in duration, so they often don’t have space in the film to unfold into full-blown genre exercises. This process of collaboration between sound and images in many ways results in the most “hybrid” music imaginable.\n\n\n\n\nFrom another perspective, however, film music cues are also remarkably genre-bound. Not necessarily bound by _musical_ genres per se (classical, country, pop, rock, etc.), but bound by the generic conventions of film itself. Due to the standardized production practices of most movies, film as a medium tends to be more amenable to genre classification—and soundtracks can play a key role in that[4]. For instance, soaring strings sketching out an American traditional folk or “cowboy” song, and the audience suddenly knows we’re in a Western. A minor key piano melody and a sultry saxophone, and we know we’re watching a film noir. Given that these generic conventions of film music most certainly exist, it makes me wonder what sorts of films Westergren was scoring as part of his job. Perhaps offbeat indie-Sundance dramas which were aiming for a kind of transcendence of genre strictures?\n\n\n\n\nIn any case, the fact that the Music Genome Project origin story involves the world of film music tells me that musical genre was on Westergren’s mind as he brainstormed—even if he sought to rebel against the concept. Additionally, the Project’s roots in soundtrack-for-hire work demonstrate that you can never take the music _business_ out of the music: commercial forces and the presence of an paying audience (real or imagined) inflect in some way all decisions musicians and music industry workers make. As Westergren’s rock band tailoring their sound in an attempt to “get known” reminds us, market considerations are basically always in a dialectical relationship with “creativity” as a musical genre forms—and this includes classical and avant-garde genres which claim to be above that kind of stuff.\n\n\n\n\nGiven the commercialized roots of the Music Genome Project, it’s a bit surprising to me that the music industry has fought Pandora as tooth-and-nail as they have. It’s common knowledge that traditional AM/FM commercial radio has been the music industry’s biggest promotional tool during the 20th century. But as traditional radio fades in influence and web radio such as Pandora ascends, the industry leaders have been notoriously less willing to jump onto the 21st-century Internet bandwagon of music promotion.\n\n\n\n\nDespite the fact that Pandora _streams_ tracks instead of allowing users to illegally download—which means, in my opinion, that record company executives should be groveling at Pandora’s feet in thanks—the major-label music industry has seemed intent upon shutting them down. Or, to be more specific, the labels’ demands (through representative organization SoundExchange) for higher royalty payments created expenses Pandora was finding impossible to sustain. Following a federal board ruling mandating increased royalty rates for web radio, in August of last year the _Washington Post_ [quoted Westergren](http://www.washingtonpost.com/wp-dyn/content/article/2008/08/15/AR2008081503367_2.html) as saying Pandora was “approaching a pull-the-plug kind of decision”. On the brink of shutting down, Westergren appealed to the company’s subscribers to contact their local representatives on behalf of web radio—and the gambit seems to have worked. The ruling was reconsidered, and [currently representatives from both web radio and the music industry are negotiating](http://www.dmwmedia.com/news/2009/02/23/cnet%3A-webcasters%2C-music-industry-battling-over-royalties) newer, more manageable royalty rates. Pandora seems to be approaching a delicate truce with the market forces which, to me, seem constitutive of its role as a source for promoting and discovering popular music. (In other words, what took the industry so long to accept current reality?)\n\n\n\n\nIn the next installment of this blog, I’ll continue my exploration of Pandora, especially the logic behind its attempt to map “genes” of music and to approach music in a mode somehow “beyond” genre.\n\n\n\n\n\n\n\n\n\n* * *\n\n\n\n\n\n\n\n[1] Walser, Robert. _Running with the Devil: Power, Gender, and Madness in Heavy Metal Music_. Middletown, CT: Wesleyan University Press, 1993. p. 29.\n\n\n\n\n\n\n\n\n\n\n\n[2] Fabbri, Franco and Iain Chambers. “What Kind of Music?”. _Popular Music,_ Vol. 2, Theory and Method (1982), pp. 131-143.\n\n\n\n\n\n\n\n\n\n\n\n[3] Bazelon, Irwin. _Knowing the Score: Notes on Film Music_. New York: Van Nostrand Reinhold Company, 1975.\n\n\n\n\n\n\n\n\n\n\n\n[4] Holt, Fabian. _Genre in Popular Music_. Chicago: The University of Chicago Press, 2007. Pp. 4-5.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},{"id":"2009-04-21-toward-the-historical-data-forge-what-happens-after-the-data-mining","title":"Toward the Historical Data Forge: What Happens After the Data-Mining?","author":"ronda-grizzle","date":"2009-04-21 08:50:18 -0400","categories":["Podcasts"],"url":"toward-the-historical-data-forge-what-happens-after-the-data-mining","content":"Bruce Robertson of the Mount Allison University Department of Classics and the Historical Event Markup and Linking Project talks about HEML: Historical Event Markup Language\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.2053632731/enclosure.mp3\"]\n"},{"id":"2009-05-04-mapping-out-the-geography-of-an-asian-american-music-blog","title":"Mapping the Digital Diaspora of a Dissertation Research Blog","author":"wendy-hsu","date":"2009-05-04 06:32:00 -0400","categories":["Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"mapping-out-the-geography-of-an-asian-american-music-blog","content":"At the onset of my field research in summer 2007, I launched a blog – [YellowBuzz.org](http://yellowbuzz.org) – with the intention to: 1) archive and organize my field notes in textual and audio-visual form; 2) convey my research purpose and progress to informant musicians and the public; 3) self-position as a “participant” in the scene. Since then, I have made over 160 posts, some directly linked and others tangentially related to my research findings about the activities and media of Asian American indie rock musicians. Over the past one and a half years, my field research blog has received attention from both print and online media.  Evidently, this blog has constructed a community consisting of musician- and music-enthusiast-visitors with an interest in Asian American and transpacific music-culture.<!-- more -->\n\nThis past January, I began tracking the blog traffic by using [Google Analytics](http://www.google.com/analytics/). This service monitors the physical location of site visitors and their interactions with the pages on the site. The geographical data are analyzed in terms of the number of visits per unit of geographical organization such as city, country/territory, sub continent region, and continent.  This information is also visualized in the form of an interactive map on which users can zoom in and out of specific locales and find site visit patterns specific to cities, countries, regions, or continents in the world.\n\nOver the last four months, I have been playing with the May Overlay function projecting geospatial patterns of the site traffic on my blog. These interactive moments have helped me imagine interesting questions such as: What is the geography of an electronic community based on the topic of “Asian American music,” the tagline of my blog? What does the geo-spatial terrain of this “digital diaspora” look like? Are there any striking patterns at each of the organizational level namely, the city, country, sub-continental region, and continent? What spatial boundaries are transcended and created in these visualizations? Or, fancifully, how does the digital geography of my blog reconfigure the more general social geography of “Asian America” online or offline?\n\nToday marks a 4-month anniversary of this thought experiment. I decided to take some screen shots of a few of the visualizations that I’ve found more meaningful in Google Analytics. This analysis uses data from a sample of 3,061 site visits collected from January 1 to April 30, 2009. I will highlight a few interesting findings below:\n\n[![blog visits in U.S. cities](http://lh4.ggpht.com/_POx44XG38pY/SfsEEpcmLGI/AAAAAAAABME/jPp5MEsm9t0/s800/Analytics_Map_UScities.jpg)](http://lh4.ggpht.com/_POx44XG38pY/SfsEEpcmLGI/AAAAAAAABME/jPp5MEsm9t0/s800/Analytics_Map_UScities.jpg)1) Here’s a map of blog visits in various U.S. cities. It appears that the visitors are concentrated in central Virginia (the home of yours truly), New York City, Boulder, Los Angeles, and San Francisco. Other than central Virginia and Boulder, these are areas of high concentration of Asian Americans and indie rock activities. I'm not quite sure how to explain the traffic flow from the Denver area (Boulder and Aurora, ranked sixth in this map) other than to link it to the thriving indie rock scene in Boulder and the physical location of an Asian/Japanese music blogger Shay of [Sparkplugged](http://sparkplugged.net/).\n\n[![site visits per country](http://lh4.ggpht.com/_POx44XG38pY/SfsEE0fNXFI/AAAAAAAABMM/LccJoFTelNY/s800/Analytics_May_countries_PieChart.jpg)](http://lh4.ggpht.com/_POx44XG38pY/SfsEE0fNXFI/AAAAAAAABMM/LccJoFTelNY/s800/Analytics_May_countries_PieChart.jpg)2) According to this chart, 76% of the site visits have occurred within the boundaries of the United States. Next on the list are Canada, United Kingdom, and Australia, all English-speaking countries with close historical ties to American music. In the continent of Asia, countries such as Taiwan, South Korea, the Philippines, and Singapore have among the highest number of visitors to my site. I attribute this pattern to my blog posts about U.S.-based artists who have a large following in these particular countries. Specifically, Hsu-nami (of New Jersey) and Johnny Hi-Fi (SF-based) has strong ties to Taiwan; Kite Operations (New-York) to South Korea; Plus/Minus (New York) to the Philippines and Taiwan.\n\n[![site visits per sub-continent region](http://lh3.ggpht.com/_POx44XG38pY/SfsEE4M02WI/AAAAAAAABMU/3_qKDX7hSoY/s800/Analytics_piechart_subcont.jpg)](http://lh3.ggpht.com/_POx44XG38pY/SfsEE4M02WI/AAAAAAAABMU/3_qKDX7hSoY/s800/Analytics_piechart_subcont.jpg)3) This last chart represents the sub-continental spread of the site visits. North America takes the lead (taking 80% of all visits). Northern Europe and Eastern Asia tie as second, followed by South-Eastern Asian and Western Europe. I’m not quite sure how to explain the high number of visits from Northern Europe other than to link it to the popularity of a Taiwanese metal band Chthonic in North Europe. Chthonic has a strong international presence, having worked with producers in Denmark and the U.S. including Rob Caggiano, the guitarist of Anthrax. In 2007, Chthonic toured with the OzzFest and established close ties with Taiwanese-American-led erhu rock group Hsu-nami.\n\nSo what does this all mean? YellowBuzz, a blog on “Asian American music”, has constructed a global, transnational readership. Asian America in the online digital environment exists beyond the boundaries of the United States and the Asian continent. These observations of transnational crossings work against the geography of Orientalism: a now-classical theory within postcolonial studies that refers the representational control of the non-west by western-produced discourse.  The digital diaspora of YellowBuzz has tampered with the so-called east-west binary.\n\nNow if I were serious about pursuing the research on the transnationality of Internet music journalism, I would look for a correlation between blog content and traffic patterns. This would require systematic, post-to-post observations. I would also consider mapping information regarding Internet access and user demographic with the intention to find links between the blog statistics and general Internet sociality. I would also look for statistical and mapping methods more powerful than Google Analytics.\n\nBut – to get back to my dissertation that asks: What paths do musicians and their music take as they establish routes crossing territories constructed by nation-states, corporations, international laws, etc? Unfortunately, these visualizations lack the analytical strength to provide an insight on the musicians’ perspective on the scene. They have offered a perspective on media, in particular in understanding the role of a music blog in constructing “Asian America.”\n\nIn the coming months, I will be working on a digital humanities project with Joe Gilbert at UVa’s [Scholars’ Lab](http://www2.lib.virginia.edu/scholarslab/) pursuing questions related to the musicians’ side of the story. I hope to unravel the terrain of musicians’ sociality within the transnational scene of indie rock music by mapping out their tours, social networks on (SNS), and record distribution. Meanwhile, I’m experiencing a bout of euphoria loving the fact that I have reclaimed a free market analytical tool offered by Google for my academic(-y) ethnomusicological thought experiment.\n"},{"id":"2009-07-24-institute-for-enabling-geospatial-scholarship","title":"Institute for Enabling Geospatial Scholarship","author":"bethany-nowviskie","date":"2009-07-24 07:17:22 -0400","categories":["Announcements","Digital Humanities","Geospatial and Temporal"],"url":"institute-for-enabling-geospatial-scholarship","content":"Through the generosity of the National Endowment for the Humanities, the Scholars' Lab will host a three-track **Institute for Enabling Geospatial Scholarship** at the University of Virginia Library in November 2009 and May 2010. This Institute will bring scholars, cultural heritage professionals, and software developers together to support and develop geospatial projects and methods in the digital humanities. The NEH's [Institutes for Advanced Topics in the Digital Humanities](http://www.neh.gov/grants/guidelines/IATDH.html) program will support travel and lodging for 40 attendees as well as Institute faculty members. Dedicated funding is available for graduate students as well as faculty attendees. The Scholars' Lab will provide $40,000 in funding for short-term scholar- and developer-in-residencies in humanities GIS to complement the Institute.\n\nThe Scholars' Lab also will develop and host an online information clearinghouse and fund visiting fellows in an effort to promote ongoing scholarly engagement, software development, and information sharing by Institute attendees around the theme of Enabling Geospatial Scholarship.\n\nSee the [Institute web site](http://www2.lib.virginia.edu/scholarslab/geospatial/index.html) for more information -- including application deadlines for each of our three \"tracks,\" on Stewardship, Software, and Scholarship.\n"},{"id":"2009-09-10-digital-therapy-luncheon-september-2009","title":"Digital Therapy Luncheon September 2009","author":"ronda-grizzle","date":"2009-09-10 08:48:47 -0400","categories":["Podcasts"],"url":"digital-therapy-luncheon-september-2009","content":"Introducing our 2009/10 Digital Humanities Fellows and Scholarship Award Winners\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.2466448832/enclosure.mp3\"]\n"},{"id":"2009-11-05-creation-of-game-worlds","title":"Creation of Game Worlds","author":"ronda-grizzle","date":"2009-11-05 04:43:31 -0500","categories":["Podcasts"],"url":"creation-of-game-worlds","content":"Writer, game designer, and UVa alumnus [Shane Liesegang](http://shaneliesegang.com/) talks about the \"Disruptive Construction of Game Worlds\"\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.2733214823/enclosure.mp3\"]\n"},{"id":"2009-11-05-olmsted-editing-to-mapping","title":"Olmsted: Editing to Mapping","author":"ronda-grizzle","date":"2009-11-05 04:00:17 -0500","categories":["Podcasts"],"url":"olmsted-editing-to-mapping","content":"Ethan Carr and Mandy Gagel of the Frederick Law Olmsted Papers discuss \"The Papers of Frederick Law Olmsted: From Editing to Mapping?\"\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.2726749166/enclosure.mp3\"]\n"},{"id":"2009-11-12-new-course-in-digital-humanities","title":"New Course in Digital Humanities!","author":"jean-bauer","date":"2009-11-12 06:58:37 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"new-course-in-digital-humanities","content":"Inspired by my fellowship at the Scholars’ Lab last year, I am teaching a course in the History Department this coming spring called, HIST 4501 “From Vellum to Very Large Databases: Historical Sources Past, Present, and Future.”   The course will examine how information about the past has been (and is being) preserved.\n\nHistorians rely on primary sources to inform and defend their arguments about the past, but digital technology is altering the form and the content of available records and, in the process, raising fundamental questions about the nature of historical analysis.   I have designed the course to be “hands on,” so students will have the chance to\n\n\n\n\t\n  * examine illuminated manuscripts\n\n\t\n  * operate an early printing press\n\n\t\n  * geo-reference historical maps\n\n\nas they explore familiar and unfamiliar ways of recording information and reflect on how these formats affect the study of history.\n\nThe course is for undergraduates and will meet on Wednesdays from 3:30-6:00pm.  For more information,  check out the course page at [http://www.jeanbauer.com/vellum_to_vldb.html](http://www.jeanbauer.com/vellum_to_vldb.html).\n\n“From Vellum to Very Large Databases” is a 4501 (Major Seminar), so students will sign up via a waitlist and then be added once they have received the instructor’s permission to enroll.\n"},{"id":"2009-11-25-neogeography-from-tower-to-town-hall","title":"Neogeography: from Tower to Town Hall","author":"ronda-grizzle","date":"2009-11-25 04:55:40 -0500","categories":["Podcasts"],"url":"neogeography-from-tower-to-town-hall","content":"Andew Turner joined us in the SLab to discuss the neogeography movement, which has emerged from the rise of easy-to-use web-based maps and emphasizes community-led and colloquial uses of geospatial tools and techniques such as online maps, GPS, and location-aware phones, and its potential applicability to higher education.\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.2841459673/enclosure.mp3\"]\n"},{"id":"2009-12-15-dynamic-web-forms-for-the-creation-of-xml","title":"Dynamic web forms for the creation of XML","author":"ethan-gruber","date":"2009-12-15 06:20:25 -0500","categories":["Research and Development"],"url":"dynamic-web-forms-for-the-creation-of-xml","content":"Among my regular tasks in the Scholars' Lab Research and Development department, I have been developing applications to enable users to easily edit XML metadata within web forms.  As those familiar with metadata creation workflows will know, methods for creating XML documents were prone to human error and required some level of technical knowledge.  With XForms, a W3C standard for creating dynamic web forms, the technical barriers for creating robust metadata can finally be removed.\n\nI began working with XForms (and the enterprise application, Orbeon, for rendering the forms and managing data interactions) in July as I improved [Numishare](http://sourceforge.net/projects/numishare/), the open source application developed to deliver the [University of Virginia Art Museum Numismatic Collection](http://coins.lib.virginia.edu/).  The UVA coin collection, described in Encoded Archival Description (EAD) is a visual site with no administrative back end.  In order to provide tools to entice users to try the new software, I had to develop a method that curators could use to quickly and easily manage artifactual information--standard creating, updating, and deleting commands.  I was able to accomplish this entirely with Orbeon and XForms.  The application is sophisticated enough to allow auto-suggest capabilities for controlled vocabulary (based on TermsComponent in Solr 1.4), as well as post directly to the Solr search index when the user saves or deletes a record.\n\nMost recently, I have been developing an XForms application that can generally be applied to the creation and management of EAD finding aid collections.  The project, titled [EADitor](http://code.google.com/p/eaditor/), has been met with interest from the archival and library coding communities.  While there is much work left to do, the application has definite potential.  It is possible to interact with EAD data that resides in the institutional repository, simplifying the process by which the guides are edited and saved back in the repo.\n\nOther institutions have developed forms for other metadata standards common to libraries, so my colleague in the Scholars' Lab, Adam Soroka, and I have started a listserv in order to facilitate better discussion and collaboration between units working on similar projects.  Hopefully, the library community will eventually use XForms applications that streamline the metadata creation process for all XML standards that are commonly used.\n"},{"id":"2009-12-15-neatline","title":"Neatline","author":"bethany-nowviskie","date":"2009-12-15 11:08:14 -0500","categories":["Digital Humanities"],"url":"neatline","content":"I've called [Neatline](http://neatline.org/), the [Digital Humanities Start-Up](http://www.neh.gov/grants/guidelines/digitalhumanitiesstartup.html) project Adam Soroka and I began developing in September, a \"contribution to interpretive humanities scholarship in the visual vernacular.\"\n\nHuh?\n\nThis project will allow scholars (and other stewards of cultural heritage) to create Web-based geospatial and temporal visualizations that build on the rich [EAD metadata](http://www.archivists.org/saagroups/ead/index.html) libraries produce in describing their archival collections and making them more discoverable -- but the crucial twist is that we didn't want to think of our Neatline visualizations as _products_ of the metadata. They're not brain-dead algorithmic output or some kind of thoughtless expression of the archivist's (nuanced, but necessarily broad) stance toward historical or literary documents of interest. (Yes, I'm asking for it; bring it on!) In other words, Neatline isn't about the parsing of placenames and automated population of timelines with data. Rather, we've conceived this tool (really, as development proceeds, this _approach_, because Neatline is emerging as an arrangement of instruments and an attitude toward their use) as a kind of _playspace for the scholarly interpretative act_. In future posts, we'll describe our development effort and I'll delve a little into the conceptual background for Neatline in the [Temporal Modelling](http://iath.virginia.edu/time) project I undertook several years ago with [Johanna Drucker](http://en.wikipedia.org/wiki/Johanna_Drucker).\n\nIn the meantime, you can read about how we're employing the [Omeka](http://omeka.org) plugin framework as a way to handle GIS services for scanned historical maps on the ScholarsLab.org project pages for [Neatline-in-progress](/research/neatline/) and our larger [Omeka plugin work](/research/omeka-plugins/), or you can check out our dedicated [project blog](http://neatline.org).\n"},{"id":"2009-12-16-large-files-and-omeka","title":"Large Files and Omeka","author":"wayne-graham","date":"2009-12-16 09:40:05 -0500","categories":["Research and Development"],"url":"large-files-and-omeka","content":"This issue came up for a friend of the Scholars' Lab today on Twitter, but it's hard to answer in 140 characters. It's a question about allowing for larger file sizes in Omeka and there are a few ways to handle this.  (Because we want our new blog to be a combination of thoughtful essays on digital scholarship and quick answers to real-world technical problems, I thought I'd post here.)\n\nSince Omeka runs on PHP, this is actually a PHP configuration issue and not something you can currently tweak in Omeka. Basically, you just need to tell PHP to allow larger files sizes that are larger than the default. A very easy way to do this is to edit the .htaccess file that Omeka ships with along the following lines:\n\n[code lang=\"bash\"]\nphp_value upload_max_filesize 20971520\nphp_value post_max_size 20971520\n[/code]\n\nI'll note here that doing things this way only affects your Omeka project. Another way to go about this is to add the above to the Apache configuration that defines from where Omeka should be served. For example:\n\n[code lang=\"bash\"]\n\n<VirtualHost *:80>\n\nServerName www.coolomeka.org\nDocumentRoot /var/www/omeka\n\n<Directory \"/var/www/omeka\">\n    Options FollwSymLinks\n    AllowOverride All\n    Order allow,deny\n    Allow from all\n</Directory>\n\n    ErrorLog logs/omeka_error_log\n    TransferLog logs/omeka_transfer_log\n\n    php_value upload_max_filesize 20M\n    php_value post_max_size 20M\n</VirtualHost>\n[/code]\n\nLastly, you can edit the php.ini file (usually in /etc/php.ini or /etc/php5/apache2/php.ini). Just do a search in the file and change the following settings:\n\n[code lang=\"bash\"]\n  memory_limit = 32M\n  post_max_size = 20M\n  upload_max_size = 20M\n[/code]\n\nYou typically don't need to reload Apache (as long as you did not edit the Apache configuration file) to get these settings to work.\n\nFor more info on this, check out these resources\n\n\n\n\t\n  * [Description of core php.ini directives](http://php.net/manual/en/ini.core.php)\n\n\t\n  * [PHP Upload Configuration](http://www.radinks.com/upload/config.php)\n\n\t\n  * [PHP Directives Related to (Large) File Upload](http://www.developershome.com/wap/wapUpload/wap_upload.asp?page=php2)\n\n\n"},{"id":"2010-01-07-calculating-county-to-county-distances-with-gis","title":"Calculating county-to-county distances with GIS","author":"kelly-johnston","date":"2010-01-07 10:40:08 -0500","categories":["Geospatial and Temporal"],"url":"calculating-county-to-county-distances-with-gis","content":"In the Scholars' Lab we recently worked with a researcher whose study areas focused on several groups of US counties.  Of interest was the distance from every county within a group to every other county in that same group. We used geographic information systems (GIS) software to calculate these distances.\n\n<!-- more -->GIS software creates, manages, analyzes, and visualizes geographically referenced data.  Environmental Systems Research Institute (ESRI) in Redlands, California, produces ArcGIS desktop, a GIS software suite.  The following examples use ArcGIS ArcMap software version 9.3.1 at the ArcInfo product level.  Instructions for accessing GIS software at the University of Virginia are here: [http://guides.lib.virginia.edu/gis](http://guides.lib.virginia.edu/gis).  If you are not affiliated with UVA, contact your local IT support person or ESRI for information on accessing this GIS software.\n\n**Calculating polygon centroids** – When working with polygon features (like county boundaries) in GIS it is often necessary to locate the geographic center or centroid of each polygon as a point feature.  In ESRI’s ArcGIS desktop software the ‘Feature To Point’ tool creates polygon centroids.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2010/01/FeatureToPoint1.jpg)](http://www.scholarslab.org/calculating-county-to-county-distances-with-gis/attachment/featuretopoint-2/)\n\nFor more information see the ArcGIS online help for the ‘Feature To Point’ tool:\n\n[http://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?id=1798&pid=1790&topicname=Feature_To_Point_%28Data_Management%29](http://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?id=1798&pid=1790&topicname=Feature_To_Point_%28Data_Management%29)\n\n** **\n\n** **\n\n** **\n\n** **\n\n** **\n\n** **\n\n** **\n\n** **\n\n** **\n\n** **\n\n**Creating a point distance table** – Given a set of point features, GIS software can calculate the straight-line distance from each point in the set to every other point in the set.   The output distance table contains one row for each point-to-point combination along with the calculated distance.  In ESRI’s ArcGIS desktop software the ‘Point Distance’ tool creates a point distance table.\n\n[![Point Distance](http://www.scholarslab.org/wp-content/uploads/2010/01/PointDistance.jpg)](http://www.scholarslab.org/calculating-county-to-county-distances-with-gis/attachment/pointdistance/)\n\nFor more information see the ArcGIS online help for the Point Distance tool: [http://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?id=1353&pid=1347&topicname=Point_Distance_%28Analysis%29](http://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?id=1353&pid=1347&topicname=Point_Distance_%28Analysis%29)\n\n** **\n\n** **\n\n** **\n\n** **\n\n** **\n\n**Example using the ‘Feature To Point’ and ‘Point Distance’ tools – **Given a polygon dataset representing boundaries of a group of US counties, calculate the distance between each county in the group and every other county in the group.\n\n\n\n\n  1. After ensuring the county polygon boundary file is projected using a distance-preserving projection, select the county polygons for the study area.[![](http://www.scholarslab.org/wp-content/uploads/2010/01/Counties1-1024x763.jpg)](http://www.scholarslab.org/geospatial-and-temporal/calculating-county-to-county-distances-with-gis/attachment/counties-2/)\n\n\n  2. Convert the selected county polygons to county polygon centroid points using the ‘Feature To Point’ tool. [![Counties with Centroids](http://www.scholarslab.org/wp-content/uploads/2010/01/CountiesCentroids-1024x763.jpg)](http://www.scholarslab.org/geospatial-and-temporal/calculating-county-to-county-distances-with-gis/attachment/countiescentroids/)\n\n\n  3. Generate the point distance table for all county centroid points created in step 2 using the ‘Point Distance’ tool.  Distance is expressed in the linear unit of the input dataset, which is meters in our example.[![](http://www.scholarslab.org/wp-content/uploads/2010/01/Matrix1.jpg)](http://www.scholarslab.org/geospatial-and-temporal/calculating-county-to-county-distances-with-gis/attachment/matrix-2/)\n"},{"id":"2010-01-27-the-1907-massie-map-of-albemarle-co-is-now-in-the-portal","title":"The 1907 Massie map of Albemarle Co.","author":"dave-richardson","date":"2010-01-27 10:11:57 -0500","categories":["Geospatial and Temporal"],"url":"the-1907-massie-map-of-albemarle-co-is-now-in-the-portal","content":"[caption id=\"attachment_482\" align=\"alignright\" width=\"109\"][![The 1907 Massie Map of Albemarle Co., VA](http://www.scholarslab.org/wp-content/uploads/2010/01/Massie1907_thumb500.jpg)](http://www.scholarslab.org/geospatial-and-temporal/the-1907-massie-map-of-albemarle-co-is-now-in-the-portal/attachment/massie1907_thumb500-2/) The 1907 Massie Map of Albemarle Co., VA[/caption]\n\nWhile going through our archives of scanned maps, we recently ran across a copy of **Frank A. Massie’s 1907 “A new and historical map of Albemarle County, Virginia” **[Special Collections, University of Virginia Library], commonly referred to as the Massie map, which contains a wealth of detailed historical information for the county in which the University of Virginia sits.\n\n<!-- more -->After obtaining a more recent scan of the map from Special Collections and Andrew Curly (of the Library’s Digital Production Services), we georectified the digital map and added it to our [geospatial data portal](http://gis.lib.virginia.edu/). Accessing the map through our portal ([here](http://gis.lib.virginia.edu:8080/geonetwork/srv/en/metadata.show?id=544&currTab=simple) or[ here](http://gis.lib.virginia.edu/items/544)) allows you to not only view the map at high resolution in your browser, but also to [view the Massie map in Google Earth](http://gis.lib.virginia.edu:8080/geoserver/wms/kml?layers=Massie%3AMassie1907_2483), as well as being able to overlay the Massie map over other base maps and other geospatial data layers (for example, by making a WMS call to our portal’s server from your desktop GIS; or by pulling the Massie map into your webpage-embedded dynamic map using [Open Layers](http://openlayers.org/)).\n\nAmong the many interesting historical features on the maps, are the locations of:\n\n\n\n\t\n  * The residences and named estates of prominent county landowners, including the oldest house in the county, plus various churches and schools, numerous dams, mills, and quarries, and local geologic and mineral resources;\n\n\t\n  * Where Revolutionary war prisoners where held (the Barracks that held Hessian troops and from which Barracks Road was named, as well as where Baron and Baroness de Reidesel resided) plus old militia rendezvous points;\n\n\t\n  * Routes and encampments of various military campaigns (notably those of British Lieutenant Colonel Banastre Tarleton’s 1781 raid on Charlottesville and Gov. Thomas Jefferson’s subsequent flight, General T. J. “Stonewall” Jackson’s Confederate Army’s march in 1862, General George A. Custer’s unsuccessful Union Army raid in 1864, and General Philip Sheridan’s Union Army’s march in 1865 along with locations of many of the mills and bridges Sheridan had destroyed);\n\n\t\n  * Existing and proposed rail lines of many now-nonexistent railroads;\n\n\t\n  * Locations where various murders and river drownings occurred.\n\n\nAdditionally, the map contains tables showing city and county data from the 1900 census and a 1906 household survey (listing, among other things: total property values and taxes; numbers of livestock (1000’s of cattle, horses, hogs, and sheep, but only 24 goats!), wagons, watches, clocks, sewing machines, and pianos; number of manufacturers with their capital, number employees, and total wages; farm acreage; and enrollment figures for the white and colored public schools.\n\nThere’s also an inset showing the major rail lines and railroad junctions for the state of Virginia.\n"},{"id":"2010-02-10-lisa-rosner-the-anatomy-murders","title":"Lisa Rosner: the Anatomy Murders","author":"ronda-grizzle","date":"2010-02-10 07:30:06 -0500","categories":["Podcasts"],"url":"lisa-rosner-the-anatomy-murders","content":"_Up the close\nAnd down the stair\nVisualizing the worlds\nOf Burke and Hare_\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.3381074823/enclosure.mp3\"]\n"},{"id":"2010-02-15-more-on-pandora-genres","title":"More on Pandora:  genres, genomes, and musical taste...","author":"jason-kirby","date":"2010-02-15 04:39:52 -0500","categories":["Digital Humanities","Visualization and Data Mining"],"url":"more-on-pandora-genres","content":"Hello. In my last blog, I began my discussion of [Pandora.com](http://www.pandora.com), the streaming audio website which offers a new kind of web radio to listeners. Enter a “seed” song into Pandora’s search engine, and the site will create a streaming “station” composed of songs that resemble your seed song. This process is powered by the Music Genome Project, a massive research endeavor which began in the early 2000s and is based out of the company’s Oakland, California headquarters.\n\n\n\n\nHow is Pandora’s song-recommendation engine different than web radio platforms that came before it? Well, the majority of other online radio stations, such as last.fm, operate off a system called collaborative filtering. What is collaborative filtering? In layperson’s terms, collaborative filtering involves matching one user’s taste to another’s (or a series of other people). On a site like [last.fm](http://last.fm), over time a user amasses a playlist of songs they’ve expressed a preference for—a sort of musical taste profile. Last.fm’s search tools automatically identify other users with whom your tastes seem to overlap, and uses this information to power “radio” stations you can stream on the site. The process is pretty simple, and [based on personal intuition and the data existing users have already entered into the system](http://www.wired.com/culture/lifestyle/news/2003/07/59522). Collaborative filtering powers aspects of many media websites, such as Amazon.com’s personal recommendation feature for shoppers.\n\n\n\n\n<!-- more -->It does have some limitations for online radio listeners, however. As Pandora’s founder Tim Westergren pointed out in a 2006 interview with Leo Laporte and Amber MacArthur, collaborative filtering-powered online radio stations have a tendency to only recommend what is broadly popular in contemporary pop music. While independent-label music certainly has a strong presence on last.fm, a quick scan of various users’ profiles on the site may suggest that Westergren has a point. Even among “indie” users on last.fm, there’s a whole lot of Death Cab for Cutie and Modest Mouse ruling the playlists (nothing against either of these bands). Collaborative filtering doesn’t necessarily ensure that the site’s users will discover truly obscure stuff they hadn’t heard of before. And in keeping with my interest in genre boundaries vis-à-vis Internet radio, in interviews Westergren has attributed the problem to the mainstream music business’ interest in keeping consumers bracketed into genre-specific niches. In the aforementioned chat with Laporte and MacArthur, Westergren cited the [“age-old problem in the music industry”](http://twit.tv/itn6) wherein a tiny percentage of music released by a given label typically accounts for nearly all its sales—a problem codified by genre boundaries.\n\n\n\n\nPandora, through its Music Genome Project, aims to circumvent this problem, by offering its users a new kind of recommendation engine. As I mentioned in my earlier post, the Music Genome is a systematic endeavor to deconstruct and analyze individual pop songs using over 400 “musical attributes” that the company has identified. These attributes include everything from tempo, to vocal timbre, to harmonic movement—even sound production aspects like echo and reverb. In other words, it is essentially a musicological approach in the strictest sense of the word. The focus is on sound itself, rather than a band’s cultural associations with other bands (as is the case in collaborative filtering). Indeed, Westergren bragged in the aforementioned interview that “when we recommend to you a piece of music, we don’t even _know_ how popular it is.”\n\n\n\n\nInstead, what the Music Genome Project entails is the company’s roughly fifty analysts sitting down in the Oakland, CA headquarters and methodically tagging a given song using these 400+ attributes. Westergren has described the process in ways akin to the scientific method, noting that a percentage of songs the analysts deconstruct are reviewed twice for quality. The songs, categorized by attributes, are added to the Project’s over 500,000 songs (and counting) accumulating in the company’s database. Songs sharing a similar musical “DNA” are then automatically matched and linked by Pandora’s search engine when you enter in a “seed” song. Westergren has called the Genome “kind of like a musical taxonomy,” and I don’t think this language is accidental. As Fabian Holt has pointed out about musical genres, “Discourse on the temporal dimensions of categories is saturated with organicist metaphors, as in discussions of how genres are _born_, how they _grow_, _mature_, _branch off_, _explode_, and _die_.”1 Even though Pandora in fact aims to get _around_ genre, it seems to me that this biologic language informs the company’s mission and direction.\n\n\n\n\n\nIn any case, as a Pandora user, I have often benefited from the happy accidents occasioned by the way the Music Genome Project works. For instance, I entered in Pandora as a \"seed\" Bob Dylan's song \"Tonight I'll Be Staying Here With You\", a lilting, mid-tempo country-rock stroll. The Genome built a streaming station for me that included folk-rocky chestnuts by relatively obscure ‘60s and ‘70s groups like UFO and Earth Opera. It's likely that I would not have heard about these groups without Pandora, or at least that I would've heard about them years from now in another context. In this regard, it seems that Westergren does have something to boast about regarding his claim that Pandora's search engine connects listeners with \"invisible\" music in a way that mainstream, genre-bound, multinational music corporations just can't.\n\n\n\n\nOn the other hand, there are several notable gaps in the logic and execution of Pandora and the Music Genome Project model. The first gap I feel compelled to point out is a very practical one. Returning to my example of the station based around \"Tonight I'll Be Staying Here With You,\" Pandora is skilled in giving a user a lot of what they like. Enter in a twangy rock song like Dylan’s and you’ll get a station with loads of twangy rock songs. But there can be too much of a good thing; namely, I find homogeneity of songs’ _tempo_ an issue on Pandora stations. “Tonight I’ll Be Staying Here With You” is a bit plodding, and I’ve found that over a few hours of playing this station, I mostly get one plodding song after the next. This can be useful in terms of finding hidden gems, but makes for monotonous, even frustrating listening over a span of a few hours.\n\n\n\n\nI do know that Pandora makes much of its “Thumbs Up/Thumbs Down” feature, which allows the user to indicate her or his preference for a given song. Pandora’s algorithms will adjust the playlist’s direction (ever so slightly) upon a “Thumbs Down” for a song you don’t care for. In a 2006 interview with the _New York Times_, Westergren describes this feature as [a concession to human subjectivity](http://www.nytimes.com/2006/09/03/arts/music/03leed.html?_r=1&pagewanted=all) (within an otherwise “objective” platform), and I agree. The “Thumbs Up/Thumbs Down” feature requires active listening and participation on the user’s part—generally a good thing, I’ll admit. But what if I want to just sit back with a cold beverage and let the music play? The Genome’s platform, as it currently works, seems unable to deliver the ebbs and flows in tempo and musical texture which I enjoy in a good mixtape or college radio show.\n\n\n\n\nThese kind of practical gaps in Pandora’s service point me toward a larger theoretical problem worth discussing.  In its insistence upon musical sound as _the_ key ingredient for making song recommendations, the Music Genome Project willingly suspends belief in some basic social facts about the way music works. Music is undeniably social, cultural, and political. It’s the soundtrack to our lives as we dance, eat dinner, exercise, commute to work, fall in love, and so on. Music blasts out of loudspeakers at political rallies. We argue with friends over drinks about the relative merit of this or that musical group. And music is always part of a commercial marketplace, even in this age of file-sharing. Given all this, I find Westergren’s claim that a band’s marketplace popularity is [“completely irrelevant to what we do”](http://twit.tv/itn6) a wee bit disingenuous, or at the very least requiring a willing suspension of disbelief regarding music’s social and marketplace role.\n\n\n\n\nThe Music Genome Project’s near-exclusive focus on sound itself, coupled with its organicist rhetoric regarding “musical DNA”, seems to suggest the company believes it can map out music in its totality—that it can “crack the code” of music, so to speak. As an aspiring musicologist, this reminds me a bit of another massive scholarly endeavor which worked toward a similar goal of cataloging music: Alan Lomax’s Cantometrics project. Developed by Lomax in the late 1950s and into the ‘60s, Cantometrics was a project wherein Lomax and several co-researchers analyzed the performance styles of (mostly traditional “folk”) songs from hundreds of different cultures around the world, tagging them with a variety of traits. These performance traits, such as vocal timbre, were organized into a computerized system wherein elements of the different musics could be compared. Lomax made the bold claim that one could draw conclusions about the social structure of a given society based on some of these performance traits (societies noted for a certain style of singing were sexually repressive, for instance). Of course, this claim was quite controversial, and has been challenged by other scholars since as overly reductive and essentialist. Fortunately, the Music Genome Project doesn’t attempt to make the connection between music and social structures the way Cantometrics did; indeed, as I said, the Genome Project’s rhetoric seems to _deny_ aspects of the social world, if anything.\n\n\n\n\nHowever, in the desire to systematically categorize and compare different aspects of music, one could say that the Genome Project and Cantometrics spring from a shared wellspring of human curiosity. One issue with this categorizing mission, though, is the problem of sample size. Lomax’s research was criticized for not casting a broad enough net in collecting these comparable performance traits. One could ask similar questions about the Music Genome Project’s scope. As I mentioned, the company’s website points out that its database currently features over 500,000 songs, and counting. This seems like a lot, but how useful is that number, when one considers the thousands and thousands of songs which are released commercially every year? And how can one ensure that multiple varieties, styles, and (yes, even) genres of music are adequately represented within those 500,000 songs?\n\n\n\n\nAdditionally, Pandora shares potentially problematic assumptions with Cantometrics regarding humans’ ability to fully categorize and catalog the world, to reduce music to its essence. This descends from the Enlightenment idea that our natural world is fully knowable through empirical and objective observation. That bedrock assumption has been the basis for the natural sciences, and one can see its influence on a project like Alan Lomax’s. The problem is: while an empirical observation approach might work well for classifying different varieties of tree frogs, when one wades into the murky waters of human behavior, it’s a lot more difficult to claim objectivity. Indeed, for myself and for a growing number of musicologists and humanities scholars more broadly, it is basically impossible to claim objectivity in one’s understanding of the world.\n\n\n\n\nThis is not to say that Pandora explicitly makes a claim of total objectivity, on their website or elsewhere. But as with Cantometrics, the fact that the company breaks songs down into discrete components and then makes comparisons and connections based on those components suggests that they believe music is knowable in some objective way. Pandora hasn’t made public a list of its over 400 “musical attributes”, but [shares a handful of them](http://blog.pandora.com/faq/#92) on their website’s Frequently Asked Questions page. Some of the attributes they share make a lot of sense, and could even be called “objective”: major or minor key tonality, for instance. But consider an attribute like “headnodic beats”: in its FAQ entry, Pandora’s analysts admit they created the term themselves (it describes hip-hop beats which are strong, but not forceful enough to dance to). Given that probably almost no one outside of the Pandora offices uses this term, it can’t reasonably be called objective. This is not to say that an identification of some subjectivity within Pandora’s research model makes the whole enterprise come crashing down. Rather, I just wish to point out that while company prides itself on the cold objectivity of a computer algorithm choosing your music for you, human beings with subjective viewpoints created the components which power that algorithm.\n\n\n\n\nRelated to this, both Pandora and Cantometrics raise questions regarding musical gatekeepers, tastemakers, and their authority. Indeed, as ethnomusicologist Steven Feld has mused in response to Lomax’s work, “What are the sources of authority, wisdom, and legitimacy about sounds and music? Who can know about sound? Is musical knowledge public, private, ritual, esoteric?”.2 Many researchers of pop music, pop culture, and genre agree that this issue of _who_ is doing the classifying, categorizing, and ranking is a really important question. For instance, snobby clerks at your local independent record store may decide that Gillian Welch’s music belongs in the “folk” rather than “rock” section of the store. But where does the authority behind their judgment come from?   Their judgment is informed by their life experiences and backgrounds as (mostly) well-educated middle-class white males.\n\n\n\n\nThese gatekeepers’ judgments are also informed by a deep knowledge of various musical genres: the ability to distinguish glam from punk from grunge, and so on. Since Pandora’s fifty music analysts perform a similar function, I find this aspect of their job paradoxical: though the company seems to pride itself on getting beyond musical genre, these analysts must be extremely well-versed in genre in order to do their jobs well. In a recent video post on Pandora’s blog, Westergren states that the purpose of the Music Genome Project is ultimately to connect musicians with audiences, in ways the traditional music business can’t.3 This is notably egalitarian rhetoric; it works off the assumption that consumers and musicians are empowered enough to seek each other out, and that they don’t need tastemakers dictating what music they should like.\n\n\n\n\nAs I noted above, however, the computerized system that listeners use to connect with musicians is designed and maintained by a group of (relatively) elite tastemakers. And in Westergren’s public statements about these analysts’ qualifications, I read a certain degree of anxiety over what kind of authority is vested in that role of analyst. In his 2006 interview with Leo Laporte and Amber MacArthur, Westergren pointed out that while all their analysts are regularly-gigging musicians, in order to carry out the depth of analysis required for the Music Genome Project, one really “need[s] an academic background”. Thus, in addition to being a working musician, an analyst employed by Pandora also needs at least a four-year undergraduate degree in music theory.\n\n\n\n\nOn a practical level, this makes a lot of sense to me. If you’re going to employ folks to analyze songs for you, wouldn’t you want them to have an understanding of musical principles on several different levels? On the other hand, on a theoretical level, Pandora’s insistence on both “street” and “book” smarts from its analysts demonstrates an unresolved subliminal conflict over whether \"brains and corporate no-how\" or \"gut, ‘Id’ feelings\" are what shape the music we listen to. Thus, in this way, Pandora and the Music Genome Project struggle with these issues of taste and knowledge hierarchies just like other public pop prognosticators, even as their seemingly objective research platform denies this social fact.\n\n\n\n\nThis may read as though I am beating up on Pandora, but I hope the position I’m staking out is subtler than that. Rather, I have simply been attempting to point out some slight contradictions of logic within the Music Genome Project’s overall research platform. On a practical, user’s level, I enjoy the site. And to be fair to Pandora’s employees, on a certain level they seem to recognize the issues I am brining up here. For instance, in a recent post on Pandora’s official blog by one of its music analysts, Michael Zapruder likens evaluating songs to judging a baby beauty contest, and then points out,\n\n\n\n\n\"_The idea that all music is equal and deserves equal rights is somehow fundamentally a democratic idea; as is the corresponding idea that the public, and not some small cadre of experts, is the best judge of musical quality. But the fact that some music not only attracts more listeners, but also seems to mean more to more people over a longer period of time, indicates that there is actually something fundamentally unequal about music as well_.\"4\n\n\n\n\nIn other words, perhaps this issue of taste isn’t an “either/or” problem, but rather a “both/and” one. And by its nature, it’s most likely a problem with no definitive answer.\n\n\n\n\nIt seems that in his blog entry, Pandora employees like Zapruder are trying to find a _practical_, everyday way of working around and through this problem—and I can’t fault them for that. Certainly, the academic in me bristles when I see Pandora present something like “headnodic beats” as some kind of objective criteria for judging music. But on a practical level, it seems that these classifications, even if they’re vague (such as “vinyl ambience,” or what have you) are perhaps vague at least partly in the service of the _listener’s_ experience—of trying to match users to interesting new music. It doesn’t seem that the point of the Genome is to categorize musical attributes simply for the sake of categorization. Rather, the point seems to be to put that information to use, making musical connections for the listener. So perhaps it’s a utilitarian reason why the Music Genome cuts certain logical corners on the “objective vs. subjective” question.\n\n\n\n\nUltimately, Pandora’s service rests upon the assumption that sound itself is the only aspect which really matters when analyzing different forms of music. It assumes that sound automatically trumps the sociocultural boundaries of genre, taste, and marketplace. This isn't true, of course: in the real world we live in, rhetoric surrounding genre and taste guide the musical choices we all make, from Walmart AC/DC lovers to bebop nerds. But the Music Genome Project’s fiction regarding the supremacy of sound is an important, if very one-sided, position to have out there in the world. In fact, it’s almost counter-cultural in a way, because journalists and advertisers often focus so much on _image_ when considering contemporary pop music. Pandora’s vision is a kind of imagined musical utopia, making a particularly 21st-century-specific stand for the importance of musical sound—a stand made possible by the shared _cultural_ resource of the Internet.\n\n\n\n\nFinally, closing with an idea my professor Fred Maus pointed out to me, when you're confronted with enjoying a song you didn't think you would like on Pandora (you enter in a Turbonegro song as your “seed” and are rewarded with a Poison song, for example), that tells us something important about genre boundaries. Your bemusement proves that musical genres exist. They're cultural; they don't hold up to objective scrutiny. And they're based on something more than just musical sound; they're built around assumptions that have to do with hierarchies of taste and class. Thus, paradoxically, we can learn quite a bit about the rules of genre from a website devoted to transcending those rules.\n\n\n\n\n\n* * *\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1  Holt, Fabian. _Genre in Popular Music_. Chicago: University of Chicago Press, 2007. Pg. 14.\n\n\n\n\n2  Feld, Steven. “Sound Structure as Social Structure.” _Ethnomusicology_, Vol. 28. No. 3 (Sept. 1984), pp. 383-409.\n\n\n\n\n3  http://blog.pandora.com/pandora/archives/2009/03/index.html March 15, 2009 entry.\n\n\n\n\n4  http://blog.pandora.com/pandora/archives/2009/02/index.html      February 25, 2009 entry.\n"},{"id":"2010-02-18-digital-therapy-cesaire-and-hawthorne","title":"Digital Therapy: Cesaire and Hawthorne","author":"ronda-grizzle","date":"2010-02-18 07:17:22 -0500","categories":["Podcasts"],"url":"digital-therapy-cesaire-and-hawthorne","content":"Graduate students Alex Gil and Ryan Cordell present their recent work on  digital editions of works by Nathaniel Hawthorne and Aimé Césaire.\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.3390392681/enclosure.mp3\"]\n"},{"id":"2010-03-25-mr-voronoi-meet-the-us-state-boundaries","title":"Mr. Voronoi, meet the US state boundaries","author":"kelly-johnston","date":"2010-03-25 10:57:34 -0400","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"mr-voronoi-meet-the-us-state-boundaries","content":"In the Scholars' Lab we are working with remarkably detailed datasets showing changes to US political boundaries over time.  We've all been fascinated with visualizations where the familiar outlines of the US states emerge from thousands of boundary changes to their underlying counties over the last few hundred years.  Did you know Virginia once spanned from the Atlantic Ocean to the Mississippi River?\n\n[![Virginia](http://www.scholarslab.org/wp-content/uploads/2010/03/VirginiaToMiss.png)](http://www.scholarslab.org/geospatial-and-temporal/mr-voronoi-meet-the-us-state-boundaries/attachment/virginiatomiss/)\n\nWe're developing a new web-based tool for visualizing these historic boundary changes and it's nearly ready for prime time.  We'll  announce the beta release here soon.\n\nSo with the knowledge that US state boundaries have already been subject to drastic change over time, let's have some fun with geographic information systems to visualize drastic mathematically-induced changes to those familiar US state boundaries.\n\nFor our experiment, let's keep all our current state capital cities right where they are since they are laden with the necessary infrastructure of government.  But we'll move the state boundary lines [Voronoi-style](http://mathworld.wolfram.com/VoronoiDiagram.html) so anywhere you travel in each of our new states you'll be closer to the state capital than any other state capital.  In other words, when you're standing anywhere inside our newly outlined Virginia, you will always be closer to the Virginia state capital, Richmond, than any other state capital.  That seems very efficient.  Let's have a look.\n\n[![US States with capitals](http://www.scholarslab.org/wp-content/uploads/2010/03/USAnow2-1024x651.png)](http://www.scholarslab.org/geospatial-and-temporal/mr-voronoi-meet-the-us-state-boundaries/attachment/usanow2/)\n\nHere's that familiar grade-school wall map of the lower 48 US states and their capital cities.   Now let's tweak the map with GIS software to reconfigure the states, Voronoi-style.\n\n[![US Voronoi states with capitals](http://www.scholarslab.org/wp-content/uploads/2010/03/USAthen2-1024x655.png)](http://www.scholarslab.org/geospatial-and-temporal/mr-voronoi-meet-the-us-state-boundaries/attachment/usathen2/)\n\nWow, what a difference Voronoi makes.\n\nLet's measure just how much the states have changed in our new layout.   In absolute terms, Utah and New Mexico make the biggest land grabs while Texas and California lose the most real estate.  But as a percentage of their current area, Rhode Island is the big winner ballooning in size by over 240% while Massachusetts shrinks 60%.\n\nTo visualize the state-by-state changes, Todd Burks from neighboring Clemons Library overlayed the two maps.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2010/03/ToddMashup-1024x655.jpg)](http://www.scholarslab.org/geospatial-and-temporal/mr-voronoi-meet-the-us-state-boundaries/attachment/toddmashup/)\n\nIntrigued?  [Read more](http://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?id=1349&pid=1347&topicname=Create_Thiessen_Polygons_%28Analysis%29) about Voronoi and Thiessen polygon GIS techniques.\n"},{"id":"2010-04-06-scholars-lab-newsletter","title":"Scholars' Lab Newsletter","author":"ronda-grizzle","date":"2010-04-06 11:28:19 -0400","categories":["Announcements"],"url":"scholars-lab-newsletter","content":"We're pleased to announce that the inaugural issue of our monthly newsletter is now available for download. The newsletter will highlight projects currently in progress in the Scholars' Lab, report on professional activities of the Scholars' Lab staff, and list monthly events. If you have suggestions for projects that you'd like to see highlighted in future newsletters, please email me at [rag9b@virginia.edu](mailto:rag9b@virginia.edu).\n\n\n[Scholars' Lab News for April 2010 (PDF version)](http://www.scholarslab.org/announcements/scholars-lab-newsletter/attachment/2010april/)\n\n\n\n\n[Scholars' Lab News April 2010 (text-only version)](http://www.scholarslab.org/announcements/scholars-lab-newsletter/attachment/2010april-2/)\n"},{"id":"2010-04-14-omeka-timeline-plugin","title":"Omeka Timeline Plugin","author":"wayne-graham","date":"2010-04-14 11:19:44 -0400","categories":["Research and Development"],"url":"omeka-timeline-plugin","content":"As part of our ongoing efforts on our [Neatline](http://www.neatline.org) grant, we needed to include a way of displaying temporal information and interacting with other data stored in Omeka. Just about the time we were starting to write this code, CHNM announced their [Plugin Rush](http://omeka.org/blog/2010/02/18/plugin-rush-2010/) which pays an honorarium to give folks some incentive to pitch in and develop a plugin or two. Since we were going to develop the plugin anyway, we're donating this back to the Omeka project, but we thought this might be a good opportunity to talk a little more about the development cycle for Omeka plugins, and hopefully inspire others to get involved.  <!-- more -->\n\nThe specifications for the [Timeline plugin](http://omeka.org/c/index.php/Plugin_Rush_2010#Timeline_.281.1-1.0.29) are wonderfully documented and explained on the Omeka wiki. This actually illustrates a great practice that is all too often ignored...explicitly stating what some software should do. Taking the time to think through the \"what\" a piece of software should do will save  you time in the long run as it forces you to think about how everything fits together, alleviating ambiguity and allowing you to focus on the task at hand.\n\nFor this specification, there were two requirements:\n\n\n\n\t\n  * The plugin should create a helper function for creating a [SIMILE  Timeline](http://www.simile-widgets.org/timeline/) widget from an array of items.  The helper function should allow you to  specify which metadata elements the time data should come from, as well  as the element that specifies the caption (by default, the Dublin Core  Title element).\n\n\t\n  * The timeline should allow for time intervals (start and end dates) and points in  time (singe date).\n\n\nWhile there are two requirements, it's a good idea to break this us a little more into individual (atomic) tasks. First, we need to take a look at the [SIMILE Timeline Widget documentation](http://code.google.com/p/simile-widgets/wiki/Timeline). Taking a quick look at their [Getting Started](http://code.google.com/p/simile-widgets/wiki/Timeline_GettingStarted) guide, this seems pretty straight forward\n\n\n\n\t\n  1. [Include the Timeline javascript](http://code.google.com/p/simile-widgets/wiki/Timeline_GettingStarted#Step_1._Link_to_the_API)\n\n\t\n  2. Create a div container in the HTML view (e.g. <div id=\"timeline\"></div>\n\n\t\n  3. Format the items from Omeka as [Timeline Events](http://code.google.com/p/simile-widgets/wiki/Timeline_GettingStarted#Step_5._Add_Events)\n\n\t\n  4. Add the Timeline.create() call to the Omeka HTML view\n\n\nThe specification states that this needs to be a helper function. If you're not familiar with this term, a helper function is a block of code that does some of the computation for another piece of code. In many frameworks, helper functions aren't actual objects, but pieces of procedural code that can be accessed from across the application that help add functionality that isn't exactly proper to place in a model or controller; essentially these are statically accessible functions (in coding terms) that can be called from properly instantiated objects (in this case a view).\n\nNow that we have good idea of what the code should do, let's turn to some actual code. Omeka uses the [Zend Framework](http://framework.zend.com/) to keep from doing a lot of repetitive programming, so most of the syntax of what we need to do is driven by how Zend handles PHP. On top of the Zend Framework, Omeka implements its own plugin infrastructure, so there are a few things we need to take in to account in our design.\n\nThe Zend Framework is a [model-view-controller (MVC) framework](http://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller) designed to organize code for maintainability and DRY-ness ([Don't Repeat Yourself](http://en.wikipedia.org/wiki/Don%27t_repeat_yourself)). One the the hallmarks of most MVC applications is its physical separation of files and functionalities. In the case of Omeka plugins, the hierarchy is generally split into model, view, controller, and tests directory. For the Timeline plugin, since we are developing a helper function, we use a slightly modified directory structure:\n\n[code lang=\"bash\"]\nTimeline\n|__helpers\n|__tests\n|__views\n[/code]\n\nWe also need some mechanism to tell Omeka about a plugin. This metadata is currently provided in a file called plugin.ini. This file is pretty straight forward, but  let's go over it briefly:\n\n[code lang=\"php\"]\n\n[info]\nname=\"Timeline\"\nauthor=\"Scholars' Lab\"\ndescription=\"SIMILE Timeline for Omeka\"\nlink=\"http://omeka.org/codex/Plugins/Timeline\"\nomeka_minimum_version=\"1.0\"\nomeka_tested_up_to=\"1.2\"\nversion=\"1.1\"\ntags=\"Timeline, simile, chronology, time, temporal\"\n\n[/code]\n\nThis file is what the Omeka admin interface uses to display information about your plugin. Two things that may not be initially obvious are the** omeka_minimimum_version** and **omeka_tested_up_to** lines. One fact you'll learn about software development, especially with API development, is as projects mature, the needs of the API grow along with them. You want to be able to mitigate potential issues should the plugin API change by explicitly setting the minimum revision number that your plugin is tested against (you can get older revisions from the SVN tags repo at [https://omeka.org/svn/tags/](https://omeka.org/svn/tags/)).\n\n\n<blockquote>**Note:** you can run multiple versions of Omeka on your machine for testing by checking out separate versions of the software in you web tree. For instance, you can have **localhost/omeka1.0**, **localhost/omeka1.1**, **localhost/omeka_trunk**. Setting this up is beyond the scope of this post (be sure to set up separate databases), but if you have questions, leave a comment.</blockquote>\n\n\nThe next thing we need to do is tell Omeka what to do with our plugin. The top-level plugin.php file contains instructions ([hooks](http://omeka.org/codex/Plugin_API/Hooks)) to tell the Admin interface what to do when a user installs or uninstalls the plugin. This is where we let Omeka know that items tagged with \"Timeline\" should use the Timeline Plugin, to set up some logging to help us debug when something goes wrong, and some default [routes](http://omeka.org/codex/Plugin_API/Hook/define_routes) to help make \"pretty\" URLs.\n\nNow, with all the preliminary setup taken care of, now we can start developing the helper function. First, let's examine the Controller which tells Omeka what to do when an action is requested by the framework. This is actually a fairly straight forward:\n\n[code lang=\"php\"]\n\n<?php\nclass Timeline_TimelinesController extends Omeka_Controller_Action\n\n{\n\tprivate $logger;\n\n\tpublic function init()\n\t{\n\t\t$this->_modelClass = 'Item';\n\t\t$writer = new Zend_Log_Writer_Stream(LOGS_DIR . DIRECTORY_SEPARATOR . \"timeline.log\");\n\t\t$this->logger = new Zend_Log($writer);\n\t}\n\n\tpublic function showAction()\n\t{\n\t\t$this->view->item = $this->findById();\n\t}\n\n}\n[/code]\n\nLet's go over briefly what's going on here. There are some semantics in the way in which these Controller objects are named which are inherited from the way in which Zend handles Controllers. The **Timeline_TimelinesController** follows the convention of the \"package\" (the plugin) name, underscore, plural controller name (to handle multiple controllers), and finally \"Controller\" (which explicitly tells a programmer what function the Object performs). Because this is a Framework, we also want to be able to inherit a lot of behaviors without needing to code them ourselves, which is handled by the \"extends Omeka_Controller_Action\" (this is the base [CRUD](http://en.wikipedia.org/wiki/Create,_read,_update_and_delete) class for Omeka which overrides and extends the [Zend_Controller_Action](http://framework.zend.com/manual/en/zend.controller.action.html) object). The \"important\" part of the Controller code is really the showAction function which sets a variable named \"item\" in the view which contains a reference to the ID of an Omeka object. The rest just sets up a logger to keep track of what's going on.\n\n\n<blockquote>**Note**: If you run in to problems with this plugin, it is most likely related to logging. For more on this, see the documentation on [Retrieving Error Messages](http://omeka.org/codex/Retrieving_error_messages).</blockquote>\n\n\nNow we can get into the guts of the actual helper function. When you boil down the code, most of this is JavaScript with some strategically placed PHP. What we did is create a helper function named \"createTimeline\" which actually does the work for us. This takes two required items, a div reference to associate the Timeline on your page, and an array of Omeka Items with which to populate the Timeline.\n\n[code lang=\"php\"]\nfunction createTimeline($div, $items = array(), $captionElementSet = \"Dublin Core\", $captionElement =  \"Title\", $dateElementSet = \"Dublin Core\", $dateElement =  \"Date\" ) {\n\t\techo js(\"prototype\");\n\t\tglobal $mets;\n\t\t$mets = array($captionElementSet, $captionElement, $dateElementSet, $dateElement);\n\t\t?>\n\t\t<!--  we have to load the script in this funny way because we need to get the tag into the head of the doc\n\t\t\tbecause of the the funky way Simile Timeline loads its sub-scripts  -->\n\t\t<script type=\"text/javascript\">\n\t\t\tscripttag = document.createElement(\"script\");\n\t\t\tscripttag.src = \"http://static.simile.mit.edu/timeline/api-2.3.0/timeline-api.js?bundle=false\";\n\t\t\tscripttag.type = \"text/javascript\";\n\t\t\t$$(\"head\")[0].insert(scripttag);\n\n\t\t\tif (typeof(Omeka) == \"undefined\") {\n\t\t\t\tOmeka = new Object();\n\t\t\t}\n\n\t\t\tif (!Omeka.Timeline) {\n\t\t\t\tOmeka.Timeline = new Object();\n\t\t\t}\n\n\t\t</script>\n\n\t\t<script type=\"text/javascript\" defer=\"defer\">\n\t\t\tOmeka.Timeline.timelinediv = $(\"<?php echo $div;?>\");\n\n\t\t\tOmeka.Timeline.events = [\n\t\t\t<?php\n\t\t\t\tfunction event_to_json($item) {\n\t\t\t\t\tglobal $mets;\n\t\t\t\t\treturn \"{ 'title' : '\" . getMet($item, $mets[0], $mets[1]) . \"',\n\t\t\t\t\t'start' : '\" . getMet($item, $mets[2], $mets[3]) . \"',\n\t\t\t\t\t'description' : '\" . getMet($item, \"Dublin Core\", \"Description\") . \"',\n\t\t\t\t\t'durationEvent':false }\";\n\t\t\t\t}\n\t\t\t\techo implode(',',array_map('event_to_json', $items));\n\t\t\t\t?>\n\t\t\t\t];\n\n\t\t</script>\n\t\t<?php\n\t     echo js(\"createTimeline\");\n\t\t?>\n\t\t<script type=\"text/javascript\">\n\t\t\tEvent.observe(window, 'load', onLoad);\n\t\t\tEvent.observe(document.body, 'resize', onResize);\n\t\t</script>\n\n\t\t<?php\n}\n\n[/code]\n\nThere's a lot going on here, and there is a mix of PHP in the JavaScript. The first thing is making sure the prototype.js library is included, then declaring a variable named \"mets\" in the [global scope](http://php.net/manual/en/language.variables.scope.php) (to make it available to other variable scopes). After we've declared $mets, get in to the JavaScript to include on the page and introducing a new JavaScript [Namespace](http://en.wikipedia.org/wiki/Namespace_%28computer_science%29) (Omeka.Timeline) which allows you to extend this code in other views.\n\nThe second script block actually formats Omeka items that you've called as Timeline Events in the [JSON](http://www.json.org/) format calling a helper method we also include in the code:\n\n[code lang=\"php\"]\nfunction getMet($item, $elementSet, $element) {\n\t $tmp = $item->getElementTextsByElementNameAndSetName($element, $elementSet);\n\t return addslashes( $tmp[0]->text ) ;\n}\n\n[/code]\n\nThis function returns the metadata for an Omeka item, which is then used the createTimeline's sub-method of event_to_json to properly construct an event for Timeline. After all the JSON strings are created, we \"glue\" all the array elements with a comma with the [implode](http://php.net/manual/en/function.implode.php) function.\n\nAs you can see, not a lot of code actually needs to be written to add functionality to Omeka. With a little research, and some pointers on syntax, extending Omeka can be done quite quickly and doesn't require a degree in computer science. If you're interested in getting started on a plugin, I highly recommend the [Omeka dev list](http://groups.google.com/group/omeka-dev); the community is growing and questions are answered quickly (usually by folks on the Omeka development team) and is a great way to learn about the technical issues surrounding developing software using the Omeka platform.\n\n\n## Resources\n\n\n\n\n\n\t\n  * [Omeka Plugin API](http://omeka.org/codex/Plugin_API)\n\n\t\n  * [Omeka Developer List](http://groups.google.com/group/omeka-dev)\n\n\t\n  * [Timeline Source Code](https://addons.omeka.org/svn/plugins/Timeline/trunk/)\n\n\t\n  * [Timeline Documentation](http://omeka.org/codex/Plugins/Timeline)\n\n\t\n  * [Zend Framework](http://framework.zend.com/)\n\n\t\n  * [Zend Framework Documentation](http://framework.zend.com/manual/en/)\n\n\n"},{"id":"2010-04-20-automating-omeka-deployment-with-capistrano","title":"Automating Omeka Deployment with Capistrano","author":"wayne-graham","date":"2010-04-20 15:53:25 -0400","categories":["Research and Development"],"url":"automating-omeka-deployment-with-capistrano","content":"If you've done much web development, you'll know that deploying applications can be a real pain. Typically you get some code (like Omeka), FTP it to your server, run the install, then go grab some plugins and themes and FTP them to your server. If you're a bit more sophisticated, you may have put this in to an source code management (SCM) system like [git](http://git-scm.com/), [mercurial](http://mercurial.selenic.com/), or [subversion](http://subversion.apache.org/), which then changes your workflow to editing on your local machine, committing the changes to your SCM, logging on to the command line interface for your server, running an update on the code, praying nothing breaks; if it does, you then try to roll back to a working version (you remembered to run svn info on the code before updating so you know what number to go back to). Even if everything goes swimmingly, that's a lot of steps and way more applications than I like to fool with, and since it's essentially doing the same thing over and over again, wouldn't it be nice to automate this process?   <!-- more -->\n\nEnter [Capistrano](http://www.capify.org/index.php/Capistrano)...If you've not used this before, essentially this automates the deployment of web applications to your server environment.  It's written in Ruby, but allows you to deploy ANY type of web application (we use it for Cocoon, Rails, Java, and PHP applications in the Scholars' Lab). If you've got a larger shop, you may also take a look at a web interface called [Webistrano](http://labs.peritor.com/webistrano) which allows non-programmer types to deploy software through a web interface.\n\nTo show off the power of this software, I thought I'd write up how we use capistrano to deploy Omeka in various environments. The setup can be a little complex, but there are some good tutorials for getting started (see [Setting up a Rails Server and Deploying with Capistrano on Fedora from Scratch](http://net.tutsplus.com/tutorials/ruby/setting-up-a-rails-server-and-deploying-with-capistrano-on-fedora-from-scratch/) and the [Capistrano Getting Started](http://www.capify.org/index.php/Getting_Started)). The following code snips assume you have successfully installed capistrano and use Subversion as your SCM (if you need SVN hosting, you can start a new project on [Google Code](http://code.google.com/hosting/createProject); you can also use [Github](https://github.com/) if you declare the git scm in the code).\n\nThe first step in getting your Omeka project automated for capistrano is ensuring both the capistrano and railsless-deploy gems are installed (if you're not a ruby-ist, [gem](http://docs.rubygems.org/read/book/1) is a package manager for Ruby applications and libraries):\n\n[code lang=\"bash\"]\nsudo gem install capistrano railsless-deploy\n[/code]\n\nCapistrano installs a new command on your system called \"capify\" which sets up the boilerplate for capistrano. Just execute the capify script in your project directory:\n\n[code lang=\"bash\"]\ncd /path/to/project/trunk\ncapify .\n[/code]\n\nThis will create two files, Capfile in your root directory and a config/deploy.rb. You'll need to edit the Capfile ever so slightly to add the requirement for the railsless-deploy gem. It should read as follows:\n\n[code lang=\"ruby\"]\nload 'deploy' if respond_to?(:namespace) # cap2 differentiator\n# Dir['vendor/plugins/*/recipes/*.rb'].each { |plugin| load(plugin) }\n\nrequire 'rubygems'\nrequire 'railsless-deploy'\nload    'config/deploy'\n[/code]\n\nNow we just need to do some setup in the config/deploy.rb file to tell capistrano about Omeka. This is where you need to know a little about how your server is set up and you may need to slightly change your server set up in order to use capistrano. The way capistrano works is that it creates a releases directory on your path that holds \"deployments\" of you project. The latest version of the project is then symlinked the project directory into the releases. This allows you to very quickly undo a deployment that goes awry.\n\nAs an example, we deploy projects to /usr/local/projects, so our omeka project would get deployed to /usr/local/projects/omeka. Capistano will create a few directories in /usr/local/projects/omeka:\n\n\n\n\t\n  * **releases** (timestamped directories of your application)\n\n\t\n  * **shared** (for log files, SCM cache, files you don't want to be overwritten)\n\n\t\n  * **current** (symlink to latest directory in the releases directory)\n\n\nIf you're setting up Omeka, you will need to redirect the base Directory to the \"current\" symlink. Here's the vhost entry we use for Omeka as an example (this is an Ubuntu server; you may need to change the log file path if you are deploying to another operating system).\n\n[code lang=\"bash\"]\n<VirtualHost *:80>\nServerName your.server.org\nDocumentRoot /usr/local/projects/omeka/current/\n<Directory \"/usr/local/projects/omeka/current\">\nOptions FollowSymLinks\nAllowOverride All\nOrder allow,deny\nAllow from all\n</Directory>\n\nErrorLog /var/log/apache2/omeka_error.log\nTransferLog /var/log/apache2/omeka_access.log\n</VirtualHost>\n[/code]\n\nNow, to edit the config/deploy.rb file to set things up for automated deployments.\n\n[code lang=\"ruby\"]\n# You must always specify the application and repository for every recipe. The\n# repository must be the URL of the repository you want this recipe to\n# correspond to. The deploy_to path must be the path on each machine that will\n# form the root of the application path.\n\nset :application, 'omeka'\nset :repository, 'http://your.svn.path/repo/trunk'\n\nset :deploy_to, \"/usr/local/projects/#{application}\"\nset :deploy_via, :remote_cache\nset :user, 'deployer'\nset :runner, user\nset :run_method, :run\n\ndefault_run_options[:pty] = true\n\nssh_options[:username] = user\nssh_options[:host_key] = 'ssh-dss'\nssh_options[:paranoid] = false\n\nrole :app, 'www.coolomekaapp.org'\nrole :web, 'www.coolomekaapp.org'\nrole :db, 'www.coolomekaapp.org'\n\n# ===============\n# Custom Tasks\n# ===============\nnamespace :deploy do\ndesc 'Make sure the archives directory has the proper permissions'\ntask :chmod_archive_dir, :except=>{:no_release => true} do\n\nrun \"chmod g+rw #{current_path}/archives\"\nend\ndesc 'Sets up the intitial db.ini config'\ntask :upload_database_config, :except=>{:no_release => true} do\ndb_config = <<-INI\n[database]\nhost     = \"your.db.host\"\nusername = \"db_user\"\npassword = \"db_password\"\nname     = \"db_name\"\nprefix   = \"omeka_\"\n;port     = \"\"\nINI\n\nput db_config, \"#{current_path}/db.ini\"\n\nend\n\ndesc 'Move archives directory so it doesn't get overwritten during deployments'\n task :move_archive_dir, :except=>{:no_release => true} do\n run \"mv #{current_path}/archives #{shared_path}\"\n end\n\n desc 'Just svn up the directory'\n task :svn_up, :except => {:no_release => true} do\n run \"svn up #{current_path}\"\n end\n\n desc 'Link archives folder to shared directory'\n task :link_archives_dir, :except=>{:no_release => true} do\n run \"cd #{current_path} && ln -snf #{shared_path}/archives archives\"\n end\n\nend\n\n# ======================\n# Task Event Hooks\n# ======================\n\nafter 'deploy:symlink', 'deploy:upload_database_config', 'deploy:link_archives_dir'\nafter 'deploy:cold', 'deploy:chmod_archives_dir', 'deploy:move_archives_dir'\nafter 'deploy', 'deploy:cleanup'\n[/code]\n\nOk, there's a lot going on here. I'll briefly explain what's going on, but there should be enough here for you to start hacking. But let's see what tasks capistrano know about and you can call. If you are still in your project directory, just type cap -T to list all the capistrano tasks. Your output should look similar to this:\n\n[code lang=\"bash\"]\ncap deploy                        # Deploys your project.\ncap deploy:check                  # Test deployment dependencies.\ncap deploy:chmod_archive_dir      # Make sure the archives directory has the ...\ncap deploy:cleanup                # Clean up old releases.\ncap deploy:cold                   # Deploys and starts a `cold' application.\ncap deploy:migrate                # Run the migrate rake task.\ncap deploy:migrations             # Deploy and run pending migrations.\ncap deploy:pending                # Displays the commits since your last deploy.\ncap deploy:pending:diff           # Displays the `diff' since your last deploy.\ncap deploy:restart                # Restarts your application.\ncap deploy:rollback               # Rolls back to a previous version and rest...\ncap deploy:rollback:code          # Rolls back to the previously deployed ver...\ncap deploy:setup                  # Prepares one or more servers for deployment.\ncap deploy:start                  # Start the application servers.\ncap deploy:stop                   # Stop the application servers.\ncap deploy:symlink                # Updates the symlink to the most recently ...\ncap deploy:update                 # Copies your project and updates the symlink.\ncap deploy:update_code            # Copies your project to the remote servers.\ncap deploy:upload                 # Copy files to the currently deployed vers...\ncap deploy:upload_database_config # Sets up the intitial db.ini config\ncap deploy:web:disable            # Present a maintenance page to visitors.\ncap deploy:web:enable             # Makes the application web-accessible again.\ncap invoke                        # Invoke a single command on the remote ser...\ncap log:tail_log                  # Tail the rails log file\ncap shell                         # Begin an interactive Capistrano session.\n\nSome tasks were not listed, either because they have no description,\nor because they are only used internally by other tasks. To see all\ntasks, type `cap -vT'.\n\nExtended help may be available for these tasks.\nType `cap -e taskname' to view it.\n[/code]\n\nTo use a specific capistrano task, you just type the command listed. But let's get back to the actual script and go over that briefly. Part of the installation process of Omeka requires you to reset the permissions of the archives directory. This is handled by the chmod_archive_dir. However, because of the way that capistrano deploys applications, the archives folder would get overwritten in every deployment. To get around this, we move the archives directory to the shared folder, then create a symlink from the current directory to the shared/archives directory.\n\nThere's a task to upload_database_config that you can have in your cap script (we deploy out of private repos), but if you're deploying out of a public repo, you may want to just put the db.ini file on the server in the shared directory and symlink it into the current_path. Lastly, there are times where you just need to do an \"svn up\" (or git pull) to update something small and not need to do a full deployment. This is where the cap deploy:svn_up helps out....guess what it does :)\n\nCapistrano also provides task even hooks to execute specific tasks after specific events. In this script, when you do a cold deployment (when you are setting things up for the first time), the script will change the permissions on the archives directory, then move the archives directory to the shared directory. When you do a normal deploy (after the directory has gotten a proper symlink), the script will upload the database config, then symlink the archives directory and run a cleanup (keep the last 5 versions you deployed).\n\nWhile there's still a bit of up-front set up to do on your server, capistrano significantly speeds up your ability to to consistently deploy software, quickly roll-back if (that changes to \"when\", if you write code long enough) problems occur, and reinforces a development process that involves SCM!\n\n\n## Resources\n\n\n\n\n\n\t\n  * [Capistrano](http://www.capify.org/index.php/Capistrano)\n\n\t\n  * [Automated PHP Deployment with Capistrano](http://www.jonmaddox.com/2006/08/16/automated-php-deployment-with-capistrano/)\n\n\t\n  * [Railsless-deploy](http://github.com/leehambley/railsless-deploy)\n\n\t\n  * [Capistrano Tutorials](http://www.capify.org/index.php/Tutorials)\n\n\n"},{"id":"2010-05-07-gis-the-rare-tartan-plaid-point-dispersion-problem","title":"GIS: The (rare) Tartan-Plaid Point Dispersion Problem ","author":"dave-richardson","date":"2010-05-07 11:07:20 -0400","categories":["Geospatial and Temporal"],"url":"gis-the-rare-tartan-plaid-point-dispersion-problem","content":"Have you ever wondered what would happen to your map of points if while converting your coordinates from latitude/longitude in degrees, minutes, seconds (DMS) to decimal degrees (DD) you messed up the math?  Ever seen a weird tartan-like plaid pattern emerge on your map from points that were suppose to be uniformly spread out over the known extent?  Or wonder why coordinates are much more commonly stored as decimal degrees by computer GIS applications instead of the degrees-minutes-seconds most of us learn growing up?  If so, this blog entry from the Scholars’ Lab at the University  of Virginia Library is for you! <!-- more -->\n\nFirst a little digression to explain latitude and longitude, and why computer GISs generally prefer decimal degrees when expressing lat/lon as a coordinate pair.\n\nLatitude and Longitude is a spherical coordinate system for describing location upon a sphere (or upon an object that’s approximately a sphere, like the Earth).  Just as there are 360 degrees in a circle, there are 360 degrees of longitude (numbered 180 W (-180) to 0 to 180 E (+180) on either side of the Greenwich Prime Meridian) and 360 degrees of latitude (numbered 90 S (-90) to 0 to 90 N (+90) from the south Pole to the Equator (the Prime Parrallel, so to speak) to the North Pole... and back again on the other side of the globe, to complete the circle).  Each degree can be subdivided into 60 minutes, and each minute subdivided into 60 seconds.  Further subdivision of seconds is expressed as fractions or decimals.  Thus you could express the geographic location of the Scholars’ Lab at UVA as being at 38º 02’ 12.3540” N, 78 º 30’ 19.7928” W (or +38º 2’ 12.3540”, -78 º 30’ 19.7928”).\n\nComputer GIS programs all want the northing and easting coordinate pair saved as just two numbers (one number for latitude, one number for longitude) instead of three different fields to contain the degrees, minutes, and seconds for latitude and another three fields for longitude.  This makes it much easier for the computer to plot location.  Many GIS programs also prefer the coordinates to be ordered longitude, latitude since that mimics X, Y coordinates.  Since there are 60 minutes in a degree and 60 seconds in a minute (or 3600 seconds in a degree [60 x 60]), you could write the location of the Scholars’ Lab as -78 – (30/60) – (19.7928/3600), 38 + (02/60) + (12.3540/3600) which is -78.505498°, 38.036765° in decimal degrees.\n\nNow back to the emergent tartan-plaid problem.  What would happen if instead of converting to decimal degrees, you simple wrote out the degrees-minutes-seconds numbers in the format DD.MMSSssss ?  The Scholars Lab location would become -78.30197928, 38.02123540.  But the computer GIS would still interpret this as decimal degrees, and would compress all points falling within a 1x1 degree box into just the first 6/10ths x 6/10ths of the box, with a gap without points filling the rest of the box.  Spread over a large region, this would result in a tartan plaid-like pattern emerging.\n\nSo what is supposed to look like this:\n\n[![tartan example 2](http://www.scholarslab.org/wp-content/uploads/2010/01/tartan2-300x133.jpg)](http://www.scholarslab.org/geospatial-and-temporal/gis-the-rare-tartan-plaid-point-dispersion-problem/attachment/tartan2/)\n\nWould end up looking like this:\n\n[![tartan example 1](http://www.scholarslab.org/wp-content/uploads/2010/01/tartan1-300x133.jpg)](http://www.scholarslab.org/geospatial-and-temporal/gis-the-rare-tartan-plaid-point-dispersion-problem/attachment/tartan1/)\n\nAnd both the correct and incorrect versions together:\n\n[![tartan example 3](http://www.scholarslab.org/wp-content/uploads/2010/01/tartan3-300x133.jpg)](http://www.scholarslab.org/geospatial-and-temporal/gis-the-rare-tartan-plaid-point-dispersion-problem/attachment/tartan3/)\n\nNot that this is a common mistake people using GIS make by any stretch, but when someone has a question about why their points are all coming out misaligned with strange empty striping patterns, it can take a little while to deduce what’s going on if you’ve never seen the results of such a mistake before.\n"},{"id":"2010-05-07-introducing-davila","title":"Introducing DAVILA","author":"jean-bauer","date":"2010-05-07 11:10:46 -0400","categories":["Digital Humanities","Grad Student Research","Visualization and Data Mining"],"url":"introducing-davila","content":"Jean Bauer, former Scholars' Lab Graduate Fellow in Digital Humanities announces: \"I have just released my first open source project.  HUZZAH!\"\n\nDAVILA is a database schema visualization/annotation tool that  creates “humanist readable” technical diagrams.  It is written in [Processing](http://processing.org/) with the [toxiclibs](http://toxiclibs.org/) physics  library and released under GPLv3.  DAVILA takes in the database’s schema  and a pipe separated customization file and uses them to produce an  interactive, color-coded, annotated diagram similar in format to UML.   There are many applications that will create technical diagrams based on  database schema, but as a digital humanist I require more than they can  provide.  <!-- more -->\n\nTechnical diagrams are wonderfully compact ways of conveying  information about extremely complex systems.  But they only work for  people who have been trained to read them.  If you design a database for  a historian, and then hand him or her a basic E-R or UML diagram, you  will end up explaining the diagram’s nomenclature before you can talk  about the database (and oftentimes you run out of time before getting  back to the research question underlying the database).  This removes  the major advantage of technical diagrams and can also create an  unnecessary divide between the technical and non-technical members of a  digital humanities development team.\n\nI have become fascinated by how documenting a project (either in  development or after release) can build community.  I’m not just talking  about user generated documentation (ala wikis), but rather the feeling  created by a diagram or README file that really takes the time to  explain how the software works and why it works the way it does.  There  is a generosity and even warmth that comes from thoughtful, helpful  documentation, just as inadequate documentation can make someone feel  stupid, slighted, or unwanted as a user/developer.  I will be writing on  this topic more in the months to come (perhaps leading up to an  article).  In the meantime, check out DAVILA and let me know what you  think.\n\nProject homepage: [http://www.jeanbauer.com/davila.html](http://www.jeanbauer.com/davila.html)\n\n\n"},{"id":"2010-05-07-julie-meloni-n-dimensional-archives","title":"Julie Meloni: N-dimensional Archives","author":"ronda-grizzle","date":"2010-05-07 12:03:11 -0400","categories":["Podcasts"],"url":"julie-meloni-n-dimensional-archives","content":"Julie Meloni, Jerome McGann, and Bethany Nowviskie discuss ways of reconsidering the multivalent cultural record in a digital age\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public.2014484138.02014484145.3871948813/enclosure.mp3\"]\n"},{"id":"2010-05-11-why-ruby","title":"Why Ruby?","author":"wayne-graham","date":"2010-05-11 11:27:02 -0400","categories":["Research and Development"],"url":"why-ruby","content":"Stemming from [a Twitter conversation](http://twitter.com/dougreside/status/12881732120) last month, I thought it would be a good idea to describe -- in more than the 140 character bursts that Twitter allows -- why we at the [Scholars' Lab](http://lib.virginia.edu/scholarslab/) often promote Ruby, opposed to one of the other 4 or 5 languages we develop with. This isn't an attempt to declare one language \"the best,\" but is meant to lay out some of the fundamental reasons why we use Ruby in the context of our digital humanities work and why we think it's a nice language to suggest to folks just starting to program.<!-- more -->\n\n\n## Qualification\n\n\nPeople often think of the Scholars' Lab as a [Ruby](http://www.ruby-lang.org/en/) (and [Rails](http://rubyonrails.org/)) shop, which isn't quite the case. We code in many different languages.  In the past several moths, we have written [The Mind is a Metaphor](http://metaphors.lib.virginia.edu) in Ruby (with Rails). [For Better For Verse](http://prosody.lib.virginia.edu/) uses [Wordpress](http://wordpress.org/) with [TEI](http://www.tei-c.org/index.xml) and [JSP](http://java.sun.com/products/jsp/), and some more recent work on a William Faulkner audio archive employs [Cocoon](http://cocoon.apache.org/2.1/) with [Solr](http://lucene.apache.org/solr/). In addition to those collaborations with UVa faculty, we've been writing plugins for [Omeka](http://omeka.org/) (and dusting off our PHP skills) and have created a [discovery interface](http://gis.lib.virginia.edu/) for our GIS infrastructure in Ruby (with [Sinatra](http://www.sinatrarb.com/)). If you analyze the technologies we currently deploy, it turns out we use [Cocoon](http://cocoon.apache.org/) + [Solr](http://lucene.apache.org/solr/) more than anything else, though we're starting to move away from that particular stack as our approach for tool development.\n\nThe Scholars' Lab has a lot of experience with all types of languages, and depending on the circumstances, we choose different tools to accomplish any given task. However, after quite a bit of time helping people get started on a programming path, I've come to appreciate some of the features Ruby provides in getting new programmers up to speed.\n\n\n## Learning Curve\n\n\nEvery language has a learning curve. However, once you get the hang of some of the basics of computer languages (flow control, data structures, objects, etc.), the biggest differences come from syntax. All web languages make certain programming exercises easy, and once you buy in to the way in which that language handles programming constructs, moving between languages for experienced programmers becomes a simpler exercise in exploring syntax and built-in functionality.\n\nAs one of my computer science professors posited, generally the first language you learn governs the way you code until something significantly better comes along. For a lot of folks getting in to programming for the first time, this usually means either taking a class or finding someone to show you the basics. If you're in higher education, this has typically meant the tool of choice is PHP. However, having seen the look of bewilderment on the faces of enough graduate students and faculty members as I attempt to explain the difference between sprintf and printf (printf returns the length of the formatted String and sprintf returns the formatted String), I've come to believe that the syntax of a programming language (and it's readability) is an exceptionally important part of a language, especially when teaching basics of software construction.\n\n\n### Method Chaining\n\n\nWithout getting into how the PHP and Ruby [duck-type](http://en.wikipedia.org/wiki/Duck_typing)[ primitive data types](http://en.wikipedia.org/wiki/Primitive_data_type) and [data structures](http://en.wikipedia.org/wiki/Data_structure), one big difference in syntax between the two is how one combines multiple method calls together. Ruby uses method chaining for objects whereas PHP uses \"bolted-on\" functions (think of these as order-of-operations from your high school Algebra class). Let's look at this brief example of addressing and sorting an associative array in PHP and its equivalent in Ruby:\n\n[code lang=\"php\"]\n$projects = array(\"solr\" => 4, \"php\" => 1, \"rails\" => 2, \"jsp\" => 3);\n$keys = array_keys($projects);\nsort($keys);\n$sorted = array_slice($keys, 0, 3);\n[/code]\n\nIf you're a little more advanced, you might refactor (rewrite) the code to look more like this (methods anonymously \"bolted-on\" to one another):\n\n[code lang=\"php\"]\n$projects = array(\"solr\" => 4, \"php\" =>  1, \"rails\" => 2, \"jsp\" => 3);\n$sorted = array_slice(sort(array_keys($projects)), 0, 3);\n[/code]\n\nNow, the same examples in Ruby syntax:\n\n[code lang=\"ruby\"]\nprojects = {\"solr\" => 4, \"php\" =>  1, \"rails\" => 2, \"jsp\" => 3}\nsorted = projects.keys.sort.slice(0,3)\n[/code]\n\nOr, even more concisely:\n\n[code lang=\"ruby\"]\nprojects = {\"solr\" => 4, \"php\" =>  1, \"rails\" => 2, \"jsp\" => 3}\nsorted = projects.keys.sort[0..3]\n[/code]\n\nI've found that Ruby's method chaining syntax makes more sense to new programmers than the more mathematical \"bolted-on\" syntax.\n\n\n### Blocks\n\n\nRuby has a neat construct that you use all over the place to create anonymous functions (a technical term for creating specific functionality without defining a new function to define the action). Let's take a function to sort an array of projects. First, in PHP:\n\n[code lang=\"php\"]\n\nfunction sort_projects_by_count($a, $b)\n{\n    if($a -> counts == $b -> counts)\n    {\n        return 0;\n    }\n    return($a -> counts > $b -> counts) ? +1 : -1;\n}\n\nusort($projects, \"sort_projects_by_count\");\n\n[/code]\n\nAnd the same thing in Ruby:\n\n[code lang=\"ruby\"]\nprojects.sort do |a, b|\n    a.counts <=> b.counts\nend\n[/code]\n\nOk, so this is a bit of an unfair comparison, but here is an analogous version of the Ruby code in PHP:\n\n[code lang=\"php\"]\nusort($projects, create_function($a, $b, 'if($a->counts == $b->counts){\n    return 0;}return ($a->counts > $b->counts ? +1 : -1));\n[/code]\n\nNo matter how you slice it, Ruby syntax just feels more human. Even if you don't know exactly what's going on, looking up one operator in the Ruby syntax as opposed to following the logic flow and determining what \"? +1 : -1\" means (it's shorthand for an if-then statement) makes the act of reading code much easier.\n\n\n## Monkeypatching\n\n\nIf you're not familiar with the term, \"monkeypatching\" is what programmers call changing or extending a base class (like an array or string object) to add functionality or change the way it works. Let's say you really need to be able to test a string to see if it looks like an integer, you could create a monkeypatch along these lines:\n\n[code lang=\"ruby\"]\nclass String\n    def is_int?\n        self =~ /^[-+]?[0-9]*$/\n    end\nend\n[/code]\n\nThis code snip extends the String class and uses a [Regular Expression](http://en.wikipedia.org/wiki/Regular_expression) (regex) to test if a given String is an integer (number) by simply calling \"is_int?\" (notice the question mark at the end of the definition; this is used for methods that return a Boolean value). That's a little advanced, but it does show off a very useful piece of functionality of the language that allows you to do a better job dealing with a [duck-typed language](http://en.wikipedia.org/wiki/Duck_typing).\n\n\n## Frameworks\n\n\nMany people when talking about Ruby associate its use in web development with the Rails [MVC framework](http://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller). Just as PHP has [Zend](http://framework.zend.com/), [CodeIgniter](http://codeigniter.com/), [CakePHP](http://cakephp.org/), [symfony](http://www.symfony-project.org/), etc., Ruby has [Rails](http://rubyonrails.org/), [Merb](http://www.merbivore.com/), [Sinatra](http://www.sinatrarb.com/), [Camping](http://camping.rubyforge.org/), and many more. Rails is the 900-pound gorilla of Ruby frameworks, and has a lot of nice features to get new applications off the ground quickly and some really great online guides to setting things up (I frequent [RailsGuides](http://guides.rubyonrails.org/)). Since we often suggest Rails to our collaborators I'll focus on this framework, but there are several other frameworks out there to choose from.\n\nThink of Ruby (or PHP for that matter) as a pile of building materials: you can to build anything you want if you know how to put everything together. Rails, on the other hand, is like a prefab house where workers pour a foundation, set the house up, and then leave you to add the drywall, siding, windows, and roof. If you need a new component for your prefab house, a sales representative is standing by to help immediately ship you what you need.\n\n\n### Generators\n\n\nTo extend the previous metaphor, generators are like sales representatives that allow you to place orders for new aspects of your site.  To create all the erb templates (the default templeting language for Rails), controller methods, model, database migrations, routes, and tests for a new project with a single line, you might run something like the following:\n\n[code lang=\"ruby\"]\nscript/generate scaffold topic title:string description:text\n[/code]\n\nI moved away from using scaffolding pretty quickly, but it does provide a nice starting point for new programmers to build interactions with data models.\n\n\n### Templates\n\n\nThe default templating engine in Rails is [erb](http://www.ruby-doc.org/stdlib/libdoc/erb/rdoc/) which provides a convenient method for generating views of  your models. One of erb's most important features is the use of \"partials,\" pieces of code that are used in multiple views by calling the render method. I often replicated this behavior in PHP by calling an include somewhere in a view.\n\n\n### ActiveRecord\n\n\nThe key to database interactions in Rails is ActiveRecord. As an SQL expert, I have to admit this part of the Rails framework drove me a bit batty at first, but then again I've been writing SQL for over 10 years (my colleagues often roll their eyes when I start writing it with relational algebra nomenclature), so allowing a framework to abstract this particular piece took some getting used to. If you're new to programming, though, this means you don't have to learn SQL but instead can use Ruby-style syntax to interact with your database without necessarily needing to care what your [RDBMS](http://en.wikipedia.org/wiki/Relational_database_management_system) back-end happens to be.\n\nTake this example of looking up a book review where you have both a \"book\" and \"review\" table. In PHP you would do something like this (this snip will only work with a MySQL connection but has some sub-selection stuff going on):\n\n[code lang=\"php\"]\nfunction query($sql)\n{\n    global $conn;\n    return mysql_query($sql, $conn);\n}\n\nfunction recent_reviews($count)\n{\n    $query = query(sprintf('SELECT b.book_title, r.review_id, r.created_at, r.id, review_counter.review_total\n        FROM reviews r, books b,\n            (SELECT count(*) AS review_total FROM reviews) AS review_counter\n        WHERE r.book_id = b.book_id\n        ORDER BY created_at DESC\n        LIMIT %d', $count);\n\n    return $query\n}\n\nprint_r(recent_reviews(5));\n[/code]\n\nNow, for the ActiveRecord equivalent:\n\n[code lang=\"ruby\"]\nreviews = Review.find(:limit => 5, :order => \"created_at DESC\");\nputs reviews.inspect\n[/code]\n\nBecause of the way in which ActiveRecord sets up its model associations, you'll have access to the different name scopes to print out the same information, just in far less code. However, if you really want to, you can pass your SQL to get more granular control over the syntax:\n\n[code lang=\"ruby\"]\nreviews = Review.find_by_sql(\"SELECT b.book_title, r.review_id, r.created_at, r.id, review_counter.review_total\n        FROM reviews r, books b,\n            (SELECT count(*) AS review_total FROM reviews) AS review_counter\n        WHERE r.book_id = b.book_id\n        ORDER BY created_at DESC\n        LIMIT ?\", count);\n[/code]\n\nOne of the real beauties of the ActiveRecord methods is that as long as you're using the generic ActiveRecord syntax, your data persistence layer can be pretty much any RDBMS and be changed with a couple lines in the configuration file. The trade-off however, is that you lose a few things and can make slightly more work for yourself than you might anticipate. One important caveat is that ActiveRecord doesn't create foreign keys when you set up reference fields. This is actually by design as it's using an object-oriented idiom (an object should validate the presence of another, without the underlying persistence layer enforcing any type of constraint), but I find myself adding these in to ensure that the RDBMS takes advantage of the pre-calculated indexes to improve overall performance.\n\nI should also mention that I think the ActiveRecord model has some real limitations. As you develop your models, you will most like be tweaking its fields, which in turn requires new migrations, and you may forget which fields are actually in your models. There are plugins that help with this, but you do need to take additional steps to have this information placed somewhere convenient (I use a pre-commit git hook that calls the [annotate gem](http://github.com/ctran/annotate_models) to dynamically annotate my model schemas).\n\n\n### Security\n\n\nYou'll notice in the last examples I was doing some funny stuff in both the PHP and Ruby examples to protect against [SQL injection attacks](http://en.wikipedia.org/wiki/SQL_injection). If you're using the ActiveRecord methods of addressing objects, Rails will take care of this for you. If you're using PHP, you'll either need to do this yourself (sprintf is commonly used) or rely on a framework to parametrize your statements (you don't want someone deleting everything in your database).\n\nYou also need to protect yourself from [Cross-site Scripting Attacks](http://en.wikipedia.org/wiki/Cross-site_scripting) (XSS) by escaping HTML from fields with dynamic content. erb has a helper function html_escape (h is the shorthand) which escapes this data. In Rails 3, this will change slightly and erb will automatically html_escape model output unless you explicitly tell it not to escape the field. One less thing to remember!\n\n\n### Testing\n\n\nTesting is important, and I try to preach its virtues every chance I get. After finishing a project, the typical programmer won't touch the code again until the application breaks. Good testing will save you (or the person that inherits the code) a lot of time discovering exactly what broke the application.\n\nLet's face it: testing is a pain in PHP, and I rarely see it done well. What I've really enjoyed about Ruby development is the fact that no matter how you code there is an appropriate testing framework available (I use [rspec](http://rspec.info/)). There's a strong emphasis on not only unit testing, but also integration and acceptance testing. There are also libraries that give you an idea of how well your [code is tested](http://github.com/relevance/rcov), something I sorely missed from my Java coding endeavors. One other huge plus is that every gem on the [rubygems.org](http://www.rubygems.org) site includes test-coverage metrics to give you an idea of how well the code you want to install is tested.\n\nPHP also has testing frameworks with [PHPUnit](http://www.phpunit.de/) and [PHPSpec](http://code.google.com/p/phpspec/) being rather popular. I won't say too much about the PHP testing frameworks other than to say that there are analogous frameworks for writing and running tests in PHP and Ruby. However, I've noticed a slightly more concerted effort to think through the inclusion of the tools in Ruby and their integration into the coding workflow than I've experienced with PHP. With the latter language, I've often fell in to the trap of writing the code, getting it to where I want it, and then, really as an afterthought, writing basic unit tests to get rather skimpy code coverage.\n\nAs a case in point, a mantra in Rails development is [TATFT](http://rubyhoedown2008.confreaks.com/05-bryan-liles-lightning-talk-tatft-test-all-the-f-in-time.html).\n\n\n\n[BryanL on TATFT](http://vimeo.com/1534976) from [Bryan Liles](http://vimeo.com/bryanl) on [Vimeo](http://vimeo.com).\n\n\n## Deployment\n\n\nTwo typical objections raised when contemplating Ruby development are that \"Ruby doesn't scale\" and that server setup is a real pain. These are valid concerns, but as with many open source projects with a large number of fanatical supporters, the Ruby community has steadily made improvements in these areas. Actually, for the vast majority of our readership, these issues can be filed away in the solved category.\n\n\n### Scaling\n\n\nI've always found objections to scaling a bit troubling. Scaling is one of those over-used terms that means different things to different people, but most of the \"Rails doesn't scale\" comes from Twitter's experience with the framework. They found, as they scaled horizontally (adding more servers) to handle loads of 11,000+ requests per second, that a bottleneck existed at the data persistence level as Rails doesn't, by default, provide a mechanism to to address multiple databases. Twitter has since moved parts of their code base to [Scala](http://www.scala-lang.org/) but has retained the majority of their code in Rails and has developed some rather ingenious messaging capabilities to talk to the appropriate abstraction layers that one needs in very large enterprise applications.\n\nWhile Twitter shows that Rails is capable of scaling (with lots of work), quite honestly the likelihood of any of our applications needing this level of engineering is slim. I will say, however, that there are relatively simple methods of scaling with your infrastructure should you start running into performance issues. We have, for example, an application written in pure Ruby on Rails deployed as a Tomcat application (the details of which are completely outside the scope of this article, but the application gets all the benefits of an Enterprise class Java environment with the ease of Rails development).\n\n\n## Server Support\n\n\nThe Ruby language is included in most (if not all) modern Linux package systems and makes installation a snap. The other \"major\" piece of software you'll need is RubyGems (a package manager for Ruby libraries), which is also generally available as a managed package.\n\n\n<blockquote>**Note**: There is a major change occurring with the development of Rails 3. Rails is moving from a system of system-wide gems to application-level gems with the introduction of [GemBundler](http://gembundler.com/). This approach is a more stable method of deploying application requirements which not only allows you to ensure that application libraries are properly resolved, but also provide better granular control over which libraries are deployed in specific contexts.</blockquote>\n\n\nThere was a time where deploying Rails applications was a real bear. Then along came [Phusion Passenger](http://www.modrails.com/) (aka mod_rails). This allows you to run Rails (actually any Rack-based application) through Apache and Nginx without any other port management, service process monitoring, file cleanup, etc. As long as Apache is running, so is your Rails app!\n\n\n## Community of Support\n\n\nThe Rails community is pretty great in getting folks off the ground. As with any technology there are a fair number of curmudgeons, but leaders in the community as quick to remind people to be nice (see Yahuda Katz's [The Blind Men and the Elephant: A Story of Noobs](http://yehudakatz.com/2010/02/09/the-blind-men-and-the-elephant-a-story-of-noobs/)). There are several corporations backing Rails development ([EngineYard](http://www.engineyard.com) is a big one) and when the Google Summer of Code for Rails program wasn't continued, the Rails community was able to raise $100,000 in three days to support a [Ruby Summer of Code](http://rubysoc.org/).\n\nThere are a number of really good podcasts ([Ruby5](http://ruby5.envylabs.com/) and [The Ruby Show](http://5by5.tv/rubyshow) are good), vodcasts ([Railscasts](http://railscasts.com/)), tutorial sites ([ACSIIcasts](http://asciicasts.com/)), blogs ([RailsDispatch](http://www.railsdispatch.com/), [ThoughtBot](http://thoughtbot.com/), [EngineYard](http://www.engineyard.com/blog/)), open source projects (lots on [github](http://www.github.com)), open source books ([Rails Tutorials](http://www.railstutorial.org/book)), and some really good reference books from [The Pragmatic Programmers](http://pragprog.com/). There's even some humor...\n\n\n\n\n## Summary\n\n\nAs much as it drives language purists crazy when I say it, there's nothing that you can do in Ruby that you can't replicate in PHP (and vice-versa). In my experience, getting people set up with a functioning web application is far easier with Ruby than it is with PHP (and less prone to [spaghetti code](http://en.wikipedia.org/wiki/Spaghetti_code)), and the deployment options make Ruby (plus a framework) a great starting point for new programmers to get their feet wet with web application development (check out [Heroku](http://www.heroku.com)). If you're an experienced developer, does it make sense to drop PHP and rewrite your code base? Absolutely not. However, at some point, you will be faced with the prospect of needing to migrate a legacy application where Ruby may make a lot of sense. As someone who has spent quite a bit of time de-tangling spaghetti code from Perl CGI and mixed HTML and PHP pages, I'm hoping that the people that will eventually be migrating my Ruby code will not need to perform the level of coding archaeology we've needed to perform.\n\nLike most things in life, choosing the correct tool for the job needs some careful consideration and planning. Ruby makes a lot of sense for getting applications off the ground quickly and reinforcing good practices like testing, code separation, and readability that I find important in forming new digital humanities programmers. Web development over the last decade has become exceptionally complex (AJAX, web services, web standards, multiple browsers, etc.) and the real hope is that by using Ruby and Rails as an approach, people will be inspired to continue down a development path to both enrich their own scholarship and impact the larger digital humanities community without becoming frustrated by syntax. This is a bit of an experiment which we and other digital humanities shops are undertaking, and in which we're inviting everyone to participate.  No matter the language, we should all be engaged in teaching best practices in project design and management, in software development techniques, in the construction of usable and elegant interfaces, and in the application of these things to humanities scholarship, through which everyone wins!\n\n\n## Other Resources\n\n\n\n\n\n\t\n  * [Rails for PHP Developers](http://railsforphp.com/)\n\n\t\n  * [7 Reason I switched back to PHP after 2 years on Rails](http://www.oreillynet.com/ruby/blog/2007/09/7_reasons_i_switched_back_to_p_1.html)\n\n\t\n  * [RubyGuides](http://guides.rubyonrails.org/)\n\n\t\n  * [JRuby](http://www.jruby.org/)\n\n\n"},{"id":"2010-05-17-frontiers-in-spatial-humanities","title":"Frontiers in Spatial Humanities","author":"bethany-nowviskie","date":"2010-05-17 14:11:29 -0400","categories":["Announcements","Digital Humanities","Geospatial and Temporal"],"url":"frontiers-in-spatial-humanities","content":"**[UPDATE: [video for the \"Frontiers\" event](/2010/06/01/frontiers-in-spatial-humanities-video/) is now available!]**\n\nWe're crowd-sourcing the keynote to the final round of the Scholars' Lab/NEH 2009-2010 [Institute for Enabling Geospatial Scholarship](http://lib.virginia.edu/scholarslab/geospatial).  With all of these fantastic [attendees](http://www2.lib.virginia.edu/scholarslab/geospatial/participants.html#scholarship) on hand -- not to mention the [Institute faculty](http://www2.lib.virginia.edu/scholarslab/geospatial/index.html#faculty) -- how could we let the opportunity slip by?\n\n\n\n#### Frontiers in Spatial Humanities:\nLightning Presentations\n\n\n\nWe are pleased to host 40 rapid-fire, 2-minute demos of boundary-pushing projects in spatial humanities.  The scholars presenting their work come from 27 different institutions, and were competitively selected to attend this prestigious program, funded by the [National Endowment for the Humanities](http://neh.gov/odh).  Some of our Institute faculty will also offer brief glimpses of their work as part of a whirlwind tour of emerging work in humanities GIS.\n\nWhile admission to the Institute itself is now closed, \"Frontiers in Spatial Humanities\" and the reception that follows are open to the public!\n\nI'd like to thank the NEH for its generous funding of our training program, and the University of Virginia Library for supporting the Scholars' Lab -- as well as the \"Frontiers\" reception, to which you're all invited!\n\nThursday, **May 27th**, 3:30-5:00pm\nHarrison-Small Auditorium\n\n[![](http://rclslab.files.wordpress.com/2010/05/geoinst-poster.jpg?w=194)](http://rclslab.files.wordpress.com/2010/05/geoinst-poster.jpg)\n\nFor more information about the SLab and our NEH-funded [Institute for Advanced Topics in the Digital Humanities](http://www.neh.gov/grants/guidelines/IATDH.html), please visit:\n[http://lib.virginia.edu/scholarslab/geospatial/](http://www2.lib.virginia.edu/scholarslab/geospatial/)\n"},{"id":"2010-06-01-frontiers-in-spatial-humanities-video","title":"Frontiers in Spatial Humanities (video)","author":"joe-gilbert","date":"2010-06-01 05:53:38 -0400","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Podcasts"],"url":"frontiers-in-spatial-humanities-video","content":"A video stream of the final event of our NEH-funded [Institute for Enabling Geospatial Scholarship](http://lib.virginia.edu/scholarslab/geospatial) (or [#geoinst](http://search.twitter.com/search?q=%23geoinst) as it's known on Twitter) is now available!  Thanks to all our wonderful participants for making these lightning talks, collectively entitled \"Frontiers in Spatial Humanities,\" so thought-provoking.\n\n\n\n\n\n\n\nThe Scholars' Lab/NEH Institute for Enabling Geospatial Scholarship was held at the University of Virginia Library May 25-27, 2010 and concluded with a set of two-minute, three-slide lightning talks by Institute attendees on their own spatial humanities projects and works-in-progress.\n"},{"id":"2010-06-17-expanding-the-capabilities-of-omeka","title":"Expanding the Capabilities of Omeka","author":"ethan-gruber","date":"2010-06-17 10:00:59 -0400","categories":["Research and Development"],"url":"expanding-the-capabilities-of-omeka","content":"Because I have a keen interest in the description of cultural heritage artifacts and in doing interesting things with metadata, in recent months I have developed a handful of Omeka plugins to meet these interests.  My first foray into plugin development for the application was with the [EAD Importer](https://addons.omeka.org/svn/plugins/EadImporter/).  The EAD Importer, as the name suggests, extracts item-level metadata (along with a bit of collection-level metadata, like rights) from Encoded Archival Description finding aids and generates a CSV file which can be imported through the CSV Import plugin developed by the Omeka crew.  The plugin would be useful to archivists who would like to use Omeka to build online exhibits of their collections.  I took this framework a step further to create a plugin that is capable of importing any [flat XML](https://addons.omeka.org/svn/plugins/GenericXmlImporter/trunk/) into Omeka by transforming that file into a CSV file.\n\nMost recently, I have turned my attention to expanding the descriptive abilities of Omeka into the realm of collections of artwork.  Omeka items are described with Dublin Core, which is capable of describing _anything_, though not particularly well.  I developed [VraCoreElementSet](https://addons.omeka.org/svn/plugins/VraCoreElementSet/trunk/), which incorporates VRA Core fields into the Edit Item form.  VRA Core is a much more semantically appropriate schema for describing art and artifacts.  Since it was conceived as an XML standard (not strictly a flat list of fields), some elements have hierarchical sub-componenets.  For example, a work may have several agents involved in its production, and each agent has a name as well as a role, culture, birth date, and, as the case may be, a death date.  The VraCoreElementSet plugin creates a table for agents so that a user may enter this data separately.  Then in the Edit Item form, the user may select VRA Core agents from a drop down menu restricted by the records in the agents table.  Items may also be exported to schema-compliant VRA Core XML.  There is still some work remaining on this plugin, but it is well on its way toward completion.\n\nNow that the Scholars' Lab has contributed EAD Importer and VRA Core Element Set plugins, Omeka may attract new institutional users from the library, archive, and museum fields, who may have otherwise settled for proprietary applications to disseminate their digital collections.\n"},{"id":"2010-06-30-wms-vs-tilecaching","title":"WMS vs. tilecaching","author":"adam-soroka","date":"2010-06-30 03:59:35 -0400","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"wms-vs-tilecaching","content":"In our work on [Neatline](http://neatline.org/), we have made a deliberate choice to start by constraining ourselves to map-sources that are quickly and easily provided through [WMS](http://www.opengeospatial.org/standards/wms). This leaves out (for now) two popular sources of map imagery; [Google Maps](http://maps.google.com/) and [Open Street Map](http://www.openstreetmap.org/). I'm going to explain why we made that choice, and why, when we do come to make these sources usable with Neatline, we will do so with great care and with an eye to scholarly method.  <!-- more -->\n\nAll two-dimensional maps (as opposed to globes) are [projected](http://en.wikipedia.org/wiki/Map_projection). That is, the curved three-dimensional surface of the Earth is transformed onto a flat two-dimensional surface. This can be done in an infinite variety of ways, many of which have been mathematically characterized and named by cartographers, for whom they are necessary tools. We must note, however, that no such transform can obtain a perfect representation of a section of the Earth. The mapmaker must choose which qualities to preserve and in what measures. Is it more important to provide an accurate depiction of relative areas or of relative lengths? Is the area around Greenland to be kept in the focus of accuracy, or that around New Zealand?\n\nEach map therefore carries with it from its creation certain choices like these, part of the arguments the map makes about the world by its very construction. We chose WMS on which to start building our tools because, amongst other reasons, it allows for the transmission of projection information as part of its operation. This fact allows us to produce imagery from historical maps (themselves in any number of projections) and maintain the original choices the mapmaker made. Google Maps and Open Street Map are not WMS sources. They can be described as tile caches, huge reservoirs of rendered imagery. As such, they offer their own choices about how the world is to be projected. (Google's choice has become so closely associated with Google that it is known widely as \"the Google projection\".)\n\nNow we come to an important technical distinction; WMS services are able (depending on the capabilities of the specific software in use) to reproject their contents. That is, in response to a specific request for imagery, they can produce the imagery in a projection different from the one in which it was stored. [GeoServer](http://geoserver.org/display/GEOS/Welcome), the software we are using for Neatline, has a library of thousands of projections to which users can add more as desired. This allows us to take imagery from a WMS source and lay it under a historical map layer while maintaining the original projection for that of the map as a whole. Tile caches, by and large, do not allow for this. (Google Maps offers its one projection, and Open Street Map offers two.) This means that in order to lay historical map imagery over a layer from one of these sources, we would have to reproject the foreground (historical imagery) overriding the choices of the mapmaker and introducing additional choices of our own about what facets of the geographies at stake are to be preserved and which abandoned.\n\n(Neogeographers will remark that georectifying a digital image introduces similar issues. This is true, but unavoidable for our purposes. We would like to avoid compounding the matter in a way that is subtle and hard to detect.)\n\nWe are working out means by which we can provide the undeniable utility of popular tilecaching services in a way that is respectful of the historical context and story of map artifacts. Until we do, we will continue to concentrate on the more flexible and sophisticated apparatus provided by WMS.\n"},{"id":"2010-07-23-on-xforms","title":"On XForms","author":"ethan-gruber","date":"2010-07-23 13:08:30 -0400","categories":["Research and Development"],"url":"on-xforms","content":"Several months ago, I wrote a post about my [XForms](http://www.w3.org/TR/xforms11/) development in the Scholars' Lab as part of a research project. I'm currently working on two research projects that utilize the standard: [EADitor](http://code.google.com/p/eaditor/) (Encoded Archival Description management and dissemination framework) and [Numishare](http://code.google.com/p/numishare/) (geared towards online delivery of numismatic collections, though other artifacts can be represented). Despite its promise, XForms has not quite swept up the library world yet (though it is most definitely generating some buzz). The W3C standard is a definition for creating dynamic webforms that handle complex, hierarchical XML data--the type of stuff libraries deal with daily. However, only in recent years have XForms processors matured to the point they are ready for mass-market consumption. There are numerous private firms developing XForms applications, including Wachovia, Cisco, and Pfizer. It is also used to some degree in the academic community. As far as I am aware, not many institutions are running it in production, though some are rapidly moving in that direction. The [XForms4Lib listserv](https://list.mail.virginia.edu/mailman/listinfo/xforms4lib) created in the fall has 80 members from across North American and European academia.\n\nWhich brings me to my point. <!-- more -->\n\nMatt Zumwalt, active code4lib member and Ruby on Rails/institutional repository developer, [boldly declared XForms to be dead](http://yourmediashelf.com/blog/2010/07/20/writing-on-the-wall-xforms-has-been-dead-for-years/). I offer this critique:\n\n\n<blockquote>There are some inaccuracies in this post that I would like to address. First of all, HTML 5 forms do not supplant XForms as an option for collecting user inputted data. HTML5 is much simpler, and thus has broader appeal. XForms enables the creation of much, much more complex models, with far more sophisticated controls and validation. Moreover, if XForms was a dead language in January 2008, with the release of the HTML5 specification, and that IBM had dropped support, then why do you suppose the XForms 1.1 specification was released in October 2009, edited by a representative of IBM?\n\nNo, XForms is very much alive. It has a small, but very active community, which is especially visible with the Orbeon development community. XForms is best used as a definition of dynamic forms that are processed server-side, not in the browser (which pushes a lot of processing demand onto the user, which isn't good). There are some good, open source frameworks out there. Orbeon is the best, and has many users from both industry and academia, including Pfizer, Leap Frog, Wachovia, UCSB, Stanford, and the National Archives. In fact, Orbeon XForms applications form a large part of the enormous workflow of the NARA Electronic Records Archives project, which is a multi-year project contracted to Lockheed Martin and has a financial backing of close to a half a billion dollars (I have heard). XForms, dead?\n\nA lot of the design flaws you describe are in actuality implementation flaws. Development of a Rails-based framework seems to me like an enormous waste of time and money. You can adapt the MODS editor developed by Brown to such a task. It has already been proven that you can interact with metadata delivered through REST from a Fedora repository. And MODS is fairly simple as a a metadata standard. Care to take a stab at TEI or EAD?\n\nWhen you began your research in 2007, Orbeon was a fairly young application. But the standard and its delivery and processing applications have evolved since then. Only in the last two or three years has XForms grown into a viable solution. Moreover, since it is a W3C standard, you can pick your forms up and migrate them to a new framework fairly easily. Is your Rails application sustainable in the long term? Are today's jQuery functions going to work in 2015's browsers? These are things you need to consider when contemplating a web form standard.</blockquote>\n\n\nFedora is a Tomcat application. So is Solr. So is adore-djatoka, which UVA/Hydra utilizes for jpeg2000 delivery. And so is Orbeon. ActiveFedora and any Rails-based MODS editor seem to me like the third wheel in the repository relationship. But in all seriousness, the sustainability of a boutique Rails application that is heavily dependent on the javascript functions of 2010 should be a serious concern to repository developers. jQuery is all the rage today, but it could blow away in the wind five years from now. This is the very thing that the XForms working group set out to prevent when they introduced a standard approach to dynamic webforms.\n"},{"id":"2010-08-18-code-reviews-and-the-digital-humanities","title":"Code Reviews and the Digital Humanities","author":"wayne-graham","date":"2010-08-18 04:47:57 -0400","categories":["Digital Humanities","Research and Development"],"url":"code-reviews-and-the-digital-humanities","content":"The following was a response I made in an email exchange with Tom Elliot of the [Pleiades Project](http://pleiades.stoa.org/) and Bethany Nowviskie. Our conversation was prompted by Tom's inquiry on planning, budgeting for, and conducting a code review as part of a grant-funded project. What follows is a slightly modified (and expanded) version of that email conversation.\n\nTesting and code review is something that has been on my mind a lot lately as our shop has been shifting its focus from boutique, one-off projects, to building upon frameworks maintained by other organizations. As these code bases continue to grow, we need to ensure that subtle changes to the core functionality of the underlying systems do not propagate into bugs in our code. We also need a way to handle this situation quickly and efficiently when this does arise. This was especially reinforced by two recent projects our group undertook to migrate nearly decade-old software on to new servers.\n\nIf you ask anyone in the office, they will most likely roll their eyes when I start beating the testing drum. <!-- more --> These are great tools for not only generating pretty green and red bar charts, but also documenting the intention of the programmer in writing the code, and zeroing in on bugs where they occur without weeks of hunting. However, this is only one of the tools in the chest for writing solid code, sans bugs. In fact, there are a lot of sophisticated, freely available, automated tools that help programmers of all skill levels not only write more consistent code, but also zero in on potential performance issues and just plain smelly code (that they obviously wrote just to get running and fully intended to go back and fix later).\n\nOver the years, tools that measure code complexity (like [PMD](http://pmd.sourceforge.net/), [PHPMD](http://phpmd.org/about.html), and [flog](http://ruby.sadi.st/Flog.html)), code dependency analyzers ([JDepend](http://www.clarkware.com/software/JDepend.html), [PHPDepend](http://pdepend.org/news.html), and [rcov](http://eigenclass.org/hiki.rb?rcov)), copy/paste detection (in PMD, [flay](http://ruby.sadi.st/Flay.html), and [phpcpd](http://github.com/sebastianbergmann/phpcpd)), and enforcing coding standards (a la PHPCode Sniffer and rails_best_practices), along with not only unit and integration tests (in whatever style you choose), but a code coverage analysis reports that provides feedback on which lines were executed, go a long way in reducing the number of bugs in code. These tools are really pre-emptive step in writing stronger, more elegant, and ultimately more sustainable code, all before once gets to the point of performing a human code review.\n\nWhile I don't need to be building software per-se, I have started experimenting with the Hudson continuous integration server as a dashboard to get a quick snapshot of how these different metric tests all play together in the code that our team writes. It is no longer good enough to simply have code functioning, we need the code to pass certain thresholds of quality and sustainability before we can release. Where we find issues in the code, like test coverage, high cyclomatic complexity, lots of copy-n-pasted code, or high volatility in dependency scans, we can sit down and perform a rather focused mini code review (resembling the pair-programming idiom) on that section of code to refactor a better solution or approach To this end, we're currently working on a set of baseline testing and reporting tools for our projects. Currently, we have Ant scripts for our PHP and Java projects, and a gem bundle for Rails and Sinatra projects.\n\nWhile we take this approach in the Scholars' Lab, we were wondering if there were others out there that had opinions or experiences to share about code review during development? If you do, leave a comment, write a post, or tweet at us (@scholarslab, @nowviskie, @wayne_graham) -- and at @paregorios, who started the conversation in the first place. We'd love to hear about your best practices (and even horror stories) and philosophy on what constitutes good software and useful code reviewing, including whether you think current trends in open source development constitute a good-enough review for DH projects.\n\n\n## Further Resources\n\n\nJava\n\n\n\n\t\n  * [PMD](http://pmd.sourceforge.net/)\n\n\t\n  * [JDedend](http://www.clarkware.com/software/JDepend.html)\n\n\t\n  * [junit](http://www.junit.org/)\n\n\t\n  * [Clover](http://www.atlassian.com/software/clover/)\n\n\t\n  * [Javadoc](http://www.oracle.com/technetwork/java/javase/documentation/index-jsp-135444.html)\n\n\t\n  * [Checkstyle](http://checkstyle.sourceforge.net/)\n\n\nPHP\n\n\t\n  * [PHP Depend](http://pdepend.org/news.html)\n\n\t\n  * [PHPMD](http://phpmd.org/about.html)\n\n\t\n  * [phpcpd](http://github.com/sebastianbergmann/phpcpd)\n\n\t\n  * [PHP Codesniffer](http://pear.php.net/package/PHP_CodeSniffer/redirected)\n\n\t\n  * [phpDocumentor](http://www.phpdoc.org/)\n\n\nRuby\n\n\t\n  * [Flog](http://ruby.sadi.st/Flog.html)\n\n\t\n  * [Flay](http://ruby.sadi.st/Flay.html)\n\n\t\n  * [metric_fu](http://metric-fu.rubyforge.org/)\n\n\t\n  * [rdoc](http://rdoc.sourceforge.net/)\n\n\nJavascript\n\n\t\n  * [QUnit](http://docs.jquery.com/QUnit)\n\n\t\n  * [JSCoverage](http://siliconforks.com/jscoverage/)\n\n\nContinuous Integration\n\n\t\n  * [Hudson](http://hudson-ci.org/)\n\n\t\n  * [Phing](http://phing.info/trac/)\n\n\t\n  * [CruiseControl](http://cruisecontrol.sourceforge.net/)\n\n\nIssue Tracking\n\n\t\n  * [Google Code](https://code.google.com/hosting/)\n\n\t\n  * [GitHub](http://github.com/)\n\n\t\n  * [Project Kenai](http://kenai.com/)\n\n\t\n  * [Jira](http://www.atlassian.com/software/jira/)\n\n\t\n  * [Redmine](http://www.redmine.org/)\n\n\n"},{"id":"2010-09-09-omeka-solr-and-tei","title":"Omeka, Solr, and TEI","author":"ethan-gruber","date":"2010-09-09 07:50:16 -0400","categories":["Digital Humanities","Research and Development"],"url":"omeka-solr-and-tei","content":"One of the most vital tools that computers bestow upon the humanities scholar is the ability to rapidly sort and group data that are relevant to the scholar's own research needs.  A digital collection of several thousand artifacts is useful, but it is even more useful if, for example, the user can filter the results for lithographs created or published by a certain person or corporate identity.  Omeka's built-in search mechanism is fairly simple, and it may suffice for most collections, but it may also fall short of providing the kind of advanced querying abilities that scholars are growing accustomed to with other digital collections, such as Northwestern's [Winterton Collection](http://repository.library.northwestern.edu/winterton/browse.html#actiontgetAllPhotos) or [modern library catalogs](http://search.lib.virginia.edu) such as the one released publicly here at the University of Virginia Library in July.  Apache Solr is an open-source Java-based search index that provides this functionality.\n\n<!-- more -->\n\nFolks in the Scholars' Lab and other U.Va. Library departments have been using Solr for a number of years.  I have used it for nearly a dozen different projects since 2007, when Bess Sadler (now with Stanford's Digital Library Systems and Services group) introduced it to the department.  About two months ago, I began work on a Solr plugin for Omeka which would post public collection items to a Solr index.  The search results then would be rendered in the public theme.  A table in the Omeka database contains all of the elements that the user may select as facets, displayable fields, or sortable fields, and the user may check boxes in a form in the administrative panel to customize the Solr results.  Collections, item types, and tags may also be selected as facet, displayable, or sortable fields, and thumbnail images may be displayed in the search results.  The simple admin interface to the variety of Solr options outlined above can transform your Omeka collection into a great resource that visitors can manipulate to meet their own research interests.\n\nYesterday, I released [SolrSearch 0.9](http://omeka.org/codex/Plugins/SolrSearch).  In this most recent version of the plugin, text nodes from XML files attached to items are indexed for full text searching.  SolrSearch, then, is an important plugin to install in conjunction with [TeiDisplay](http://omeka.org/codex/Plugins/TeiDisplay), a plugin the Scholars' Lab developed for rendering Text Encoding Initiative (TEI) XML files.  Therefore, not only can a user read TEI transcriptions of textual works, but search the collection for words or phrases in these works as well.  SolrSearch will feature a hit highlighting option in a future version so that the user may see their search keywords in context.\n\nI know of at least one institution that is using SolrSearch (at least, in an experimental state) for their collection, so hopefully as more people begin to use it, a larger developer group can form around adapting Solr features to Omeka.  Solr is useful for controlled vocabulary services, and it would be great to maximize the application's capabilities.\n"},{"id":"2010-09-10-the-methodological-turn","title":"The Methodological Turn","author":"bethany-nowviskie","date":"2010-09-10 14:26:09 -0400","categories":["Announcements","Digital Humanities"],"url":"the-methodological-turn","content":"Exactly how does one acquire the “tools of the trade” in digital humanities research?\n\nThursday, **September 16th**\n**4pm** in the Scholars' Lab\n\n[Ray Siemens](http://web.uvic.ca/~siemens/) from the University of Victoria is Director of the Digital Humanities Summer Institute and President of the Society for Digital Humanities/Société pour l'étude des médias interactifs (SDH/SEMI). Ray will talk about training for digital humanities research including the types of courses offered at DHSI, the students and established scholars who participate, and how these groups work together to enhance methodological training.\n\n[Julie Meloni](http://www.academicsandbox.com/), an INKE Fellow at the University of Victoria, will describe her vision for a self-paced but peer-guided independent learning system for mid-career scholars who wish to refocus their research in the digital humanities, but who have only traditional and analog training.\n\nPlease join us in the SLab for light refreshments and conversation with our speakers!\n"},{"id":"2010-09-15-scholars-lab-fall-newsletter","title":"Scholars' Lab Fall Newsletter","author":"ronda-grizzle","date":"2010-09-15 11:50:01 -0400","categories":["Announcements"],"url":"scholars-lab-fall-newsletter","content":"The Scholars’ Lab Fall 2010 newsletter (pdf) is now available for download.\n\n\n#### [Download](http://tinyurl.com/SLabFall2010news)\n\n\nIn this edition, we introduce our four new Grad Fellows, as well as our 2010 Scholar in Residence and visiting scholars, and we’ve included the complete Fall 2010 schedule of events.\n\nPlease be aware that the event schedule may change, so check the official events calendar for updated information. [SLab Event Calendar](http://tinyurl.com/Slabevents)\n"},{"id":"2010-09-16-weve-come-a-long-way-baby","title":"We've come a long way, baby.","author":"bethany-nowviskie","date":"2010-09-16 07:04:50 -0400","categories":null,"url":"weve-come-a-long-way-baby","content":"[![](http://www.scholarslab.org/wp-content/uploads/2010/09/Screen-shot-2010-09-16-at-10.15.28-AM-124x300.png)](http://www.scholarslab.org/wp-content/uploads/2010/09/LiteratureSearching.pdf)Thanks to Megan Brett, Research Database and Records Manager at the [Montpelier Foundation](http://www.montpelier.org/), we are able share with you a piece of ephemera from UVa Library's computing past: a pamphlet on \"Computer Literature Search.\"\n\n\n<blockquote>**\"Why use a computer search?** Consider the time it takes to search manually through the many issues of printed indexes. The computer searches these indexes in seconds; the search is faster, more comprehensive, and often more precise, as there are more subject access points and greater flexibility in combining terms in a computer search.\"</blockquote>\n\n\nThe pamphlet continues with an offer to split evenly the costs of search with Library patrons -- \"based on computer connect-time and on the number and format of citations printed.\"  Check out a PDF of the pamphlet, [here](http://www.scholarslab.org/wp-content/uploads/2010/09/LiteratureSearching.pdf) (1mb).  It is coded \"10-84.\" Is this from 1984?\n\nPlease comment if you can shed light on the date of the pamphlet, or want to share memories of early digital and computer-assisted scholarship at UVa.  We'd also be very happy -- in the semester in which we've rolled out a [new Virgo interface](http://search.lib.virginia.edu) based on [Project Blacklight](http://projectblacklight.org) (first prototyped here in the [Scholars' Lab](http://lib.virginia.edu/scholarslab)!) -- to see more ephemera from UVa Library's long engagement with digital research.\n\n"},{"id":"2010-09-24-synchronizing-development-databases","title":"Synchronizing Development Databases","author":"wayne-graham","date":"2010-09-24 10:27:06 -0400","categories":["Research and Development"],"url":"synchronizing-development-databases","content":"As a developer, I routinely work on multiple machines during the course of a project. One of the biggest pains is working on a database-driven project is that I often need to move the data on machine X to machine Y, make changes, then move the updated data from machine Y back to machine X.\n\nBack in the day (ok, so like last week), I would typically write a mysqldump/pgdump script that would dump the data to a tarball, then scp the data around as needed. If it were really important, I might take the time to set up rsync, or even a master/slave configuration for the data. What I found, however, is that I could \"break\" my development databases and I did this often, wasting time recovering from this foolishness. There had to be a better way. <!-- more -->\n\nTurns out there is. If you've ever deployed anything to [heroku](http://www.heroku.com), you'll find they have a really neat way to allow you to synchronize your databases. From the command line, you can pull the database running on the server to your local database (and it actually doesn't matter if you're running sqlite, mysql, or postgresql locally, it just works) with:\n\n[code lang=\"bash\"]\nheroku db:pull mysql://user:pass@localhost/mydb\nheroku db:pull sqlite://path/to/my.db\n[/code]\n\nNeed to push changes to the server?\n\n[code lang=\"bash\"]\nheroku db:push\n[/code]\n\nBehind the scenes, heroku is using the [taps](http://rubygems.org/gems/taps) gem, so you can actually use this same technique for your local setups.\n\nThe following will walk through a \"typical\" (e.g. the way I have my dev system set up) use case for integrating taps in to your workflow. I use a Mac, so if you're on Linux or worse (Windows), you'll need to slightly adjust some of these directions.\n\nThe first thing you need to do is make sure that your gems are up-to-date. From a terminal, issue this command:\n\n[code lang=\"bash\"]\nsudo gem update --system\n[/code]\n\nNow, we need the taps gem:\n\n[code lang=\"bash\"]\nsudo gem install taps\n[/code]\n\nThis will take a while as the library dependencies are calculated, and the documentation is generated, but you will some something along these lines:\n\n[code lang=\"bash\"]\ndevbox:~ user$ gem install taps\nBuilding native extensions.  This could take a while...\nSuccessfully installed json_pure-1.4.6\nSuccessfully installed rack-1.2.1\nSuccessfully installed sinatra-1.0\nSuccessfully installed mime-types-1.16\nSuccessfully installed rest-client-1.4.2\nSuccessfully installed sequel-3.15.0\nSuccessfully installed sqlite3-ruby-1.3.1\nSuccessfully installed taps-0.3.12\n8 gems installed\nInstalling ri documentation for json_pure-1.4.6...\nInstalling ri documentation for rack-1.2.1...\nInstalling ri documentation for sinatra-1.0...\nInstalling ri documentation for mime-types-1.16...\nInstalling ri documentation for rest-client-1.4.2...\nInstalling ri documentation for sequel-3.15.0...\nInstalling ri documentation for sqlite3-ruby-1.3.1...\nInstalling ri documentation for taps-0.3.12...\nInstalling RDoc documentation for json_pure-1.4.6...\nInstalling RDoc documentation for rack-1.2.1...\nInstalling RDoc documentation for sinatra-1.0...\nInstalling RDoc documentation for mime-types-1.16...\nInstalling RDoc documentation for rest-client-1.4.2...\nInstalling RDoc documentation for sequel-3.15.0...\nInstalling RDoc documentation for sqlite3-ruby-1.3.1...\nInstalling RDoc documentation for taps-0.3.12...\n[/code]\n\nYou will need to have this installed on each of the boxes you want to be able to push/pull to/from.\n\nOk, assuming you've got the taps gem installed on all the computers you want to use, you need to fire up the taps server on each box that actually responds to the push/pull requests. This is a simple [Sinatra](http://www.sinatrarb.com/) application that runs and listens for push/pull requests. To fire this up, issue the command:\n\n[code lang=\"bash\"]\ntaps server mysql://user@localhost:port/database tapsusername tapspassword\n[/code]\n\nLet's unpack this a little. The taps server needs to know what database to connect to, and a secret user/password to use. Let's say you're running MAMP with the default mysql server and accounts running, and you want to be able to sync your Omeka database. Your connection string would look like this:\n\n[code lang=\"bash\"]\ntaps server mysql://root@localhost:8889/omeka tapuser IeEf643\n [/code]\n\nNow we can test that the server is running by pointing your browser at [http://localhost:5000](http://localhost:5000). You should see something along these lines after using the username and password you set with the server:\n\n[![](http://www.scholarslab.org/wp-content/uploads/2010/09/taps_server-300x149.jpg)](http://www.scholarslab.org/slab-code/synchronizing-development-databases/attachment/taps_server/)\n\nNow this doesn't actually do anything, just ensures that you have the server up-and-running. Now to get the data loaded on another box...\n\nAssuming you're on another computer now (and that you're not blocking port 5000 on the host machine), you issue a pull command (assuming you've already created the omeka database in the MAMP phpMyAdmin):\n\n[code lang=\"bash\"]\n\ntaps pull mysql://root@localhost:3306/omeka http://tapuser:IeEf643@remoteip:5000\n\n[/code]\n\nAgain, assuming you don't have a firewall port blocking issues, you should see the tables getting propagated on your system:\n\n[code lang=\"bash\"]\nReceiving schema\nSchema:        100% |==========================================| Time: 00:00:22\nReceiving data\n25 tables, 228 records\nomeka_item_ty: 100% |==========================================| Time: 00:00:00\nomeka_tags:    100% |==========================================| Time: 00:00:00\nomeka_entity_: 100% |==========================================| Time: 00:00:00\nomeka_items:   100% |==========================================| Time: 00:00:00\nomeka_section: 100% |==========================================| Time: 00:00:00\nomeka_element: 100% |==========================================| Time: 00:00:00\nomeka_record_: 100% |==========================================| Time: 00:00:00\nomeka_tagging: 100% |==========================================| Time: 00:00:00\nomeka_users_a: 100% |==========================================| Time: 00:00:00\nomeka_element: 100% |==========================================| Time: 00:00:00\nomeka_element: 100% |==========================================| Time: 00:00:00\nomeka_entitie: 100% |==========================================| Time: 00:00:00\nomeka_data_ty: 100% |==========================================| Time: 00:00:00\nomeka_item_ty: 100% |==========================================| Time: 00:00:00\nomeka_options: 100% |==========================================| Time: 00:00:00\nomeka_entitie: 100% |==========================================| Time: 00:00:00\nomeka_mime_el: 100% |==========================================| Time: 00:00:00\nomeka_section: 100% |==========================================| Time: 00:00:00\nomeka_items_s: 100% |==========================================| Time: 00:00:00\nomeka_process: 100% |==========================================| Time: 00:00:00\nomeka_collect: 100% |==========================================| Time: 00:00:00\nomeka_exhibit: 100% |==========================================| Time: 00:00:00\nomeka_files:   100% |==========================================| Time: 00:00:00\nomeka_plugins: 100% |==========================================| Time: 00:00:00\nomeka_users:   100% |==========================================| Time: 00:00:00\nReceiving indexes\nResetting sequences\n\n[/code]\n\nYou should now have a functional copy of all your data from the server machine. Now all you have to do is make your changes, then push those changes back to the server.\n\n[code lang=\"bash\"]\n\n$ taps push mysql://root@localhost:8889/omeka http://tapuser:IeEf643@localhost:5000\nSending schema\nSchema:        100% |==========================================| Time: 00:00:20\nSending data\n25 tables, 0 records\nomeka_item_ty: 100% |==========================================| Time: 00:00:00\nomeka_tags:    100% |==========================================| Time: 00:00:00\nomeka_section: 100% |==========================================| Time: 00:00:00\nomeka_entity_: 100% |==========================================| Time: 00:00:00\nomeka_items:   100% |==========================================| Time: 00:00:00\nomeka_element: 100% |==========================================| Time: 00:00:00\nomeka_tagging: 100% |==========================================| Time: 00:00:00\nomeka_users_a: 100% |==========================================| Time: 00:00:00\nomeka_record_: 100% |==========================================| Time: 00:00:00\nomeka_entitie: 100% |==========================================| Time: 00:00:00\nomeka_element: 100% |==========================================| Time: 00:00:00\nomeka_element: 100% |==========================================| Time: 00:00:00\nomeka_item_ty: 100% |==========================================| Time: 00:00:00\nomeka_options: 100% |==========================================| Time: 00:00:00\nomeka_data_ty: 100% |==========================================| Time: 00:00:00\nomeka_entitie: 100% |==========================================| Time: 00:00:00\nomeka_section: 100% |==========================================| Time: 00:00:00\nomeka_mime_el: 100% |==========================================| Time: 00:00:00\nomeka_process: 100% |==========================================| Time: 00:00:00\nomeka_items_s: 100% |==========================================| Time: 00:00:00\nomeka_collect: 100% |==========================================| Time: 00:00:00\nomeka_plugins: 100% |==========================================| Time: 00:00:00\nomeka_files:   100% |==========================================| Time: 00:00:00\nomeka_exhibit: 100% |==========================================| Time: 00:00:00\nomeka_users:   100% |==========================================| Time: 00:00:00\nSending indexes\nResetting sequences\n\n[/code]\n\n\n## So what can go wrong?\n\n\nThis is a young project, so there are a few things you should know. As of the time of writing this (taps v 3.12.0), there are a few issues being worked on:\n\n\n\n\t\n  * Foreign Keys get lost in the schema transfer\n\n\t\n  * Tables without primary keys will be incredibly slow to transfer. This is due to it being inefficient having large offset values in queries.\n\n\t\n  * Multiple schemas are currently not supported\n\n\n**I strongly suggest only using this in a development setting for non-Rails projects**. Rails-based projects have a special object for table relations which help manage keys. If you're doing heavy database development, use the tools your database provides (mysqldump/pgdump) to create snapshots of your data! Script it, crontab it, download it!\n"},{"id":"2010-10-15-open-access-week-events","title":"Open Access Week Events","author":"bethany-nowviskie","date":"2010-10-15 12:19:42 -0400","categories":["Announcements"],"url":"open-access-week-events","content":"![OA Week](http://www2.lib.virginia.edu/scholarslab/images/OA120-240webbanner.jpg)You are cordially invited to an\n\n**Open Access Week Luncheon**\nMonday, October 18 at noon in the Scholars' Lab\n\nThe Scholars’ Lab is proud to celebrate [Open Access Week](http://openaccessweek.org) with a conversation led by Associate Professor of Education **Brian Pusser** (chair of last year's Faculty Senate Task Force on Scholarly Publications and Authors' Rights), and UVa Associate General Counsel **Madelyn Wessel**.  We hope you’ll join us for a tasty lunch, some interesting and educational short films, and a lively discussion of issues surrounding authors' rights and open access to scholarly work.\n\nThen, come back that very afternoon for...\n\n**Wine, Cheese, & Legalese **\nwith Madelyn Wessel\n\nMonday, October 18 at 4:00 p.m. in the  Scholars' Lab\n\nThis is an event designed especially for graduate students and those who advise them: wine, cheese, and straight talk about copyright, fair use, and your dissertation.  Madelyn has a story or two that will curl your hair.\n"},{"id":"2010-10-19-its-alive-introducing-the-early-american-foreign-service-database","title":"It's [A]live!: Introducing The Early American Foreign Service Database","author":"jean-bauer","date":"2010-10-19 11:42:33 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"its-alive-introducing-the-early-american-foreign-service-database","content":"It is with great pleasure, and no small amount of trepidation, that I announce the launch of the [Early American Foreign Service Database](http://www.eafsd.org) (EAFSD to its friends).  While the EAFSD has been designed as an independent, secondary source publication, it also exists symbiotically with my dissertation \"Revolution-Mongers: Launching the U.S. Foreign Service, 1775-1825.\"\n\nI created the EAFSD to help me track the many diplomats, consuls, and special agents sent abroad by the various American governments during the first fifty-years of American state-building.  Currently the database contains basic information about overseas assignments and a few dives into data visualization (an interactive Google map and Moritz Stefaner's Relation Browser).\n\nI have been a reluctant convert to the principles of Web 2.0, and I keenly feel the anxiety of releasing something before my perfectionist tendencies have been fully exhausted.  The pages of the EAFSD are therefore sprinkled with requests for feedback and my (hopefully humorous) under construction page, featuring Benjamin West's unfinished masterpiece the \"American Commissioners of the Preliminary Peace Agreement with Great Britain.\"\n\nOver the next few months (and coming years) I will be adding more information to the database, allowing me to trace the social, professional, and correspondence networks from which American foreign service officers drew the information they needed to represent their new (and often disorganized) government.  I will also be enhancing the data visualizations to include hypertrees, time lines, and network graphs.\n\nThis launch has been over two years in the making.  As I look back over that time, I am amazed at the generous support I have received from my colleagues at the University of Virginia and the Digital Humanities community writ large.  I wrote an extended [acknowledgments page](http://www.eafsd.org/acknowledgements) for the EAFSD, my humble attempt to recognize the help and encouragement that made this project possible.\n\nLaunching the EAFSD also gives me a chance to test [Project Quincy](http://projectquincy.rubyforge.org), the open-source software package I am developing for tracing historical networks through time and space.  The EAFSD is the flagship (read guinea pig) application for Project Quincy.  I hope my work will allow other scholars to explore the networks relevant to their own research.\n\nTo that end the EAFSD is, and always will be, open access and open source.\n"},{"id":"2010-10-22-old-school-hydro","title":"\"Old School Hydro\" in the Scholars' Lab","author":"admin","date":"2010-10-22 10:39:05 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"old-school-hydro","content":"Please join us on November 4th (or look for our podcast) to get your feet wet with _Old School Hydro: Modern and Historic Surveying Aboard the NOAA Ship “Thomas Jefferson!”_\n![rumsey map](http://www2.lib.virginia.edu/scholarslab/images/rumsey-edelson.jpg)\nThursday, November 4\n3:00 p.m.\nScholars' Lab\n\nDuring the summer of 2010, U.Va. History professor and [Scholars' Lab](http://lib.Virginia.edu/scholarslab) GIS collaborator Max Edelson took a berth aboard the NOAA Ship Thomas Jefferson as it charted the waters off the western Keys of Florida. For a week, he learned about modern coastal surveying and hydrography first hand and interviewed the TJ’s officers and scientists about their experiences using sonar-based sensing to measure the extent of the Deepwater Horizon oil spill. To better get a grasp on the first rigorous colonial surveys of Florida created in the 1760s and 1770s, he enlisted some of the crew to recreate early modern methods by tracing the contours and measuring the depths of a harbor in Key West. When asked what they were up with their lead lines and sextants by puzzled crew mates, they replied, “We’re off to do some old-school hydro.”\n\nThis talk describes the art and science of surveying and mapmaking in and around the Florida Keys across 250 years.\n\nAll Scholars' Lab events are free and open to all. No registration is required.\n\nWe hope to see you in the Scholars' Lab! And check out our full [calendar of events](http://www2.lib.virginia.edu/scholarslab/about/events.html) for the Fall semester.\n"},{"id":"2010-10-29-smarter-paper-maps","title":"Smarter Paper Maps","author":"kelly-johnston","date":"2010-10-29 09:27:50 -0400","categories":["Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"smarter-paper-maps","content":"It’s a quiz.  I’ll name the required skills, you name the profession.  Go.\n\n\n\n\t\n  * Identifying map projections and coordinate systems\n\n\t\n  * Interpreting map scale\n\n\t\n  * Understanding techniques of cartographic relief\n\n\t\n  * Interpolating latitude & longitude\n\n\t\n  * Calculating geographic extent rectangles\n\n\nToo easy?  Well the profession I’m describing is not _Geographic Information Systems guru_ or _Cartographer_ or _Neogeographer_.   In fact, my list describes just a few of the skills you’ll need to be a first-rate _Map Cataloger_.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2010/10/MapCatalogingBooks-300x225.jpg)](http://www.scholarslab.org/digital-humanities/smarter-paper-maps/attachment/mapcatalogingbooks/)\n\nThanks to a thoughtful invitation from Jennifer Roper, I learned a bit about map cataloging alongside her UVA Library Cataloging and Metadata Services team at a workshop led by one of the very best, Paige Andrew of Penn State University.  Of course Mr. Andrew sports a world map necktie.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2010/10/PaigeAndrew-222x300.jpg)](http://www.scholarslab.org/digital-humanities/smarter-paper-maps/attachment/paigeandrew/)\n\nThe session made clear to me many of the skills needed to be a strong map cataloger are the same skills needed to be proficient in Geographic Information Systems.  That skills list above would serve as a good starting outline for a GIS 101 syllabus and all those skills are key to the map cataloging workflow.\n\nAnd Mr. Andrew’s traveling map collection was a treat to explore with examples of bleeding edge maps, segmented maps, pop-up maps, remote sensing maps, tourist maps, and maps with and without covers and neatlines.  Who knew map catalogers must be skilled in both map unfolding for preservation and map re-folding (harder!) for panel measurements.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2010/10/AssortedMaps-300x225.jpg)](http://www.scholarslab.org/digital-humanities/smarter-paper-maps/attachment/assortedmaps/)\n\nToday was eye-opening for me in thinking about using a geographic search to discover paper maps in a library catalog.  With catalogers now capturing the latitude and longitude of each corner of the map as part of their workflow, finding every paper map that intersects any point on the earth becomes possible.  It's sometimes called searching by geographic extent.  Just point to any spot on a map of the world and find all the maps in the collection that cover that spot.\n\nGeographic search relies on the kind of extent rectangle metadata map catalogers now create every day.  We’ve implemented geographic search for digital datasets in our GIS Portal [http://gis.lib.virginia.edu](http://gis.lib.virginia.edu/) by creating just such extent rectangle metadata for every digital dataset.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2010/10/GIS-Portal-300x265.jpg)](http://www.scholarslab.org/digital-humanities/smarter-paper-maps/attachment/gis-portal/)\n\nSeems map catalogers are setting the stage for that same kind of discovery for paper map collections.  Paper maps are getting smarter every day.\n"},{"id":"2010-11-09-senior-developer-position","title":"Are you our new Senior Developer?","author":"wayne-graham","date":"2010-11-09 04:19:17 -0500","categories":["Announcements"],"url":"senior-developer-position","content":"Still fairly fresh on the scene, but drawing on a long history in digital humanities and spatial and data-driven work at the University of Virginia, the UVa Library's [Scholars' Lab](http://lib.virginia.edu/scholarslab) has developed [great projects](http://www.scholarslab.org/) and hosted [amazing events](http://www2.lib.virginia.edu/scholarslab/about/events.html) over the past three years. We now have an opportunity to add new collaborators to round out our team! (So stay tuned for future postings later this semester.)\n\nToday, [we're looking for a Senior Developer](http://jobs.virginia.edu/applicants/Central?quickFind=62652) who can build, test, and debug code and who loves to get 'under the hood' to figure out how things work.\n\n\n### Senior Developer, Scholars' Lab\n\n\nAs a senior web and software developer reporting to the Head of R&D for the [Scholars' Lab](http://www.scholarslab.org/), you will be responsible for enhancing, maintaining, and optimizing projects related to digital research and scholarship. Not only should you enjoy writing well organized, highly tested code, but you should enjoy working with a great group of teammates and scholarly stake holders to solve hard problems in both software engineering and the digital humanities. You will need to fit into a fast-paced, interdisciplinary environment where technology enables creative vision -- and where you can take good advantage of the \"20% time\" that all Scholars' Lab and [Department of Digital Research & Scholarship](http://lib.virginia.edu/scholarslab) faculty and staff are granted to pursue professional development and their own (often collaborative) R&D projects.\n\n**Responsibilities:**\n\n\n\n\t\n  * Write scalable, well-factored, well-tested application code\n\n\t\n  * Lead development projects\n\n\t\n  * Work with support teams to find and fix application bugs\n\n\t\n  * Work with cross-functional teams to develop design solutions\n\n\n**Specialized Knowledge and Skills:**\n\n\n\n\t\n  * In-depth knowledge of multiple programming languages including PHP, Perl, Ruby, and Java\n\n\t\n  * Full-time experience with PHP and Ruby frameworks (e.g. Zend and Rails)\n\n\t\n  * Proven ability to quickly learn new systems and environments\n\n\t\n  * Experience working on open source projects\n\n\t\n  * Efficient problem solving techniques\n\n\t\n  * Excellent verbal and written communication skills\n\n\t\n  * Knowledge of multiple RDBMSes (PostgreSQL, Oracle, MySQL)\n\n\t\n  * Javascript skills (AJAX, DHTML, jQuery and similar JS frameworks)\n\n\t\n  * Familiarity with version control systems (Subversion and Git)\n\n\t\n  * Experience Test-Driven development (PHPUnit, SimpleTest, RSpec, Shoulda, etc.)\n\n\t\n  * Linux experience a plus (RHEL, Fedora Core)\n\n\n\n\n### If this sounds like you...\n\n\nwe invite you to [APPLY FOR THE JOB](http://jobs.virginia.edu/applicants/Central?quickFind=62652   ).\n\nSalary is commensurate with experience, and expected to range between approximately $58,000 and $106,000 per annum. We're looking to fill this position quickly, so please don't delay!\n"},{"id":"2010-11-23-humanities-design-ux-architect","title":"Are you our new Humanities Design (UX) Architect?","author":"bethany-nowviskie","date":"2010-11-23 09:08:33 -0500","categories":["Announcements"],"url":"humanities-design-ux-architect","content":"Building on a nearly twenty-year history in digital humanities and spatial and data-driven scholarship at the University of Virginia, the [Scholars' Lab](http://lib.virginia.edu/scholarslab) has developed [great projects](http://www.scholarslab.org/) and hosted [amazing events](http://www2.lib.virginia.edu/scholarslab/about/events.html) at UVa Library over the past three years.  In addition to our current search for a full-time, permanent [Senior Developer](/2010/11/09/senior-developer-position/), we now have the opportunity to add some UX love to the work of our R&D; division.\n\nWe're looking for a Humanities Design (UX) Architect who can create and guide exciting, professional user experiences, is passionate about the quality of his or her HTML and CSS, and wants to be part of a team that does great work in the digital humanities.\n\n\n\n### Humanities Design (User Experience) Architect\n\n\nAs the Humanities Design Architect of UVa Library's department of Digital Research and Scholarship, you will be responsible for the design and implementation of effective and inspiring digital resources for teaching and scholarship. We are looking for someone who is highly technically skilled and a talented designer, and who has a deep background in humanities or social science scholarship. This general faculty position in the Scholars' Lab is for a true \"hybrid academic\"  -- someone who can communicate effectively with humanities and social science scholars, help guide the work of graduate students in our digital humanities fellowship program, and focus intently on the presentation and interaction layer for next-generation digital scholarship.\n\nWe seek a person who can fit into a fast-paced, interdisciplinary environment where good design truly matters.  Because we collaborate as peers with UVa faculty and graduate students, we are looking for someone with a personal research agenda, who will take excellent advantage of the “20% time” all Scholars' Lab faculty and staff are granted to pursue professional development and their own projects.\n\nThis is a two-year, general faculty position, with possibility of renewal.  Salary will be commensurate with experience.  Benefits are excellent, and include generous funding for continuing education and travel, retirement plans, and 22 days of vacation per year.\n\n\n\n#### Primary Responsibilities:\n\n\n\n\n\n\n  * Conduct user research for creating user models\n\n\n  * Drive the interaction process, feature discussions, and functional requirements for Scholars' Lab projects\n\n\n  * Create wireframes and prototypes\n\n\n  * Conduct informal usability tests\n\n\n  * Work closely with R&D Team\n\n\n\n\n\n#### Specialized Knowledge and Skills:\n\n\n\n\n\n\n  * Passionate about interactive experience across a variety of media (web, mobile) with a strong desire for innovation\n\n\n  * Experience as a project manager or technical team leader on scholarly projects\n\n\n  * Experience with user-centered design patterns and methodologies\n\n\n  * Experience running user testing and conducting accessibility testing\n\n\n  * Comfort with complexity and ambiguity, and a passion for the challenges of the humanities and social sciences\n\n\n  * Advanced understanding of user interface client technologies such as Javascript, AJAX, HTML, CSS, etc.\n\n\n  * Experience creating standard user experience deliverables, including\n\n\n    * Site Maps\n\n\n    * Process flows\n\n\n    * Personas\n\n\n    * Use Cases\n\n\n    * Concept Models\n\n\n    * Wireframes\n\n\n    * Interactive prototypes\n\n\n\n\n\n  * Strong presentation and communication skills\n\n\n  * Expertise in current design tools\n\n\n  * and a sophisticated personal research agenda related to the user experience of digital humanities and social science projects.\n\n\n\n\n### If this sounds like you...\n\n\nwe invite you to [APPLY FOR THE JOB!](http://is.gd/hElfB)\n\nConsideration of applications will begin immediately and continue until the position is filled.\n"},{"id":"2010-12-08-podcasts","title":"Podcasts!","author":"ronda-grizzle","date":"2010-12-08 09:41:41 -0500","categories":["Podcasts"],"url":"podcasts","content":"Greetings, Scholars' Lab fans. We have three new podcasts to share since we last updated. Please enjoy!\n\nIntroducing our 2010/2011 Scholars' Lab Fellows - Tom Finger, Jared Benton, Chris Clapp, and Alex Gil.\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.5154837771/enclosure.mp3\"]\n\nRay Siemens (University of Victoria) and Julie Meloni (INKE Fellow at the University of Victoria) discuss opportunities and approaches for training humanities scholars in digital methods.\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.5154837767/enclosure.mp3\"]\n\nJo Guldi (Harvard Society of Fellows) discusses _The City Made of Words : Text Mining the Spaces of Subaltern Agency in Britain, 1848-1919_.\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.5154837763/enclosure.mp3\"]\n"},{"id":"2010-12-14-mark-sample-haunts-place-play-and-trauma","title":"Mark Sample: Haunts: Place, Play, and Trauma","author":"ronda-grizzle","date":"2010-12-14 05:45:48 -0500","categories":["Podcasts"],"url":"mark-sample-haunts-place-play-and-trauma","content":"Prof. Mark Sample re-imagines locative media tools as platforms for renegotiating space and telling stories in literary, critical, and creative contexts.\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.5484615210/enclosure.mp3\"]\n"},{"id":"2010-12-15-building-omeka-exhibits-with-fedora-repository-content","title":"Building Omeka Exhibits with Fedora Repository Content","author":"ethan-gruber","date":"2010-12-15 08:53:53 -0500","categories":["Digital Humanities","Research and Development"],"url":"building-omeka-exhibits-with-fedora-repository-content","content":"Our NEH-funded [Neatline](/research/neatline/) project has inspired the [Scholars' Lab](http://lib.virginia.edu) to develop or enhance several new [Omeka](http://www.omeka.org) plugins recently. (See our full [list](/research/omeka-plugins/).)\n\nOne of these is [FedoraConnector](http://omeka.org/codex/Plugins/FedoraConnector), which is designed to enable administrators to attach Fedora datastreams (a digital object -- whether image, XML like TEI or EAD, or video) to Omeka items.  This is fundamentally different from attaching files to an item--the datastream is not duplicated and stored within Omeka's archive.  Rather, a reference to the Fedora object (PID) is stored within a new table in the Omeka database that associates the item with the URL of the datastream that is accessed (and rendered) with Fedora's REST API.  The plugin also supports importing Dublin Core and MODS metadata into the DC Element Set in Omeka.  The importers can be extended to map from any metadata standard into DC.\n\nThe benefit to this architecture is that it enables dynamic rendering of the most current version of the Fedora object, and thus there is no issue about storing duplicate files in the Omeka disk space that can be deprecated by updates to the original Fedora object.  Additionally, FedoraConnector can take advantage of institutional-specific services that are developed for delivering content.  For example, thumbnail and medium-sized page images are rendered in real time by querying the University of Virginia Library's JPEG2000 server and requesting deliverables at a specific dimension.  Disseminators, or handler functions for rendering Fedora content based on mime-type and/or datastream type, are extensible.\n\n[caption id=\"attachment_1195\" align=\"alignright\" width=\"110\" caption=\"TEI document from Fedora\"][![TEI document from Fedora](http://www.scholarslab.org/wp-content/uploads/2010/12/segmental-tei-110x110.png)](http://www.scholarslab.org/digital-humanities/building-omeka-exhibits-with-fedora-repository-content/attachment/segmental-tei/)[/caption]\n\nEarlier this year, we released a beta version of a plugin for rendering TEI files into HTML within Omeka.  Called [TeiDisplay](http://omeka.org/codex/Plugins/TeiDisplay), this plugin was enhanced by the insertion of several hooks that execute FedoraConnector functions (if FedoraConnector is installed) to render TEI XML datastreams on the fly directly from the repository.  TeiDisplay supports, as the documentation for the plugin indicates, selection of customized XSLT stylesheets and two display types: entire document and segmental view (with table of contents and by-section rendering).  Indeed, documents coming from Fedora can be rendered dynamically with the same set of options.\n\nBut what about indexing the document?  This is why the Scholars' Lab developed [SolrSearch](http://omeka.org/codex/Plugins/SolrSearch) last summer to replace Omeka's default mySQL search with the more advanced search options afforded by Solr, an open source search index.  SolrSearch supports facets, sorting, hit highlighting, and a handful of other options.  Originally designed to index the full text of Omeka files with a text/xml mime-type, SolrSearch was enhanced to index the full text of Fedora datastreams with a text/xml mime-type as well, enabling full text searching, faceted browsing, and hit highlighting of the aforementioned TEI files referenced from a repository.\n\n[caption id=\"attachment_1188\" align=\"aligncenter\" width=\"300\" caption=\"Solr search of TEI file in Omeka\"][![solr](http://www.scholarslab.org/wp-content/uploads/2010/12/solr-300x116.png)](http://www.scholarslab.org/digital-humanities/building-omeka-exhibits-with-fedora-repository-content/attachment/solr/)[/caption]\n\nSo in essense, the range of plugins the Scholars' Lab has created for Omeka can enable creation of attractive and cutting-edge public user interfaces for collections of Fedora objects.  Coupled with our Neatline plugins, which are all about geospatial and temporal interpretation of archival collections, this work bridges a well-recognized gap between the  volume of digital content housed in sophisticated repositories and the curators, scholars, and end users who seek access to it and wish to interpret it in online exhibits.\n"},{"id":"2011-01-06-mapping-an-asian-american-indie-rock-digital-diaspora","title":"Mapping an Asian American Indie Rock Digital Diaspora","author":"wendy-hsu","date":"2011-01-06 08:59:57 -0500","categories":["Digital Humanities","Geospatial and Temporal","Grad Student Research"],"url":"mapping-an-asian-american-indie-rock-digital-diaspora","content":"[My dissertation project](http://beingwendyhsu.info/?page_id=6) investigates the musical and social life of current independent rock  musicians of Asian descent. This research looks at the music,  interviews, and social interactions of these musicians. How do I do  this?\n\nPrior to working with UVa's [Scholars Lab](http://lib.virginia.edu/scholarslab/),  my method of field research had been participant observation: attending  concerts, doing formal and informal interviews, interacting with the musicians' friends and fans, listening to their recorded music, organizing local  performances on their tours…an immersion in these musicians’  multi-faceted musical life. As soon as I began my field research, I  discovered that the notion of “the field” has changed because of the  prevalent usage of digital social media among the musicians of my study.  The Internet, is no longer just a means of communication between me and  my informants. Digital social media make up an important site of social  interactions and creative expressions. Not only that, it is the key to  social networking and community building for these musicians. Thus the  “field” of my investigation came to include the digital social terrain  that I navigate within the scope of dissertation research.\n\nThis post focuses on the map of one of the bands that I study: [The Kominas](http://komin.as/).  The Kominas is a South Asian American punk band that spawned in Boston,  now based in Philadelphia. Recombining sounds from the Boston  ska-and-crust-punk scene with 1970s Bollywood movies and Bhangra music  from their parents’ dusty tape collection, The Kominas evokes a  radically transnational sonic landscape. [[Example “Par Desi”](http://soundcloud.com/pocoparty/the-kominas-wild-nights-in-guantanamo-bay-05-par-desi)] Since 2006,   the band has been vigorously creating a translocal social terrain via  face-to-face interactions through touring and online social networking.  The Kominas’ do-it-yourself network is comprised of Muslim-,  South-Asian-identified, and other [taqwacore](http://en.wikipedia.org/wiki/Taqwacore)-inspired musicians, listeners, artists, filmmakers, and bloggers.\n\nIn this post, I ask:  What does The Kominas’ \"digital diaspora\" look  like geographically and spatially? First, I will describe the digital  methods I used to map this community.\n\n**Digital Methods – Web-scraping and Visualization**\n\nTo create such a map, I designed and executed out a two-phase method. Phase 1 is [web-scraping](http://en.wikipedia.org/wiki/Web_scraping),  the process of mining data from the Internet. This process entails  first, locating a source of useful geographic data, and then harvesting  this information programmatically. I was interested in two sets of data,  specifically: the physical location of the band’s performance tours;  and the self-reported (physical) location of the friends in an online  community. The first set of data, regarding performance locations, was  found on The Kominas’ official website. The information regarding friend  locations was found in its most complete form on the social networking  site [Myspace](http://myspace.com/thekominas).\n\nTo extract and process these data sets, with the help of [Joe Gilbert](http://www.scholarslab.org/contributors/jfg9x/),  I wrote a program using Ruby to parse out the relevant information in  the source code of the profile pages of The Kominas' Myspace friends.  The Kominas [as of April 2010] had close to 3,000 friends on Myspace.  These are all Myspace users who have requested to become friends with  The Kominas, or vice versa. Using [Mechanize](http://mechanize.rubyforge.org/mechanize/),  a Ruby gem, the program extracted all the geographically related text  from the Myspace profile pages of 2,867 friends. Using the [Geokit](http://geokit.rubyforge.org/), a ruby gem that implements the [Google Geocoder](http://www.coldfusionjedi.com/demos/googlegeocode/test.cfm), the program translated this information into a set of spatial coordinates, specifically, latitude and longitude.\n\nPhase 2 - geospatial visualization – is the process of turning the harvested data into a meaningful visualization. Using [OpenLayers](http://openlayers.org/),  an open-source mapping program, I created a dynamic map containing all  the points of the physical locations of the band’s Myspace friends and  performance tours. To contextualize the reading of the physical points, I  added various map layers. For example, I added a Google street map  layer to label the visualization with the proper name of countries and  cities. The rest of my efforts were spent to refine the map, to make it  readable and meaningful.\n\n\n## **[The Kominas' Digital Diaspora Map: GO!](http://beingwendyhsu.info/wp-content/uploads/2010/12/kominasmap3.html)**\n\n\n[![](http://beingwendyhsu.info/wp-content/uploads/2010/11/Cluster_continent_KominasMap-300x176.png)](http://beingwendyhsu.info/wp-content/uploads/2010/12/kominasmap3.html)\n\nTo interact with the map, click on the above image. This screenshot  shows the global distribution of The Kominas’ Myspace friends. The  reddish pink clusters represent the friend density in the respective  locales. The size of the cluster is an approximate representation of the  number of friends in one location.\n\nA baselayer of the world’s regions – marked by various shades of  green in the background - helps contextualize the friend distribution  across continental boundaries. At a macro level, this map articulates a  radically transnational and inter-continental distribution of friends.  Areas of high friend density include: North America, Europe, and Asia.  The story of translocality becomes more complex as we zoom in on the map  to get more geographic detail. In my dissertation, combining maps,  music analysis, and interviews, I examine how the members of The Kominas  position themselves geographically and ethnically _vis a vis _their vastly transnational world.\n\n**Questions and Concerns**\n\nThese maps tell a story, a particular kind of story that situates a  humanist study of a music-culture within a particular geographic  context. In the context of my dissertation, these maps add a spatial  texture to the understanding of the translocal social terrain of a  U.S.-based musicians of Asian descent. And the visualization process  helps me  to analyze the musicians' questioning of their sense  of  ethnic and national belonging  and  to situate the ethnographic  details  of my 24-month field research  within  a global context.\n\nHere are some general questions and concerns that I've  encountered in creating and using these dynamic maps. To express density  using a clustering pattern, I used an algorithm that balances point  density and readability, so that the contrast between the smallest and  the largest clusters is adjusted. In this case, a single-point cluster  can be seen and the largest concentration of the friends of the  northeast of the United States doesn’t dominate the entire map. This  presents the question, am I interested in representing the mathematical  reality of this friend community? Or is there some part of the story  that I was more interested in telling? Which level of detail is most  useful?\n\nI've discovered that these maps do not provide any answers to my  research questions. They, in fact, present an interpreted reality that  generate further useful questions. A map is certainly not a dissertation  chapter; but it provides a spatial and geographical context for the  musical and social experiences of the musicians in my study.\n\nHow I use these maps, of course, depends on the narrative that I want  to tell. At a very macro, global level, zoomed all the way out, these  maps can look very similar across bands: with large clusters in the  North American region, some clustering in Europe, and some but less in  other regions of the world. NOT SO INTERESTING…\n\nOf interest to me, in my dissertation, are the patterns of the band's  transnational connections to musicians and fans in Asia. What is the  band’s friend distribution in Asia? Is it useful to compare the  Asia-based friend distribution across band? I have shown two screenshots  of two bands' friend distribution in Asia. On the top is The Kominas.  On the bottom is [Kite Operations](http://www.kiteoperations.com/), a New-York-based noise rock band.\n\n[![](http://beingwendyhsu.info/wp-content/uploads/2011/01/Asia_041010-300x142.png)](http://beingwendyhsu.info/wp-content/uploads/2011/01/Asia_041010.png)\n\n[![](http://beingwendyhsu.info/wp-content/uploads/2011/01/KiteOperations_Asia_041010_rv-300x135.png)](http://beingwendyhsu.info/wp-content/uploads/2011/01/KiteOperations_Asia_041010_rv.png)\n\nThis comparison presents interesting results: These two maps show  that The Kominas, a South Asian American punk band has created a social  geography much more concentrated in South and Southeast Asia; whereas  Kite Operations, with 3/4 of the members being of Korean descent, has  stronger friend presence in East Asia, specifically in South Korea. The  difference in friend distribution shown by these images can provide a  sketch for illustrating a different \"Asia\" as created through the  cultural practice of \"friending\" on Myspace by American artists of Asian  descent.\n\n**Combining Digital Methods with “Conventional Methods”**\n\nThese digital methods seem to have an orthogonal relation to more  conventional ethnographic methods. Until these new digital methods  become accepted in ethnomusicology and cultural anthropology, I must  find a way to integrate the new with the old. [Yes, I have  thought-experimented with a set of [digitally engaged ethnographic methods](http://www.hastac.org/blogs/wendyhsu/long-due-introduction-toward-digital-ethnography).] Here are some ideas for this integration:\n\n\n\n\t\n  * Showing the map to the musician-informants: Asking them if  they are surprised by the results of my study. Asking them questions  about how they feel about these places in the world? Personal or musical  connections to these places?\n\n\t\n  * Toward a Geospatial Music Analysis: Many musicians that I  study are pre-occupied with geography. In their lyrics, they often  discuss being trapped or living in a limbo between two worlds. They talk  about their feelings regarding certain meaningful place and space in  their music. It’d be potentially fruitful to juxtapose the musical and  social geographies of a single band.\n\n\t\n  * Mapping genre/sonic differences: Here I suggest the  possibility of incorporating sonic qualities such as tempo, timbre,  volume, studio effects, and language/dialect into geospatial information  technology and system. Such a tool would be immensely powerful for the  study of the world’s music-cultures at the local and global level. For  example, the [World Musical Map project](http://www.musicalworldmap.org/) by Ozan Aksoy based at the [New Media Lab](http://www.newmedialab.cuny.edu/) at the Graduate Center of CUNY explores the rupture between audio  boundaries and actual national borders. Another example is Lee Byron's  visualization of the [listening history on Last.FM](http://www.visualcomplexity.com/vc/project_details.cfm?id=460&index=17&domain=Music).\n\n\nHere's my attempt to start a digital (ethno)musicology. Are there any other takers?\n\n\n## **[The Kominas' Digital Diaspora Map: It's Your Turn. GO!](http://beingwendyhsu.info/wp-content/uploads/2010/12/kominasmap3.html)**\n\n\nTips:\n\n\n\n\t\n  * Double-click to zoom in on the map\n\n\t\n  * Upper-left: turn on/off various layers: Google Street/Satellite;  world's regions; Muslim-majority countries; clusters (friend density);  friends (individual points);** **gigs.\n\n\t\n  * Scroll on the map by clicking + holding + moving the cursor\n\n\n"},{"id":"2011-01-24-2011-12-grad-fellows","title":"2011-2012 Graduate Fellows' Program","author":"ronda-grizzle","date":"2011-01-24 06:52:45 -0500","categories":["Announcements","Grad Student Research"],"url":"2011-12-grad-fellows","content":"The Scholars’ Lab is proud to host, for the fourth year, a prestigious UVa Library Graduate Fellowship program in Digital Humanities.\n\nThis fellowship is designed to support advanced graduate students doing significant and innovative work in DH by making resources and expertise available in all areas of digital humanities research -- from content creation and data management, to advice on intellectual property issues. Funding for the Fellowship program was established by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew & Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities.\n\nAmong other projects, past Scholars' Lab fellows have used Ruby on Rails and MySQL to create a database of the Early American foreign service, GIS tools to explore geospatial patterns of pipe development in Native American settlements, computational linguistic techniques to evaluate the authorship of a Victorian novel and to explore patterns in early Biblical apocalyptic literature, and have used web APIs for social networking sites to explore Asian-American indie rock culture. More information about our Fellows can be found on the [community page](http://www2.lib.virginia.edu/scholarslab/about/people.html) at the Scholars' Lab website.\n\nApplication information and instructions for the fellowship program are available from [the Scholars' Lab website](http://www2.lib.virginia.edu/scholarslab/about/fellowship.html).\n\nApplication packets are due no later than March 1, 2011.\n"},{"id":"2011-01-31-putting-american-community-survey-data-to-work","title":"Putting American Community Survey Data to Work","author":"chris-gist","date":"2011-01-31 05:33:50 -0500","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"putting-american-community-survey-data-to-work","content":"This past week, I read an [article](http://www.wired.com/autopia/2010/10/more-people-biking-to-work/) that claims the number of people \"getting around\" by bicycle is steadily growing. The article references the American Community Survey (ACS) and the League of American Bicyclists (LAB). Considering I am a certified instructor from the LAB, I wanted to check the data for myself (and map it). <!-- more -->\n\nI originally went to [Social Explorer](http://www.socialexplorer.com) (SE) to view and map the data. While SE allowed me to easily view data and make some quick thematic maps, it has some limitations. SE only allows you to map one attribute at a time and does not allow for custom attributes (like the rate change in bicycle riding between 2005 and 2009). Also SE only goes back to 2006 for the ACS.  My journey next took me to [American FactFinder](http://factfinder.census.gov) (AFF), the U.S. Census's data portal.\n\nIn AFF, I was able to get all the transportation survey data and decided to map rate changes in mass transit, bicycling and walking.  I had to manipulate the downloaded Excel file to make it \"GIS ready.\"\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2010/10/fullMap150.png)\n\n\n\n\nThere are some interesting things going on within this map.  First, why do a majority of the counties have no data?  The ACS only surveys communities with greater than 65K in population.  Therefore, a majority of U.S. counties are not included in the ACS.  Several counties have data for either 2005 or 2009 but rate change can only be calculated for counties with both.  Secondly, how can adjacent counties have such wild differences in rates of change for bicycle usage?  This question is much tougher to answer.  However, the most likely issue is inaccuracies in the ACS survey results stemming from small sample sizes and differences in the way questions are asked.  The LAB article does mention these issues in passing.\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2010/10/fullMapMass150.png)](http://www.scholarslab.org/geospatial-and-temporal/putting-american-community-survey-data-to-work/attachment/fullmapmass150/)\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2010/10/fullMapWalk150.png)](http://www.scholarslab.org/geospatial-and-temporal/putting-american-community-survey-data-to-work/attachment/fullmapwalk150/)\n\n\n\n\nKeeping in mind the classification bins are not uniform across these maps, I think there may be some interesting parallels between counties that have lost mass transit and gained walkers/bicyclists, etc.\n\n\n\n\nI made a few more maps showing the density of cyclists in 2005 and 2009 at a larger scale.\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2010/10/bikeVA.png)](http://www.scholarslab.org/geospatial-and-temporal/putting-american-community-survey-data-to-work/attachment/bikeva/)\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2010/10/bikeNEcoor.png)](http://www.scholarslab.org/?attachment_id=)\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2010/10/bikeCO.png)](http://www.scholarslab.org/geospatial-and-temporal/putting-american-community-survey-data-to-work/attachment/bikeco/)\n\n\n\n\nIf you would like more information about the U.S. Census, the ACS, Social Explorer, GIS or bicycling, please contact me at cgist[at]virginia.edu.  Larger version of the maps are also available.\n"},{"id":"2011-02-03-spring-2011-newsletter","title":"Spring 2011 Newsletter","author":"ronda-grizzle","date":"2011-02-03 09:35:08 -0500","categories":["Announcements"],"url":"spring-2011-newsletter","content":"Our Spring '11 ([PDF](http://tinyurl.com/SLabSpring11)) newsletter is now available for download. Our newsiest newsletter yet includes articles about employment opportunities in the SLab, the applications deadline for our Graduate Fellows program, SLab staff adventures, SLab collaborations, and THATcamp Virginia 2010. Download your copy today!\n\n[http://tinyurl.com/SLabSpring11](http://tinyurl.com/SLabSpring11)\n"},{"id":"2011-02-04-web-applications-specialist","title":"Are you our new Web Applications Specialist?","author":"wayne-graham","date":"2011-02-04 10:51:07 -0500","categories":["Announcements"],"url":"web-applications-specialist","content":"Are you an enthusiastic Web developer with an interest in the humanities or cultural heritage? UVa Library seeks a Web Applications Specialist to help develop software in our internationally-recognized [Scholars' Lab](http://lib.virginia.edu/scholarslab). The ideal candidate is detail-oriented, eager to work collaboratively, and stays involved with the latest Web and digital humanities technologies.\n\nWe're seeking someone passionate about tackling technical problems in the digital humanities – preferably a person with both a technical and liberal arts background, prepared to build next-generation DH interfaces and tools. Our new Web Application Specialist will also be able to take advantage of the “20% time” that all Department of Digital Research & Scholarship faculty and staff are granted to pursue professional development and their own (often collaborative) R&D projects. This is a full-time, permanent position at UVa.\n\n\n### Web Applications Specialist\n\n\nAs a Web Applications Specialist reporting to the Head of R&D for the Scholars' Lab, you will be responsible for building, testing, and debugging code, developing documentation, and assisting at troubleshooting. You should possess an attention to detail and a high level of accountability and responsibility. We're looking for someone who enjoys technical challenges, likes to figure out how things work, and stays involved in the latest Web and digital humanities technologies. You will need to be able to fit in to a creative and collaborative environment. Want to join us as we create great scholarly interfaces?\n\n\n\n\t\n  * 2-3 years of experience developing web applications with tech skills demonstrated via one or more of the following:\n\n\t\n    * your open source work\n\n\t\n    * your github repository\n\n\t\n    * your awesome blog\n\n\t\n    * your code samples from side projects\n\n\t\n    * or your production web site (handling real traffic)\n\n\n\n\n\t\n  * knowlege of SQL, git, svn, HTML, CSS, Javascript\n\n\t\n  * ability to work with technical and non-technical collaborators thanks to your great communications skills\n\n\t\n  * experience with software development (maybe even including Agile methodologies)\n\n\n\n\n### Duties and Responsibilities:\n\n\n\n\n\n\n\n\n\t\n  * Build, test, and debug code\n\n\t\n  * Write test cases\n\n\t\n  * Estimate coding projects\n\n\t\n  * Provide consultation on collaborative projects\n\n\t\n  * Develop documentation\n\n\t\n  * Assist in the debugging and system troubleshooting for existing software written in a variety of languages and platform\n\n\n\n\n\n\n\n### Qualifications:\n\n\n\n\n\n\t\n  * 1+ years full-time experience with web development (Rails and PHP preferred)\n\n\t\n  * 2+ years experience of standards compliant HTML, CSS, and Javascript\n\n\t\n  * Javascript skills (AJAX, JQuery or similar JS framework)\n\n\t\n  * Experience with Test Driven Development (Shoulda, RSpec, PHPUnit)\n\n\t\n  * Experience with relational database management systems (MySQL, Postgresql)\n\n\t\n  * Familiarity with version control systems\n\n\t\n  * Understanding of software life cycle\n\n\t\n  * Strong foundation in OO programming and practices\n\n\t\n  * Experience with [Omeka](http://omeka.org) a plus\n\n\n\n\n### If this sounds like you...\n\n\nWe encourage you to [APPLY FOR THIS JOB](http://jobs.virginia.edu/applicants/Central?quickFind=63332 ). Salary is commensurate with experience, and expected to range between approximately $43,500 and $75,500 per annum. We’re looking to fill this position quickly, so please don’t delay!\n\nConsideration of applications will begin immediately and continue until the position is filled.\n"},{"id":"2011-02-10-dh-dev-picks","title":"DH Dev Picks","author":"wayne-graham","date":"2011-02-10 11:30:07 -0500","categories":["Research and Development"],"url":"dh-dev-picks","content":"Part of mission here at the Scholars' Lab is provide guidance for folks working on digital projects. As such, I do my best to keep up with trends in software development. For a while I've just been adding these to my [delicious account](http://www.delicious.com/wsgrah) to make it a bit easier to find references later. However, recent trends in the way Yahoo! is handling its properties (specifically with delicious), made the think a bit harder about this approach and I thought I might try something else. The idea here is to have a regular series of posts with links that I find interesting and I think are are of some utility to other DH developers.\n\n\n\n\t\n  * [Awesome Fontstacks](http://awesome-fontstacks.com/): If you're a developer who does design, or a designer who codes, browser support for beautiful fonts opens a lot of doors for creating compelling presentations of your work.\n\n\t\n  * [Using Git to manage a web site](http://toroid.org/ams/git-website-howto): Web workflows vary widely, but wouldn't it be nice to use your SCM for deploying your web application? This post describes a method of using git to deploy web pages.\n\n\t\n  * [Getting Comfortable With SSH](http://jcsalterego.github.com/2011/02/04/getting-comfortable-with-ssh.html): I use keys, aliases, and remote tunneling as part of my workflow, and this post provides a nice introduction to all of these. I would be remiss, however, if I didn't point out something they highlight on the post: this method is pretty insecure. After you get your feet wet, be sure to check out a much more secure method over on [Github](http://help.github.com/working-with-key-passphrases/)\n\n\t\n  * [My Github Resume](http://resume.github.com/): Use github? This site does a nice job creating a resume/cv of your code commits\n\n\t\n  * [31 CSS Code Snippets To Make You A Better Coder](http://www.designyourway.net/blog/resources/31-css-code-snippets-to-make-you-a-better-coder/): If you're like me, I can't keep browser-specific differences in the implementation of the CSS spec straight in my head. These code snippets help. Add them to a program like [Snippely](http://code.google.com/p/snippely/), [SnipMate](http://www.vim.org/scripts/script.php?script_id=2540),\nor your favorite IDE, you can throw them in your code as needed.\n\n\t\n  * [960 Grid on jQuery-Mobile](http://jeromeetienne.github.com/jquery-mobile-960/): I'm a fan of 960 grid and jQuery. While these techniques for use on mobile devices has a way to go, this technique does show some promise.\n\n\t\n  * [Maze Algorithms](http://weblog.jamisbuck.org/): Great set of posts on solving tough problems algorithmically. These are also nice if you're not familiar with the pseudo-code commonly used to express discrete algorithmic logic. Very clear explanation of the problem and the approach used to solve the issues.\n\n\n"},{"id":"2011-02-15-scholars-lab-and-chnm-partner-on-omeka-neatline","title":"Scholars' Lab and CHNM Partner on \"Omeka + Neatline\"","author":"admin","date":"2011-02-15 08:58:57 -0500","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"scholars-lab-and-chnm-partner-on-omeka-neatline","content":"The[ Scholars' Lab](http://lib.virginia.edu/scholarslab/) at the University of Virginia Library and the Center for History and New Media ([CHNM](http://chnm.gmu.edu)) at George Mason University are pleased to announce a collaborative \"Omeka + Neatline\" initiative, supported by $665,248 in funding from the [Library of Congress](http://loc.gov/).\n\nThe Omeka + Neatline project's goal is to enable scholars, students, and library and museum professionals to create geospatial and temporal visualizations of archival collections using a [Neatline toolset](/research/neatline/) within CHNM's popular, open source [Omeka](http://omeka.org/) exhibition platform.  Neatline, a \"contribution to interpretive humanities scholarship in the visual vernacular,\" is a project of the UVa Library Scholars' Lab, originally bolstered by a Start-Up Grant from the [Office of Digital Humanities](http://neh.gov/odh) at the National Endowment for the Humanities. Omeka is an award-winning web-publishing platform for the display of cultural heritage and scholarly collections and exhibits, funded by the Institute of Museum and Library Services, Alfred P. Sloan Foundation, Andrew W. Mellon Foundation, and Samuel H. Kress Foundation.\n\nThis two-year initiative will allow CHNM and the Scholars' Lab to expand and regularize a partnership that developed informally between the two centers over the course of the past year.  Collaboration has already resulted in improvements to the core functionality of Omeka by CHNM and has led the Scholars' Lab to produce [a number of prototype plugins](/research/omeka-plugins/) making Omeka a more attractive and viable option for scholarly partnerships with larger libraries and cultural heritage institutions. These include: improved data import (including EAD, a common archival standard); Solr-powered searching and browsing; and Fedora-based repository services.  Further development will improve existing plugins, add preservation workflows, and refine the Neatline toolset for integration and sophisticated editing and scholarly annotation of historical maps, GIS layers, and timelines. Enhancements to Omeka's core APIs, improved documentation, regular \"point\" releases, and a new Exhibit Builder will strengthen Omeka's already large and robust user and developer communities.\n\nOmeka + Neatline is one of six contract awards made by the Library of Congress in a program that aims both to improve the Library's own content management and content delivery infrastructure and to contribute to collaborative knowledge sharing among broader communities concerned with the sustainability and accessibility of digital content. In July of 2010, the Library of Congress targeted approximately $3,000,000 toward Broad Agency Announcements covering three areas of research interest related to these goals. Technical proposals were openly solicited from expert, multi-disciplinary communities in both academic and commercial settings in three areas: Ingest for Digital Content, Data Modeling of Legislative Information, and Open Source Software for Digital Content Delivery.\n\nIn addition to guiding software development work at the Scholars' Lab and CHNM, project directors [Tom Scheinfeldt](http://foundhistory.org/) and [Bethany Nowviskie](http://nowviskie.org/) will use the Omeka + Neatline project as an opportunity to document and disseminate a model for open source, developer-level collaborations among library labs and digital humanities centers.\n"},{"id":"2011-02-17-dh-dev-links-for-214","title":"DH Dev Links for 2/14","author":"wayne-graham","date":"2011-02-17 04:41:15 -0500","categories":["Research and Development"],"url":"dh-dev-links-for-214","content":"There seem to be quite a few links this week loosely grouped around interface design, with some other geeky goodness mixed in.\n\n\n\n\t\n  * [Isotope jQuery Plugin](http://isotope.metafizzy.co/): Seriously one of the coolest jQuery plugins I've seen in a while. When I get some time, I plan to spend some time with this plugin and Solr results for a couple of projects...\n\n\t\n  * [Duke, Exeter, and Howe I Wish Computer Science was Taught](http://andrewcbrown.com/2011/02/07/duke-exeter-and-how-i-wish-computer-science-was-taught/): Good post on some of the shortcomings of the current state of computer science education.\n\n\t\n  * [Queuing Theory Books Online](http://web2.uwindsor.ca/math/hlynka/qonline.html): Eventually you'll find a problem that could use some queuing. This is a great list of books that give a great theoretical handling of the subject.\n\n\t\n  * [Here is the Biggest Mistake You Will Make on Amazon EC2](http://www.edukatr.com/here-is-the-biggest-mistake-you-will-make-on-amazon-ec2/): With the popularity of cloud computing, there are some things you need to remember when deploying to platforms like EC2. Take away from the article...be careful rebooting server instances!\n\n\t\n  * [What is the On-Boarding Process for new Employees at Twitter](http://www.quora.com/Twitter-Inc-company/What-is-the-on-boarding-process-for-new-employees-at-Twitter): While most DH centers have pretty small teams, I'm always interested in how bigger companies bring new developers in and cultivate their cultural beliefs. This is a nice piece on how Twitter brings new people in.\n\n\t\n  * [How to Use Dropbox to Organize Your Startup's DH Project's Documents](http://blog.revenueloan.com/2011/02/07/howto-use-dropbox-to-organize-your-startups-documents/): Just replace \"startup\" in this article with \"your project\" and this has some good tips for keeping your project documents organized.\n\n\t\n  * [Pull Request Diff Comments](https://github.com/blog/785-pull-request-diff-comments): Github now allows you to comment on individual lines on pull requests! Very useful for code reviews.\n\n\t\n  * [Landing Page Best Practices](http://visualwebsiteoptimizer.com/split-testing-blog/landing-page-best-practices/): Remember Flash splash pages? This post has some great tips on designing landing pages to engage users in the content of your site.\n\n\t\n  * [IE 9 Beta](http://windows.microsoft.com/ie9): Let's face it, a lot of people don't have access to other browsers for various reasons (can't install software on work computer). Even though IE is painful, Microsoft swears IE 9 will be better...\n\n\t\n  * [Better grids: Lessons learned from Design for Developers](http://robots.thoughtbot.com/post/3217276323/better-grids-lessons-learned-from-design-for): Thoughtbot is putting on a series of training sessions to teach design principals to developers. Take away on this...3 and 4 column grid systems provide a solid foundation that introduces clarity to your web layouts.\n\n\t\n  * [Documentation is Freaking Awesome](http://warpspire.com/talks/documentation/): Enough said.\n\n\t\n  * [Paul Irish on HTML 5 Boilerplate](http://ontwik.com/html5-2/paul-irish-on-html5-boilerplate/): Anyone who's looked at any HTML I've done in the last 6 months will notice that it starts with HTML 5 Boilerplate (usually with some 960gs). This is a nice presentation by Paul on its development and use.\n\n\t\n  * [Reuse your JavaScript as jQuery Plugins](http://www.engineyard.com/blog/2011/reuse-your-javascript-as-jquery-plugins/): Nice post on converting JS scripts in to jQuery plugins using a couple of different patterns. Definitely worth a look if you are writing any Javascript.\n\n\t\n  * [How to use Dropbox as a git server](http://www.intermediaware.com/blog/1085): Can't/Don't want to use github as your SCM? This tutorial shows how you might use Dropbox to replicate your git repo (you'll need to adjust the paths if you're a Windows user).\n\n\t\n  * [Announcing TileMill: A Modern Map Design Studio Powered by Open Source](http://developmentseed.org/blog/2011/feb/16/announcing-tilemill-modern-map-design-studio-powered-open-source): The folks at DevelopmentSeed are doing some amazing work. If you have need to create some maps, TileMill looks like a nice piece of software to get you running without needing to install a large server infrastructure!\n\n\t\n  * [Hacked Floppy Disk Plays Starwars Music](http://vodpod.com/watch/799407-hacked-floppy-disk-plays-star-wars-music-as-awesome-as-it-): Just for fun\n\n\n"},{"id":"2011-02-23-head-of-outreach-consulting","title":"Are you our new Head of Outreach & Consulting?","author":"bethany-nowviskie","date":"2011-02-23 04:37:50 -0500","categories":["Announcements"],"url":"head-of-outreach-consulting","content":"We're excited to announce a fantastic job opportunity: a faculty-level position at the University of Virginia Library with responsibility for running public services in our growing and well-respected [Scholars' Lab](http://lib.virginia.edu/scholarslab).\n\n\n\n### Head of Outreach and Consulting\n\n\nAre you an excellent and enthusiastic communicator with a strong background in technological approaches to humanities and social science scholarship? UVa Library seeks a **Head of Outreach and Consulting** to coordinate public services in our internationally-recognized Scholars' Lab. The ideal candidate is detail-oriented, eager to work collaboratively, and able to represent -- to internal and external audiences -- UVa Library’s involvement in the digital humanities. This supervisory position is responsible for day-to-day operations in the Scholars’ Lab (overseeing staff dedicated to geospatial, data-driven, and text-based research consultation) and plays a key role in our program for [Graduate Fellows in the Digital Humanities](http://www2.lib.virginia.edu/scholarslab/about/fellowship.html).  This position reports to the Library's Director of Digital Research & Scholarship.\n\nThe position is for a true “hybrid\" or \"alternative academic” — someone with a strong service ethic and sense of hospitality who can also collaborate as a true intellectual partner with faculty and graduate students, enabling next-generation DH in the Scholars’ Lab. The Head of Outreach and Consulting should also be able to take good advantage of the “20% time” that all Department of Digital Research & Scholarship faculty and staff are granted to pursue professional development and their own (often collaborative) R&D; projects.  This is a full-time, permanent general faculty position at UVa.\n\n\n\n### Primary Responsibilities:\n\n\nOutreach and Scholars’ Lab Management: oversight of day-to-day operations and consulting services, coordination of intellectual programming (speaker series, workshops, etc.) and organization of Graduate Fellows program; Scholarly Projects Consultation: development of intake process, workplans, and MoUs for new scholarly collaborations, in consultation with Scholars’ Lab R&D; Education and Professional Development: pursuit of own scholarly R&D; agenda related to the humanities or social sciences and publication of results and/or presentation at appropriate conferences.\n\n\n\n### Specialized Knowledge and Skills:\n\n\nWorking knowledge of digital humanities technologies and directions.  Strong public service orientation and interest in guiding faculty projects from conceptualization to the formulation of workable project plans.  Excellent communication skills, including the ability to present complex technical information to a generalist audience and a clear understanding of the perspectives and needs of the professoriate. Previous experience in public service in an academic library setting and experience in scholarly research, writing, and web development preferred.\n\n**Education:**  Masters Degree or PhD in humanities or social sciences.\n**Experience:** 3 years experience with project management and/or hands-on development of digital projects related to digital humanities or cultural heritage.\n**Salary and Benefits:**  Salary commensurate with experience and competitive depending on qualifications. General faculty status. Excellent benefits, including 22 days of vacation; TIAA/CREF and other retirement plans along with generous funding for travel and professional development.\n\n\n\n### If this sounds like you...\n\n\nwe invite you to [APPLY FOR THE JOB!](http://jobs.virginia.edu/applicants/Central?quickFind=63447)\n\nConsideration of applications will begin immediately and continue until the position is filled.\n\n"},{"id":"2011-02-28-speaker-series-jeremy-boggs","title":"Speaker Series: Jeremy Boggs","author":"ronda-grizzle","date":"2011-02-28 06:48:28 -0500","categories":["Announcements","Digital Humanities"],"url":"speaker-series-jeremy-boggs","content":"Jeremy Boggs, last semester's scholar-in-residence at the [Scholars' Lab](http://lib.virginia.edu/scholarslab), will give a talk at **2pm** on Thursday, **March 3**. Boggs is Associate Director of Research at the [Center for History & New Media](http://chnm.gmu.edu) at George Mason University, as well as a Ph.D. candidate in GMU's Department of History.\n\nDuring his tenure at CHNM, where he was a founding member of the [THATCamp](http://thatcamp.org/) movement, Boggs has been deeply involved in developing conversations about the definition of Digital Humanities and how best to prepare scholars to work on digital projects. His Scholars' Lab talk, entitled **A Plea for Open Digital Humanities Work: or, A DH Grad Student Reflects on Years of 'Study'**, argues that defining a canonical set of skills for digital humanists is ultimately a fruitless endeavor. Instead, Boggs urges the development and promotion of digital humanities tools that are open, well-documented, and learnable, allowing scholars to create projects from these tools that carry these same characteristics. If the digital humanities community can promote a culture of open projects and toolsets, scholars will be able to explore and expand their skills by learning from collleagues' work.\n\n[Dr. Bethany Nowviskie](http://nowviskie.org) will act as respondent for this talk.\n[caption id=\"attachment_1346\" align=\"alignnone\" width=\"300\" caption=\"Image released under Creative Commons license by Flickr user John Fawcett\"][![](http://www.scholarslab.org/wp-content/uploads/2011/02/JohnFawcett-boggs-300x300.jpg)](http://www.scholarslab.org/digital-humanities/speaker-series-jeremy-boggs/attachment/johnfawcett-boggs/)[/caption]\n"},{"id":"2011-03-04-workshop-data-visualization","title":"Workshop: Data Visualization","author":"ronda-grizzle","date":"2011-03-04 07:37:38 -0500","categories":["Announcements"],"url":"workshop-data-visualization","content":"### Workshop: An Introduction to VIDI\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/03/qthomasbower_vizwkshop-224x300.jpg)](http://www.scholarslab.org/slab-events/workshop-data-visualization/attachment/qthomasbower_vizwkshop/)Wednesday, **March 9** at **10:00 a.m. – Noon**\nin Alderman Library, Room 317\n\nAaron Presnall and Dante Chinni of [The Jefferson Institute](http://www.jeffersoninst.org/)\nPresented by the Scholars’ Lab, SHANTI & The Department of Politics\n\nGot data, but not certain how it might be displayed digitally with its particular numeric, spatial and temporal attributes? This workshop promises an introduction to [VIDI](http://www.dataviz.org/), a user friendly suite of [Drupal](http://drupal.org/) modules for displaying, graphing and mapping data, with illustrations from the [Patchwork Nation](http://www.patchworknation.org/) project.\n"},{"id":"2011-03-10-welcoming-eric-rochester","title":"Welcoming Eric Rochester!","author":"bethany-nowviskie","date":"2011-03-10 06:04:41 -0500","categories":["Announcements"],"url":"welcoming-eric-rochester","content":"[![](http://www.scholarslab.org/wp-content/uploads/2011/03/DCP_1060-Medium-183x300.jpg)](http://www.scholarslab.org/announcements/welcoming-eric-rochester/attachment/dcp_1060-medium/)The [Scholars' Lab](http://lib.virginia.edu/scholarslab) is very pleased to welcome Dr. Eric Rochester as the new Senior Developer on our R&D team!\n\nEric is an accomplished computational linguist and digital humanities scholar and developer. His past work includes appointments with the Oxford University Press and Georgia's Linguistic Atlas projects, as well as consultancies and programming positions at a number of technology firms. Eric's doctorate is in English from the University of Georgia, where he concentrated on medieval literature and wrote a dissertation entitled _Schwa: A Dictionary Pronunciation Database System_.  _Schwa_ examines the production of lexicographical pronunciations, both from a technical and a theoretical perspective, and establishes a set of best practices, well-grounded in the history and present state of dictionary pronunciation. To test his research and theorizing, Eric implemented a lexicographical pronunciation system -- and he'll be bringing that brand of thoughtful, iterative scholarly software development to his new role in the SLab.\n\nYou can find more information about Eric at [his website](http://www.ericrochester.com/) and follow [@erochest](http://twitter.com/erochest) on Twitter. He starts on Monday, and we're thrilled that he and his wife Jackie and daughter Melina are joining the Scholars' Lab family!\n"},{"id":"2011-03-18-unsworth-talk","title":"Unsworth to speak at UVa","author":"bethany-nowviskie","date":"2011-03-18 14:17:25 -0400","categories":["Announcements"],"url":"unsworth-talk","content":"The UVa Digital Humanities Speaker Series presents:\n\n\n### John Unsworth on \"Idiosyncrasy at Scale: Data Curation in the Humanities\"\n\n\nFriday, March 25\n3:00pm (reception follows)\nSouth Lawn Auditorium (NAU 101)\n\nThis talk is co-sponsored by [IATH](http://iath.virginia.edu), [SHANTI](http://uvashanti.org), and the [Scholars' Lab](http://lib.virginia.edu/scholarslab)\n\n\n\n### Abstract:\n\nUnsworth will argue that, in the past, the humanities have been characterized by data sets that are significantly smaller than those in the sciences, but also significantly more idiosyncratic. Now we face a situation where humanities data, at least in textual form, and soon enough in other forms, is large-scale. Can it still be idiosyncratic? What are the trade-offs between the requirements of curation and the needs of the users for whom that data is curated? What models or initiatives are out there that could help us grapple with idiosyncrasy at scale?\n\n[John Unsworth](http://www3.isrl.illinois.edu/~unsworth/) is dean of the Graduate School of Library and Information Science, University of Illinois, Urbana-Champaign.  John was the founding director of IATH at UVa, and remains one of the most prominent figures in the field of digital humanities.\n"},{"id":"2011-03-25-welcoming-jeremy-boggs","title":"Welcoming Jeremy Boggs!","author":"bethany-nowviskie","date":"2011-03-25 09:05:43 -0400","categories":["Announcements","Digital Humanities"],"url":"welcoming-jeremy-boggs","content":"We're thrilled to announce that Jeremy Boggs will be joining the [Scholars' Lab](http://lib.virginia.edu/scholarslab) and the staff of UVa Library's Digital Research & Scholarship department this June, in the role of Humanities Design Architect.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/03/jeremy.jpg)](http://www.scholarslab.org/announcements/welcoming-jeremy-boggs/attachment/jeremy/)[Jeremy](http://clioweb.org) comes to us from the wonderful [Center for History and New Media](http://chnm.gmu.edu/) (CHNM) at George Mason University, where he serves as Associate Director of Research and as development manager for [Omeka](http://omeka.org), an open-source web publishing system for cultural heritage collections.  Jeremy was also one of the originators of [THATCamp](http://thatcamp.org), the popular technology and humanities \"unconference.\"\n\nHe is completing his doctoral dissertation at Mason, \"The Designing Historian,\" and the UVa community has benefited from Jeremy's presence this year as a visiting scholar in the Scholars' Lab.\n\nIn his new role, Jeremy will be advising faculty and grad students on design and user experience aspects of their digital projects in both the humanities and humanistic social sciences and contributing to ongoing work in Scholars' Lab R&D;, with special attention to \"[Neatline](http://www.scholarslab.org/announcements/scholars-lab-and-chnm-partner-on-omeka-neatline/),\" a set of tools we're developing with funding from the Library of Congress and in collaboration with CHNM.\n\nWe look forward to welcoming Jeremy and his wife Jill to the Scholars' Lab family this summer!\n"},{"id":"2011-03-28-welcoming-our-201112-graduate-fellows","title":"Welcoming our 2011/12 Graduate Fellows","author":"ronda-grizzle","date":"2011-03-28 09:33:26 -0400","categories":["Announcements","Grad Student Research"],"url":"welcoming-our-201112-graduate-fellows","content":"The Scholars' Lab is pleased to welcome our 2011/12 cohort of [Graduate Fellows in Digital Humanities](http://www2.lib.virginia.edu/scholarslab/about/fellowship.html):\n\n**Gabriel Hankins** from the Department of English, **Randi Lewis** from the Corcoran Department of History, and **Edward Triplett** from the McIntire Department of Art.\n\nGabriel's fellowship project will explore global modernisms by \"modeling the network of textual relationships between global modernist writers and the social and institutional networks of the League of Nations project.\"\n\nRandi will use GIS technology to create digital maps showing \"the changing patterns of Salem trade and the global reach of Salem's maritime economy\" during the Early Republic.\n\nEdward will create a database of geo-referenced fortress-monasteries and churches created or occupied by military orders in medieval Spain to begin the process of exploring \"how the military orders created a fortified border between Christianity and Islam while displaying varying degrees of cultural permeability across this same border.\"\n\nPlease plan to join us at the September 2011 Digital Therapy Luncheon where we'll formally introduce our new Fellows, and they will present a short introduction to their projects.\n"},{"id":"2011-04-01-scholars-lab-becomes-lab-for-digital-bs","title":"Scholars' Lab becomes Laboratory for Digital Byzantine Sigillography","author":"bethany-nowviskie","date":"2011-04-01 06:55:01 -0400","categories":["Announcements"],"url":"scholars-lab-becomes-lab-for-digital-bs","content":"FOR IMMEDIATE RELEASE:\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/04/1bbcb4e21269fc72.jpg)](http://www.scholarslab.org/announcements/scholars-lab-becomes-lab-for-digital-bs/attachment/1bbcb4e21269fc72/)Through the generosity of an anonymous donor, the [Scholars' Lab](http://lib.virginia.edu/scholarslab) at the [University of Virginia Library](http://lib.virginia.edu/) is proud to announce a change in name and concentration.  Effective April 1st, the SLab (which has hitherto supported work in GIS, qualitatitive and quantitative analysis, and interpretive and textual scholarship in the humanities and social sciences) will sharpen its focus and be known as the _Laboratory for Digital Byzantine Sigillography_.\n\n\"Among major research libraries,\" said University Librarian Karin Wittenborg, \"UVa has made the most longstanding investments in Digital BS.  Digital BS has enriched the landscape of the humanities and social sciences immeasurably,\" she continued. \"It's just in the air in the former Scholars' Lab.  You can almost smell it!\"\n\nDepartment director [Bethany Nowviskie](http://twitter.com/nowviskie) agrees. \"At UVa Library, we look for \"shovel-ready\" projects in the digital humanities and social sciences.  In terms of Digital BS, scholars here have been piling it higher and deeper for decades.\"\n\nThe name change comes with a significant shift to teaching and training, collaborations with UVa faculty, and admissions criteria for the lab's Graduate Fellowships in Digital Humanities.  Current Graduate Fellow [Alex Gil](http://twitter.com/elotroalex) was reached for comment at his dusty carrel in the bowels of Alderman Library:  \"As a doctoral candidate in English, you might expect me to be concerned about this new focus on digital Byzantine sigillography.  Far from it!  My dissertation is just full of digital BS!  I'm ecstatic, and now plan to take an extra four to five years to immerse myself in it.\"  Former fellow and ethnomusicologist [Wendy Hsu](http://twitter.com/wendyfshu) will defend her dissertation in the Lab later this month, before taking up a Mellon postdoctoral fellowship at Occidental College.  She looks forward to \"spreading all this Virginia BS westward.\"\n\nOutreach & Training Specialist [Ronda Grizzle](http://twitter.com/ronda_at_uva) was careful to specify that the Lab will retain its two internal units: Consultation Services and Research & Development.  One representative of Consultation Services, [Kelly Johnston](http://twitter.com/kellygjohnston), was mucking around in the field and unavailable for comment beyond the following text message, sent to Grizzle repeatedly: \"Sphragistics FTW.\"  Meanwhile, [Wayne Graham](http://twitter.com/wayne_graham), head of BS R&D, looks forward to fresh projects: \"We intend to give a whole new meaning to the word 'vaporware.'\"  New R&D hires [Eric Rochester](http://twitter.com/erochest) (Senior Developer) and [Jeremy Boggs](http://twitter.com/clioweb) (Design Architect) expressed excitement at the opportunity to \"ride the wave of digital BS at UVa.\"\n\nSome may see UVa Library's new, exclusive concentration on digital Byzantine sigillography as a corrective to the broadening into meaninglessness of the phrase \"digital humanities.\"  Nowviskie disagrees: \"We're an open community of practice.  Everyone is welcome under the big tent of Digital BS.\"\n"},{"id":"2011-04-14-project-launch-spatial-humanities","title":"project launch: \"Spatial Humanities!\"","author":"bethany-nowviskie","date":"2011-04-14 12:24:12 -0400","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"project-launch-spatial-humanities","content":"Over the past two years, with generous support from the National Endowment for the Humanities, the Scholars’ Lab at the University of Virginia Library has hosted an _Institute for Enabling Geospatial Scholarship_. Today we're pleased to announce the launch of \"Spatial Humanities,\" a community-driven resource for place-based digital scholarship:\n\n[http://spatial.scholarslab.org/](http://spatial.scholarslab.org/)\n\nThis site responds to needs identified in conversation with our 21 Institute faculty members and 56 participants (humanities scholars, software developers, and map & GIS librarians).  It includes:\n\n\n\n\t\n  * an evolving, crowdsourced catalog of research resources, projects, and organizations;\n\n\t\n  * a set of framing essays on the spatial turn across the disciplines by Dr. Jo Guldi of the Harvard Society of Fellows;\n\n\t\n  * GIS-related feeds from Q&A sites and other forms of social media;\n\n\t\n  * and a peer-reviewed, occasional publication for step-by-step tutorials in spatial tools and methods.\n\n\nPlease help us keep this resource current by contributing to it! You can:\n\n\t\n  * use Zotero to freely upload research citations, projects, and links to groups;\n\n\t\n  * contribute your own tutorials and helpsheets in “Step By Step” format for peer review and formal publication;\n\n\t\n  * adopt the #geoinst hashtag on Twitter and Delicious;\n\n\t\n  * ask related questions and offer help on DH Answers or the GIS Stack Exchange;\n\n\t\n  * and post your commentary on the essays we’ve shared.\n\n\nLearn more about our NEH Institute:\n\n[http://spatial.scholarslab.org/about/](http://spatial.scholarslab.org/about/)\n\nand about how you can contribute to the \"Spatial Humanities\" site:\n[\nhttp://spatial.scholarslab.org/contribute/]( http://spatial.scholarslab.org/contribute/)\n\nMany thanks to the NEH, the staff of the Scholars’ Lab, our Institute advisory board and faculty, and the scores of Institute participants and fellows who helped to define the project!\n"},{"id":"2011-04-19-announcement-bagitphp-library","title":"Announcement: BagItPHP Library","author":"eric-rochester","date":"2011-04-19 06:16:01 -0400","categories":["Announcements","Research and Development"],"url":"announcement-bagitphp-library","content":"The Scholars' Lab is pleased to announce the initial release of a PHP library implementing [BagIt 0.96](https://wiki.ucop.edu/display/Curation/BagIt). BagIt is a specification from the [Library of Congress](http://www.loc.gov/) for bundling and transmitting multiple files along with their meta-data. You can check out the project page at [http://github.com/scholarslab/BagItPHP/](http://github.com/scholarslab/BagItPHP/).\n\nOur work on BagItPHP stems from the open source [\"Omeka + Neatline\"](/2011/02/15/scholars-lab-and-chnm-partner-on-omeka-neatline/) project, a collaboration of the [Scholars' Lab](http://lib.virginia.edu/scholarslab) with the Roy Rosenzweig Center for History and New Media.  \"Omeka + Neatline\" is supported by the Library of Congress.\n\n\n\n#### Downloads\n\n\nYou can download the library either as a [ZIP](http://github.com/scholarslab/BagItPHP/zipball/master) or [tarball](http://github.com/scholarslab/BagItPHP/tarball/master), or you can clone the repo with git:\n\n[sourcecode language=\"bash\"]\ngit clone git://github.com/scholarslab/BagItPHP\n[/sourcecode]\n\n\n#### Use: Creating Bags\n\n\nTo create a bag, simply instantiate a new `BagIt` object with the name of a directory that doesn't exist, add files to it, and package it into a tarball with the name of the bag:\n\n[sourcecode language=\"php\"]\nrequire_once 'lib/bagit.php';\n\n$bag = new BagIt('./new-directory');\n\n$bag->addFile('./exhibit/index.html', 'index.html');\n$bag->addFile('./exhibit/imgs/1.png', 'imgs/1.png');\n$bag->addFile('./exhibit/imgs/2.png', 'imgs/2.png');\n\n$bag->package('./new-directory');\n// The bag package will be created named ./new-directory.tgz.\n[/sourcecode]\n\n\n#### Use: Reading Bags\n\n\nTo read a bag, simply open an existing back, validate it (optional), fetch remote resources, and iterate over the files, copying them or processing them in some other way.\n\n[sourcecode language=\"php\"]\nrequire_once 'lib/bagit.php';\n\n$bag = new BagIt('./existing-bag.zip');\n\n$bag->validate();\nif (count($bag->getBagErrors()) == 0) {\n    $bag->fetch->download();\n\n    foreach ($bag->getBagContents() as $filename) {\n        copy($filename, 'final/destination/' . basename($filename));\n    }\n}\n\n[/sourcecode]\n\nFor more information about the methods that are available, please see the documentation.\n\n\n#### Let Us Hear from You\n\n\nIf you're using this library or have any feedback on it, we'd love to hear from you! We are relying on the [GitHub issues tracker](https://github.com/scholarslab/BagItPHP/issues) for code feedback, so you can file bugs or other [issues there](https://github.com/scholarslab/BagItPHP/issues). If you have a more general question, feel free to post here.\n"},{"id":"2011-04-19-jeremy-boggs-a-plea-for-open-digital-humanities-work","title":"Jeremy Boggs: A Plea for Open Digital Humanities Work","author":"ronda-grizzle","date":"2011-04-19 08:58:54 -0400","categories":["Podcasts"],"url":"jeremy-boggs-a-plea-for-open-digital-humanities-work","content":"Hello there, Scholars' Lab fans!\n\nVisiting Scholar Jeremy Boggs spoke in the Scholars' Lab on March 3, giving a talk entitled **A Plea for Open Digital Humanities Work: or, A DH Grad Student Reflects on Years of 'Study'**.\n\nJeremy will join the faculty of the Scholars' Lab full time in June, as [our new Humanities Design Architect](/2011/03/25/welcoming-jeremy-boggs). We hope you'll enjoy the podcast of his talk.\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.6793487437/enclosure.mp3\"]\n"},{"id":"2011-04-25-unsworth-idiosyncrasy-at-scale","title":"John Unsworth, \"Idiosyncrasy at Scale\"","author":"admin","date":"2011-04-25 06:38:33 -0400","categories":["Podcasts"],"url":"unsworth-idiosyncrasy-at-scale","content":"### Idiosyncrasy at Scale: Data Curation in the Humanities\n\n\nOn March 25th, [John Unsworth](http://www3.isrl.illinois.edu/~unsworth/), Dean of the [Graduate School of Library and Information Science](http://www.lis.illinois.edu/) at the University of Illinois, Urbana-Champaign spoke as part of the UVa Digital Humanities Speaker Series.  This is a speaker series jointly sponsored by SHANTI, IATH, and the Scholars' Lab at UVa Library.\n\nIn this talk, Unsworth announced that Indiana University and the University of Illinois were soon to launch the [HathiTrust](http://www.hathitrust.org/about) Research Center -- which will develop \"cutting-edge software tools and cyberinfrastructure to enable advanced computational access to the growing digital record of human knowledge.\" A [formal press release](http://newsinfo.iu.edu/news/page/normal/18245.html) for the project has since appeared.\n\nAs usual, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://www.google.com/url?sa=t&source=web&cd=1&ved=0CBUQFjAA&url=http%3A%2F%2Fitunes.apple.com%2Fus%2Fitunes-u%2Fscholars-lab-speaker-series%2Fid401906619&rct=j&q=scholars%27%20lab%20itunes&ei=FI61TdiZNo-Dtge0g_3pDg&usg=AFQjCNGGTBvTY5QpL9aRCKh7rjEOtlLAUQ&sig2=KBrhIc1DK814RPqoAB85Tg&cad=rja).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.7592112318/enclosure.mp3\"]\n"},{"id":"2011-05-02-welcoming-david-mcclure","title":"Welcoming David McClure!","author":"wayne-graham","date":"2011-05-02 07:42:01 -0400","categories":["Announcements"],"url":"welcoming-david-mcclure","content":"[![](http://www.scholarslab.org/wp-content/uploads/2011/05/pic.jpg)](http://www.scholarslab.org/announcements/welcoming-david-mcclure/attachment/pic/)We're delighted to welcome our new Web Applications Specialist, [David McClure](http://twitter.com/#!/clured), to the Scholars' Lab R&D team.\n\nDavid graduated from Yale University with a degree in Humanities in 2009, and has been working as an independent web developer in San Francisco, New York, and Madison, Wisconsin for the last few years. David is the creator of [Public Poetics](http://publicpoetics.org/), an elegant PHP interface for collaborative, web-based commentary on poetry.  He characterizes this project as a design experiment in addressing \"a problem of content 'over-accumulation' that tends to plague many existing systems of textual annotation and commenting.\"  Not only does David have a strong aesthetic sense and background in web application development, he also has experience in computer graphics, having worked with graphics libraries while developing software for the Cognitive Science department at Yale.\n\nDavid is also an avid outdoorsman.  His essay \"[Mountain Magic on the Pyrenean High Route](http://www.backpackinglight.com/cgi-bin/backpackinglight/pyrenean.html)\" was published in _Backpacking Light_ online.\n\nDavid McClure will start with us in late May, and we're excited to bring him on as a collaborator!\n"},{"id":"2011-05-03-myron-gutmann-data-access","title":"Myron Gutmann on Data Access","author":"admin","date":"2011-05-03 05:56:30 -0400","categories":["Announcements"],"url":"myron-gutmann-data-access","content":"The UVa Digital Humanities Speaker Series presents:\n\n\n### Myron Gutmann on \"Data Access for Research and Teaching in the Twenty-First Century\"\n\n\nFriday, May 6\n4:00pm (reception follows)\nMonroe Hall, Room 120\n\nThis talk is co-sponsored by [IATH](http://iath.virginia.edu), [SHANTI](http://uvashanti.org), the [Scholars' Lab](http://lib.virginia.edu/scholarslab), and the College of Arts & Sciences Quantitative Collaborative Lecture Series\n\n<!-- more -->\n\n### Abstract:\n\nThe scientific community is facing new opportunities and new requirements in the ways that data are managed and made available for future research. The biggest change that we see is the dramatic increase in the volume of data produced by observations, experiments, and simulations, which has turned what was already a steady stream of data into a flood.  That rising tide of data is being shared by research networks that span the globe, calling for new infrastructure and new architectures that will allow researchers to make use of data from around the world and engage in new long-distance collaborations. These new collaborations now mostly involve researchers, but the availability of new forms of data and the creation of new mechanisms for sharing those data make it possible to expand access in a meaningful way to students and citizen scientists. At the same time, policy makers are moving forward rapidly to require that data from publicly-financed research projects be shared with other researchers, while they simultaneously concern themselves with protecting the privacy and confidentiality of human research subjects. This presentation will discuss these changes in the data preservation and sharing environment, especially as they relate to data for the social, behavioral and economic sciences, and suggest ways that all the potential stakeholders in the process -- funding agencies, universities, data archives, libraries, researchers, teachers, and students can work together in the future to get the most out of our data investments.\n\nMyron Gutmann is Head of the [National Science Foundation's Social, Behavioral & Economics Directorate](http://www.nsf.gov/dir/index.jsp?org=SBE) and [Professor in the Department of History](http://www.lsa.umich.edu/history/facstaff/facultydetail.asp?ID=71) at the University of Michigan.\n\n"},{"id":"2011-05-13-welcoming-eric-johnson","title":"Welcoming Eric Johnson!","author":"bethany-nowviskie","date":"2011-05-13 10:18:42 -0400","categories":["Announcements"],"url":"welcoming-eric-johnson","content":"We're proud to announce that [Eric Johnson](http://twitter.com/#!/edmj) will join the faculty of UVa Library's Digital Research and Scholarship department this July as Head of Outreach & Consulting.  In his new role, Eric will direct day-to-day public services in the Library's vibrant [Scholars' Lab](http://lib.virginia.edu/scholarslab/) and help to coordinate our intellectual programming as well as our transformative program for Graduate Fellows in the Digital Humanities, now entering its fifth year.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/05/Screen-shot-2011-05-13-at-10.50.58-AM.png)](http://www.scholarslab.org/announcements/welcoming-eric-johnson/attachment/screen-shot-2011-05-13-at-10-50-58-am/)Eric comes to us from [The Thomas Jefferson Foundation (Monticello)](http://www.monticello.org/site/about/thomas-jefferson-foundation) where he currently serves as New Media Specialist, guiding Monticello's connection to its user community and the development of organizational media policies and services.  While at the Foundation, Eric participated in a number of digital initiatives, including the Jefferson Encyclopedia and the Thomas Jefferson Libraries project, and coordinated more traditional library services for users such as reference and research assistance for Foundation staff and visiting fellows.  He holds degrees in history from the College of William & Mary and George Mason University, as well as an MS in Library and Information Science from FSU, and has two decades of experience in public service in the library and cultural heritage sector.  Eric's recent and forthcoming publications showcase his thoughtful approach to user engagement in digital scholarship, and his deep commitment to libraries as places of community and hospitality for scholars, students, staff, and the general public.\n\nWe look forward to welcoming Eric and his wife, Sheryl, to the Scholars' Lab family this summer!\n"},{"id":"2011-06-09-student-jobs","title":"grad student jobs in Scholars' Lab R&D","author":"bethany-nowviskie","date":"2011-06-09 11:52:40 -0400","categories":["Announcements","Grad Student Research"],"url":"student-jobs","content":"Are you a maker? A builder? A tinkerer at heart?\n\nThe [Scholars’ Lab](http://lib.virginia.edu/scholarslab) in Alderman Library seeks energetic and imaginative UVa graduate students from all disciplines looking to hone their digital skills. Interested in cutting-edge technologies for the digital humanities and interpretive or humanistic social sciences? Want hands-on experience in project management and the design of innovative, beautiful, and useful tools for scholarship and cultural heritage?  We encourage you to apply for this apprenticeship opportunity with our [Research and Development team](http://scholarslab.org)!\n\n[![get excited and make things](http://www.scholarslab.org/wp-content/uploads/2011/06/sign1.png)](http://www.scholarslab.org/announcements/student-jobs/attachment/sign1/)Scholars’ Lab R&D is engaged in a number of projects leveraging textual encoding techniques, advanced search services, geo-temporal visualization, relational database systems, and modern Web development frameworks.  The intent of these assistantship positions is to allow you to work as part of a team, learning design, coding, and project management skills that will be invaluable to your own research and make you highly marketable within the broad field of the digital humanities.\n\nStudents will have the opportunity to partner on Scholars' Lab collaborations with UVa faculty and to help test [Omeka plug-ins](/research/omeka-plugins/) and craft geo-temporal scholarly arguments using [Neatline](/2011/02/15/scholars-lab-and-chnm-partner-on-omeka-neatline/), a tool we're building with funding from the Library of Congress and in partnership with the [Rosenzweig Center for History and New Media](http://chnm.gmu.edu).  We'll also be undertaking a brand-new project with our R&D assistants in Fall 2011: \"Crowdsourcing Interpretation.\"\n\nThese are 10-hour-per-week positions with the opportunity to work longer hours in the summers and between semesters.  We hope to hire several  grad students, who may also be interested in becoming Scholars' Lab [Graduate Fellows in Digital Humanities](http://www2.lib.virginia.edu/scholarslab/about/fellowship.html).\n\nUVa graduate students may apply through [CavLink](https://virginia-csm.symplicity.com/students/index.php).  Search for \"Scholars’ Lab Research & Development Assistant.\"\n"},{"id":"2011-06-15-myron-gutmann-data","title":"Myron Gutmann, \"Data Access for Research and Teaching in the Twenty-First Century\"","author":"ronda-grizzle","date":"2011-06-15 08:35:44 -0400","categories":["Announcements","Podcasts"],"url":"myron-gutmann-data","content":"### Data Access for Research and Teaching in the Twenty-First Century\n\n\nOn May 6th, [Myron Gutmann](http://www.nsf.gov/news/news_summ.jsp?cntn_id=115316&org=OLPA&from=news), Head of the NSF's Social, Behavioral & Economics Directorate and Professor in the Department of History at the University of Michigan, spoke as part of the UVa Digital Humanities Speaker Series.  Mr. Gutmann's talk was jointly sponsored by the Scholars' Lab at UVa Library, SHANTI, IATH, and the College of Arts & Sciences Quantitative Collaborative.\n\nIn his talk, Gutmann discusses changes in the twenty-first century data access and preservation environment, especially as relate to data for social, behavioral and economic sciences. He suggests ways that all potential stakeholders in the process — funding agencies, universities, data archives, libraries, researchers, teachers, and students — might work together to get the most out of our data investments.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://www.google.com/url?sa=t&source=web&cd=1&ved=0CBUQFjAA&url=http%3A%2F%2Fitunes.apple.com%2Fus%2Fitunes-u%2Fscholars-lab-speaker-series%2Fid401906619&rct=j&q=scholars%27%20lab%20itunes&ei=FI61TdiZNo-Dtge0g_3pDg&usg=AFQjCNGGTBvTY5QpL9aRCKh7rjEOtlLAUQ&sig2=KBrhIc1DK814RPqoAB85Tg&cad=rja).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.8349921266/enclosure.mp3\"]\n"},{"id":"2011-06-17-omeka-development-with-vagrant","title":"Omeka Development with Vagrant","author":"eric-rochester","date":"2011-06-17 12:09:12 -0400","categories":["Announcements","Research and Development"],"url":"omeka-development-with-vagrant","content":"<blockquote>_For the latest information about this system, please see the [README](https://github.com/scholarslab/cookbooks/blob/master/README.mkd) file. It contains this information, plus I'll keep it up-to-date._</blockquote>\n\n\nOne of the biggest annoyances in a developer's life is managing the software required to work on several different projects. Usually, this doesn't just mean having different systems or programming languages, but having different versions of them as well. It isn't a chop-your-limb-off problem, but more a death-by-1000-paper-cuts.\n\nOne solution is [virtual machines](http://en.wikipedia.org/wiki/Virtual_machine). They promise so much: A separate machine that can get as trashy as needed, without actually messing up the sacred laptop or desktop. Unfortunately, they can also be difficult or time-consuming to set up properly, and when a developer has to do that maybe once a week or more for new projects, it adds up quickly.\n\nEnter [Vagrant](http://vagrantup.com/). This makes defining, setting up, maintaining, and using a VM easy.\n\nUsing Vagrant, I've been working to get an out-of-the-box Omeka VM working for development. I haven't quite succeeded, but it's getting there. This involves a base box (not available yet) and a [Chef Solo cookbook](https://github.com/scholarslab/cookbooks) with instructions for setting up the VM.\n\nHere's how to use it to create a VM environment to develop a theme, plugin, or site with Omeka. These instructions work on a Mac, but they assume only that you have access to a Terminal/console window, a text editor, and a web browser, so with the right changes, it should work on any system.\n\n\n<blockquote>**Note**: Some of the resources (like the base box) aren't publicly available right now. We're working on that. Watch this space for more information.</blockquote>\n\n\n\n\n#### Get Vagrant and VirtualBox\n\n\nVagrant's written in Ruby, so assuming you have Ruby and RubyGems installed, just do this:\n\n[sourcecode language=\"bash\"]\ngem install vagrant\n[/sourcecode]\n\nGetting VirtualBox is more complicated. Check [the VirtualBox website](http://www.virtualbox.org/) for how to install it.\n\n\n#### Setting up the Working Directory\n\n\nOnce the software is installed, you'll need to set up a working directory and initialize it for Vagrant. You'll also need to download the Chef cookbooks that you'll use.\n\n[sourcecode language=\"bash\"]\nmkdir omeka-project\ncd omeka-project\ngit clone https://github.com/opscode/cookbooks.git\ngit clone git://github.com/scholarslab/cookbooks.git slab-cookbooks\nvagrant init omeka-project PATH-TO/base-centos32.box\n[/sourcecode]\n\nThe last command created a file called \"Vagrantfile\". (It also pointed to a file that won't exist on your system. We're working on a URL for hosting the base box. When it's available, use that URL in place of PATH-TO.) Go ahead and open it up in your favorite text editor. Vagrantfile is just Ruby, nothing scary there. We need to add a few lines. At the bottom of the file, just before the \"end,\" insert these:\n\n[sourcecode language=\"ruby\"]\nconfig.vm.provision :chef_solo do |chef|\n  chef.cookbooks_path = [\"cookbooks\", \"slab-cookbooks\"]\n  chef.add_recipe \"omeka\"\nend\nconfig.vm.forward_port('mysql', 3306, 3333)\nconfig.vm.forward_port('apache2', 80, 8080)\n[/sourcecode]\n\nThe first four lines tell Vagrant to set up the system using [Chef Solo](http://www.opscode.com/chef/), and they tell Chef to use the cookbooks we downloaded from GitHub and to use the \"omeka\" recipe. The last two lines tell Vagrant to set up port forwarding so we can access the web server and database from the host machine, without needing to log onto the VM.\n\n\n#### Set up Omeka\n\n\nNow we're ready to set up Omeka. By default, the system assumes that your Omeka code is in a subdirectory of your working directory and that it is named \"omeka.\" (This — and many other things — are configurable, but that's beyond the scope of this post.)\n\nThese commands will download the latest version of Omeka (as of the time I'm writing this) and change permissions on the archive directory so the web server can write to it.\n\n[sourcecode language=\"bash\"]\ncurl -O http://omeka.org/files/omeka-1.3.2.zip\nunzip omeka-1.3.2.zip\nmv omeka-1.3.2 omeka\nchmod -R a+rwx omeka/archive/\n[/sourcecode]\n\n\n<blockquote>There used to be something here about setting up your \"db.ini\" file. The Omeka Chef recipe now takes care of that for you.</blockquote>\n\n\n\n\n#### Start the VM\n\n\nEverything's in place. Now it's time to start the VM. From the console, just enter this command:\n\n[sourcecode language=\"bash\"]\nvagrant up\n[/sourcecode]\n\nA lot of lines will scroll by. Many minutes will pass. Apache, PHP, and MySQL will be installed. When you get your prompt back, you should be ready to go.\n\nYou probably missed it, but these lines were near the beginning of all that output:\n\n[sourcecode language=\"text\"]\n[default] -- mysql: 3306 => 3333 (adapter 1)\n[default] -- apache2: 80 => 8080 (adapter 1)\n[default] -- ssh: 22 => 2222 (adapter 1)\n[/sourcecode]\n\nThese tell how you can communicate with your newly minted VM. Since it's using port forwarding, you can pretend like you're talking to your host box, but using the ports listed above:\n\n[sourcecode language=\"bash\"]\nmysql -uomeka -pomeka --protocol=TCP --port=3333 omeka\nopen http://localhost:8080/\nvagrant ssh\n[/sourcecode]\n\n\n#### Finishing the Omeka Installation\n\n\nNow you need to finish setting up Omeka. Just point your browser to [the Omeka instance (http://localhost:8080)\n](http://localhost:8080) running on the VM and fill in the installation information like you normally would. Nothing special here.\n\n\n#### Developing\n\n\nThe Omeka code running the site is on your host machine, in the omeka/ directory that you created above. You can put the plugins and themes that you want to use into there, and you can edit them as you like.\n\n\n#### Closing Down\n\n\nWhen you're done for the day and you want your resources back, you can just suspend the VM by calling this:\n\n[sourcecode language=\"bash\"]\nvagrant suspend\n[/sourcecode]\n\nWhen you're done with the project and you want to destroy the VM, the database, and everything on it, give this command:\n\n[sourcecode language=\"bash\"]\nvagrant destroy\n[/sourcecode]\n\n\n#### Next Steps, or What Can I Do Since It's Vaporware?\n\n\nFor you, if you're interested in this, you may want to become familiar with Vagrant by looking at their instructions for [Getting Started](http://vagrantup.com/docs/getting-started/index.html).\n\nFor me, I wrote this today in the spirit of releasing early and often, and there's lots for me to do. Here's a little of what I'm planning initially:\n\n\n\n\t\n  * Make the base box publicly available;\n\n\t\n  * Write a README for the cookbook and Chef metadata for the Omeka Recipe;\n\n\t\n  * More recipes for more systems;\n\n\t\n  * Add phpunit, phpmd, and other PHP systems to help improve code quality; and\n\n\t\n  * Add a [Rakefile](http://rake.rubyforge.org/) with tasks for running phpunit, dumping the database, and other things.\n\n\nHopefully this will make all of our lives easier. Stay tuned.\n"},{"id":"2011-06-22-untimely-coding-scholars-lab-weekly-roundup","title":"Untimely Coding: Scholars’ Lab Weekly Roundup","author":"david-mcclure","date":"2011-06-22 08:00:36 -0400","categories":["Research and Development"],"url":"untimely-coding-scholars-lab-weekly-roundup","content":"Although the air conditioning here in Alderman Library keeps the office a bit cooler than the summer heat that’s descended on Charlottesville, things are definitely heating up in the Scholars’ Lab!\n\nMay, June, and July each mark the arrival of a new face in the lab.  In May, I was delighted to make the move to Virginia and join the team as a web developer.  [Jeremy Boggs](http://clioweb.org/) (of [Omeka](http://omeka.org/about/staff/) and [CHNM](http://chnm.gmu.edu/) fame) started as our new Humanities Design Architect at the beginning of this month. In the second week of July, Eric Johnson will come on board as our new Head of Outreach and Consulting. Eric Rochester, our Senior Developer who started in March, is starting to seem like an old-timer…\n\nWith all the fresh intellectual energy bouncing off the walls, we’re making good progress on a number of exciting projects:\n\n\n\n\n  * **BagIt plugin for Omeka** – What if you have a big collection of files in one location and you want to move them onto your Omeka site?  Depending on the size of the collection and the locations of the files, this could be anything from a chore to a nightmare.  The BagIt specification was designed to provide an easy and reliable way to package and transmit files from one location to another.  Using the BagitPHP library that Eric released last month, I’ve been putting the finishing touches on an Omeka plugin that makes it possible to import and export “bags” from your Omeka site.\n\n\n\n\n  * **Timeline plugin for Omeka** – With the arrival of Jeremy (Omeka’s Development Lead), we’ve been mapping out the next development cycle of the [Omeka + Neatline](http://neatline.org/) project.  First up is the Timeline plugin, which may get a significant update in the form of a new JavaScript widget to power the core timeline functionality.  The current version of the plugin uses a timeline application that doesn’t play well with the larger Omeka code ecosystem.  Jeremy is looking into a number of alternatives that would clear up the conflicts and make it possible to create a more robust and feature-rich interface for adding content to a timeline.\n\n\n\n\n  * **Continuous integration with Jenkins** – As part of an ongoing effort to build testing and quality control into our development process, we’ve been exploring the possibility of hooking up our Omeka plugin repositories to a continuous integration server like [Jenkins](http://jenkins-ci.org/).  With Jenkins running in the background, each new piece of code that we submit to one of our projects triggers a “build,” which runs through the software and checks to see the new version of the code passes a series of tests.  If the tests pass, the code gets automatically deployed.  This makes it possible to iterate applications quickly and aggressively without having to worry about the possibility of broken code slipping out the door unnoticed.\n\n\nIn addition to these regularly scheduled efforts, we’ve also been pursuing a number of smaller projects, experiments, and collaborations.  We were delighted to have the opportunity earlier this month to hear cultural historian [Tim Sherratt](http://wraggelabs.com/) talk about his adventures using linked open data and third-party API development to help rediscover the lives of thousands of non-white Australians who were affected by the “White Australia” policies of the 20th century.  The slides for his presentation, “Confessions of an Impatient Historian,” are available on [Slideshare](http://www.slideshare.net/wragge/confessionspdf).  During his visit, Tim also pointed us in the direction of JSTOR’s “[Data For Research](http://dfr.jstor.org/)” resource, which served as the basis for some interesting explorations into how JSTOR keeps track of information like word counts and citation relationships.\n\nMeanwhile, Eric has been working on setting up virtual machines with Vagrant (see his [blog post](/2011/06/17/omeka-development-with-vagrant/) from last week) and I’ve been building an experimental link aggregator on Django designed to serve as a testing platform for non-traditional approaches to organizing comments and other types of text-based discussions.\n\nStay tuned for exciting code to come!\n"},{"id":"2011-06-27-andrew-hankinson-applications-of-the-music-encoding-initiative-in-optical-music-recognition","title":"Andrew Hankinson, \"Applications of the Music Encoding Initiative in Optical Music Recognition\"","author":"ronda-grizzle","date":"2011-06-27 06:52:11 -0400","categories":["Podcasts"],"url":"andrew-hankinson-applications-of-the-music-encoding-initiative-in-optical-music-recognition","content":"### Applications of the Music Encoding Initiative in Optical Music Recognition\n\n\nOn April 20th, the Scholars' Lab welcomed visiting scholar Andrew Hankinson, a PhD candidate in the Distributed Digital Music Archives and Libraries Lab at the Schulich School of Music at McGill University.\n\nMr. Hankinson's talk introduced work at the Distributed Digital Music Archives and Libraries Lab on creating optical music recognition systems for recognizing printed music books, specifically focusing on the use of MEI to encode and preserve musical works. Hankinson also presented applications in notation data analysis, searching, and discovery of printed music works.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://www.google.com/url?sa=t&source=web&cd=1&ved=0CBUQFjAA&url=http%3A%2F%2Fitunes.apple.com%2Fus%2Fitunes-u%2Fscholars-lab-speaker-series%2Fid401906619&rct=j&q=scholars%27%20lab%20itunes&ei=FI61TdiZNo-Dtge0g_3pDg&usg=AFQjCNGGTBvTY5QpL9aRCKh7rjEOtlLAUQ&sig2=KBrhIc1DK814RPqoAB85Tg&cad=rja).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.8395768334/enclosure.mp3\"]\n"},{"id":"2011-06-27-scholars-lab-grad-fellows-digital-therapy-talks","title":"Scholars' Lab Grad Fellows, \"Digital Therapy\" Talks","author":"ronda-grizzle","date":"2011-06-27 06:53:05 -0400","categories":["Podcasts"],"url":"scholars-lab-grad-fellows-digital-therapy-talks","content":"### Scholars' Lab Grad Fellows Chris Clapp, Tom Finger and Alex Gil\n\n\nOur Grad Fellows in Digital Humanities are asked to make a final presentation the results of their work at springtime luncheon talks. This April, our 2010-2011 fellows (Chris Clapp, Department of Economics; Tom Finger, Corcoran Department of History; and Alex Gil, Department of English) spoke in the Scholars' Lab.\n\n\n#### Chris Clapp\n\n\nChris (our first Econ fellow) discussed his research on the effects of congestion pricing policies on  commuter behavior, residential location decisions, and ultimately  congestion itself.\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.7587118138/enclosure.mp3\"]\n\n\n\n#### Tom Finger\n\n\nTom discussed his dissertation research highlighting the growth of the North Atlantic grain trade between the United States and Great Britain during the nineteenth century, offering a case study of the ways in which technologies, ecosystems, and human social groups interact over large scale economic systems.\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.8388458488/enclosure.mp3\"]\n\n\n\n#### Alex Gil\n\n\nAlex showed a series of mock-ups that exemplify his concept for \"deep representation\" of texts in a digital environment. His talk focused on showing where we are now in terms of digital scholarly editing, and where we can go from here. Alex used examples from his own editorial work on Aimé Césaire’s _Et les chiens se taisaient_.\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.8395768094/enclosure.mp3\"]\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://www.google.com/url?sa=t&source=web&cd=1&ved=0CBUQFjAA&url=http%3A%2F%2Fitunes.apple.com%2Fus%2Fitunes-u%2Fscholars-lab-speaker-series%2Fid401906619&rct=j&q=scholars%27%20lab%20itunes&ei=FI61TdiZNo-Dtge0g_3pDg&usg=AFQjCNGGTBvTY5QpL9aRCKh7rjEOtlLAUQ&sig2=KBrhIc1DK814RPqoAB85Tg&cad=rja).\n"},{"id":"2011-07-05-fedora-connector-new-and-improved","title":"Fedora Connector - New and Improved","author":"david-mcclure","date":"2011-07-05 06:16:31 -0400","categories":["Announcements","Research and Development"],"url":"fedora-connector-new-and-improved","content":"This week we’re pleased to announce that we’ve finished up the first pass on a major redesign of our Fedora Connector plugin for Omeka.  Fedora Connector makes it possible to connect an Omeka site to a [Fedora Commons](http://www.fedora-commons.org/) repository.  After entering basic information about the location of the Fedora repository, you can create linkages between “datastreams” in Fedora and the native Omeka items on your site.  Once those associations are configured, you can import all of the metadata from Fedora into Omeka with a single click.\n\nYou can [grab the code on GitHub](https://github.com/scholarslab/FedoraConnector).\n\nWe’ve completely redesigned the front-end administrative interfaces and made a lot of plumbing upgrades to the code that communicates with Fedora.  One of the challenges with this type of “glue” software – code that crosses back and forth between different systems and data standards – is that it’s always something of a losing game to try to cover all possible use cases.  For example, Fedora Connector needs to query the XML in a Fedora datastream for a particular set of nodes that can be used to populate the native Dublin Core fields for Omeka items.  That's pretty straightforward if the datastream is already encoded in Dublin Core – but if it’s not, the plugin needs a \"conversion table\" of sorts so that it knows that node X in Dublin Core corresponds to node Y in whatever schema the target datastream is marked up in.\n\nTrying to code out all possible combinations of metadata standards would devolve into a big game of programming whack-a-mole.  Some permutations are obvious.  For example, the new version of Fedora Connector ships with code to convert from the commonly used MODS metadata format.  But the edge cases aren’t so obvious, and beyond a certain point it becomes a fool’s errand to try to guess to hard about software will be used in the wild.\n\nLuckily, this is just the sort of situation where open source really shines!  Eric and Wayne concocted a highly clever system that essentially creates a “sub-plugin” ecosystem inside of the Fedora Connector plugin that makes it possible to modularly snap in new chunks of code to handle specific metadata formats.  This way, if you need the plugin to communicate with an unsupported metadata standard on your Fedora server, you can just write out the XPath queries that map onto the Dublin Core fields, and, with a minimal amount of easy-on-the-eyes boilerplate, plug the new importer into Fedora Connector.\n<!-- more -->\nHere’s how it works.  New importers just inherit from an abstract class called FedoraConnector_AbstractImporter, which handles all of the utility tasks of fetching content from Fedora and writing new values to the Omeka database.  All you have to do is define two functions.  The first, called canImport(), essentially just announces to the sub-plugin system that datastreams of metadata type X (whatever standard your code is supporting) can now be imported.  And the second, called getQueries(), takes the name of Dublin Core value and returns the queries needed to find the corresponding value(s) in the datastream content.\n\nFirst, though, we need to tell PHP where to find the abstract class:\n\n[sourcecode language=\"php\"]\nrequire_once FEDORA_CONNECTOR_PLUGIN_DIR . '/libraries/FedoraConnector/AbstractImporter.php';\n[/sourcecode]\n\nNext, declare your new class and tell it to inherit from the abstract base class:\n\n[sourcecode language=\"php\"]\nrequire_once FEDORA_CONNECTOR_PLUGIN_DIR . '/libraries/FedoraConnector/AbstractImporter.php';\n\nclass NameOfFormat_Importer extends FedoraConnector_AbstractImporter\n{\n\n}\n[/sourcecode]\n\nMake sure to name your class with the format [NameOfFormat]_Importer so that Fedora Connector can see it.  Now, add the declarations for the two functions:\n\n[sourcecode language=\"php\"]\nrequire_once FEDORA_CONNECTOR_PLUGIN_DIR . '/libraries/FedoraConnector/AbstractImporter.php';\n\nclass NameOfFormat_Importer extends FedoraConnector_AbstractImporter\n{\n\n\tpublic function canImport($datastream)\n\t{\n\n\t}\n\n\tpublic function getQueries($name)\n\t{\n\n\t}\n\n}\n\n[/sourcecode]\n\nIn canImport(), you just need to return a true or false value depending on whether or not the passed in datastream matches the format that your importer is designed to handle.  This can be done by just checking for equality and returning the result of the evaluation:\n\n[sourcecode language=\"php\"]\nrequire_once FEDORA_CONNECTOR_PLUGIN_DIR . '/libraries/FedoraConnector/AbstractImporter.php';\n\nclass NameOfFormat_Importer extends FedoraConnector_AbstractImporter\n{\n\n\tpublic function canImport($datastream)\n\t{\n\t\treturn ($datastream->metadata_stream == ‘NameOfFormat’);\n\t}\n\n\tpublic function getQueries($name)\n\t{\n\n\t}\n\n}\n\n[/sourcecode]\n\nNow, all that's left is to process the Dublin Core $name variable getting passed into the getQueries() function and return an array of XPath queries that will pluck the corresponding nodes out of a document marked up in the format that you need to accommodate.  Just run a switch-case on the $name and run through all the Dublin Core values:\n\n[sourcecode language=\"php\"]\nrequire_once FEDORA_CONNECTOR_PLUGIN_DIR . '/libraries/FedoraConnector/AbstractImporter.php';\n\nclass NameOfFormat_Importer extends FedoraConnector_AbstractImporter\n{\n\n\tpublic function canImport($datastream)\n\t{\n\t\treturn ($datastream->metadata_stream == ‘NameOfFormat’);\n\t}\n\n\tpublic function getQueries($name)\n\t{\n\n\t\tswitch ($name) {\n\n\t\t\tcase 'Title':\n\n\t\t\t\t$queries = array(\n\t\t\t\t'//*[local-name()=\"mods\"]/*[local-name()=\"titleInfo\"]'\n\t\t\t\t\t. '/*[local-name()=\"title\"]'\n\t\t\t\t);\n\n\t\t\tbreak;\n\n\t\t\tcase 'Creator':\n\n\t\t\t\t$queries = array(\n\t\t\t\t'//*[local-name()=\"mods\"]'\n\t\t\t\t\t. '/*[local-name()=\"name\"][*[local-name()=\"role\"] = \"creator\"]'\n\t\t\t\t);\n\n\t\t\tbreak;\n\n\t\t\t[...handle all DC values...]\n\n\t\t}\n\n\t\treturn $queries;\n\n\t}\n\n}\n\n[/sourcecode]\n\nStick the file in the FedoraConnector/Importers directory and you're good to go!  We've also implemented a nifty method of adding custom \"renderers\" to the plugin - code that controls the display of various data formats on the public-facing Omeka site.  Check out the [README.md](https://github.com/scholarslab/FedoraConnector/blob/master/README.md) file on GitHub for details, and watch for more development on the plugin in the near future.\n"},{"id":"2011-07-05-tim-sherratt-confessions-of-an-impatient-historian","title":"Tim Sherratt, Confessions of an Impatient Historian","author":"ronda-grizzle","date":"2011-07-05 06:06:14 -0400","categories":["Podcasts"],"url":"tim-sherratt-confessions-of-an-impatient-historian","content":"### Confessions of an Impatient Historian\n\n\nOn June 8th, the Scholars' Lab welcomed visiting scholar [Tim Sherratt](http://wraggelabs.com/), digital historian, web developer and cultural data hacker who's been developing online resources relating to archives, museums and history since 1993. Tim is currently employed by the National Museum of Australia, as well as being an Adjunct Associate Professor in the Digital Design and Media Arts Research Cluster at the University of Canberra. He is one of the organizers of THATCamp Canberra and is a member of the interim committee of the new Australian Association for the Digital Humanities.\n\nTim's talk investigated what happens when you equip an impatient historian with some rudimentary coding skills and a borderline obsession. Reflecting on his experiences in and around a number of Australia’s national cultural institutions, he considered how digitally-enhanced impatience can change our relationship both with our sources and the institutions that hold them.\n\nExplore Tim's digital tools and renegade APIs at [WraggeLabs Emporium](http://wraggelabs.com/emporium/).\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://www.google.com/url?sa=t&source=web&cd=1&ved=0CBUQFjAA&url=http%3A%2F%2Fitunes.apple.com%2Fus%2Fitunes-u%2Fscholars-lab-speaker-series%2Fid401906619&rct=j&q=scholars%27%20lab%20itunes&ei=FI61TdiZNo-Dtge0g_3pDg&usg=AFQjCNGGTBvTY5QpL9aRCKh7rjEOtlLAUQ&sig2=KBrhIc1DK814RPqoAB85Tg&cad=rja).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.8576900213/enclosure.mp3\"]\n"},{"id":"2011-07-21-introduction-to-linked-open-data-at-rare-books-school","title":"Introduction to Linked Open Data at Rare Book School","author":"eric-rochester","date":"2011-07-21 10:15:20 -0400","categories":["Research and Development"],"url":"introduction-to-linked-open-data-at-rare-books-school","content":"Yesterday, I was fortunate to be invited by [Andrew Stauffer](http://www.engl.virginia.edu/faculty/stauffer_andrew.shtml) and [Bethany Nowviskie](http://nowviskie.org/) to present at their [Rare Book School](http://www.rarebookschool.org/) course, [Digitizing the Historical Record](http://rarebookschool.org/courses/libraries/l65/). I talked about [Linked Open Data](http://en.wikipedia.org/wiki/Linked_Data) (LOD), and afterward, [Dana Wheeles](http://twitter.com/#!/bluesaepe) talked about the [NINES](http://www.nines.org/) project and how they use RDF and LOD.\n\n\n\n\nI tried to present a gentle, mostly non-technical introduction to LOD, with an example of it in action. Hopefully, this posting will be a 50,000 foot overview also.\n\n\n\n\n### The Linked Open Data Universe\n\n\n\n\n\n[ ![Linked Open Data cloud](http://richard.cyganiak.de/2007/10/lod/lod-datasets_2010-09-22.png) ](http://lod-cloud.net/) _Linking Open Data cloud diagram, by Richard Cyganiak and Anja Jentzsch. [http://lod-cloud.net/](http://lod-cloud.net/)_\n\n\n\n\n\n\nThe first thing to know about LOD is that it’s everywhere. Look at the [Linked Open Data cloud diagram](http://lod-cloud.net/) above. All of these institutions are publishing data that anyone can use, and their data references others' data also.\n\n\n\n\n### Linked Data vs Open Data vs RDF Data\n\n\n\n\nFirst we need to unpack the term _Linked Open Data_:\n\n\n\n\n**Linked** is an approach to data. You need to provide context for your data; you need to point to other’s data.\n\n\n\n\n**Open** is a policy. Your data is out there for others to look at and use; you explicitly give others this permission.\n\n\n\n\n**Data** is a technology and a set of standards. Your data is available using an RDF data model (usually) so computers can easily process it.\n\n\n\n\n_(See [Christopher Gutteridge’s post](http://blogs.ecs.soton.ac.uk/webteam/2011/07/17/linked-data-vs-open-data-vs-rdf-data/) for more about this distinction.)_\n\n\n\n\n### Five Stars\n\n\n\n\nCreating LOD can seem overwhelming. Where do you start? What do you have to do? It’s not an all or nothing proposition. You can take what you have, figure out how close you are to LOD, and work gradually toward making your information a full member of the LOD cloud. The LOD community talks about having four-star data or five-star data. Here are what the different stars denote:\n\n\n\n\n\n\n  1. You’ve released the data using any format under an **open** license that allows others to view and use your data;\n\n\n  2. You’ve released the data in a **structured** format so that some program can deal with it (e.g., Excel);\n\n\n  3. You’ve released the data in a **non-proprietary** format, like CVS;\n\n\n  4. You’ve used **HTTP URIs** (things you can type into your web browser’s location bar) to identify things in your data and made those URIs available on the web so others can point to your stuff;\n\n\n  5. You explicitly **link** your data to others’ data to provide context.\n\n\n\n\n_(This is all over the web. [Michael Hausenblas’ explanation with examples](http://lab.linkeddata.deri.ie/2010/star-scheme-by-example/) is a good starting point.)_\n\n\n\n\n### Representing Knowledge\n\n\n\n\nA large part of this is about representing knowledge so computers can easily process it. Often LOD is encoded using [Resource Description Framework (RDF)](http://en.wikipedia.org/wiki/Resource_Description_Framework). This provides a way to model information using a series of statements. Each statement has three parts: a subject, a predicate, and an object. Subjects and predicates _must_ be URIs. Objects can be URIs (linked data) or data literals.\n\n\n\n\nThe predicates that you can use are grouped into _vocabularies_. Each vocabulary is used for a specific domain.\n\n\n\n\nWe’re getting abstract, so let’s ground this discussion by looking at a specific vocabulary and set of statements.\n\n\n\n\n#### Friend of a Friend\n\n\n\n\nFor describing people, there’s a vocabulary standard called [Friend of a Friend (FOAF)](http://www.foaf-project.org/). I’ve used that on my web site to provide information about me. (The file on my website is in [RDF/XML](http://en.wikipedia.org/wiki/RDF/XML), which can be frightening. I’ve converted it to [Turtle](http://en.wikipedia.org/wiki/Turtle_(syntax)), which we can walk through more easily.)\n\n\n\n\nI’ll show you parts of it line-by-line.\n\n\n\n\n(Ahem. Before we start, a disclaimer: I need to update my FOAF file. It doesn’t reflect best practices. The referencing URL isn’t quite the way it should be, and it uses deprecated FOAF predicates. That said, if you can ignore my dirty laundry, it still illustrates the points I want to make about the basic structure of RDF.)\n\n\n\n\nFirst,\n\n\n\n    \n    <code>@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n    </code>\n\n\n\n\nThis just says that anywhere `foaf:` appears later, replace it with the URL `http://xmlns.com/foaf/0.1/`.\n\n\n\n    \n    <code>[] a <http://xmlns.com/foaf/0.1/Person>;\n    </code>\n\n\n\n\nThis is a statement. `[]` just means that it’s talking about the document itself, which in this case is a stand-in for me. The predicate here is `a`, which is a shortcut that’s used to tell what type of an object something is. In this case, it says that I’m a person, as FOAF defines it.\n\n\n\n\nAnd because the line ends in a semicolon, the rest of the statements are also about me. Or more specifically, about `[]`.\n\n\n\n    \n    <code>foaf:firstName \"Eric\";\n    foaf:surname \"Rochester\";\n    foaf:name \"Eric Rochester\";\n    foaf:nick \"Eric\";\n    </code>\n\n\n\n\nThis set of statements still have the implied subject of me, and they use a series of predicates from FOAF. The object of each is a literal string, giving a value. Roughly this translates into four statements:\n\n\n\n\n\n\n  * Eric’s first name is “Eric.”\n\n\n  * Eric’s given name is “Rochester.”\n\n\n  * Eric’s full name is “Eric Rochester.”\n\n\n  * Eric’s nickname is “Eric.”\n\n\n\n\nThe next statement is a little different:\n\n\n\n    \n    <code>foaf:workplaceHomepage <http://www.scholarslab.org/> .\n    </code>\n\n\n\n\nThis final statement has a URI as the object. It represents this statement:\n\n\n\n\n\n\n  * Eric’s workplace’s home page is “[http://www.scholarslab.org/](http://www.scholarslab.org/)”.\n\n\n\n\nIf this was a little overwhelming, thank you for sticking around this far. Now here’s what you need to know about modeling information using RDF:\n\n\n\n\n\n\n  1. Everything is expressed as subject-predicate-object statements; and\n\n\n  2. Predicates are grouped into vocabularies.\n\n\n\n\nThe rest is just details.\n\n\n\n\n### Linked Open Data and the Semantic Web\n\n\n\n\nDuring my presentation, someone pointed out that this all sounds a lot like the [Semantic Web](http://en.wikipedia.org/wiki/Semantic_web).\n\n\n\n\nYes, it does. LOD is the semantic web without the focus on understanding and focusing more on what we can do. Understanding may come later—or not—but in the meantime we can still do some pretty cool things.\n\n\n\n\n### So What?\n\n\n\n\nThe benefit of all this is that it provides another layer for the internet. You can use this information to augment your own services (e.g., Google augments their search results with RDF data about product reviews) or build services on top of this information.\n\n\n\n\nIf you’re curious for more or still aren’t convinced, visit the [Open Bibliographic Data Guide](http://obd.jisc.ac.uk/). They make a business case and articulate some use cases for LOD for libraries and other institutions.\n\n\n\n\n### For Example\n\n\n\n\nDiscussing LOD can get pretty abstract and pretty meta. To keep things grounded, I spent a few hours and threw together a quick demonstration of what you can do with LOD.\n\n\n\n\nThe Library of Congress’ [Chronicling America](http://chroniclingamerica.loc.gov/) project exposes data about the newspapers in its archives using RDF. It’s five-star data, too. For example, to tell the geographic location that the papers covered, it links to both [GeoNames](http://www.geonames.org/) and [DBpedia](http://dbpedia.org/About). The LoC doesn’t provide the coordinates of these cities, but because they express the places with a link, I can follow those and read the latitude and longitude from there.\n\n\n\n\nI wrote a [Python](http://www.python.org/) script that uses [RDFlib](http://www.rdflib.net/) to read the data from the LoC and GeoNames and writes it out using [KML](http://en.wikipedia.org/wiki/Kml). You can view this file using Google Maps or Google Earth.\n\n\n\n\nHere’s the results of one run of the script. (I randomly pick 100 newspapers from the LoC, so the results of each run is different.)\n\n\n\n\n  \n[View Larger Map](http://maps.google.com/maps?f=q&source=embed&hl=en&geocode=&q=https:%2F%2Fgithub.com%2Ferochest%2Floc-chronicling-map%2Fraw%2Fmaster%2Fdata%2Fnewspapers.kml&aq=&sll=38.063606,-78.505873&sspn=0.011741,0.016093&ie=UTF8&t=h&ll=34.64296,-115.5352&spn=26.67204,84.64626)\n\n\n\n\nYou can find the source for this example on both Github and BitBucket:\n\n\n\n\n\n\n  * [https://github.com/erochest/loc-chronicling-map](https://github.com/erochest/loc-chronicling-map)\n\n\n  * [https://bitbucket.org/erochest/loc-chronicling-map/overview](https://bitbucket.org/erochest/loc-chronicling-map/overview)\n\n\n\n\n### Resources\n\n\n\n\nThroughout this post, I’ve tried to link to some resources. Here are a few more (not all of these will be appropriate to a novice):\n\n\n\n\n\n\n  * [The Wikipedia page on linked data](http://en.wikipedia.org/wiki/Linked%5C_Data).\n\n\n  * [The Open Bibliographic Data Guide](http://obd.jisc.ac.uk/), which provides rationales for LOD.\n\n\n  * [A portal to LOD resources and tools](http://linkddata.org/).\n\n\n  * [A portal maintained by the W3C](http://www.w3.org/wiki/LinkedData).\n\n\n  * [The LOD cloud](http://lod-cloud.net/).\n\n\n  * [The four rules of LOD](http://www.w3.org/DesignIssues/LinkedData.html).\n\n\n  * [The five rules](http://lab.linkeddata.deri.ie/2010/star-scheme-by-example/).\n\n\n  * [Linked vs Open vs Data](http://blogs.ecs.soton.ac.uk/webteam/2011/07/17/linked-data-vs-open-data-vs-rdf-data/).\n\n\n  * [A book on publishing LOD on the internet](http://linkeddatabook.com/book).\n\n\n\n"},{"id":"2011-08-04-this-week-in-open-source","title":"This week in Open Source","author":"wayne-graham","date":"2011-08-04 02:30:00 -0400","categories":["Research and Development"],"url":"this-week-in-open-source","content":"After reading a post on one of my favorite blogs, [Giant Robots Smashing Into Other Giant Robots](http://robots.thoughtbot.com/post/8221237451/this-week-in-open-source), I was inspired to start this series chronicling highlights in our Open Source development efforts.\n\nIt was a busy week for the Scholars' Lab R&D team, with updates to the [FedoraConnector](https://github.com/scholarslab/FedoraConnector), [NeatlineMaps](https://github.com/scholarslab/NeatlineMaps), and [Timeline](https://github.com/scholarslab/Timeline) plugins for [Omeka](https://github.com/omeka/Omeka), as well as updates to our Vagrant [cookbooks](https://github.com/scholarslab/cookbooks) (and an [Omeka dev example](https://github.com/scholarslab/FalmouthDevEnv)), and [BagItPHP](https://github.com/scholarslab/BagItPHP) library.\n\n\n## FedoraConnector\n\n\nDavid McClure ([davidmcclure](https://github.com/davidmcclure)) greatly improved the workflow for updating metadata from FedoraCommons objects in to an Omeka instance. Users can now set not only system-wide defaults for polling updates to metadata records from a Fedora server, but also on a per-field basis within individual records, giving users greater control over their metadata pulls.\n\n\n## NeatlineMaps\n\n\nDavid McClure ([davidmcclure](https://github.com/davidmcclure)) also has begun work on a major refactor of the NeatlineMaps plugin on the refactor branch of the project. There is now much smarter support for dynamically determining a map's bounding box ([c4090fe](https://github.com/scholarslab/NeatlineMaps/commit/c4090fe1c37cb7547dfe11b309d50290d357b9a2)). Just a note if you are checking out this branch: this commit ([032adf3](https://github.com/scholarslab/NeatlineMaps/commit/032adf36eb72ec5d6cd9ece34a5ad625168c1f23)) adds the map in the item view, but is default code and displays the map incorrectly due to a bug in the projection. Next up is a major revamp of the administration interface, which will provide a facade for GeoServer inside of Omeka, making it easy to manage collections of maps and assign them to servers and namespaces.\n\n\n## Timeline\n\n\nJeremy Boggs ([clioweb](https://github.com/clioweb)) has re-implemented the Timeline plugin to use the [jQuery Timeglider plugin](http://timeglider.com/jquery/). This update ([b078df8](https://github.com/scholarslab/Timeline/commit/b078df8169cc30ce2706d3c564161bef44ea3330)) replaces the plugin's use of the SIMILE Timeline and begins to add support for a custom JSON output ([51eb12a](https://github.com/scholarslab/Timeline/commit/51eb12a1c8cdcc37d9d96f72223e7cbee99aa09b)) for Omeka records for use in Timeglider timelines.\n\n\n## Vagrant\n\n\nEric Rochester ([erochest](https://github.com/erochest)) has been working on standardizing our development, testing, and deployment environments using virtual servers with [Vagrant](http://vagrantup.com/). He has set up a repository of [cookbooks](https://github.com/scholarslab/cookbooks) which automate the setup of development environments for various projects. For a cool example, check out the [dev environment](https://github.com/scholarslab/FalmouthDevEnv) for our project with Louis Nelson, chronicling the architectural development of [Falmouth Jamaica](http://falmouth.lib.virginia.edu/).\n\n\n## BagitPHP\n\n\nWe spent some time this week moving our Omeka plugins (and other libraries) to a new Jenkins server. In getting the PEAR/Pecl packages properly set up, Wayne Graham ([waynegraham](https://github.com/waynegraham)) updated the ant script for the various testing and code reports that we run on our software  ([465fa89](https://github.com/scholarslab/BagItPHP/commit/465fa89cf2c9cdc763018acde72c82be0f21e6bb)).\n\n\n## Jenkins\n\n\nTo better automate our testing environment, David McClure ([davidmcclure](https://github.com/davidmcclure)) worked out the final issues with automating the testing of our plugins with multiple versions of Omeka. We are currently targeting the [current stable tag of Omeka](https://github.com/omeka/Omeka/tree/stable-1.4), and the [master branch](https://github.com/omeka/Omeka). We are working on turning everything green, but the major work of integrating the plugins in to the test environment is complete. It took a bit of tinkering to get the plugins to build correctly when they're not sitting inside of an Omeka installation tree (as they would in a standard configuration). Watch this space for a more detailed post about how to replicate this set up.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/08/omeka-jenkins-300x175.png)](http://www.scholarslab.org/slab-code/this-week-in-open-source/attachment/omeka-jenkins/)\n"},{"id":"2011-08-08-life-liberty-and-the-pursuit-of-mappiness","title":"Life, Liberty, and the Pursuit of Mappiness","author":"kelly-johnston","date":"2011-08-08 05:35:18 -0400","categories":["Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"life-liberty-and-the-pursuit-of-mappiness","content":"Mr. Jefferson ended his best-known sentence with “_Life, Liberty and the pursuit of Happiness_.\"   The only thing missing was maps.\n\n[![Life Liberty Pursuit of Happiness](http://www.scholarslab.org/wp-content/uploads/2011/08/LifeLibertyPursuitBlogSlide-300x225.png)](http://www.scholarslab.org/digital-humanities/life-liberty-and-the-pursuit-of-mappiness/attachment/lifelibertypursuitblogslide/)\n\nIn the Scholars' Lab, we're all about the spatial goodness.   Inspired by Kansas State University’s [Seven Deadly Sins maps](http://www.wired.com/culture/education/magazine/17-09/st_sinmaps), we set about converting the qualities of life, liberty, and the pursuit of happiness into quantities we could visualize through maps.  Brainstorming commenced on how best to measure the unmeasurable.   Mappiness ensued.\n\nWe calculated a score for _Life_, for _Liberty_, and for the _Pursuit of Happiness_ for each county in the continental U.S. and mapped how each county's score deviated from the mean.  So our maps highlight extremes, both high and low.\n\n**[![Life](http://www.scholarslab.org/wp-content/uploads/2011/08/Life-1024x791.jpg)](http://www.scholarslab.org/digital-humanities/life-liberty-and-the-pursuit-of-mappiness/attachment/life/)\n**\n\n_Life_ mapped combined male and female [Life Expectancy At Birth](http://www.plosmedicine.org/article/info:doi/10.1371/journal.pmed.0050066#pmed-0050066-sd002) data for 1999.** **\n\n**[![Liberty](http://www.scholarslab.org/wp-content/uploads/2011/08/Liberty-1024x791.jpg)](http://www.scholarslab.org/digital-humanities/life-liberty-and-the-pursuit-of-mappiness/attachment/liberty/)\n**\n\n_Liberty_ mapped US Census datasets for 2000 to measure the [institutionalized population](http://factfinder.census.gov/servlet/MetadataBrowserServlet?type=subject&id=GQ_USF1&dsspName=DEC_2000_SF1&back=update&_lang=en) as a percent of the total population.\n\n[![Pursuit of Happiness](http://www.scholarslab.org/wp-content/uploads/2011/08/PursuitOfHappiness-1024x791.jpg)](http://www.scholarslab.org/digital-humanities/life-liberty-and-the-pursuit-of-mappiness/attachment/pursuitofhappiness/)\n\n_Pursuit of Happiness_ mapped the ratio of [arts, entertainment, and recreation](http://bhs.econ.census.gov/econhelp/resources/ae-71/SEC_AE-71.html) establishments to the total population from the 2002 US Economic Census.\n\nTransforming datasets from a spreadsheet to a map takes advantage of our  human ability to consume mass quantities of information visually.  Rows of numbers stashed away in academic journals and US  Census tables come alive when mapped to show comparisons with their neighbors both near and far.  Patterns and clusters appear.  [New questions are asked.](http://spatial.scholarslab.org/spatial-turn/) New answers come.  New maps emerge.\n\nAnd here's good news:  If you find our measures of Mr. Jefferson's famous phrase lacking, software tools to [combine and manipulate datasets](/2011/01/31/putting-american-community-survey-data-to-work) that may share only common geographic markers can now [make cartographers  of us all](http://spatial.scholarslab.org/).\n\nHappy mapping!\n"},{"id":"2011-08-10-web-development-template-rails-3-1-html-5-boilerplate-960-gs","title":"Web Development Template: Rails 3.1, HTML 5 Boilerplate, 960.gs","author":"eric-rochester","date":"2011-08-10 04:54:16 -0400","categories":["Research and Development"],"url":"web-development-template-rails-3-1-html-5-boilerplate-960-gs","content":"One of the best things about web development is that there are so many tools around to make the job easier.\n\n\n\n\n\nOne of the worst things about web development is that there are so many tools around, and they just don't play well together.\n\n\n\n\n\nOften, they're not fighting publicly. Instead, they're digging at each other passive-aggressively and making everyone's life slightly unpleasant.\n\n\n\n\n\nI've taken three of these tools and put them together in a directory and tried to smooth things over:\n\n\n\n\n\n\n\n  * [Ruby on Rails 3.1](http://rubyonrails.org/), a framework for developing web applications in Ruby;\n\n\n  * Some useful Ruby Gems;\n\n\n  * [HTML5 Boilerplate](http://html5boilerplate.com/), a template for creating web pages using the latest technologies and best practices; and\n\n\n  * [960 Grid System](http://960.gs/), a set of CSS rules for laying out content on web pages in columns.\n\n\n\n\n\nPutting these together isn't difficult, and it's not really a lot of work. But now you don't have to do any of that. You can just clone the template from GitHub and start building things:\n\n\n\n\n\n<blockquote>\n[https://github.com/scholarslab/rails31-template](https://github.com/scholarslab/rails31-template)\n</blockquote>\n\n\n\n\n\nFor more details on how to use this, see the [README file](https://github.com/scholarslab/rails31-template/blob/master/README.mkd).\n\n\n\n"},{"id":"2011-08-15-last-week-in-open-source","title":"Last week in Open Source","author":"wayne-graham","date":"2011-08-15 05:42:51 -0400","categories":["Research and Development"],"url":"last-week-in-open-source","content":"Another busy week in the Scholars' Lab R&D offices. If you have anything to contribute, remember, pull requests are welcome!\n\n\n## Rails 3.1 Template\n\n\nEric Rochester ([erochest](https://github.com/erochest)) spent some time building a [template for Rails 3.1](https://github.com/scholarslab/rails31-template) projects which include [HTML 5 Boilerplate](http://html5boilerplate.com/) and [960gs](http://960.gs/) for erb template layouts, along with [rspec-rails](https://rubygems.org/gems/rspec-rails), [annotate](https://rubygems.org/gems/annotate), [faker](https://rubygems.org/gems/faker), [webrat](https://rubygems.org/gems/webrat), [spork](https://rubygems.org/gems/spork), and [factory_girl_rails](https://rubygems.org/gems/factory_girl_rails) gems to help speed spinning up a new rails project. He wrote a nice [post on using this technique](http://www.scholarslab.org/slab-code/web-development-template-rails-3-1-html-5-boilerplate-960-gs/), then we refactored it a bit, using the rails [application templating system](http://guides.rubyonrails.org/generators.html#application-templates), as well modifying the inclusion of styles and javascripts as we read more of the sprockets 2.0 source to see how it actually handles combining CSS and Javascript.\n\nThe entire group has been working with Rails a bit more the past couple of weeks (we have lots of experience in the group with Python and PHP), and there were some humorous responses when I asked their thoughts on rails and the new asset pipeline:\n\n\"I like it a lot more than I used too...\" - Jeremy Boggs\n\n\"The asset pipeline is easy enough to override...\" - Eric Rochester\n\n\"Meh...\" - David McClure\n\n\n## Octopress\n\n\nEric Rochester [reworked his blog](http://www.ericrochester.com/) in [Octopress](http://octopress.org/), a [Jekyll](http://github.com/mojombo/jekyll) framework for blogging ([0b331a9](https://github.com/erochest/erochest.github.com)). No more database overhead, just straight up HTML goodness! If you feel Wordpress is overkill for your needs, and love markdown (or other minimalist markups), you should definitely check this technique out.\n\n\n## Chef\n\n\nEric Rochester found a [bug in the Chef when using httpd on CentOS 6](http://tickets.opscode.com/browse/COOK-665) ([pull request](https://github.com/opscode/cookbooks/pull/178)). This update changes the default [pid](http://en.wikipedia.org/wiki/Process_identifier) file for the httpd daemon and updates the Apache configuration to appropriately assign the pid as expected. There is really nothing worse than a stale pid file that stalls a daemon.\n\n\n## NeatlineMaps\n\n\nDavid McClure is getting closer to finishing a major refactor of the [Neatline Maps plugin](https://github.com/scholarslab/NeatlineMaps) for Omeka, which makes is possible to display .tiff files hosted in [GeoServer](http://geoserver.org/display/GEOS/Welcome) with a \"slippy\" map courtesy of [OpenLayers](http://www.openlayers.org). The current [maps branch](https://github.com/scholarslab/NeatlineMaps/tree/maps) of the plugin has quite a number of UX improvements that ease the building of composite maps from of a number of files and associate them with Omeka items.\n\n\n## The Mind is a Metaphor\n\n\nWayne Graham has started the work of migrating a Rails 2 application to Rails 3 using upgrade plugins and rake tasks. Not only is this application upgrading frameworks, but also is being upgraded to run on Ruby 1.9. As support for Ruby 1.8.7 will reach its [EOL](http://redmine.ruby-lang.org/issues/4996), with normal maintenance until June 2012 and security fixes until June 2013. After spending some time grinding on this, enough has changed that I'm going to try a new tack and merge models and controllers into a blank project because of the difference in how rails sets itself up depending on if 1.9 or 1.8 is used to initialize the project.\n\n\n## Git Flow\n\n\nWe are attempting to standardize our git workflow to make a bit more sense and be \"safer.\" After reading the great piece by Vincent Driessen (\"[A successful Git branching model](http://nvie.com/posts/a-successful-git-branching-model/)\" [pdf](http://github.com/downloads/nvie/gitflow/Git-branching-model.pdf)), we've begun utilizing [gitflow](https://github.com/nvie/gitflow).  Essentially it  breaks down to a master branch (the release), a develop branch (your next version), release versions (archive of past releases). You then create local \"feature\" branches that you merge in to the develop branch, merging up the tree as needed. I personally am digging the fact that to merge my local branches I no longer have to type:\n\n[sourcecode language=\"bash\"]\ngit co develop\ngit merge --no-ff mybranch\ngit branch -d mybranch\ngit push origin develop\n[/sourcecode]\n\nThere are only two lines now:\n\n[sourcecode language=\"bash\"]\ngit flow feature finish mybranch\ngit push origin develop\n[/sourcecode]\n"},{"id":"2011-08-17-charlottesvilles-street-car-system-in-gis","title":"Charlottesville's Street Car System in GIS","author":"chris-gist","date":"2011-08-17 05:27:38 -0400","categories":["Digital Humanities","Geospatial and Temporal"],"url":"charlottesvilles-street-car-system-in-gis","content":"# Background\n\n\nDid you know that Charlottesville once had streetcars?  Since moving to town, I've heard tales of the once-thriving transportation system that connected Fry's Spring, UVa and downtown.  It wasn't until an inquiry came in from a student looking for GIS data for the system that I investigated it.\n\n[caption id=\"attachment_1917\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/muleCar-1024x520.jpg)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/mulecar/) Mules pulling streetcar on Main St. - Special Collections, University of Virginia Library[/caption]\n\nI first found the following 1890 map which shows the holdings and plans for the Charlottesville Land Company.  The map highlights the existing streetcar system and plans to extend the system into their new neighborhoods.  We also found a large number of streetcar-related images from the [Holsinger Photo Collection](http://search.lib.virginia.edu/?f%5Bdigital_collection_facet%5D%5B%5D=Holsinger+Studio+Collection&sort=date_received_facet+desc).\n\n[caption id=\"attachment_1922\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/venable-1024x605.jpg)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/venable/) Charlottesville Land Co., 1890 - Special Collections, University of Virginia Library[/caption]\n\nThe Charlottesville Land Co. map is intriguing.  Not only does it show the eventual street grid for Belmont and Rose Hill, it has unrealized plans for the southwest quadrant of the city.  The extensive streetcar plan designed to service these new developments piqued my interest. After a little research, I found a book called _\"Forward is the Motto of Today\": Street Railways in Charlottesville, Virginia, 1866-1936 _by Jefferson Randolph Kean.  The UVa Library has two copies, one in Special Collections and one in general circulation.  The book gives the entire history of the rail system from its modest beginnings with mule-drawn cars down Main St. to an extensive electric system.  One of the best qualities of the book is that it has maps drawn by the book's publisher, Harold E. Cox.\n\n[caption id=\"attachment_1919\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/1891map-1024x836.jpg)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/1891map/) Charlottesville streetcar system 1891 from \"Forward is the Motto of Today\": Street Railways in Charlottesville, Virginia, 1866 - 1936 - H.E. Cox, reproduced with permission[/caption]\n\nThe 1891 map shows the main trunk line which connects the UVa Corner to the C & O station on the east end of downtown via Main Street. Notice the Fry's Spring R.R. line from the train station area to the Hotel Albemarle.  This line was eventually abandoned and replaced with one following the Jefferson Park Avenue route.  The Belmont R.R. line servicing \"The Grove\" (now Belmont Park) was also later abandoned.\n\n[caption id=\"attachment_1920\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/1895map-1024x684.jpg)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/1895map/) Charlottesville streetcar system 1895 from \"Forward is the Motto of Today\": Street Railways in Charlottesville, Virginia, 1866 - 1936 - H.E. Cox, reproduced with permission[/caption]\n\nThe 1895 map shows planned expansion to Woolen Mills, Fifeville, and 10th and Page neighborhoods.  The section downtown (South Street, 1st Street, and Market Street) that was built but abandoned is particularly interesting.  The entire section was never more than two blocks from the main line, which probably explains why it was removed.\n\n[caption id=\"attachment_1929\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/1920map-1024x705.jpg)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/1920map/) Charlottesville streetcar system 1920 from \"Forward is the Motto of Today\": Street Railways in Charlottesville, Virginia, 1866 - 1936 - H.E. Cox, reproduced with permission[/caption]\n\nThe 1920 map shows an expansion through UVa to service Lambeth Field and down Jefferson Park Ave. to Fry's Spring.  This map also depicts the \"pass-bys\" (represented with the double lines in five locations), car barn (on Ridge St.), and turn-arounds at the end of all the lines.\n\n\n# GISing the Data\n\n\nReviewing Kean's work and other maps led me to some interesting questions.  What exact streets did the trolleys use?  Are any of the facilities remaining?  If the Charlottesville Land Co.'s streetcar plan was implemented and still existed, how many Charlottesville citizens would be served?  How do you answer these questions?  Here is where GIS can help.\n\nSteps in spatial analysis:\n\n1. Georeference the maps.  Georeferencing is the process of taking scanned maps and geolocating them - generally through the use of control points - for use in GIS or other tools.  Check [here](http://webhelp.esri.com/arcgisdesktop/9.3/index.cfm?TopicName=Georeferencing_a_raster_dataset) for more information.\n2  Digitize the relevant features using the ArcMap editor.  More information [here](http://help.arcgis.com/en/arcgisdesktop/10.0/help/index.html#//001t00000001000000.htm).\n3. Perform spatial analysis.  In this case, creating a service area around the lines using 1/4 mile buffer.  That distance is considered a serviceable walking distance.\n\n[caption id=\"attachment_1939\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/CLC-1024x768.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/clc/) Georeferenced Charlottesville Land Co. map[/caption]\n\nThe above image shows the georeferenced Charlottesville Land Co. map with 40% transparency overlaid on an [Open Street Map](http://www.openstreetmap.org/) (OSM) layer.\n\n[caption id=\"attachment_1940\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/1920-1024x768.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/1920/) 1920 map with OSM base map[/caption]\n\nThis copy of the 1920 map was the first one I acquired using a handheld camera (opposed to scanning as with the other version).  Notice my fingers in the lower left holding the book open.  While nowhere near optimal, this shows that even poor quality photographs of maps can be georeferenced with decent results. Once all the maps were georeferenced, I was able to digitize all the pertinent streetcar system features.  I chose to show all lines from the maps including conceptual, planned, and abandoned.  I also included all the pass-bys, turn-arounds and support buildings including the electric generation plant on the Rivanna River near Woolen Mills (not on the map above but found using aerial images).\n\n[caption id=\"attachment_1948\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/trolleyFeatures-1024x791.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/trolleyfeatures/) Digitized Charlottesville streetcar system overlayed on modern aerial imagery[/caption]\n\nThe above map shows all the streetcar routes, realized or otherwise.  It includes all the system features except the power plant adjacent to the Rivanna River.  A PDF showing the full extent of the system (with the ability to toggle layers on and off) is available [here](http://people.virginia.edu/~dcg6b/CvilleTrolleySystem.pdf).\n\nSo what if the Charlottesville streetcar system was fully realized and still existed today?  How many Charlottesville citizens would it serve?   Using spatial analysis techniques, we can answer that question.  To do this, I downloaded the 2010 census population counts and joined them to a block-level boundary layer.  I then created the walking buffer of 1/4 mile around the streetcar routes and aggregated the census blocks within the buffer to get the answer.\n\n[caption id=\"attachment_1974\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/trolleyCensus-1024x768.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/trolleycensus/) Proposed Charlottesville streetcar system with 1/4 mile service area buffer overlayed on 2010 census blocks[/caption]\n\nThe above map shows the full streetcar system in black with the service buffer around it in red.  The background is the 2010 census blocks.  Notice the two donut holes and the white areas.  The white areas in the map are Albemarle County.  At this point, we are only interested in city residents.  However, if you wanted to get the full extent of the service population, the UVa Grounds would have to be included (the eastern section of the service area is mainly over the river and unpopulated areas).  The next step is to use a technique called a spatial join to aggregate the total population for each block to the streetcar service area.  More on that [here](http://help.arcgis.com/en/arcgisdesktop/10.0/help/index.html#//005s0000002n000000.htm).\n\n[caption id=\"attachment_1979\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/bufferJoin.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/bufferjoin/) Showing attributes of spatial join aggregation[/caption]\n\nYou can see by the last column in the above table that the streetcar service area covers around 33,267 city residents.  I say around because any block that intersected the service area buffer boundary was counted in full even though a percentage of people living within that block may not be within the buffer.  Using our block level tabular data, we can gather some basic statistics including total population for the city.\n\n[caption id=\"attachment_1980\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/sum.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/sum/) Using the basic statistics feature in ArcMap[/caption]\n\nThe 2010 total population for Charlottesville is 43,475 which mean that approximately 76% of the city's population would be served by the streetcar system.\n\n\n# Some More Pictures\n\n\nThe Holsinger Collection in the UVa Library's Special Collections has many great pictures of the streetcar system.  Here are some of my favorites along with a few other images.\n\n[caption id=\"attachment_1918\" align=\"aligncenter\" width=\"470\"]![](http://www.scholarslab.org/wp-content/uploads/2011/08/X02377B-1024x809.jpg) Streetcar near Rotunda - Holsinger Collection - Special Collections, University of Virginia Library[/caption]\n\n[caption id=\"attachment_1981\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/Y20866B-1024x801.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/y20866b/) Main St. looking east, electric street car about to pass Christian's Pizza (third full building on the right) - Holsinger Collection - Special Collections, University of Virginia Library[/caption]\n\n[caption id=\"attachment_1982\" align=\"aligncenter\" width=\"450\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/X1803B.jpg)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/x1803b/) Looking north towards power plant, Rivanna River on the right - Holsinger Collection - Special Collections, University of Virginia Library[/caption]\n\n[caption id=\"attachment_1983\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/X06208B1-copy-1024x861.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/x06208b1-copy/) Working on the tracks, looking north up JPA Extended - houses in background exist today - Holsinger Collection - Special Collections, University of Virginia Library[/caption]\n\n[caption id=\"attachment_1986\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/X06114B11-1024x837.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/x06114b1-2/) Troops marching north up Rugby along tracks, Fayerweather Hall on the left, Mad Bowl on the right - Holsinger Collection - Special Collections, University of Virginia Library[/caption]\n\n[caption id=\"attachment_1985\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/X06208B3-1024x854.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/x06208b3/) Ridge St. with car barn in background, bridge over tracks and church exist today, car barn now the Greyhound station bus entrance - Holsinger Collection - Special Collections, University of Virginia Library[/caption]\n\n[caption id=\"attachment_1987\" align=\"aligncenter\" width=\"470\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/Y08206B-1024x810.png)](http://www.scholarslab.org/digital-humanities/charlottesvilles-street-car-system-in-gis/attachment/y08206b/) Installing tracks on University Ave. near the Corner - Holsinger Collection - Special Collections, University of Virginia Library[/caption]\n\nOf course once you know where to look, evidence of the streetcar system is around.\n\n[caption id=\"attachment_1988\" align=\"aligncenter\" width=\"470\"]![](http://www.scholarslab.org/wp-content/uploads/2011/08/trolley-005-1024x768.jpg) Intersection of University and Rugby looking southeast[/caption]\n\nIf you look closely in the crosswalk there, you can see the old track as it turns off University Ave. onto Rugby Ave.\n\n[caption id=\"attachment_1991\" align=\"aligncenter\" width=\"470\"]![](http://www.scholarslab.org/wp-content/uploads/2011/08/trolley-008-1024x768.jpg) Closeup of exposed streetcar track in crosswalk across Rugby at University[/caption]\n"},{"id":"2011-08-24-announcing-the-praxis-program","title":"Announcing the Praxis Program","author":"bethany-nowviskie","date":"2011-08-24 06:16:48 -0400","categories":["Grad Student Research"],"url":"announcing-the-praxis-program","content":"We've radically re-imagined the teaching and training we do in Scholars' Lab R&D, and today are excited to launch the [Praxis Program](http://praxis.scholarslab.org)! This is a pilot, we hope, of big things to come, and it is meant to complement our work with [Graduate Fellows in Digital Humanities](http://www2.lib.virginia.edu/scholarslab/about/fellowship.html), a program now entering its fifth year at UVa Library.\n\nIn pilot mode, the Praxis Program will fund six University of Virginia graduate students from a variety of disciplines to apprentice with us for an academic year as we design and build _Prism,_ a new tool for \"crowd-sourced\" textual analysis, visualization, and humanities interpretation. (More about that, later.)\n\nOur goal is fairly lofty: recognizing that methodological training in the digital humanities is often absent or catch-as-catch-can at the graduate level, we are using the Praxis Program to experiment with an action-oriented curriculum _live and in public,_ -- hoping to attract local allies as well as partners in labs and centers at other institutions (which could, in future, work as nodes in a larger Praxis Program network). Above all, we want to situate our contribution to methodological training within a larger conversation about the changing demands of the humanities in a digital age.\n\nThe Praxis Program will equip knowledge workers for emerging faculty positions and [alternative academic careers](http://mediacommons.futureofthebook.org/alt-ac) at a moment in which new questions can be asked and new systems built. We'll share our [evolving curriculum](http://praxis.scholarslab.org) and our faculty, staff, and students alike will be [blogging about their experience](http://www.scholarslab.org/category/praxisprogram/).\n\nSo watch us this year as we see what it takes to produce thoughtful DH scholars who are comfortable designing effective user experiences, writing and working with open source code, engaging broad audiences, managing teams and budgets, and theorizing their work within the rich tradition of humanities computing.\n"},{"id":"2011-08-24-mapping-the-earthquake","title":"Mapping the Earthquake","author":"chris-gist","date":"2011-08-24 12:40:08 -0400","categories":["Geospatial and Temporal"],"url":"mapping-the-earthquake","content":"One good thing about living in this age is instant access to information.  What could be better than that?  Maps!\n\nThe USGS has up-to-the-minute maps for earthquakes all over the world.  For the latest Virginia events click [here](http://earthquake.usgs.gov/earthquakes/recenteqsus/Maps/US2/37.39.-79.-77.php).  You can find their main earthquake page [here](http://earthquake.usgs.gov/earthquakes/).\n\nThe USGS also has a crowd-sourced program - called Do You Feel It? - where users can gauge the quake at their location and report back to help build the map below.  More on that program [here](http://earthquake.usgs.gov/earthquakes/dyfi/).\n\n[caption id=\"\" align=\"aligncenter\" width=\"367\"][![City map](http://earthquake.usgs.gov/earthquakes/dyfi/events/se/082311a/us/se082311a_ciim.jpg)](http://earthquake.usgs.gov/earthquakes/dyfi/events/se/082311a/us/index.html) Crowd-sourced intensity map - USGS[/caption]\n\nThe good people at [Development Seed](http://developmentseed.org/) created some cool maps just after the largest Virginia quake using publicly-available data and some tools from the guys at [MapBox](http://mapbox.com/).  Please click [here](http://developmentseed.org/blog/2011/aug/23/map-todays-east-cost-earthquake-available-mapbox) to see how they quickly mashed up the earthquake data to make some great maps.\n\n[caption id=\"\" align=\"aligncenter\" width=\"500\"][![](http://farm7.static.flickr.com/6202/6074536202_f060ba45dc.jpg)](http://tiles.mapbox.com/mapbox/#!/map/map_1314132938521) http://tiles.mapbox.com/mapbox/#!/map/map_1314132938521[/caption]\n\n\n\nThough, I think the most interesting visualization is this [animated one](http://youtu.be/IKE7MLNdtcg), showing the earth rippling like a pond.\n\n\n\n[caption id=\"\" align=\"aligncenter\" width=\"435\"][![](http://www.scholarslab.org/wp-content/uploads/2011/08/earthquakeWave.png)](http://www.youtube.com/watch?v=IKE7MLNdtcg) YouTube: http://www.youtube.com/watch?v=IKE7MLNdtcg[/caption]\n"},{"id":"2011-08-26-praxis-program-ethos-and-charter","title":"Praxis Program Ethos and Charter","author":"jeremy-boggs","date":"2011-08-26 07:25:35 -0400","categories":["Grad Student Research"],"url":"praxis-program-ethos-and-charter","content":"As Bethany already explained in her [last post](/2011/08/24/announcing-the-praxis-program/), the [Praxis Program](http://praxis.scholarslab.org) will give a few graduate students the opportunity to learn on-the-job in a humanities shop. The program centers around building a project, named Prism, for public use and critique. For the next academic year, students will work on that project, and in the process critically engage a host of activities we think vital for knowledge workers in the digital age.\n\nNo doubt this is hands-on learning: Wayne, Eric R., and David will have students eyeball-deep in programming (and those oh-so-important unit tests!), while Joe and I talk about design and usability. Bethany, Julie, and Eric J. will take turns preparing students for project management and outreach, budgets, and team-building. There’s a rough schedule and list of topics on the [Praxis site](http://praxis.scholarslab.org), but the direction and shape of the year will depend on how each week goes.\n\nMuch more than learning to design, code, and manage grants, the Praxis Program strives to realign attitudes about humanities graduate training. We have a developing ethos for this project, one we hope Praxis participants adopt, adapt, and expand:\n\n\n\n\n  * Work is finished only when it’s as good as it can be, which is usually not when you think it is.\n\n\n  * Like most DH work, learning is on-going and interactive. While this can be frustrating, it can also be exciting and liberating. Embrace it.\n\n\n  * Don’t ask permission to look into or try something. Just do it, and let us know how it goes.\n\n\n  * Be willing and ready to share anything you learn and anything you produce. If you’re not learning, you should be teaching, and vice versa.\n\n\nStudents in the program are not code monkeys doing what we tell them to. They will have an incredible amount of agency in shaping the final project and the program as a whole. That agency is reflected in the very first thing we asked Praxis students to do: Create a project charter. The charter is an informal agreement among the group about how the project will be built and maintained and how credit on the project will be given.\n\nTo prompt thinking about the charter, we asked the Praxis team to address a few questions:\n\n\n  * What conflicts might arise from this collaboration, and how would you recommend resolving those conflicts?\n\n\n  * What would be your policy about authorship and credit for works derived from this collaboration, and why? (E.g. a journal article about the project, an article about your personal contribution to the project, a credits page on the project website)\n\n\n  * What would be your policy about maintaining/sustaining the project once this is over?\n\n\n  * How might you deal with colleagues leaving in the middle of the project, or new colleagues coming into the project?\n\n\n  * What kind of license would you want to apply to this project, and why?\n\n\n  * Who is, or should be, the audience for this charter? Would you publicize it? If so, how? If not, why not?\n\n\n  * What are the desired outcomes of the project? Which outcomes do you consider to be _required_ rather than merely desired? (examples: release of working software, release of open source code, formal and informal scholarly publications, professional development / CV enhancement)\n\n\n  * Who are signatories to the charter? What does this decision imply about the working relationship between Praxis students and Scholars’ Lab faculty and staff?\n\n\nLook for their first-draft, individual answers on this blog in the coming days. Next week, we’ll draft and post our collaboratively-written charter!\n"},{"id":"2011-08-29-let-the-process-begin","title":"Let the process begin:","author":"brooke-lestock","date":"2011-08-29 17:21:02 -0400","categories":["Grad Student Research"],"url":"let-the-process-begin","content":"Hello, digital universe! (You’ll have to excuse my childlike enthusiasm; this is all fairly new to me.)\n\nI have to admit that I’m a bit nervous, because of my novice DH status, to have so much control over the design and progress of a program that has the potential to really transform graduate student training at a time that seems ripe for it. On the other hand, I realize how lucky I am to enter this program and the DH “field” (if it can/should be defined as a field) with a blank slate - or at least a slate that has only some rudimentary XML chalked into the corners along with a lot of cloudy eraser marks. This year will take us from 0 – 60, and for my own professional development and CV adornment purposes, I’m not ashamed to say that I’m thrilled. But if that was my only motivation for joining the Praxis Program, I’d be doing myself and the program (and graduate students everywhere) a huge disservice. Likewise, if as a group we Praxisers focus only on _our_ project on _our_ terms (i.e., the success of _Prism_), we’ll do a disservice to any of the communities of scholars or researchers who could benefit from our work.\n\nIn this last sentence I realize I’ve addressed two of the charter prompts which started me scribbling notes like a mad scientist when I first read them: the prompts relating to the Praxis Program's audience and outcomes. As for the audience, because the specifics of the project are not completely determined as this point, the design of this charter must and naturally will move beyond a project-specific ethos or modus operandi. In writing this charter at the very outset, we are setting guidelines that can be adopted by _any_ project of this nature, and thus making our “work” available and relevant to the interests of a larger community.\n\nTo readdress the word “work”: In Bethany’s article, “[Where Credit Is Due](http://nowviskie.org/2011/where-credit-is-due/),” she speaks persuasively of the responsibility of tenure and promotions committees to “assess quality in digital humanities work – _not_ in terms of product or output – but quality that is embodied in an evolving and continuous series of transformative _processes_.” The evaluation of the Praxis Program should be undertaken with this in mind, and so I would hesitate to apply specific end-product requirements to a pilot program like this. That being said, we will be working on a digital tool and should aim at being able to release software and open source code. Our ultimate goal, though, should be to account for the _process_, which absolutely necessitates publicizing and archiving along the way (though like Sarah, I can’t quite speak comfortably about what archiving entails). Accounting for the process also requires that we aim to produce formal and informal scholarly publications on the project’s development, and that we pursue every opportunity to make our process available to the public and open for discussion. Long live the process. ****\n"},{"id":"2011-08-29-live-and-in-public","title":"Live and in public!","author":"sarah-storti","date":"2011-08-29 15:19:30 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"live-and-in-public","content":"To prepare for our meeting last week, all Praxis Program participants read the following pieces:\n\n\n\n\n  * Bethany Nowviskie, [\"Where Credit is Due.\"](http://nowviskie.org/2011/where-credit-is-due/)\n\n\n  * Stan Ruecker and Milena Radzikowska, [\"The Iterative Design of a Project Charter for Interdisciplinary Research.\"](http://mtroyal.academia.edu/MilenaRadzikowska/Papers/326958/The_Iterative_Design_of_a_Project_Charter_for_Interdisciplinary_Research)\n\n\n  * Siemens, et. al. [\"INKE Administrative Structure, Omnibus Document.\"](http://journals.uvic.ca/index.php/INKE/article/view/546/245)\n\n\nThese links are also available [here](http://praxis.scholarslab.org/topics/toward-a-project-charter/), but so that you, my reader, may easily follow my references (and explore for yourself) I resubmit them now. First of all, I would like to express my delight that, as Bethany put it in her [blog post of the 24th](/2011/08/24/announcing-the-praxis-program/), “we are using the Praxis Program to experiment with an action-oriented curriculum _live and in public_.” That emphasis on “live and in public” had me searching back through the above-posted materials, specifically Ruecker and Radzikowska’s piece, as I thought about my blog post for this week. I recall nodding vigorously and perhaps even whisper-shouting “yes!” to myself in the library last Monday when I came across this policy, under the subheading “Professional dignity”:\n\n“We will attempt to keep communications transparent, for example, by copying everyone involved in any given discussion, and by directly addressing with each other any questions or concerns that may arise.”\n\nThe prioritization of transparency, here meant to ensure that members of the project maintain healthy professional relationships, sounds like an excellent strategy. I would love to see something similar in our charter: it seems like an easy and probably very effective way to reduce conflict and encourage mutual respect. But I become even more excited when I imagine the “everyone involved” to include the general public—that is, you, dear reader. Ruecker and Radzikowska’s paper continues as follows:\n\n“This policy of transparency is another simplifying strategy. If too much back-channel discussion takes place, it can become very difficult for everyone to understand what decisions are being made and why, especially on a geographically distributed team.”\n\nThe audience for this blog could easily be described as a “geographically distributed team” of commentators and interested parties. Though the central Praxis Program group consists of local participants who are fortunately able to get together every single week for two hours, we are very interested in sharing what we do—whether it succeeds brilliantly or fails just as brilliantly—with _you_. I feel exceptionally lucky to have been selected to work with this talented and enthusiastic group, and desire passionately to make this kind of thing happen for other graduate students.  What decisions are being made, and why? You, reader, should feel that you are able to answer these questions as we go along. I am sure that some kind of policy about public access was always intended to go into our charter, but here is my formal declaration of support for it. The more we put out there—the more publicly we do this thing—the better for our geographically distributed team. It might not always be pretty, but at least we'll have a record of how, exactly, we got wherever it is we end up.\n\nLast week, Jeremy called these blog posts “first-draft” components of the charter: it is a thrill to know that this is the beginning of our _live and in public_ adventure.\n\n---\n\nEpilogue: Archiving was the other topic I wanted to write about, though with my relative newness to most things DH I felt less than qualified to do so in an intelligent way: I’m not exactly sure how it works. I would, however, like for us to talk about it as we consider what will go into the charter.\n"},{"id":"2011-08-29-owning-up-the-praxis-program","title":"Owning up the Praxis Program","author":"alex-gil","date":"2011-08-29 11:59:05 -0400","categories":["Grad Student Research"],"url":"owning-up-the-praxis-program","content":"On joining the Praxis Program, I knew I was in for something new. As part of the most recent generation of DH'ers at Uva, I've had time to develop a healthy dose of envy for the heroic age of SpecLab or the early years of NINES (not so long ago to be honest), when the DH demi-gods were said to roam the halls of Bryan. In the past couple of years, there have been informal attempts to revive the gall and vision of those who (just) came before us... without much success. Perhaps it was time to give up. After all, UVa continues to be a DH powerhouse without the shop-apprentice model of (not-so) yore. Perhaps the problem was that our impromptu efforts were tinged with nostalgia. When I was invited to become part of the Praxis Program, I knew this was something different, something new. Finally, we had a shop-apprentice model that I could make my own, that we could make our own.\n\nTo talk about ownership in the hour of open access and crowdsourcing may seem oxymoronic, but I beg to differ. Before I came to the academy I was a salaried worker for more companies than I would care to enumerate. Though the service-industry's book of mantras includes a line or two on how the company belongs to everyone, no one really buys that. Once in the academy, I've had a chance to help several faculty members with their projects where all I got in return was a footnote of appreciation. Even the countless ENWR courses I've been deputized have felt alien. In the end, the only thing I felt belonged to me where my most solitary scribblings and my toothbrush. I realize now that what makes the difference is creative direction. I too want to own what I create, but in my previous brushes with collaboration, I've always felt the only 'I' came from the top. When I saw that the first assignment of the Praxis Program was for us to design our own charter, I knew I was in for something new, something that is already starting to feel like my own.\n\nWith that in mind, here are some of the things I would like to see in the final version of our charter:\n\n\n\n\t\n  * **Credit should be non-hierarchical**: Though the program that's allowing us to build this project has a steward, the project itself should be credited to all of us.\n\n\t\n  * **Detailing the contributions**: Though we all get credit for the project, we should still publish a list of detailed contributions for audiences which require more details.\n\n\t\n  * **One for all and all for one**: Though we each can end up focusing more on those things suited to our individual calling, we should all be equal partners in the overall progress of the project.\n\n\t\n  * **Departures**: In the unlikely event that one of us leaves the project, that person should always receive credit by dates worked and contributions made, and have the right to reference the project on their vita.\n\n\t\n  * **New Members**: New participants should be given credit by date of arrival.\n\n\t\n  * **License**: I vote wholeheartedly that we offer everything we make open-access, open-source through a [Creative Commons Attribution](http://creativecommons.org/licenses/by/3.0/) license. Bethany provides excellent rationale in [\"Why, of why, CC-BY?\"](http://nowviskie.org/2011/why-oh-why-cc-by/).\n\n\t\n  * **Taking ownership**: I say we codify in the charter our commitment to promote the project publicly, to link it to our online personas, to make it truly our own. Each member should be allowed to list the project on their Vitas or Webpages under current projects or whatever appropriate equivalent\n\n\t\n  * **Non-representative democracy**: All major aspects of the project should be decided on a 2/3 vote with full quorum. This means we should also codify what we consider to be these major aspects.\n\n\n"},{"id":"2011-08-29-preliminary-praxis-charter-ideas","title":"Preliminary Praxis Charter Ideas","author":"ed-triplett","date":"2011-08-29 11:54:45 -0400","categories":["Grad Student Research"],"url":"preliminary-praxis-charter-ideas","content":"In only its second week, the Praxis Program does not yet have a well established Identity. Creating a project charter will help narrow our focus, and allow us to establish rules of operation. As a group, we are subject to a number of challenges inherent in being the first members of the program, as well as the added hurdle of learning to work in an interdisciplinary group. The umbrella of the “humanities” does a poor job of representing who we are as scholars, and we each bring something different to the Praxis table. As we discussed last week, even “English” does not properly unite the vast range of interests held by five members of our group who work in that department. We should therefore be wary of the view that we all speak the same humanist language even before we add our new “digital” vocabulary. On one hand, as someone not as well versed in textual analysis as other members of the program, I would find it very valuable to read an article each week chosen by one other member of the group that they believe represents a methodology that has been helpful in their field. On the other hand, I do not think that our individual skills should be rounded off in the desire to create a common experience and contribution for everyone. As collaborators, we do not need a formal hierarchy, but we should have individual responsibilities that play to our strengths.\n\nWe all had individual goals when we applied to this program, but I don’t believe I am alone in assuming that professional/ CV development lies at the heart of our interest.  That said, one of the strengths of this program is its ability to introduce some of us to the rules, positives and negatives of online publishing. The mystique of traditional publishing can act as a deterrent for graduate students to publish their work, yet it is always in our best professional interest to do so. In the hope of breaking down my own hesitance with regard to publishing I hope to digitally publish at least one well-crafted and edited article relating to my experience in this program. The Praxis blog should serve as our collaborative identity, and there should be a credits page linked to the site, yet we are still hired as individuals, and we cannot simply expect our future employers to be enlightened with regard to collaborative authorship as they look over our CVs.\n\nThe final issue that I have not yet come to a conclusion on is the Praxis Program’s level of self-promotion. This program has the ability to be our “boldest” representation online as scholars and there is a fine line between keeping our audience interested in our progress and shouting every minor accomplishment. I am personally rather new to using social media for professional development, and I would greatly value a discussion of how this will work in our program.\n"},{"id":"2011-08-29-thoughts-on-our-charter","title":"Thoughts on our Charter","author":"annie-swafford","date":"2011-08-29 19:09:57 -0400","categories":["Grad Student Research"],"url":"thoughts-on-our-charter","content":"Although I have experience in designing my own digital project (through NINES) and in working on pre-existing ones (through Documents Compass’s Adams and Madison papers projects), I’m getting my first taste of being on a team that will work together to design and manage a project.  Since my thinking tends to be more detail-oriented than abstract, the act of coming up with a charter to govern our interactions before we have a sense of the project itself is a challenge, but a welcome one.\n\nHere are some thoughts on our charter:\n\n\n\n\t\n  * **Equal Credit: **Like Alex, I think that we should find a way to have all Praxis Program participants get equal credit, although I’m not sure of the best method.  Would designating authorship with alphabetical order be the clearest, or should we just say “by the Praxis Program” and add our names in a footnote?\n\n\t\n  * **Publicity Opportunities:**  I think we should each be able to submit articles or attend conferences about our personal contributions to the project, and we should be able to add it to our CVs, but we should always openly acknowledge that this project is a joint venture.  We shouldn’t be able to put someone else’s article about the project on our CV unless we actually helped that individual write the article.\n\n\t\n  * **Sharing the Work:** Although I think we should all try our hands at every aspect of the project, it seems reasonable that some people might prefer to spend more time on some elements than others.  Therefore, once we’ve settled on the features we want our project to have, maybe we should divide it into subsections and appoint each member to head a subsection.\n\n\t\n  * **Conflict Management: **Since conflicts may arise regarding features or the intended audience of Prism, setting up some sort of voting system to resolve conflicts might be a good idea.  However, I feel as though the head of a subsection should maybe have a larger say in matters directly involving the subsection, since we don’t want someone to be forced to work on a feature he/she believes is doomed to fail.\n\n\t\n  * **Maintaining the Project:** I would imagine that we as a group would not want to run the project for the rest of our days, so we should maybe see if one person wants to be in charge of it after the project is over, at which point the rest of us would become the “advisory board,” or to see if Scholar’s Lab would like to take charge while still listing us as authors/founders.\n\n\t\n  * **Outcomes:** In addition to releasing working software, I would also expect us to release the open source code and to use the project for our professional development.\n\n\n"},{"id":"2011-08-30-chartering-the-unknown","title":"Chartering the Unknown","author":"lindsay-o’connor","date":"2011-08-30 09:53:20 -0400","categories":["Grad Student Research"],"url":"chartering-the-unknown","content":"Like [Alex](/2011/08/29/owning-up-the-praxis-program/), I’m excited for what seems to be a true shop-apprentice approach to learning [this whole list](http://praxis.scholarslab.org/) of skills, methods, and programs. Before I came to UVa I held a few jobs with a company that hired people based on general knowledge and potential rather than specific technical know-how, and I learned to use new technological tools as my responsibility increased with each position. While I assume we’re accepted to graduate school not for our current knowledge but for our potential to do great work later on, the generally ability v. specific skills parallel doesn’t quite work for a field in which the product is criticism, scholarly writing, or humanistic knowledge (whatever that is). But DH refigures or even expands the endpoints of humanistic inquiry and allows for different forms of education and training along the way. Somebody decided I have the potential for insightful literary analysis, and I’m glad the kind folks in the SLab have decided there’s hope for me to add some technical and managerial tools to my professional toolkit. But it's also quite new and difficult to have to draft a charter for a project I know almost nothing about so far. How much will the knowledge and skills we gain through the Praxis Program's weekly workshops change our ideas about  self-governance and credit?\n\nLike [Edward](/2011/08/29/preliminary-praxis-charter-ideas/), I’m interested in and concerned about interdisciplinarity. [One of the model charters](http://mtroyal.academia.edu/MilenaRadzikowska/Papers/326958/The_Iterative_Design_of_a_Project_Charter_for_Interdisciplinary_Research) we examined suggests an interdisciplinarity that is necessarily collaborative, with participants both “thinking across boundaries” and “communicating with people unlike [ourselves].” I’m much more used to the former but am definitely looking forward to the latter, and to working with people from the same disciplinary background as me (literature) who are now working in “non-traditional” positions. To make this work, I hope we can take the advice of the charter linked above and “map the relevant conceptual territory” early on in order to de- and re-territorialize not only our disciplinary backgrounds but also our individual theories, methodologies, and working idiosyncrasies. (Mine seem to include a tendency toward spatial thinking and obnoxious theory references. Sorry, team.)\n\nLike [Sarah](/2011/08/29/live-and-in-public/), I hope for professional dignity and transparency at all steps in the process. We’re lucky to have our core group all here in Charlottesville, but Sarah is right to point out that our working group doesn’t end there. Since we’re going to be working on crowdsourcing interpretation and since [Brooke](/2011/08/29/let-the-process-begin/) emphasizes “accounting for the process,” I wonder if we might invite blog readers to help us with our charter as we draft it over the next few weeks. Some crowdsourced input and interpretation by experts and amateurs near and far might keep us apprised of issues we’re overlooking, mishandling, or failing to anticipate.\n\nI admire [Annie](/2011/08/29/thoughts-on-our-charter/) for looking forward to sharing work and sharing credit. I can already see that she’ll be great at keeping details straight and bringing us down from abstractions and What Ifs. I too hope we can share credit equally, and taking leadership on individual components, whether technical or managerial, seems like the right idea. I’m still hesitant to say much more about how this will all work without talking more about what the particular goals of this project will be. I’m all for various forms of publication and the release of open-source software, but I’d like to talk more about what that software is going to do before we get too far into this charter. This requires even more looking ahead to unknown outcomes than the dissertation prospectus I’m struggling with right now!\n"},{"id":"2011-08-31-vim-gui-font-magic","title":"Vim GUI Font Magic","author":"eric-rochester","date":"2011-08-31 07:25:00 -0400","categories":["Grad Student Research"],"url":"vim-gui-font-magic","content":"Yesterday, [Wayne](/people/wayne-graham/) tweeted [this](http://twitter.com/#!/wayne_graham/status/108550158442184704):\n\n\n\n<blockquote>\n@erochest just dropped some vim wizardry on the @scholarslab: open system fonts, set the guifont from the settings selected in gui\n</blockquote>\n\n\n\nI thought I should describe what I did, for several reasons. First, it's kind of cool, in a geeky vim-lover kind of way. Second, it's not something you do everyday, so it's helpful to have it written down. I'm pretty surprised that I remembered it as quickly as I did.\n\n\n\n## Step 1: Select the Font Using the Standard Font Dialog\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/08/Screen-shot-2011-08-31-at-10.32.05-AM-300x239.png)](http://www.scholarslab.org/praxis-program/vim-gui-font-magic/attachment/screen-shot-2011-08-31-at-10-32-05-am/) In Vim, pop open the GUI to select the font by typing in this command: `:set guifont=*`. It should open up the standard font dialog box for your platform.\n\nSelect the font you want to use and close the dialog.\n\n\n\n## Step 2: Add the Font to Your .vimrc file.\n\n\n\nNow you need to make the change permanent by adding it to your `.vimrc` file.\n\nOpen this file using the command `:e ~/.vimrc` (on Linux, UNIX, and Mac) or `:e ~/_vimrc` (on Windows). Then add this to the bottom:\n\n\n\n<blockquote>`set guifont=`</blockquote>\n\n\n\nAt this point, you need to insert the current value of `guifont`. Do this by pressing Ctrl-R (while still in insert mode) and entering this in the command bar at the bottom of the screen:\n\n\n\n<blockquote>`=&guifont;`</blockquote>\n\n\n\nHit enter. The name and size of the font should be appended to the `set guifont` line you started.\n\nIf the font name has spaces, however, you'll need to escape those. This command should do the trick:\n\n\n\n<blockquote>`:s/ /\\ /g`</blockquote>\n\n\n\nUnfortunately, this does a little more than you want. It also escapes the space after `set`. Take that out manually.\n\nNow, the line should look something like this:\n\n\n\n<blockquote>`set guifont=DejaVu Sans Mono:h13`</blockquote>\n\n\n\n\n\n## Bonus\n\n\n\nSome nice fonts I've used with Vim over the years include these:\n\n\n\n\n\n  * [DejaVu Sans Mono](http://dejavu-fonts.org/wiki/Main_Page);\n\n\n  * [Bitstream Vera Sans Mono](http://ftp.gnome.org/pub/GNOME/sources/ttf-bitstream-vera/1.10/); and\n\n\n  * [Lucida Sans Typewriter Regular](http://www.microsoft.com/typography/fonts/font.aspx?FMID=630) (free with many Microsoft products).\n"},{"id":"2011-09-01-praxis-program-week-2","title":"Praxis Program week 2","author":"eric-johnson","date":"2011-09-01 06:00:03 -0400","categories":["Grad Student Research"],"url":"praxis-program-week-2","content":"Tuesday saw the second weekly meeting of the team involved in the Scholars' Lab's Praxis Program.  The conversation largely revolved around coalescing the [groups' thoughts](http://www.scholarslab.org/tag/charter/) on their project charter.\n\nWhile the specifics of the formal charter are still being ironed out—and will be shared shortly—the group identified four high-level principles to help guide the approach of the project:\n\n\n\n\t\n  * Open and Frequent Communication\n\n\t\n  * When in Doubt, Ask (both inter-personal & training/help)\n\n\t\n  * Getting to Know You/Staying Conscious of Interdisciplinary/Weekly Show & Tell (in other words, sharing individual expertise and interest)\n\n\t\n  * Entire group reviews claims for credit for all derivatives\n\n\nSo what's on tap for the rest of this week?  First off, the students got a crash course on installing Vim, a text editor used by members of the Scholars' Lab's development team for writing code.  Now they have to [figure out how to use it](http://praxis.scholarslab.org/tutorials/vim/).\n\nSecondly, they've got another blog post to do.  If they choose to, they can tackle some of the issues we also discussed during the meeting on [evaluating digital work](http://praxis.scholarslab.org/topics/evaluating-digital-work/).   In particular, Jeremy posed them this prompt: take a look at one or more the sites we listed.  What's one big thing that you would change?  Why would you change it?  And, perhaps most interestingly, don't forget that your change isn't just for you alone.  Keep the other members of the site's audience in mind.  How would your suggested change improve/alter their experience?\n"},{"id":"2011-09-01-vim-config-and-windows","title":"Vim Config (and Windows)","author":"eric-rochester","date":"2011-09-01 02:21:58 -0400","categories":["Grad Student Research"],"url":"vim-config-and-windows","content":"This is a set of instructions for configuring Vim for the Praxis Program. Most of it will only apply to Windows, but I've included some notes for any UNIX-like system (LINUX or Mac) at the bottom.\n\n\n\n## Windows\n\n\n\nThe bulk of this is just downloading and installing things that aren't included by Windows or by the non-existent Windows package manager.\n\n\n\n### Installing Everything\n\n\n\n\n\n\n\n\n  * **Install [Vim](http://www.vim.org/download.php).**\n\n\n\n\n  * **Install [Ruby 1.9.2](http://rubyinstaller.org/).** (You can get the [preview release here](http://rubyforge.org/frs/download.php/74977/railsinstaller-2.0.0.exe)). Make sure you click \"Add Ruby executables to your path.\"\n\n\n\n\n  * **Install [msysgit](http://code.google.com/p/msysgit/downloads/list).** Look for \"Full installer for official Git.\" During the installation select \"Git Bash Here.\" Later either select \"Use Git Bash only\" or \"Run Git from the Windows Command Prompt.\" Finally, also select \"Checkout as-is, commit Unix-style line endings.\"\n\n\n\n\n  * **Download the Windows binary for [Exuberant CTags](http://ctags.sourceforge.net/).** It comes in a ZIP file. Open it and put `ctags58ctags.exe` (it may be display as `ctags58ctags` with type Application) into `C:Program FilesVimvim73`.\n\n\n\n\n  * **Finally, download [cURL](http://curl.haxx.se/download.html).** Download the one at the bottom for \"Win32 - Generic\" labelled  \"Win32 2000/XP binary\" (it also says that it's 1.32 MB at the moment). Open and put `curl.exe`, `libcurl.dll`, `libeay32.dll`, and `libssl32.dll` into `C:Program FilesVimvim73`.\n\n\n\n\n\n\n\n### Vim Configuration\n\n\n\nFor this, you'll need to work from a command prompt. This may be a new experience for Windows users, but don't worry. It won't bite you, and it's easier than it may seem going in.\n\nFirst in Windows 7, click on the Windows button and search for \"bash.\" Open up the program it returns (\"Git Bash\").\n\nIn XP, look under the Start menu for the Git program group and select \"Git Bash\".\n\nNow, open [this script](https://gist.github.com/raw/1166018/install-vimscripts-win.sh). Copy-and-paste each line into the Git Bash Console window that's open.\n\n\n\n### Run Vim\n\n\n\nThat's it. Give it a spin.\n\nLook under the start menu, for the \"Vim 7.3\" group and select \"gVim.\" In Windows 7, you may want to pin it to your taskbar.\n\n\n\n### Fonts\n\n\n\nCheck out [Vim GUI Font Magic](/2011/08/31/vim-gui-font-magic/) for instructions on how to change the font from the truly yeechy Windows default. This also has some suggestions for nicer fonts.\n\n\n\n## UNIX (LINUX or Mac)\n\n\n\nThe process for installing this for Linux or Mac is the same. You'll want to install Vim, Git, and cURL. Chances are, they're already there.\n\nFor Mac, you'll probably want to either download the DMG file from the [MacVim](http://code.google.com/p/macvim/) project or use something like [Homebrew](http://mxcl.github.com/homebrew/) and [Xcode](http://developer.apple.com/xcode/).\n\nFor Linux, you'll probably want to just use your distribution's package manager (`sudo apt-get vim-gnome` for Ubuntu).\n\nAfter that, basically the same script as above should work. You'll want to use the version [here](https://gist.github.com/raw/1166018/install-vimscripts-unix.sh), however, which is modified for UNIX.\n"},{"id":"2011-09-05-evaluating-digital-work-suggestions","title":"Evaluating Digital Work: Suggestions","author":"brooke-lestock","date":"2011-09-05 18:31:42 -0400","categories":["Grad Student Research"],"url":"evaluating-digital-work-suggestions","content":"For this week's post, we were asked to evaluate a few of the digital tools we looked at for last week's meeting. This time we were armed with a list of questions, which I was particularly eager to have as someone with a fair amount of experience evaluating literary work and none whatsoever evaluating digital work. I was relieved to find that the questions are similar to those that must be asked of any scholarly article or book. What problem does it try to solve? What contribution does it make? How would it affect yours and others' work? The big one: _Does it work?_ And so on. Following these guidelines, we were asked to identify one thing we'd like to change, how we'd change it, and who would benefit from the change.\n\nThat being said, here are my suggestions (humbly submitted):\n\n\n\n\t\n  * Linguistic Atlas Projects: The site compiles a group of projects studying English dialects in the US submitted by institutions across the nation, but I think the site would benefit from some kind of uniformity of layout and content. It's intended to be a collection of these linguistic studies, but they’re treated very separately. Once you've selected one project from the side column, the other projects disappear. Some projects’ links will take you to to a map in which you can select a state to review, while others take you to a basic info page. Some projects' pages offer images, recordings, and detailed project descriptions complete with bibliographies, while others offer only maps and subject data. I imagine it's quite difficult to put projects with different methodologies in context with one another, but a uniform format would allow users to move between the different projects without becoming disoriented.\n\n\t\n  * Visualeyes: I was so excited by the possibilities of Visualeyes and so anxious to figure out how to use it, the only suggestion I'd make would be to get _more_ basic with tutorials. Visualeyes makes itself available as a non-programmers’ tool with many possible uses, and so there are many tutorials featured, but I had difficulty finding a sort of \"Visualeyes for Dummies\" video that would give me an overview starting at ground zero. The \"Visualeyes Project Guide\" document looked like a promising written guide, but its page was unavailable. I wasn't quite sure where to begin.\n\n\t\n  * OldWeather: I enjoy the interactive, collaborative nature of sites like this one and What's on the Menu. OldWeather in particular pushes for public involvement, but so much so that the bigger picture becomes unclear. The home page highlights the transcription \"game\" while providing just a few introductory sentences and a short intro video for the program itself. There seems to be little information available on how the data will be interpreted, what the projected outline or timeline of the work will be, how the project responds to other research being done in the field, etc. The goals of the project should be clearly advertised and delineated on the site, which will then give users a better idea of _why_ they should participate.\n\n\n"},{"id":"2011-09-05-further-evaluation-of-digital-work","title":"Further Evaluation of Digital Work","author":"annie-swafford","date":"2011-09-05 19:55:36 -0400","categories":["Grad Student Research"],"url":"further-evaluation-of-digital-work","content":" Although I feel a bit silly evaluating digital work when I’m still a comparative newbie to the DH world, I found that looking at these projects helped me not only improve my ability to imagine how projects could be altered to suit different purposes, but also understand what sort of features or documentation I would like us to incorporate in our own project.  In general, I’m a big believer in explicit instructions, and wherever possible, an explanation of why the project is important.  I also prefer projects that can be accessed/used in multiple ways (For Better For Verse), but I also like projects that have a single but incredible clear purpose that is easy to follow (What's on the Menu).\n\n\n Group 1:  [For Better For Verse](http://prosody.lib.virginia.edu/):\n\nThis is a fantastic project that can help teach students all elements of poetics, including rhyme scheme, meter, and how they relate to interpretation.  I was also very impressed by the sorting principles (you can organize the poems by title, difficulty, and author).   I can only think of a handful of features that I would like to see added:  I wish there were a way to sort by time period, an explanation of why a given scansion is correct. Also, as a TA, I wish my students could practice scanning poems they’re studying in class, so it would be nice if someday there were a way for people to contribute content so that teachers and students wouldn’t be limited to the (extensive collection of) poems there if they want to use it for class.\n\nGroup 2:  [TILE:](http://mith.umd.edu/tile/)\n\nThis seems like a really useful tool for creating digital editions where you want to clearly show which lines of a transcript correspond with which lines of the image.  I thought the sandbox feature, which lets you try the tool without downloading it was a great idea, and once I found the instructions, it became easy to use.  Although I think TILE is important, I think it would be clearer if the website had links to projects that had used this tool, or maybe a video to show what it’s capable of, because it took me some time to be able to understand its purpose and to visualize how it should be used.\n\nGroup 3:  [What’s on the Menu](http://menus.nypl.org/):\n\nThis archive features crowdsourced transcription of menus held in the New York Public Library’s rare books division.  It’s easy to add content; you just have to click on each dish and type the text as it appears on the menu, and it instantly becomes part of their searchable text.  I also thought that the site was incredibly well designed from a user perspective; the instructions were clear, and the rationale behind the project made me more interested in it than I had been when I was just looking through the menus.  I hope we can be influenced by their documentation in our own project.  I could barely come up with anything I wish were different; my only complaint is that the instructions say to ignore some typographical features, like accents, but not about other irregularities (ie. should we preserve line breaks within menu items).  Other than that, I think it’s excellent.\n"},{"id":"2011-09-05-in-which-a-novice-evaluates-digital-work","title":"In which a novice evaluates digital work","author":"sarah-storti","date":"2011-09-05 19:18:14 -0400","categories":["Grad Student Research"],"url":"in-which-a-novice-evaluates-digital-work","content":"After spending a good forty minutes browsing the [Valley of the Shadow digital archive](http://valley.lib.virginia.edu/), I must admit that I’m defeated by the prompt Jeremy gave us last week for these blog posts. I simply could not find anything about the archive I would care to change. I approached it aggressively, determined to be disappointed or confused, or perhaps annoyed by the difficulty of navigating the project, but each time I thought “aha! that’s it!” a page scroll or more careful reading revealed the solution to my imaginary problem. In lieu, therefore, of explaining what about the archive I think could be changed to improve the user’s experience, I want to point out very briefly how the folks at the Valley of the Shadow impressed the pants off me. Firstly, the archive is prefaced with an excellent description of its _raison d'être_. Upon entering, a blueprint of sorts (oooh!) describes very succinctly and logically (in my humble opinion) how the information in the archive is organized. I dove in right away, and discovered through experimentation how to navigate the archive. Those who prefer a less visual or more guided navigational experience, however, can consult the “walking tours” for each of the topic sections. My favorite discovery, though, was the “Using the Valley Project” section, which contains examples of how previous visitors have put the material to use in various (and very different) ways. At no point did I feel confused, disappointed, or annoyed. I just felt eager to continue exploring the American Civil War as witnessed by Augusta (VA) and Franklin (PA) counties.\n\nI also looked closely at the [September 11 Digital Archive](http://911digitalarchive.org/index.php), which is an incredibly enormous collection of digital items relevant in some way to the September 11, 2001 terrorist attacks on the United States of America. Selecting “Browse” from the menu on the main page takes the user to a list of links which fall under the categories Stories, Email, Documents, Images, Digital Animations, Interviews, and Audio/Video, and clicking on any link will take the user to a group of related documents… or that’s the idea: the documents should be related, somehow. I suppose that the question I would pose to the curators of this project would be: what is the archive supposed to do? Is it merely meant as a permanent storage site for digital items that are in some way associated with September 11, 2001? And here are some emails? And here are some comment cards, and here some animated political cartoons? I suppose I would recommend that the archive might be more useful and navigable were it presented with more thematic nuance: as is, items are grouped into large categories that do not really encourage the exploration of the rest of the site. With the Valley Project on my mind, I couldn’t help but feel that a chronological approach (with separate sections for items connected to the country before, during and after the attacks) might be one very effective method of organization for these materials. Unfortunately, I was unable to login to the “researcher” account I tried to register, because I wanted to check out the “enhanced access to the entire public and researcher-only collections, including personal notes and favorites storage” which comes with such an account. I imagine being able to mark or store items inside your personal account might help one work through the collection more easily, though I stand by the assertion that the archive could do with a more carefully thought-out method of organization.\n"},{"id":"2011-09-05-project-reviews","title":"Project Reviews","author":"alex-gil","date":"2011-09-05 19:54:08 -0400","categories":["Grad Student Research"],"url":"project-reviews","content":"As part of our second week assignment we were giving the task of reviewing three projects from the following [list](http://praxis.scholarslab.org/topics/evaluating-digital-work/). The list is interesting because it groups projects that have been around for a while with more recent contributions. The three that I chose actually belong to the former category. I thought it would be refreshing to re-view the familiar. I apologize beforehand if what I say below echoes criticism and praise already directed at these projects.\n\n**Zotero**\n\nI've been a user of Zotero for a while. When I discovered it a few years ago it changed the way I collected my research materials. It's been three years now that I've been compiling a large bibliography of Aimé Césaire primary sources and secondary sources. I must here confess that I use Zotero somewhat off the beaten pass as a content management system of sorts. I have transcriptions for many of my items in the note section. This allows me to use the search function through out all my items, returning the bibliographic items. I'm aware that there are better ways to search through a corpus (Solr for example), but I like the fact that I can keep all my research in one single work-space. The tags and related features allow me to record connections between primary and secondary sources that an algorithmic solution might not discover. Being able to organize the database by different categories can also be a great way of visualizing a collection. My collection at this point is enormous, and it is true that the task of adding transcriptions and making connections can be overwhelming as the collection grows, but I figure there is no hurry. As the community continues to develop around the Zotero API, I'm hoping that I will eventually be able to share my bibliographic work in the form of an online publication. It is true that the collection can be offered through the Zotero website, but this does not allow for a personalized design around particular collections. As you will hear often from me, \"analytic\" tools should always move in the direction of publication. I'm happy to see Zotero is laying the groundwork for these kinds of projects.\n\n**Whitman Archive**\n\nIn my mind the[ Whitman Archive](http://www.whitmanarchive.org/) has always been part of an imaginary triumvirate next to [The Rossetti Archive](http://www.rossettiarchive.org/) and [The Blake Archive](http://www.blakearchive.org/blake/). Like the other two it has always suffered from the \"tree\" structure. In order to get to a text you must click through narrowing categories. Once you do, you have arrived at... a text. True, you can use the search box.  It uses the Google algorithm. The logic of the Google algorithm is not necessarily the logic I would like to use for an archive. The archive is the work of Ed Folsom and Ken Price. At all levels of the work we find their imprint, criticism included. There is nothing inherently wrong with featuring the work of the principal investigators, but an archive of this scope and prominence should strive to feature the work of other scholars as well. Another question which is extremely important to me is the presentation of texts. The [typical WA edition of _Leaves of Grass_ ](http://whitmanarchive.org/published/LG/1856/whole.html)for example will include images and lightly formatted text. The design of the pages is not very conducive to reading online. Although the edition remains a great resource for the study of Whitman, it still does not provide an aesthetic experience comparable to a Penguin edition. I believe the two should not be mutually exclusive. Finally, there is the question of what you can do with the texts. So far, the Archive has partnered with [TokenX ](http://www.whitmanarchive.org/resources/tools/index.html)to allow users to do some textual processing with the materials. You can also download the XML files to use with other tools. What I would like to see in the archives of the future is more play within the space of the archive. With a bit of cleverness and resources, design can invite you in to play, and play can lead to better connections, which brings me to...\n\n**TAPoR**\n\nI started using [TAPoR ](http://portal.tapor.ca/portal/portal)first through [Voyeur Tools](http://voyeurtools.org), and it wasn't until this assignment that I started using their web interface directly, which if I'm not mistaken, is a recent offering. I discovered that the tools do not necessarily overlap, and that in general the set of tools in Voyeur Tools is more complete. Voyeur Tools also seems to be designed in such a way that the tools connect with each other, something TAPoR does not do very well. That said, TAPoR is a great tool for beginners who want to experience what textual processing can reveal about their texts. Much of the debate that I have heard about the use of statistical tools to examine texts has been directed precisely at the sorts of things that TAPoR and Voyeur Tools do well. I believe that criticism to be a result of poor usage, rather than a flaw in the tools. The conceptual claims of collocation and distribution graphs are never overstated by these projects. In this sense, the tools deliver what they promise. That said, there is plenty of room for misuse, false assumptions and 'naive' evidence. In short, use with caution.\n\n\n"},{"id":"2011-09-06-a-simple-critique-of-tile","title":"A simple critique of TILE","author":"ed-triplett","date":"2011-09-06 08:36:57 -0400","categories":["Grad Student Research"],"url":"a-simple-critique-of-tile","content":"I would like to echo Annie’s thought that I feel a little awkward critiquing DH projects of this kind given how new a lot of this is to me. I was drawn to T.I.L.E. as a user, and so while I may not yet be able to discuss the project’s inner workings, I can assess its usability. I cannot come up with any single idea that would improve the project other than that it must improve the exposition of how it works and what its purpose is. I will begin by listing a number of general mistakes that occur in the “Sandbox” version of the TILE tool:\n\n\n\n\t\n  * There is no home button to return you to the TILE site once you have clicked on the Sandbox\n\n\t\n  * OCR appears to be a critical aspect of the project, yet there is no explanation of what attention was paid to improving its results.\n\n\t\n  * Often but not always, when attempting to create an “Image tag” box, the selected area appears off the scanned page instead of where the user’s cursor is creating the box or ellipse.\n\n\t\n  * Once this incorrectly created image tagging square appears off to the right of the scanned image, there is no way to move it back to its proper location.\n\n\t\n  * After performing a line recognition, there is no way to zoom in to judge how well this was done.\n\n\t\n  * A composite scan containing a text/image composite would have been very helpful as a demonstration of the image tagging tool.\n\n\t\n  * I have trouble understanding the choice of the phrase “Semi-Automated line recognizer.” Especailly when combined with the shaky functionality of the Sandbox, it gives the impression of an “almost working line recognizer.”\n\n\t\n  * There really MUST be some visual connection between the dialogue box that pops up when image tagging, and the specific location of the image-text being tagged. I would advise looking at software such as Nowcomment for the functionality/UI for TILE.\n\n\nTo take a wider scope, there appears to be a general lack of exposition. MITH must be aware of how traffic will get to their site. I think it is not uncommon for users to discover the site in just the way I did; by clicking on one of a series of links relating to text-based DH projects. With this in mind, the problem that TILE is attempting to solve must be placed in the foreground. A lot of these problems could be solved with a simple video demonstrating the project at each stage; from scanning to OCR, to line recognition and finally image tagging.\n\nTILE introduces a lot of issues in terms of exposition of a DH project. More text explaining what inspired the project, and what it does is not always the best way to introduce and “hook” new users. Still, MITH’s decision to use such minimal text does not help. It should also be pointed out that the small screenshots devoted to the various tools are poorly chosen and fail to demonstrate anything. TILE easily could accomplish its stated goals, yet it seems the presentation and functionality of the sandbox would scare off or confuse most users before they realize the strengths of the project.\n"},{"id":"2011-09-06-temporal-mapping-crowdsourcing-and-standardization","title":"Temporal mapping, crowdsourcing, and standardization","author":"lindsay-o’connor","date":"2011-09-06 08:44:01 -0400","categories":["Grad Student Research"],"url":"temporal-mapping-crowdsourcing-and-standardization","content":"For my first DH project review, I picked the [Linguistic Atlas Projects](http://www.lap.uga.edu/) because I’m interested in regional dialect and linguistic change and because I liked the idea of a linguistic map that the name “linguistic atlas” invokes.  But when I looked closer at the site, I wanted to make it into more of a globe instead of just a collection of maps. Comparison across regions could be easier and less time-consuming for the user if the analyses had more overlap or formal standardization. Right now, each study appears differently on the website. Some provide analyses and some only provide data and a basic description. The [LAMSAS Density Estimations Maps](http://us.english.uga.edu/cgi-bin/lapsite.fcgi/lamsas/de-maps/) provide density across the region while the [LAPNW maps](http://us.english.uga.edu/cgi-bin/lapsite.fcgi/lapnw/maps/) provide isolated community locations, and these two analyses map none of the same terms. The extensive data files show many similar questions and terms, so coming up with similar analyses would not require additional research. I know we’re only supposed to suggest a single change, but I want to suggest an additional change that would also serve the goal of more extensive and accessible comparisons, this time within regions instead of just between them. Much of the data is already dated, so newer surveys would make for a more current version of the Atlas as it is, and they would also give these linguistic maps an additional dimension by allowing for comparisons over time. Users could see how dialects are changing within and across regions and demographic groups.  I realize this isn’t so much a shortcoming of the project as much as it is a next step it could take, and with Scholars’ Lab developing [Neatline](http://www2.lib.virginia.edu/scholarslab/about/projects.html#project-detail) , Linguistic Atlas might be a good candidate for an update.\n\nI also want to reflect briefly on crowdsourcing via [What’s on the Menu](http://menus.nypl.org/).  Annie helpfully pointed out some inconsistencies in the instructions on what to include and what to ignore when transcribing menus, and the [“About” page ](http://menus.nypl.org/about)mentions “cleanup” before the project is complete. I’ve seen so many restaurant menus with typos and misspellings that I’m sure there will be many variations in the language used to name and describe many menu items, and those variations and \"errors\" are part of the beauty of these objects. It would be a shame for those things to get “cleaned up;” a project like this that documents and preserves ephemera should maintain all the quirks in its archive. I’m ambivalent on crowdsourcing here; it’s of course great for getting things accomplished quickly and cheaply and it will be much better than OCR for all the different menu formats, but like OCR it might still lead to inconsistent representation or interpretation of items in the archive. This possible problem could also become a strength, however, if the archive of menu items and prices could also serve as an archive of the transcription process. If information about each transcriber were preserved and associated with the items they transcribe, What’s on the Menu (or maybe any crowdsourced project?) could serve the secondary function of demonstrating how different people interpret different objects. This issue will probably come up again as we work on Prism, so I look forward to reviewing other crowdsourced projects along the way.\n\nNow I'm asking for more standardization in one project and am resisting standardization in the other. The difference is in what level of analysis or representation is being standardized, and in a way, the Linguistic Atlas Projects might serve as a model for the data in What's on the Menu. The LAPs document linguistic variation, and I hope What's on the Menu will document variations in names, descriptions, spelling, and punctuation the way the LAPs document variations in word choice and pronunciation. The standardization I think the LAPs lack is at a higher level of analysis, not at the level of data collection. It seems that many different projects share the challenge of representing variation in the data or objects in their archives in formats that are consistent and user-friendly.\n"},{"id":"2011-09-12-crowdsourcing-interpretation","title":"Crowdsourcing Interpretation / Praxis and Prism","author":"bethany-nowviskie","date":"2011-09-12 08:34:10 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"crowdsourcing-interpretation","content":"Our goal in the Scholars' Lab [Praxis Program](http://praxis.scholarslab.org) is to address methodological training in the humanities not just through workshops and courses, but by involving graduate students in digital projects from the ground up. This means learning by creating something -- together -- with all that entails: paying attention both to vision and detail; building facility with new techniques and languages not just as an academic exercise, but _of necessity,_ and in the most pragmatic framework imaginable; acquiring the softer skills of collaboration (sadly, an undiscovered country in humanities graduate education) and of leadership (that is, of credible expertise, self-governance, and effective project management). All this also involves learning to iterate and to compromise -- and when to stop and ship.\n\nTo do this, our Praxis team needed a project. We wanted it to be a fresh one, something they could own. It was important to us that the project only be in service to the program -- that its intellectual agenda was one our students could shape, that they set the tone for the collaboration, and that -- as much as possible -- it be brand-spanking-new, free from practices and assumptions (technical or social) that might have grown organically in a pre-existing project and which we might no longer recommend.\n\nIn this inaugural year of the Praxis Program, the Scholars' Lab, in consultation with some colleagues from UVa's College of Arts and Sciences, is providing the central idea for the project. It's just too much to ask that students new to digital humanities work invent a meaningful project from whole cloth on Day 1 of the program -- especially one that, we hope, will make a meaningful intervention in the current scene of DH research and practice. That said, by the end of this year, our current Praxis team plans to have conceptualized a second project (or perhaps an extension of this one) to pass on to next year's group.\n\nHere endeth the preamble. What are we up to now?\n\nThis year, the Praxis Program is building a web-based framework, codenamed \"Prism,\" for collective marking of texts according to small and constrained (but flexible) interpretive vocabularies. Prism will enable visualization of those marks -- made by many users on the same document -- as zoomed-out, rainbow-like spectra. It will also (should we get so far!) allow for comparison and analysis of the results of users' activity (that is, their collective attention paid to certain passages of text, and the categorizations they make of those passages) by treating them as input for the data-mining techniques we can apply against large corpora of digitized texts. In other words, Prism will be a blunt but very interesting and user-friendly tool for crowd-sourcing humanities interpretation.\n\nThe basic concept has several sources. It stems in part from conversations on categories of textual interpretation, led by Johanna Drucker and Jerome McGann, in which I participated as a graduate student at [SpecLab](http://books.google.com/books/about/SpecLab.html?id=VPXCk396uPYC) (see especially chapter 2.5 of Drucker's book), as well as from a fond memory of markup games I played in my UVa Media Studies classroom and with SpecLab colleagues, including (among several others) Drucker, McGann, Andrea Laue, Worthy Martin, and Nathan Piazza. These games and discussions fed into McGann's [\"Marking Texts of Many Dimensions,\"](http://digitalhumanities.org/companion/view?docId=blackwell/9781405103213/9781405103213.xml&chunk.id=ss1-3-4) and Jerry and I spoke about our experiences of them last year, in response to a [Scholars' Lab talk](/2010/05/07/julie-meloni-n-dimensional-archives/) on \"N-dimensional Archives\" by Julie Meloni. SpecLab participants called this (quite complicated) thing \"the 'Patacritical Demon.\" It also stems from work on [folksonomy](http://en.wikipedia.org/wiki/Folksonomy) which I undertook in designing the NINES/Collex software as a postdoc with McGann, and more recent Scholars' Lab discussions about color-coded text visualization with Alison Booth, in the context of [her project](http://scholarslab.org/research-and-development/through-another-prism/) to define and mark narrative structures in biographies of women.\n\nThe concept I presented to the Praxis team last week as an inspiration for Prism is simpler in scope and beholden more to the material and pedagogical markup exercise than to text-theoretical debates. I'll say no more here than that the original game involved shared, Xeroxed page images, transparent overlays, dry-erase markers, a common interpretive prompt, and a moment in which somebody yelled \"Stop!\" and the transparencies were stacked up for discussion.\n\nMembers of the Praxis team will be describing their vision for the user interface of Prism in more detail as the weeks and months progress. In our early conversations, the whole team has seemed energized by the potential of the tool for classroom use. But it's important to say that we're not just replicating an offline pedagogical exercise in the browser.\n\nPrism updates the concept in some important -- and we think timely -- ways, some of which are meant as interventions in the current scene of DH project development and conceptualization:\n\n\n\n\n  * We recognize that there's a huge vogue for \"crowd-sourcing\" in the digital humanities right now, but have been feeling like there's potential for much more interesting work in this domain. We don't want to treat the \"crowd\" only like robots or mechanical turks -- asking for transcription labor, or refinement of OCR output, as valuable as those products may be. What would happen if we could systematize, capture, and build collective _interpretive energy_ -- on shared understandings and unexpected disagreements?\n\n\n  * We also feel ready to build on design lessons from citizen-science/citizen-scholar projects, like those created by the [Zooniverse](http://www.zooniverse.org/) group, to create a DH tool that appeals to the general public and is easy and fun and effective for pedagogical use. We'd like to be able to use Prism as a laboratory exercise for thinking about design and development in the public humanities, and on the relation of audience and user communities to the questions we can ask in DH research.\n\n\n  * Finally, in an era of mass digitization, we're keen to engage with big data in the humanities. Once the basic framework for Prism is established, we want to be able to experiment with the flow between user-friendly input and \"easy\" and attractive visualizations (like our spectra) and the deeper questions that can be asked and harder information design problems that are encountered when we move into computational linguistics & text mining techniques such as sentiment analysis.\n\n\nThis is a tall, tall order -- but neither the Scholars' Lab staff nor our Praxis students are the sort to be attracted to an unambitious project. We hope you'll follow along this year as we see just how far we can get, and what we can learn along the way.\n"},{"id":"2011-09-12-processing-praxis","title":"Processing Praxis","author":"brooke-lestock","date":"2011-09-12 18:15:43 -0400","categories":["Grad Student Research"],"url":"processing-praxis","content":"Since we've been let loose, prompt-free, to blog as we please this week, I'd like to take the opportunity to get back to \"the process.\" As the Praxis Program garners more and more attention, and we begin to produce and publicize documentation, I'm realizing how little I knew when I boldly stated \"Long live the process!\" in my first blog post just a few weeks ago. I am still convinced that the transparency of our progress is integral to the success of the program - its success as a model for similar graduate training programs in other universities and as a model for collaborative DH work - but I'm only now understanding _why_. The process is something that makes most academics, myself included, a bit uncomfortable because we spend the majority of our time and energy, to use Bethany's term from our first meeting, \"polishing.\" As graduate students, we are trained to cultivate increasingly more specific areas of expertise, to hold onto and constantly refine a piece of work before making it public, and even then we're often uncomfortable with releasing a work-in-progress to our peers. Sure, classrooms, conferences, and trusted advisors or colleagues are acceptable venues for \"testing out\" work, but there is still bound to be a considerable amount of refinement anxiety before the work is shared. Of course I'm speaking from my experience and my academic career has only just begun, but these are my impressions reinforced by what I've gleaned from more seasoned academics and professionals.\n\nThat being said, the transparency and publicity expected of us in the Praxis Program is a huge source of anxiety for me. But it's _good _anxiety.  I spend way too much time polishing (read: agonizing over) what I put \"out there\" for the group or the website - from our weekly blog posts to our working GoogleDocs for the charter or requirements gathering - that it would normally be paralyzing, but in a program as fast-paced as Praxis, there's no time for solitary perfectionism. As uncomfortable as it is to contribute an unpolished idea, sentence, post, etc., there's more at stake than my ego and more to gain than the perfect blog post. Now that Bethany's explained the basics of Prism, I'm sure there are plenty of people out there who responded exactly as I did, that is, wishing it was already a year from now so we could use it! But since I don't have a DeLorean and opportunities to develop and build a tool as potentially awesome as Prism don't present themselves every day, I'll stick to my guns and I won't betray the process. So, in addition to all the logistical training and DH street cred that the Praxis Program will provide, it will also force me to resist my urge to hoard and over-polish work, which is exactly in tune with the program's mission to \"realign graduate methodological training with the demands of the humanities in the digital age.\" Realignment: in process!\n"},{"id":"2011-09-13-a-disclaimer-and-a-declaration","title":"A disclaimer and a declaration","author":"sarah-storti","date":"2011-09-13 04:02:33 -0400","categories":["Grad Student Research"],"url":"a-disclaimer-and-a-declaration","content":"Kudos to Brooke for [her excellent blog post](/2011/09/12/processing-praxis/) this week. I believe she was writing it in the midst of a texting session with me, during which we each gave vent to plenty of grad student perfectionist angst (along the lines of \"what on EARTH can I say this week that will be worthy  the highly visible platform that is the SLab/Praxis blog??\"). I've been inspired by her honesty in writing about what many of us are thinking: transparency sounds great in theory but is pretty terrifying in practice, especially for those of us relatively new to both the academy and to the DH community. But Brooke is also right about the speed of the Praxis program: we're flying through the basics, and there is simply no time for perfection. This, I think, should count as our disclaimer for anything we may put up here, in the coming months, that may sound slightly crazy, impossible, or (heaven forbid) ill-written. We're doing our best to keep up, and in exchange, we appreciate your indulgence.\n\nThat said, I do want to add a simple paragraph's worth of unadulterated enthusiasm to the blog tonight with regard to Prism. In our meeting last Tuesday Bethany explained the basic concept, beginning with its origin in a text-analysis game involving Xeroxed texts and transparency overlays. The most exciting part about the proposed project, in my opinion, is so well-put by Bethany that I despair of rephrasing it more effectively. Here she is in [her blog post this week](/2011/09/12/crowdsourcing-interpretation/): \"What would happen if we could systematize, capture, and build collective interpretive energy — on shared understandings and unexpected disagreements?\" Yes. What would happen?  We are going to build something that allows us to peek into other readers' interpretive, individual, creative minds. Any given book club discussion, literary journal, or scholarly society gathering  testifies to the fact that everybody reads and processes texts differently: to think that with Prism we will be able to pin down a little bit of how and when that happens... I want to shout this from a rooftop somewhere! (And I suppose this blog is as good a place as any!) I'm with Brooke--I wish we could have Prism up and running tomorrow, as long as that didn't mean my internship at the Praxis Program was over. Fortunately (and I mean this in all seriousness), we have a long way to go. I'm looking forward to every stress-inducing, joyous minute.\n"},{"id":"2011-09-13-fall-2011-newsletter","title":"Fall 2011 newsletter","author":"eric-johnson","date":"2011-09-13 13:04:07 -0400","categories":["Announcements"],"url":"fall-2011-newsletter","content":"Our Fall 2011 newsletter (PDF) is now available for your reading pleasure. It's chock full of introductions: of our three new graduate fellows, of our new Praxis Program (rethinking methodological training in the digital humanities), of our new cohort of Praxis fellows, and of four new staff members. You'll also find the full schedule of fall programs and workshops and more news from and about the SLab.\n\n[Get your copy here](http://www.scholarslab.org/wp-content/uploads/2011/09/SLabFall2011news-final.pdf).\n"},{"id":"2011-09-13-getting-to-know-our-praxis-peers-samples-of-our-digital-work","title":"Getting to Know our Praxis Peers: Samples of our Digital Work","author":"ed-triplett","date":"2011-09-13 09:24:39 -0400","categories":["Grad Student Research"],"url":"getting-to-know-our-praxis-peers-samples-of-our-digital-work","content":"I would like to mention to you all that I have added two files to our collab resources that I hope will give a better idea of the kind of work I am putting together as a Scholar's Lab fellow this year. There is a transcript of the very short presentation I gave two weeks ago which describes my GIS project and briefly outlines my dissertation topic. At some point there will also be a podcast of the presentation on the Scholar's Lab page.\n\nMy purpose in posting these two files is to begin the process of making the entire group more aware of what each of us works on as individual scholars. As I discussed briefly when we began talking about the Praxis charter, I believe it would be very helpful for us to better understand our individual motivations for joining the Praxis Program. As a multi-disciplinary group, we should be aware of each others' strengths and interests as scholars, as well as the particular methodologies and approaches that are common to our particular fields. As such, I am proposing that if we do not have a digital, or \"traditional\" humanities project that we are ready to share with the program, each week one of us can submit a sample from another scholar that we believe approaches their subject in a way that represents our field, or may be relevant to PRISM. This can even take the form of a \"favorite\" article.\n"},{"id":"2011-09-13-on-demons-and-prisms","title":"On Demons and Prisms","author":"alex-gil","date":"2011-09-13 06:30:20 -0400","categories":["Grad Student Research"],"url":"on-demons-and-prisms","content":"Prism is not many things, one of them is itself... for now. There is a history behind the identity crisis. As Bethany pointed out in [her flagship post](/2011/09/12/crowdsourcing-interpretation/), Prism began as a Demon. Hearing McGann talk about it nowadays, you would think that we have found Richard Rorty's ultimate intellectual ring, the one eye that encompasses all other. The pata-critical Demon owes its name in part to Alfred Jarry's _pataphysics, \"_the science of imaginary solutions,_\"_ from which we also get Pablo Lopez's _pataphore_, \"an unusually extended metaphor.\" When the folks at the SpecLab began playing around with markers and transparencies, they were in a sense blending science with play by making literal the idea that we all read differently. Although we all knew for centuries that there was room for interpretation, footnotes and marginalia safely occupied different places on the page, reinventing the author at the moment of its undoing. The Copernican move was to take the idea of difference seriously enough to overlap it. McGann, in most other cases a visionary, hesitated before the chasm. Today, he still wants to feed commentary to the Demon. Our prism ventures out on a different path.\n\n[caption id=\"attachment_2354\" align=\"alignright\" width=\"240\" caption=\"Prism ray trace\"][![](http://www.scholarslab.org/wp-content/uploads/2011/09/prism-300x167.png)](http://www.scholarslab.org/praxis-program/on-demons-and-prisms/attachment/prism/)[/caption]\n\nThen there is the knack that some folks have to try to reduce it to the most mundane digital tools. Two in particular surface often: [Diigo ](http://www.diigo.com)and [NowComment](http://nowcomment.com/). I hope I am clear when I say, Prism they are not, and they are not for the same reason: They are not focused. A prism refracts light according to a specific set of rules. Diigo and NowComment allow for a very diffuse set of comments and monotone highlights that cannot be wrangled easily for analysis. Both are helpful to provide feedback for one reader who has a vested interested in reading the comments. If we were to read interpretation as a social phenomenon, their usefulness runs its course. Our prism understands that we all wiggle under controlled vocabularies and that it is there that our differences thrive.\n\nSo now that I've said my peace about what I think prism is not, let me leave you with a vision:\n\nOn one of those slow dry desert days where Saint Anthony receives his motley crew of visitors, he has a vision. He sees a man wearing a wig before a strange glass pyramid. He sees strange markings on several pages strewn about a table rife with even stranger machines. A ray of light flashes through the window and the wigged man fumbles for the triangle. He offers it to the light like a bishop offering the host. Miracle of miracles. The crystal gives birth to a rainbow which fills up the room with the brightest colors, like the garments of his demons.\n"},{"id":"2011-09-13-programming-for-prism","title":"Programming for Prism","author":"annie-swafford","date":"2011-09-13 08:36:35 -0400","categories":["Grad Student Research"],"url":"programming-for-prism","content":"I’d like to echo the blog posts that Brooke and Sarah made about their excitement regarding Prism and also the associated angst of blogging and transparency.  In addition to the pressure of worrying about “living up to the Scholars’ Lab blog,” I also feel discomforted by the very act of blogging itself; perhaps because I am used to imagining typed text as formal expressions of carefully thought-out ideas for journals, conferences, or a dissertation committee, it feels odd to type and make public thoughts I would more likely share in conversation.  The blog format itself seems to ask for more profound thoughts than my ideas here justify.  However, I look forward to the time when we all will have acclimated to this new way of sharing our thoughts, work, and progress in public!\n\nAlthough I am certainly looking forward to brainstorming the features of Prism and to reaping the benefits of the finished product in the classroom, I am most excited by the prospect of helping to build it.  While I have some HTML, CSS, and Javascript experience, my actual coding skills are practically non-existent (other than a preliminary knowledge of Python), and I am eager to learn and to improve my DH skill-set.  Over the last two weeks, we have learned the basics of VIM and of Bash and the command line, which, while a bit daunting, is also incredibly empowering.  My husband is a programmer, and I have watched him write code and work in Terminal with ease, and I look forward to gradually understanding more and more of what had previously seemed like multi-colored gibberish on a screen.  Although it’s hard to imagine now, at the beginning of our programming venture, that we will be able to make anything useful to anyone, I have faith that our learning speed will increase and in a few weeks, what now seems daunting will soon seem simple.\n"},{"id":"2011-09-18-imagining-end-users-for-requirements-gathering","title":"Imagining end users for requirements gathering","author":"lindsay-o’connor","date":"2011-09-18 19:51:59 -0400","categories":["Grad Student Research"],"url":"imagining-end-users-for-requirements-gathering","content":"[Bethany ](/2011/09/12/crowdsourcing-interpretation/)writes, “It was important to us that the project only be in service to the program — that its intellectual agenda was one our students could shape, that they set the tone for the collaboration, and that — as much as possible — it be brand-spanking-new, free from practices and assumptions (technical or social) that might have grown organically in a pre-existing project and which we might no longer recommend.”\n\nThis sounds like a great approach, but I worry that it leaves out a few important details. Prism does fit this description—there are many possibilities for what it will look like and how we will work together and separately to create it—but it also brings with it some theoretical assumptions that we have less freedom to critique or modify. The idea has been around for a while now, as Bethany and Alex explain, and our discussion about it last week was more Q&A and less brainstorming session than I had expected. Bethany writes, “The version of the Demon I presented to the Praxis team last week as an inspiration for Prism is simpler in scope and beholden more to the material and pedagogical than to the text-theoretical.” A few weeks ago Bethany helpfully talked about how all DH projects have implicit theoretical stances and how programming performs critical work, so I do think Prism will have a “text-theoretical” aspect.  It's an aspect of PRISM that seems non-negotiable right now. I left last week’s meeting thinking that a text-mining functionality is a goal we have to adopt, despite any reservations we might have about it, and that’s a part of Prism that is bringing out some of our differing ideas about interpretation. I realize that we are creating a tool for researchers to use in ways we anticipate and ways that we don’t, so it’s not up to us to interpret the crowdsourced interpretations. Yet I still worry about enabling facile conclusions from quantified data under the name of “literary” interpretation and what an increase in this kind of work would mean for the future of the non-alternative academy. Now that we’re working on our requirements and expectations for Prism, I support making text-mining a much later goal and focusing on creating user accounts and communities.\n\nThinking through requirements has also gotten me excited about how Prism could work as a tool for the social sciences. If Prism can collect information (demographic and otherwise) about the people tagging or highlighting text, social scientists could use it for research questions about reading communities, literacy, and education. Focusing on this possibility helps me get away from worrying about text-mining, but I’m not sure if this takes Prism out of the scope of digital _humanities_.\n"},{"id":"2011-09-19-a-transdisciplinary-ethics","title":"A Transdisciplinary Ethics","author":"brooke-lestock","date":"2011-09-19 14:40:57 -0400","categories":["Grad Student Research"],"url":"a-transdisciplinary-ethics","content":"Coincidentally (or maybe not-so-coincidentally), part of [Lindsay's post](/2011/09/18/imagining-end-users-for-requirements-gathering/) directly echoes the opening concerns of an article I'm reading for the EELS (Electronic Enabled Literary Studies) group led by Profs. Stauffer and Pasanek here at UVa. In \"Learning to Read Data: Bringing out the Humanistic in the Digital Humanities,\" Ryan Heuser and Long Le-Khac discuss the conflict between excitement and anxiety in DH work (sentiments I echoed in my last post and feel every time I approach my computer these days). The essay asks first whether we can use the quantitative methods employed in DH work while \"respect[ing] the nuance and complexity we value in the humanities,\" and then confronts the much deeper issue at-hand: \"Under the flag of interdisciplinarity, are the digital humanities no more than the colonization of the humanities by the sciences?\" (2).\n\nEven more coincidentally (scholarly synchronicity at its finest), Jahan Ramazani, in his Modern Poetry course, just assigned us the second chapter of his game-changing book, _A Transnational Poetics_. Now, I'm making a theoretical leap, but I can't help approaching Lindsay's concerns with Ramazani's words in mind. In the opening page of his second chapter, he explains that \"humanistic disciplines must draw artificial boundaries to delimit their object of study - nation, language, period, genre, and such - and so must allow for anomalies\" (23). The chapter goes on to debate the mononational narratives literary scholars build around modernist poets, and the solution Ramazani offers is one of expansive compromise: “to begin to explain how poetry helps newness enter the world,” he writes, scholars must investigate “complex intercultural relationships across boundaries … without erasing those boundaries” (47).\n\nNow that I’ve sufficiently piggybacked off of other more experienced scholars’ work, I’ll _attempt_ to address our group’s concerns about data-mining with Prism. Though I admit that this quantitative approach to literary work gives me the nervous-sweats because of its potential for producing data that ignores the “nuance and complexity” we worship as humanities scholars, the idea of  drawing “artificial boundaries” delimiting the scope of the project is something I’m even less comfortable with. I barely understand the potential implications of a tool like Prism at its most basic level, so I’m hesitant to demarcate how Prism should be used by other disciplines that could potentially do interesting things with it (Lindsay mentions the social sciences, for instance). That being said, I _completely_ agree with Lindsay that the text-mining feature should be a second-tier priority until our Alpha version is running. We already have our work cut out for us.\n\nWhat I hope I’ve made clear is that I support not a “colonization of the humanities by the sciences,” but instead a _transdisciplinary ethics_ for our project and the DH field-at-large. By that I mean we must be conscious of our methodological differences while constantly questioning disciplinary boundaries (shout-out to Ed for his [interdisciplinary show-and-tell idea](/2011/09/13/getting-to-know-our-praxis-peers-samples-of-our-digital-work/)). Ramazani quotes Edward Said's _Culture and Imperialism_, and it bears repeating here: “The fact is we are mixed in with one another in ways that most national systems of education have not dreamed of. To match knowledge in the arts and sciences with these integrative realities is, I believe, the intellectual and cultural challenge of the moment” (Ramazani 49, Said 331).\n\nP.S.: Stay tuned for next week’s blog post, which I’m sure will be a mental breakdown à la Ruby.\n"},{"id":"2011-09-19-on-interventions","title":"on interventions","author":"bethany-nowviskie","date":"2011-09-19 03:47:31 -0400","categories":["Grad Student Research"],"url":"on-interventions","content":"I started to write this as a comment on [Lindsay's latest post](/2011/09/18/imagining-end-users-for-requirements-gathering/), but then thought I should boost it a bit, so that it becomes a part of the overall conversation about next steps for Prism.\n\nI've been out of the mix of the discussions you guys have been having, so it may be that Lindsay is responding more to a building group consensus about how to use textual data than to the model \"reference interview\" introducing Prism that Jeremy conducted two weeks ago.\n\nIn case the opposite is true, I want to jump in with some clarifications. The first is that there is no aspect of this project that is not up for consideration, critique, and potential re-casting.  Because we are building the tool collaboratively, under [a charter](http://praxis.scholarslab.org/topics/toward-a-project-charter/) I hope you guys will propose to us for adoption on Tuesday, user requirements and overall vision for Prism will need to be negotiated with the group. Project-building in DH invariably requires compromise, but to get there in a healthy way, it first requires a great deal of clarity about team-members' stances and goals, and the basic theoretical orientation of the work they are trying to conduct (and enable!) together.\n\nIn the case of Prism, Lindsay's (and perhaps others') skepticism about the utility of that potential text-mining piece -- and, I think, about what algorithmic approaches mean in the overall scene of literary studies in an age of \"big data\" -- is certainly being heard. What I'm less clear on is what the team might be imagining is possible to do with the marked passages of Prism's crowd of users. I would encourage you, at this point in the planning, not to reject approaches without exploring them -- without feeling fully informed as to their potential and (more importantly, in cases where you intuit a problem) the intervention you might make, or the twist you might bring, to their use in the scholarly community. Our suggestion about the potential of this project to re-cast the vogue for mechanical \"crowd sourcing\" in DH in more interpretive and pedagogical terms is an example of one such intervention.\n\nThe role of the more experienced members of the Prism team will be to make sure we're all aware of (sadly, well-trodden) pitfalls in project design and execution. Examples of this from our first requirements conversation were the design and UX dangers of starting the project with a large or unconstrained number of user-extensible color-codings and, likewise, diving into a complex user-accounts system before specifying and architecting the basic functionalities of the tool.\n\nYou can think of those as an example of some negative course-corrections we may try to make as the group moves forward. A positive one would be my suggestion that you learn about and think fully through the possible value of data-mining approaches -- and the potential for this project to model good ones, or critique from within -- before rejecting the notion out of hand. If you ever feel us pressuring you to move in a certain direction, it will be out of a desire to help you and the overall community grow. If that's not evident in the moment, it's your job (as Lindsay has done admirably!) to tell us what you're hearing and pressure us for clarity.\n\nIn terms of the overall vision for Prism, it's true that it stems from longstanding conversations and experiments, and that we came in with some big ideas for the kinds of scholarly intervention it might make, and the directions it was possible to go. But if we had really wanted to build it exactly as specified, we'd have done so, and made up another project for you guys to sink your teeth into.\n\nThis one's just an offering, for the whole group to make of what we will.\n"},{"id":"2011-09-20-introducing-our-digital-work-songs-of-the-victorians","title":"Introducing Our Digital Work: “Songs of the Victorians” ","author":"annie-swafford","date":"2011-09-20 06:55:31 -0400","categories":["Grad Student Research"],"url":"introducing-our-digital-work-songs-of-the-victorians","content":"In response to [Ed’s suggestion](/2011/09/13/getting-to-know-our-praxis-peers-samples-of-our-digital-work/) that we introduce ourselves and our digital work, I thought I would spend some time explaining my own project on Victorian musical settings of Victorian poems, which I developed through the NINES fellowship program here at UVa.\n\nI study the intersections of music and Victorian poetry, and the final chapter of my dissertation focuses on Victorian musical settings of contemporaneous poems.  I have written papers on such topics in the past, and have discovered that it’s nearly impossible to make arguments that involve music through traditional print media unless one writes for a musical audience.  I have included excerpts of the musical score, which helps the handful of people who can read music, but even fewer of them can actually hear in their mind the music they see on the page.  I’ve seen instances where scholars have included a cd along with their article, but even that is no guarantee that the reader will listen to the music or be able to follow the audio or the argument without guidance.  As a result, I’ve developed a website that can help solve this problem.\n\nMy website features scans of the first edition printings and audio files of the songs as well as an analysis of the ways the song interprets the poems they set.  Although users can focus on each part separately (view the score, listen to the audio, or read the commentary), they will be best served by the interactive functionality of the site.  When the audio file is played, a box highlights each measure in time with the music so a user can follow along.  Additionally, when the commentary explains a particular musical effect that augments the meaning of the text, the user can click on a parenthetical note at the end of the sentence that will play that portion of the audio file and highlight the score in time with the music, so the user can follow the argument regardless of their musical prowess. I hope that this will be useful not just for my own work, but also for anyone who wants to present an argument involving music for non-musicians.\n\nThis project uses html, css, and javascript, and I am in the process of figuring out how to incorporate MEI, the scholarly music XML standard, invented by Perry Roland here at UVa.  I had hoped to be able to incorporate score following so I wouldn’t have to generate the information for each measure box by hand, but it seems as though that is too substantial a project for me to tackle, as it is not at all a solved problem.\n\nThe project is not yet live, but if you would like to see it in its current incarnation, then feel free to view the [demo video](http://www.screencast.com/t/3Nm0HcRFriSa) I made.\n\nIf any of you have suggestions on MEI incorporation, design, or score following, then please let me know! I’m always open to suggestions.\n"},{"id":"2011-09-20-mimesis-and-computer","title":"Mimesis and Computers","author":"alex-gil","date":"2011-09-20 09:52:22 -0400","categories":["Grad Student Research"],"url":"mimesis-and-computer","content":"\"Computers are inherently dumb.\" I hear this all the time, even from folks in computer science. I like to think of them as marionettes.\n\nAfter Wagner called for a [_Gesamtkunstwerk_](http://en.wikipedia.org/wiki/Gesamtkunstwerk), many European artists and thinkers reacted strongly to it (Nietzsche being the most famous case). This reaction eventually led to a modernist distrust of theater in general, and of human actors in particular. Think for example of Bertolt Brecht's _[Verfremdungseffekt](http://en.wikipedia.org/wiki/Distancing_effect)_. Somewhere in between Wagner and Brecht, the English artist [Edward Gordon Craig](http://en.wikipedia.org/wiki/Edward_Gordon_Craig) suggested that human actors should be replaced by marionettes. As you can imagine, this did not go well with the actor's guild.\n\nI hear echoes of those debates and cultural shifts in our moment, when computers are starting to resemble us more and more. Computers don't replace us always in the way that machines replaced farmers or smiths, although there are still parallels between ours and the anxieties of the industrial and agricultural revolution. And just like machines then generated monstrous forms of mechanized human labor, computers do the same (If you don't believe me, ask any of my students for [Project Tango](http://uvatango.wordpress.com/)). However, there is another anxiety I see which is not necessarily that of machine iteration replacing familiar mechanical tasks with unforeseen ones. I'm talking about our fear of marionettes. Even more specific, the fear that we will confuse the marionettes for human beings.\n\n[caption id=\"attachment_2449\" align=\"aligncenter\" width=\"240\" caption=\"Quixote fights the puppets\"][![Quixote fights the puppets](http://www.scholarslab.org/wp-content/uploads/2011/09/quixote-300x225.jpg)](http://www.scholarslab.org/praxis-program/mimesis-and-computer/attachment/quixote/)[/caption]\n\nThe true marionette is _always_ controlled by a human, so are computers... ultimately. We ventriloquise through them, and they only talk back to us according to our ridiculously precise instructions. I'm not talking about [Bina48](http://www.youtube.com/watch?v=uvcQCJpZJH8). She's kind of creepy. I'm talking about the ways in which a google search acts like an operator at the end of a 411 call; or the way that netflix suggests what we might like.\n\nThere are two approaches to figuring out what counts as a title in a large repository: we can tag it, or we can write an algorithm that does it for us. Don't worry, we're not there just yet. At some point that meta-data might pass the [Turing test](http://en.wikipedia.org/wiki/Turing_test). If it does, by definition, users will think a human did the work... but wait. When it does, by definition, users will think a human did the work... but wait...\n\nPrism is not really interested in how humans might be fooled by the marionettes more than it is in how we can fool the marionettes to behave like us. Sometimes that line is blurred. The 'text mining' component, as I have understood it, seems like the bastard child of natural language processing and web crawling. The goal here is not to count words (although that is a time-honored human activity), but to abstract semantic relationships that can be used to query large data. When we Google something, we are doing something akin to that, except we never think Google is run by [a million efficient munchkins](https://www.mturk.com/mturk/welcome). When we start getting results for our perhaps-to-be Prism queries, we use those results in public at our own risk. That there will always be Quixotes in the audience... well...\n\nTake home tweet: Even if we replace actors with marionettes, the plot stays the same.\n"},{"id":"2011-09-20-prism-is-looking-for-john-connor","title":"Prism is looking for John Connor","author":"ed-triplett","date":"2011-09-20 08:57:41 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"prism-is-looking-for-john-connor","content":"It seems the text mining issue has struck a chord with our group, so I will jump in as well. Specifically, I want to refer to Sarah’s thought that the potential danger lies with the scholar interpreting the data prism could potentially collect, not with “the machine.” This allows us to do what comes natural in the humanities: critique the conclusions a scholar makes when they attempt to make use of the data that Prism “collects.” There is a well established system in place that – especially given the growing understanding of DH processes among humanities scholars – can “sift” through scholarship that uses a tool like Prism to make poorly supported conclusions.\n\nI’d like to propose that we think about what kinds of things we can learn through Prism. Thus far we have defaulted to describe our “colors” with tongue-in-cheek phrases like “Happy passages” or “Daddy Issues passages.” I may be wrong, but I don’t think we are truly trying to “map” happiness in literature.  To make this process more tangible for our group, I think we should make it a priority to decide on a sample piece of literature and at least two possible “colors” or “expressions” that we deem valuable as crowd-interpreted data. Some of the debate about using prism to “quantify” and “mine” literary expressions or feelings may diminish if we were had a more concrete problem we’d like to solve as an example. We will thus avoid thinking about Prism as a method for mapping “happy.” Clearly there is an aspect of prism that is designed to be “hands off” and allow the crowd sourcing to be less directed and therefore more “honest.” However, many of us are simply not comfortable sending a powerful robot into the world which could be easily told to shake its metal hands and help people draw spurious conclusions before we see an example of this robot saving kittens from trees and making life better for literary scholars. Think Schwarzenegger in the original Terminator versus Schwarzenegger in Terminator 2…\n"},{"id":"2011-09-20-the-pleasures-of-programming","title":"The Pleasures of Programming","author":"lindsay-o’connor","date":"2011-09-20 13:13:47 -0400","categories":["Grad Student Research"],"url":"the-pleasures-of-programming","content":"Instead of continuing the text-mining debate, I want to reflect on our programming lesson last week and our [homework](http://praxis.scholarslab.org/exercises/programming1/) this week. I’m writing this from the Scholars' Lab grad fellows lounge, where about half of the Praxis team is working through some of our Ruby homework with Eric and Wayne’s expert, patient assistance. Annie just exclaimed with happiness when she figured out how to calculate grades with an array. We all smiled and Wayne seemed especially happy to see his student catching on and feeling good about her progress. There is something so satisfying about programming, about telling a computer to do something and having it do what you want, even if you’re just telling it to spit out a few numbers under certain conditions.  We are learning to create a program and make it respond to our commands, and that seems to give us a more intense or more satisfying pleasure than we get as users telling Google to execute a search or telling Windows to open a file. This satisfaction is particularly notable to me, and I presume to many of the other grad students in the Praxis program, because our work in the humanities is never so cut and dry with such easy, absolute measures of success and failure and with such ownership of the process from beginning to end.\n\nWhile I've never programmed before, I recall a feeling somewhat like this from my time in middle management using a CRM system in a large call center. I was good at my job because I was just tech savvy enough to learn new functionalities quickly, to intuitively find ways to make our computer programs do what I needed them to do. I was so pleased with myself when I found the most accurate search terms or devised a shortcut that made workflow faster or easier. But when I was in a supervisory role, the skills I needed to succeed were much less technical. I needed to be kind enough and flexible enough to handle whatever issues arose with the people I supervised, from tardiness and time off to explaining why we were managing human relationships with a computer system. Success on this front was much harder to measure and I rarely felt such unqualified joy at my successes because I was never quite sure when I was successful. I wonder if this will prove roughly correlative to humanities computing. Learning to program is slow and tedious and difficult, but success feels hard-earned and justified and thus pleasurable. Humanities scholarship hasn't ever brought me such a clear-cut sense of success, but it has brought me pleasures that I think mastery of computer algorithms cannot. I hope we can all enjoy the pleasures of mastery over our computers, but I hope we can do so without losing sight of the academic interests that brought us all here.\n\nAs Annie worked through the grade calculation task, she dealt with the gray areas of rounding up and rounding down and admitted that these decisions always vary. That very simple program had its limits when applied to an idiosyncratic situation, a lesson I will keep in mind as I learn more about and engage in humanities computing.\n"},{"id":"2011-09-20-vive-la-difference","title":"Vive la différence!","author":"sarah-storti","date":"2011-09-20 04:41:07 -0400","categories":["Grad Student Research"],"url":"vive-la-difference","content":"When I signed up for [David Hoover’s](https://files.nyu.edu/dh3/public/) “Out-of-the-Box Text Analysis” course at last summer’s Digital Humanities Summer Institute, I had absolutely no idea what I was getting into. Text analysis… with computers? Data mining? What? The first of our meetings felt akin to culture shock, I think, or to having a bucket of ice water thrown over my head (We are going to do _what _with the texts? Cut them up into word frequency lists??), but once I recovered myself, as it were, the rest of my fast-paced DHSI week provided me with a basic understanding of not only _how_ to use text mining tools, but, perhaps more importantly, _why_ one might want to use them. I thought now might be the perfect time to share some of what I learned in Victoria.\n\nFirstly, yes, a text mining operation can give you a set of numbers that supposedly correspond in some way to a given text. Oooooh, isn’t that scary? Well, okay, maybe it is. But what happens after that is entirely dependent upon the scholar _interpreting_ the numbers. The machine is only a machine. It is not going to change the text. Text mining tools can, however, change the ways we look at our texts.\n\nIn the course, we certainly considered questions that I had never before considered about a set of texts. They included: how well (according to word frequency lists) does the author distinguish between the “voices” of his/her different characters? What about the difference between those number sets and the number sets we get from doing the same kind of test with a different author? Is one “better” at differentiating vocabulary than another? Does this even matter? Can we make an educated guess about authorship of a disputed text based on comparisons of that text with other texts written by the two authors in question? Frequently other members of my class would push back, pretty forcefully, on the idea that we could draw hard and fast conclusions about a text using only the tools on our computers. Nobody, it seemed to me, was about to publish a paper on why Author 1 is superior to Author 2 because Author 1 uses a richer vocabulary. And these people were in the class voluntarily—they (in theory) _wanted_ to learn how to use such tools, to what various ends I could not say. I do know, however, that I never once felt that the group lost sight of the difference between an author’s text and a set of numbers.\n\nThe kind of data results we got from running these tools on our texts were simply that: data results. Mining texts for word frequency (or what you will) does not inherently devalue or damage the text. Sure, it’s possible to come at a text with a preconceived notion and repeatedly run different tests to try to prove that theory right, but the same holds true, certainly, for traditional literary criticism. I suppose my question is this: do we have any reason to be concerned, really, about what will happen if Prism does eventually allow for some kind of data mining? What is the worst that can happen? And by allowing our fears (e.g. someone will draw an irresponsible conclusion based on numbers) to dictate the direction we take Prism, aren’t we obstructing the possibility of unimagined _positive_ outcomes?\n\nWhile I do think it is important for us to feel good about the tool that _we _are building, I would also echo Bethany’s cautionary (and immensely helpful) [advice](/2011/09/19/on-interventions/) “not to reject approaches without exploring them — without feeling fully informed as to their potential and (more importantly, in cases where you intuit a problem) the intervention you might make, or the twist you might bring, to their use in the scholarly community.” “Text mining” may be a Bad Word to some of us, but the way our as-yet unrealized tool works could very well make an interesting intervention in the text mining (and DH) world, regardless of our personal preferences. Isn’t that the kind of thing we’re here at Praxis to do?\n\nTo conclude, I will make another admission: even after a week of thought-provoking and congenial collaboration between myself and my colleagues at DHSI, I would not consider myself to be a data mining kind of scholar. In fact, I would consider myself to be more bibliographically-inclined than anything else. I own both of the[ Tanselle syllabi](http://www.rarebookschool.org/tanselle/). I am invested in methodologies which make it difficult for me to see how I could implement text mining in my own work right now. These inclinations of mine do not, however, mean that I think everybody else needs to lean my way. I look forward to seeing what other people want to do with Prism, and I hope text mining does eventually become part of that. I would like to see what kind of questions people will ask about the interpretation of texts thanks to our intervention via Prism, especially because those questions are ones I would probably never ask if left to myself. After all, as my grandmother says, variety is the spice of life.\n"},{"id":"2011-09-22-richmond-virginias-place-in-gis-and-racial-discrimination-history","title":"Richmond, Virginia's Place in GIS and Racial Discrimination History","author":"chris-gist","date":"2011-09-22 07:34:45 -0400","categories":["Digital Humanities","Geospatial and Temporal"],"url":"richmond-virginias-place-in-gis-and-racial-discrimination-history","content":"Richmond, Virginia is a city steeped in history.  It is the home of [the first commercially viable electric street car system](http://chpn.net/news/2006/02/16/a-history-of-richmonds-trolleys_336/), [the world's only triple train crossing](http://richmondthenandnow.com/Newspaper-Articles/Triple-Train-Crossing.html); [the first woman-owned and African American-owned bank](http://www.encyclopediavirginia.org/Walker_Maggie_Lena_1864-1934), and some great Americans including [Bojangles Robinson](http://atdf.org/awards/bojangles.html) and [Arthur Ashe](http://www.encyclopediavirginia.org/Ashe_Arthur_1943-1993).  Not exactly the history you were thinking about, correct?  There is much more hidden history in Richmond.\n\n[caption id=\"\" align=\"alignleft\" width=\"155\"]![](http://www.csiss.org/classics/uploads/mcharg-image1.jpg) Ian McHarg[/caption]\n\n[Ian McHarg](http://www.nytimes.com/2001/03/12/arts/12MCHA.html) was born around the industrial town of Glasgow, Scotland in 1920.  After World War II, he came to the U.S. and started a career in city planning and landscape architecture.  He founded the Department of Landscape Architecture at the University of Pennsylvania and is considered innovative for his notion that design should work with the landscape instead of fighting or changing it.  He has also been credited with coming up, in the 1960s, with the idea of [map overlay](http://en.wikipedia.org/wiki/Geographic_information_system#Map_overlay) which is a fundamental GIS technique.\n\nSo, what do Richmond and McHarg have in common?  Before I can tell that story, I have to tell this one.  When I was a <del>young</del> struggling grad student, I happened to be at work in the Urban Planning Department at VCU one summer day when a PhD student from the University of California, Santa Barbara -- John Cloud -- strolled in. He told a small gathering of a few grad students and professors the story of how during the Great Depression the economic conditions were similar to those we face now. There was a foreclosure crisis and banks were not offering mortgages.  In an attempt to get the industry back on track, the Federal Housing Administration seeked for ways to estimate neighborhood risk for mortgages.  They looked for indicators to predict how neighborhoods would fare at future dates. In partnership with the Richmond Planning Commission (RPC), the FHA used Richmond as one of its major study sites.  Mr. Cloud showed us a report he pulled from the National Archives (NARA) called _Statistical Data Relative to Housing and other Planning Matters_.\n\n[caption id=\"attachment_2291\" align=\"aligncenter\" width=\"470\"]![](http://www.scholarslab.org/wp-content/uploads/2011/09/planninManners-732x1024.jpg) December 1935 Version[/caption]\n\nThis report, produced as early as 1935, uses a series of tissue paper overlays to show various themes.  A color, loose-leaf card stock map of rent by block is used as the underlay.  Mr. Cloud has found this report to be the earliest American example of the use of such overlays during his research.  He also found parallel lines of work going on in Germany during the same time period.   Please find John Cloud's detailed article on this story [here](http://www.cartogis.org/docs/proceedings/2005/cloud.pdf).  There may be older examples out there waiting to be discovered!\n\n![](http://www.scholarslab.org/wp-content/uploads/2011/09/IMG_0955-1024x768.jpg)\n\n[caption id=\"attachment_2312\" align=\"aligncenter\" width=\"470\"]![](http://www.scholarslab.org/wp-content/uploads/2011/09/IMG_0951-768x1024.jpg) Rental map under housing study area map[/caption]\n\n[caption id=\"\" align=\"alignright\" width=\"184\"]![](http://4.bp.blogspot.com/_D7-iSeG_eP0/SwGJ0Qs_hiI/AAAAAAAAABI/lOpSGsLH7ks/s1600/Homer%282%29.jpg) Homer Hoyt[/caption]\n\nAnother player in this drama, economist Homer Hoyt, was an influential researcher at FHA.  He wrote a book in 1939 about housing research techniques called _[The Structure and Growth of Residential Neighborhoods in American Cities](http://www.archive.org/details/structuregrowtho00unitrich)_.\n\nIn that book, Hoyt demonstrated the value of techniques developed during the FHA's neighborhood forecast research including the [sector model](http://en.wikipedia.org/wiki/Sector_model) and -- you guessed it -- map overlays.  He used mylar sheets in the book to do a series of overlays for Richmond, which is clearly a distilled version of the RPC report maps.\n\n[caption id=\"attachment_2290\" align=\"aligncenter\" width=\"470\"]![](http://www.scholarslab.org/wp-content/uploads/2011/09/hoytOverlays-1024x599.jpg) At right, all four of Hoyt's overlays together: Race, Age, Condition, and Rent[/caption]\n\nSo, it wasn't really Mr. McHarg who pioneered the use of overlay at all (sorry to all you landscape architects).  At least, the RPC, Mr. Hoyt, and German researchers did it some twenty-five years earlier.\n\nOn a related topic, in my recent correspondence with Mr. Cloud he informed me that the Library of Virginia (LVA) had another copy of the RPC report that was missing at least one overlay.  We agreed that I would scan the LVA's copy and send him a copy.  In return, Mr. Cloud promised to get the missing pages from the NARA copy.  However upon further investigation, I have discovered that there are at least two versions of the document.  The NARA version is dated December, 1935, and the LVA version is dated January, 1938.  There are a few discrepancies between the versions and it is hard to tell whether they were produced with different overlays or whether some layers have been lost over the years.  Specifically, there are unique overlays in each document.  The following table is an inventory of the overlays for each document.\n<table cellpadding=\"5\" align=\"center\" border=\"3\" >\n<tbody >\n<tr >\nTheme Name\n(from report)\nNARA Version\n(1935)\nLVA Version\n(1938)\n</tr>\n<tr >\n\n<td width=\"15%\" >Rental Map\n(by city block - card stock color underlay)\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n</tr>\n<tr >\n\n<td width=\"15%\" >Rental Map (by area)\n</td>\n\n<td bgcolor=\"red\" style=\"text-align: center;\" >No\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n</tr>\n<tr >\n\n<td >Relief Cases (point data)\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n</tr>\n<tr >\n\n<td >Juvenile Delinquency (point data)\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n</tr>\n<tr >\n\n<td >Adult Delinquency (point data)\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n</tr>\n<tr >\n\n<td >Infant Mortality (point data)\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n</tr>\n<tr >\n\n<td >Tuberculosis - 1934 (point data)\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n</tr>\n<tr >\n\n<td >Population - 1930 (dot density)\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n</tr>\n<tr >\n\n<td >Areas Inhabited by Negroes (area)\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n\n<td bgcolor=\"red\" style=\"text-align: center;\" >No\n</td>\n</tr>\n<tr >\n\n<td >Certain Statistical Data - 1935\n(combination of TB, relief and delinquency)\n</td>\n\n<td bgcolor=\"red\" style=\"text-align: center;\" >No\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n</tr>\n<tr >\n\n<td >Principal Thoroughfares\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n\n<td bgcolor=\"red\" style=\"text-align: center;\" >No\n</td>\n</tr>\n<tr >\n\n<td >Housing Studies\n(shows specific study areas within report)\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n</tr>\n<tr >\n\n<td >Territorial Growth\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n\n<td bgcolor=\"red\" style=\"text-align: center;\" >No\n</td>\n</tr>\n<tr >\n\n<td >Census Tracts - 1935\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n\n<td style=\"text-align: center;\" >Yes\n</td>\n</tr>\n</tbody>\n</table>\nMr. Cloud had concluded that purposeful reasons (i.e. a tear out) were to blame for the missing African American overlay from the LVA version of the RPC report. However, based on my comparison of the two different texts, I am not so sure.  I believe Mr. Cloud did not have all the information.  Did the 1939 version of the report originally have different overlays than the 1935 report?  Or were overlays removed/lost from each copy in the subsequent decades?   Of course, it wouldn't be a surprise to find out that someone purposely removed specific overlays from the PRC report given the amount of revisionist history around Richmond.\n\nHoyt listed four factors (race, age, condition, and rent) in determining neighborhood risk.  According to Hoyt's analysis, we have race, specifically the number of African Americans, as a major indicator for forecasting neighborhood risk.  Is that enough to cement Richmond's place in racial infamy?  Not quite.  However, much of his research went into a survey which concluded in a series of maps produced by the Home Owners' Loan Corporation (HOLC).  The HOLC maps were used as proof of [redlining](http://encyclopedia.chicagohistory.org/pages/1050.html),  as term coined by [John McKnight](http://www.northwestern.edu/ipr/people/mcknight.html) in the late 1960s.\n\n[caption id=\"attachment_2308\" align=\"aligncenter\" width=\"470\"]![](http://www.scholarslab.org/wp-content/uploads/2011/09/07-HOLC-map-1024x801.png) \"Redline Map\" of Richmond by the Home Owners' Loan Corporation[/caption]\n\nUndoubtedly, race -- specifically the percentage of African Americans -- played a large role in determining the hazard values for the HOLC maps.  The question is whether or not the HOLC maps caused institutional discrimination against African Americans after their production.  Amy Hillier, a researcher at the University of Pennsylvania, says [no](http://cml.upenn.edu/redlining/), at least not in Philadelphia.  According to her analysis, banks were giving mortgages in the redlined districts after the Philadelphia HOLC map was produced.  She surmises that the HOLC maps were not widely circulated outside the federal government and therefore were probably not known or used by lenders.  In fact, a majority of HOLC's mortgages were in the hazardous end of their assessment scale.  However, Hillier does conclude that the FHA policies, which were formed partially from Hoyt's research, were influential in the arena.  The legacy of these maps and policies must not be understated.\n\nThe University of Richmond has a excellent [site](http://dsl.richmond.edu/holc/) that shows the HOLC map for Richmond and explains in detail the criteria and survey data used to determine hazard rankings of which presence of African Americans trumped all other factors.  The handwritten survey reports are shocking.\n\n\n## Next Steps\n\n\nWe have now finished the process of digitizing all the RPC and Hoyt overlays.  We next will do some geostatistical analysis to compare them to the Richmond HOLC map to see how well the data fit together.  I would also like to map the new mortgages and refinancing given from the 1940s to the 1960s, à la Amy Hillier, to see the rates in the different hazard zones of the HOLC map.  [Jeremy Boggs](http://jeremyboggs.net/), a historian in our group here at the Scholars' Lab, has an interest in looking at the City of Richmond's policies during this period to gauge how they were affected by Hoyt's and HOLC's research and maps.\n\n[caption id=\"attachment_2397\" align=\"aligncenter\" width=\"470\"]![](http://www.scholarslab.org/wp-content/uploads/2011/09/map-1024x727.png) GIS map showing overlay from RPC report and HOLC hazard areas[/caption]\n\nArticle edited to make corrections to errors pointed out by Mr. Cloud in the comments section.\n"},{"id":"2011-09-25-ruby-slippers","title":"ruby slippers","author":"bethany-nowviskie","date":"2011-09-25 10:21:42 -0400","categories":["Grad Student Research"],"url":"ruby-slippers","content":"It's been an excellent Sunday morning for posts about DH and the profession(s).\n\nFirst, Desmond Schmidt [crunches the numbers](http://lists.digitalhumanities.org/pipermail/humanist/2011-September/002464.html) from a decade's worth of job postings on _Humanist_, which is the primary and longest-standing international discussion list for the digital humanities. (If you think there's a DH boom in the US, check out Desmond's per-capita analysis.) Interestingly, this survey only took PhD-level positions into account.  How have job requirements in this field evolved? Tomorrow's _Humanist_ should have a response from Dot Porter, citing an _#Alt-Academy_ essay she wrote with Amanda Gailey on \"[Credential Creep in the Digital Humanities](http://mediacommons.futureofthebook.org/alt-ac/pieces/credential-creep-digital-humanities).\"\n\nAnd here's Kathleen Fitzpatrick in the _Chronicle_, on what is really required of institutions and departments who encourage junior scholars to ['Do the Risky Thing' in Digital Humanities](http://chronicle.com/article/Do-the-Risky-Thing-in/129132/). Kathleen is amplifying and contextualizing a concern frequently voiced in the past two years, around the spate of \"cluster hires\" in DH -- which sometimes seemed to happen without thought given to the suport structures, both departmental and institutional, that new faculty would need. (I remember Patrick MurrayJohn as the first to start squawking about this on Twitter. I couldn't find his much-earlier tweets, but there's [this thread](http://digitalhumanities.org/answers/topic/who-supports-dh-at-your-institution-an-impromptu-survey) at DH Answers.) On the Chronicle piece, Kathleen and Ian Bogost make two important further points that may resonate with our [Grad Fellows](http://www2.lib.virginia.edu/scholarslab/about/fellowship.html) and [Praxis](http://praxis.scholarslab.org/) group: regarding \"[mentoring up](http://twitter.com/#!/kfitz/status/117961209692688384),\" and [pressing forward](http://twitter.com/#!/ibogost/status/117962782007230464).\n\nFinally, Natalia Cecire responds with the most acute blog post I've read on the whole so-called \"rise\" of digital humanities and its political and professional consequences: [\"It's not \"the job market\"; it's the profession (and it's your problem too).\"](http://nataliacecire.blogspot.com/2011/09/its-not-job-market-its-profession-and.html)\n\nAnd what am I doing on a quiet Sunday afternoon (besides linking together this distributed conversation)? I'm following along with our Praxis students as we learn Ruby from the ground up. This has been really satisfying to me, and not _only_ because of every way in which I agree with Steve Ramsay [on \"building.\"](http://lenz.unl.edu/papers/2011/01/11/on-building.html) It's also because, like so many digital humanists of my generation, I learned every ounce of what I know on the job, rather than in the classroom or through any formal or institutionally-supported training program -- and most of the time my learning involved being confronted with something half-built or even jury-rigged by other humanities scholars who only marginally knew what _they_ were doing. I'm not disparaging this experience! The soft skills and improvisational confidence you learn on real-world collaborative projects are invaluable -- but the rationale for addressing programming more clinically may be akin to the one for learning Latin. (Look what has been built upon it -- what you will understand! And you're not really going to pick it up as an exchange student.)\n\nI've always felt like I could hack around (read: extend, modify, steal) on the spot with a decent level of fluency -- if supplemented by a small amount of magical thinking -- but that I lacked the basic and thorough grounding that would serve me well in a variety of situations, and that would make me less dependent on others when starting from scratch.  It's time I did something about that.\n\nThe R&D; staff of the Scholars' Lab are providing our Praxis colleagues (and those of us in the SLab who need it!) with [exercises](http://praxis.scholarslab.org/exercises/), [tutorials](http://praxis.scholarslab.org/tutorials/), [lessons](http://praxis.scholarslab.org/topics/intro-to-programming/), and one-on-one sessions on learning to code. They're also sharing the materials they create with the wider world. Pretty soon, the Praxis team will move out of lesson-ville and back into on-the-job learning, as they collaboratively design and build a tool called [Prism](http://nowviskie.org/2011/praxis-and-prism/).  For now (for me, anyway), taking the time [to complete](https://github.com/nowviskie/PraxisExercises) a set of rudimentary Ruby exercises feels like the biggest gift I've given myself in a long while.\n\nWhat does this have to do with Fitzpatrick on risk? With Cecire's sharp look at the present scene? With Schmidt and Porter & Gailey and the trends? With the title of this post? I'm making a cup of tea and moving off the Praxis site into [a third set](http://ruby.learncodethehardway.org/) of exercises -- so let's leave _that_ as an exercise to the reader.\n"},{"id":"2011-09-26-play-in-the-praxis","title":"jugando a praxis","author":"alex-gil","date":"2011-09-26 10:41:54 -0400","categories":["Grad Student Research"],"url":"play-in-the-praxis","content":"I third [Lindsay](http://www.scholarslab.org/praxis-program/the-pleasures-of-programming/)'s and [Bethany](http://www.scholarslab.org/praxis-program/ruby-slippers/)'s motion to declare coding a pleasurable activity. I have been tingling with guilty pleasure for the past couple of weeks doing [the exercises](http://praxis.scholarslab.org/exercises/) that the Slab developers have prepared for us. The guilt comes from the distance between these puzzles and the dissertation chapters I have spent the better part of this year writing. I know I will not feel guilty for long. Very soon I will begin coding for my digital edition of Aimé Césaire's _Et les chiens se taisaient_ and coding will just be what I do in lieu of writing for a while.\n\nMy pseudo-catholic reaction to code was very revealing to me. After all, why should anyone feel guilty to learn a new language? Why does it feel like a forbidden art? These questions got me thinking about the alleged differences between natural languages and computer languages. In the English Department, nobody would bat an eyelid if you said you were learning Farsi. Somehow Farsi could be integrated to your work as a scholar. Coding on the other hand still feels alien. It shouldn't. The fact that it does speaks to that imaginary border between scholarship and service. As English graduate students, well-steeped in the scholarship of Benedict Anderson or Walter Mignolo, we should recognize by now that borders are reified in the performance of a nationalism. _I belong to the literary scholar nation_, _you belong to the librarian nation_, etc. I am a migrant in the fullest sense of the word. To me these borders seem silly... and dangerous.\n\n[caption id=\"attachment_2545\" align=\"aligncenter\" width=\"300\" caption=\"The languages of Git\"][![Git Languages](http://www.scholarslab.org/wp-content/uploads/2011/09/git-lang-300x136.png)](http://www.scholarslab.org/praxis-program/play-in-the-praxis/attachment/git-lang/)[/caption]\n\nAnother (false) difference between natural languages and code got me thinking further about what we're doing here. If a speaker of two languages sees a paragraph where those two languages intermingle, they recognize the meaning immediately. At first sight, computer languages don't seem to behave that way. After all, you can't really insert a PERL line at random in a Ruby script and expect the interpreter to recognize it. This difference is also artificial. There is a right way and a wrong way to mix Spanish with English, just as there is a right way to mix Java with PHP. In order to produce meaning you just have to do it the right way. Git, for example, is written mostly in C, but with substantial contributions by other languages. Junot Diaz's _The Brief and Wondrous Life of Oscar Wao_ is written mostly in English, with some Spanish thrown in the mix. Even for Spanglish speakers the text follows certain rules to make itself clear. All that to say, that for those of us who are becoming bilingual (again), it is not _caos total_, but just a new set of procedures.\n"},{"id":"2011-09-26-subject-and-object-required","title":"Subject and Object Required","author":"lindsay-o’connor","date":"2011-09-26 06:25:32 -0400","categories":["Grad Student Research"],"url":"subject-and-object-required","content":"Despite the title, this won't be about objects and coding. It's about the subject behind the requirements we gather, the people we gather those objectives from. For the past couple weeks, we’ve been bogged down in some of the practical instruction we’re getting as part of the Praxis program, so I had forgotten about some of the project planning and management instruction we got a few weeks ago. When introducing us to requirements gathering, Jeremy and Bethany modeled the methods we might use as project planners when meeting with a client for whom we’re building a tool. Jeremy asked specific questions and redirected Bethany to the issues he wanted to cover. As I wrote [a few weeks ago](/2011/09/18/imagining-end-users-for-requirements-gathering/), perhaps over-critically, this is how some of Prism’s history, and what I took to be its existing expectations, were revealed to the Praxis team. Now I want to step away from the content of that discussion and look at the form — what methods and approaches it modeled rather than what particular information it revealed. If we were to extend this model of client-focused project planning into project management and evaluation later on, we might continue to check back with our “client” Bethany to make sure the tool we build for her is coming along as she would like. It would be her goals as a user that would become our end goals as tool-builders. But this instruction on how requirements gathering works is at odds with the “Prism is whatever you want it to be” message of the Praxis program. Is our job to learn the many skills necessary in DH by building a tool for a given client’s needs, or is our job to create a tool for the broadest possible audience, being cautious not to assume too much about how it will be used? I wonder if this question is similar to a question about what kind of role in the DH community we want, or what role Praxis is preparing us to take. Are we collaborating on scholarship or are we in a support role, providing a technical service that shores up someone else’s scholarly project? If we're creating a tool for a very wide audience, how does this activity fit into our own scholarship? Are we academics or “alternative academics”?\n\nNow I realize that I was wrong to point to the specifics of Bethany’s history of Prism as the source of my confusion over just how much Prism comes with particular expectations and how much it’s really ours to create. I came into that meeting with the expectation that we could make Prism however we wanted, so I was surprised to see that we might be gathering requirements from outside the Praxis fellows. So now I'm interested in the higher level question of where we’re gathering requirements from and to whom we’ll be accountable throughout the process. Are we acting like Bethany is our client who has a scholarly project in mind but needs a technical team to help think it through and make it happen? Or do we get requirements from the entire Praxis team, with all our individual hopes and expectations but a potential user population so wide we have to somehow build for uses we will never anticipate? If it’s the latter, we would do well to take Sarah’s advice on our requirements document to heart: “we want to be careful about pigeonholing Prism” because it could have uses beyond the primary pedagogical and second-level interpretive ones we’ve talked about so much.\n\nI’m not trying to disparage either framework, but I am hoping we can talk about which one applies to the Praxis program since this seems to cut right to the point of it all. This dichotomy makes a lot of sense to me, helps me orient Praxis in relation to the solitary academic scholar I’m so familiar with, but I realize that it’s a dichotomy that [Bethany’s most recent post](/2011/09/25/ruby-slippers/) and some of the people whose work she cites all seek to break down. We all want to be visionaries and risk-takers, but sometimes I react quite critically to new things and, much like [Kathleen Fitzpatrick](http://chronicle.com/article/Do-the-Risky-Thing-in/129132/)'s grad student questioner, I might be a little too worried about the current academic “ecology” to feel confident in disrupting it. But it has only been a month.\n"},{"id":"2011-09-27-a-belated-love-letter-to-ruby","title":"A belated love letter to Ruby","author":"sarah-storti","date":"2011-09-27 06:02:30 -0400","categories":["Grad Student Research"],"url":"a-belated-love-letter-to-ruby","content":"Today in the Scholars’ Lab Grad Fellows office I had a brief conversation with Alex about learning programming.\n\nMe: Hey, Alex.\n\nAlex: Hi. How are things going for you?\n\nMe: [exasperated sigh]\n\nAlex: Ha! I actually really enjoy it. It’s soothing…\n\nMe: [incredulous eyebrows]\n\nAlex: …like Sudoku!\n\nSoothing is not the first word I would use to describe my programming language skills acquisition experience. I realize that much of the problem has to do with my schedule: trying to work my weekly Praxis hours into an even spread has proven all but impossible thus far. I haven’t given up on trying to resolve this problem, but because I’m “still in coursework” (akin to “still in diapers,” I think?) as well as a first-time TA, I often find that the weekly round of course reading, discussion section prep, and meeting scheduling pushes Praxis from the top third of my to-do list until after the Tuesday-through-Friday crunch. Like Alex, I am convinced that learning how to use Ruby is very much like learning any other language—but when I was learning French I had class (and thus practiced) every day. Such is not the case for me and Ruby at this point in time. And I’m feeling guilty.\n\nI should reiterate that I do take full responsibility for my Ruby angst. The hours I spend with colleagues and SLab folks every week are always productive, and the direct instruction we receive at our weekly Praxis meetings has been well-structured and richly informative. But I will say that in spite of these advantages it has still been just plain difficult to make enough time to do this job as well as I would like to do it. Both Alex and Lindsay bring up the separation of Academy from Other this week in their posts, and I suppose I feel as though the middle ground between these two groups is at times the most difficult to navigate. I am proud to be part of the Praxis Program; I also feel honored that I am being allowed to earn my literature degree at this prestigious institution. I just hope that I can remember why the second honor should not be incompatible with the first; that I can find enough courage to spend one extra hour per day pursing knowledge which I’m convinced is at least equally important to my scholarly and intellectual development as are my more traditional studies. It’s always difficult to step away from the volume of poetry; by the end of my morning session in the SLab today I found it really was equally difficult to break off my programming exercises practice, though I still don’t think I’ve become proficient enough to feel soothed by Ruby. _Provoked_ is more like it. But then I enjoy a little bit of confrontation.\n\nSo here’s to making time! Cheers to the newly bilingual.\n"},{"id":"2011-09-27-the-joys-of-ruby","title":"The Joys of Ruby","author":"annie-swafford","date":"2011-09-27 08:00:21 -0400","categories":["Grad Student Research"],"url":"the-joys-of-ruby","content":"It’s official: I think I might like Ruby.  Granted, I think I still slightly prefer Python, but I’m reaching the point where Ruby syntax seems to make sense and I understand methods, variables, the different types of loops, conditionals, and iterators, and I’m ready to learn more about classes, attributes, and instance variables.  The exercises assigned for this week took substantially less time than those from the week before, and I hope that this is a sign that my dream of coding proficiency from my [blog post of two weeks ago ](/2011/09/13/programming-for-prism/)will one day come true.\n\nI’m hesitant to completely proclaim my progress, however, since the exercises for this week told us what elements we needed to use (ie. write a method using a while loop), and I’m much better at following directions than I am at figuring out the directions myself (ie. discovering when and why I would need to use a while loop).  However, I suppose that knowing how to use all the building blocks of programming is half the battle, and figuring out exactly when to use them will become easier with time.\n\nNow that we know the basics, I’m looking forward to seeing how we will move from defining short, straightforward methods to building a digital tool, since it still seems like we’re a long way away.  I’m sure learning Ruby of Rails will help with the process.\n"},{"id":"2011-09-27-waxing-metaphorical-with-ruby","title":"Waxing metaphorical with Ruby","author":"brooke-lestock","date":"2011-09-27 09:55:44 -0400","categories":["Grad Student Research"],"url":"waxing-metaphorical-with-ruby","content":"I must preface this post with a few disclaimers: First, _The Wizard of Oz_ is my all-time favorite movie. Second, I am an English graduate student, so it's in my nature to wax metaphorical. And last, I'm currently in a Ruby-induced fever which has severely limited my ability to think/write clearly, so this post will be one of my more ridiculous. (I actually dreamed in code last night.)\n\nDisclaimers made, Bethany threw out a metaphor that I can't get out of my head, though it was probably intended as just a witty, catchy title: She named her blog post, [\"_ruby_ slippers\"](/2011/09/25/ruby-slippers/) (my emphasis). Because I've seen _The Wizard of Oz_ at least 100 times and Ruby is still an alien language to me, I grasped at the ruby slippers as a familiar way to allegorize my experience learning Ruby. Brilliant, I know, but bear with me here:\n\nThe Ruby slippers are magical and powerful. You have to ride a twister over the rainbow and drop a house on a witch to get them on your feet. Then you'll spend roughly an hour battling a witch to keep them on. Now, rather than twister-hopping and murdering someone, I joined the Praxis Program, but that move for me could easily be likened to riding a DH tornado to the other side of the academic rainbow. The Scholars' Lab is Munchkinland, which I guess would make the staff Munchkins (but of average height, superior intelligence, and much less prone to musical outbursts), and Bethany is (of course) Glinda the Good Witch. I've bridged the imaginary academic divide, dropped a house on the Wicked Witch of the East, and Bethany and the SL staff have convinced me to keep the Ruby slippers on my feet rather than frustratedly discard them and run screaming. But now I have to battle the Wicked Witch of the West and her flying monkeys of programming frustration. The shoes don't fit and I'm not quite comfortable on the other side of the rainbow yet, but they're on my feet, I'm on the yellow brick road, and I have a goal: Prism. And even if Prism turns out to be just an old man behind a curtain (it won't), it will never disappoint because the journey is significant in itself.\n\nBUT: The Ruby slippers are more complicated than they seem. Wearing them isn't sufficient; they come with instructions that are neither obvious nor intuitive. For Dorothy, the method for using the ruby slippers' magic is to tap her feet together three times while repeating, \"There's no place like home.\" If only it were that simple. For me to master my Ruby slippers, I need to resurrect long-dead mathematical and practical/logical thinking, then pair that with a brand-spanking-new vocabulary of strings, loops, and methods that currently makes very little sense to me. I know Ruby is magical and powerful, but I don't think I'll understand that power or my own potential for harnessing it until I find my way to the Prism \"Wizard,\" when I'll move out of theory and into (pun intended) praxis. But from what I've heard, we'll be off to see the Wizard soon enough.\n"},{"id":"2011-09-30-fall-2011-workshop-series","title":"Fall 2011 Workshop Series","author":"ronda-grizzle","date":"2011-09-30 09:35:50 -0400","categories":["Announcements"],"url":"fall-2011-workshop-series","content":"We'd like to invite you to our workshop series this fall.\n\nChris Gist and Kelly Johnston have created eight GIS workshops designed to take attendees from learning to create their first map using ArcGIS10 to mapping the world with Open Street Map. The series starts Tuesday, October 11, and will run weekly through November 29.\n\nDownload the complete schedule of GIS workshops (PDF) [here](http://www.scholarslab.org/wp-content/uploads/2011/09/2011fall_workshop_gis.pdf).\n\nNancy Kechner and Kathy Gerber of UVa ITS and the Scholars' Lab are offering six workshops introducing statistical software packages and data visualization. The series started on September 28, and runs through Wednesday, November 9.\n\nDownload the complete schedule of software workshops (PDF) [here](http://www.scholarslab.org/wp-content/uploads/2011/09/2011fall_wkshp_sw.pdf).\n\nAll Scholars' Lab workshops are free, open to all, have no prerequisites, and no registration is required. We hope you'll join us!\n"},{"id":"2011-10-02-elotroalex-re-mixed","title":"elotroalex re-mixed","author":"alex-gil","date":"2011-10-02 13:08:12 -0400","categories":["Grad Student Research"],"url":"elotroalex-re-mixed","content":"[We recently decided to get to know each other by way of the blog. There is something really odd about auto-bio pieces.  They only have meaning at the beginning of a relationship. In the spirit of the conversation starter, I thought I'd play with the genre. If I am the subject, here are my conversations.]\n\nI am a mix of places: Charlottesville, Miami, Paris, Santo Domingo, Beirut, Amsterdam, Valencia.\n\nI am a mix of places: The library, the department, the street, the salon. I am a heteroglossic fantasia in 7 languages and 7x7 idioms. Everywhere I am, I am other, _el otro_ Alex. No kidding.\n\nI study Aimé Césaire. In particular, his drama, \"And the Dogs Were Silent.\" The plot: A man rebels against everything and everyone, radically, uncompromisingly. Simple. The textual history: The author rebels against himself, radically, uncompromisingly. Not so simple.\n\nI study machine languages. Ruby, now. I am searching for the boundary between humans and machines. It is not where most people think it is.\n\nI study texts. They are strange machines, simultaneously covered and exposed. They are patient teasers. Before I had mechanical questions, I read an inordinate amount of texts. I remember most of them.\n\nIn my thesis I argue that material conditions of production over-determine texts. I am not the first to argue this, nor the last. My research on Césaire is original.\n\nI court three professional camps: Digital Humanities, Caribbean Studies and Textual Studies. I am about to court a fourth: Library Studies. I wish I had time to court Continental Philosophy and be a local community organizer.\n\nThis year I am a fellow at the Scholars' Lab, HASTAC and NINES.\n\nI publish in different places. I publish in print and on the web. I gave away my copyright once. That was a mistake. Now I give away my scholarship and ideas instead.\n\nI am one of the editors of the Planète Libre critical/genetic edition of the complete works of Aimé Césaire (2013). I am the sole editor of the digital version.\n\nOn a personal note: I am a father of two clever sons. I am cleverly married.  I have a large family. They are a mix of places too.\n"},{"id":"2011-10-03-spatial-humanities-step-by-step-mapping-wikileaks-using-google-fusion-tables","title":"Spatial Humanities Step By Step - Mapping Wikileaks using Google Fusion Tables","author":"kelly-johnston","date":"2011-10-03 10:03:36 -0400","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"spatial-humanities-step-by-step-mapping-wikileaks-using-google-fusion-tables","content":"Are you ready to participate in the [Golden Age ](http://mapbrief.com/2011/09/01/the-new-golden-age-of-cartography-has-arrived-and-its-co-ed/)of online mapping?\n\n\n\n\t\n  * June, 2009 - Google Launches [Fusion Tables](http://www.google.com/fusiontables/Home), an online tool for mapping places\n\n\t\n  * July, 2010 - [Wikileaks](http://wikileaks.org/) Publishes the Afghan War Diary, a massive dataset full of place names\n\n\t\n  * April, 2011 - Scholars' Lab [Launches \"Spatial Humanities Step By Step\"](http://www.scholarslab.org/announcements/project-launch-spatial-humanities/),  a source for peer-reviewed geo-tutorials\n\n\t\n  * October, 2011 - Devin Becker, University of Idaho Digital Initiatives Librarian, publishes Step By Step tutorials on [mapping Wikileaks using Google Fusion Tables](http://spatial.scholarslab.org/?p=1283)\n\n\nSo many new mapping tools.  So many online data sources.  So much interest in the [Spatial Turn](http://spatial.scholarslab.org/spatial-turn/) across disciplines.   Where to begin?\n\nParticipants in the NEH funded [Institute For Enabling Geospatial Scholarship](http://spatial.scholarslab.org/about/about-the-institute/) made it clear they wanted a reliable source of helpsheets and tutorials for working with data sources and mapping tools.  [Spatial Humanities Step By Step](http://spatial.scholarslab.org/step-by-step/) is that growing resource.\n\nWe created Step By Step to help aggregate high-quality (yet simple-to-follow) tutorials and provide an opportunity for folks to receive professional acknowledgment for the work that goes into creating them.\n\nIn addition to Devin Becker's newly published contribution on Mapping Wikileaks with Google Fusion Tables, we have in the pipeline tutorials on extracting and quantifying information from historic maps, using Google Earth as a gazetteer, and calculating least cost paths across a landscape.   Already posted are helpsheets for georeferencing historic maps, converting addresses to mapped locations through geocoding, mapping Global Positioning System datasets, and using Google Maps to create a HyperCities project.\n\nWe believe folks who invest the time to write easy-to-follow tutorials should receive credit for their work.  Is that you?  Then [submit your own work](http://spatial.scholarslab.org/contribute/#step-by-step) to our supportive review board:\n\n\n\n\t\n  * Patrick Florance-Tufts University\n\n\t\n  * Chris Gist-University of Virginia\n\n\t\n  * Tracey Hughes-University of California, San Diego\n\n\t\n  * Kelly Johnston-University of Virginia, Editor\n\n\t\n  * Scott Nesbit-University of Richmond\n\n\t\n  * Bethany Nowviskie-University of Virginia\n\n\t\n  * Diana Stuart Sinton-University of Redlands\n\n\t\n  * Ginny White-University of Oregon\n\n\nOur simple process: two reviewers take each submission for a spin to confirm the process is easy to follow, jargon free,  and just works.  Then with consensus, we publish.  If you'd like to volunteer to be called upon as a peer reviewer alongside our current review board please [contact me](mailto:kgj3t@virginia.edu?subject=Spatial Humanities Step By Step).\n\nI serve as the Step by Step editor and along with my colleagues on the review board we look forward to your contributions.  Check the site for more information: [http://spatial.scholarslab.org/step-by-step\n](http://spatial.scholarslab.org/step-by-step/)\n"},{"id":"2011-10-04-what-ive-learned-from-my-kindle-part-i","title":"What I've learned from my Kindle: part I","author":"sarah-storti","date":"2011-10-04 08:35:04 -0400","categories":["Grad Student Research"],"url":"what-ive-learned-from-my-kindle-part-i","content":"Though I believe the idea is to fill the “getting to know you” blog spot in turns one week at a time, events conspired this week to set me up perfectly for part one of a relatively brief expostulation on Some Things I Hold Dear as a Scholar in This Age of Digital Texts, a topic I’ve been reflecting on almost nonstop ever since we learned about the basic idea behind what will become Prism.\n\n[Like Alex](http://www.scholarslab.org/praxis-program/elotroalex-re-mixed/), and as I've acknowledged previously, much of the work I do as a scholar is centered in textual studies. I frequently find myself drawn to problems that concern the production, dissemination, transmission and translation of texts. Partly for this reason, and perhaps perversely, I have always had a very difficult time imagining myself happily using an e-reader for scholarly work (or, to be honest, even for leisure reading). However, this aversion to e-readers, which is not entirely unfounded as far as textual concerns go, caused me distress when I considered that though I do enjoy studying physical books and printed texts, I also frequently profess to be a member of what Alex calls the Digital Humanities “camp.” I had always felt that these two areas of interest mesh quite well and are complementary. But how pro-digital could I really be, I used to wonder, if I couldn’t even bring myself to come within five feet of a Kindle or a Nook?\n\nThe e-reader problem went unaddressed until I graduated from UVa with my M.A. last spring. I opened the door of my apartment one day to find an unexpected box from Amazon sitting innocently on the front step. Inside the box was a very thoughtful and generous graduation gift from my aunt and uncle: a Kindle. Magnificent! I thought. Now I don’t have to pay for an e-reader, and I can finally rid myself of the guilt I’ve been feeling about assiduously avoiding contact with any and all e-reader devices since they first appeared on the market. Of course, I decided that I would start out on the Kindle with something unrelated to what I studied. Some light summer reading, perhaps. I had a number of international flights booked in the upcoming months. Perfect.\n\nTo make a long story short, I did not take the Kindle with me to British Columbia, Greece, or London. It disappeared during the move into my new apartment and only recently, with a pang of remorse, did I discover it again. I determined, however, that this time I would actually read something on it before the week was out.  Quite happily, a favorite reading group of mine provided the perfect opportunity. At our meeting last Friday we decided that our next text for discussion would be John Henry Newman’s novel _Loss and Gain. _Copies seemed scarce, I needed one quickly, and I refused to pay for print-on-demand. Enter: [the Kindle option](http://www.amazon.com/Loss-Gain-Story-Convert-ebook/dp/B002RKTEBM/ref=sr_1_2?ie=UTF8&qid=1317708329&sr=8-2).\n\nI was ready. I was even excited. I “purchased” my copy for free, and it instantly appeared on my Kindle via Amazon \"Whispernet.\" It was magical.\n\nBut would the reading process prove to be equally satisfactory? The saga continues next week… (and will conclude with an explanation about why all of this matters in a Praxis/Prism context!)\n"},{"id":"2011-10-07-ada-lovelace-day-2011","title":"Ada Lovelace Day 2011","author":"bethany-nowviskie","date":"2011-10-07 10:59:43 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"ada-lovelace-day-2011","content":"On this [Ada Lovelace Day](http://findingada.com), I'm looking forward and back.  Here's my full post in honor of [humanities computing pioneer Susan Hockey](http://nowviskie.org/2011/ada-lovelace-day-susan-hockey/) (where you can also find links to past years' posts on Johanna Drucker, Bess Sadler, and Leah Buechley). But I'm also spending today feeling appreciative of the a fantastic group of young women -- emerging humanities and social science scholars, technologists, and cultural heritage or scholarly communications workers -- with whom we've been privileged to collaborate in the SLab.\n\nSo here's a little post in honor of our grad school gals: Scholars' Lab [Graduate Fellows in Digital Humanities](http://www2.lib.virginia.edu/scholarslab/about/fellowship.html) and [Praxis Fellows](http://praxis.scholarslab.org/) past and present.\n\nJean Bauer\nBeth Bollwerk\nAbby Holeman\nDr. Wendy Hsu\nBrooke Lestock\nRandi Lewis\nLindsay O'Connor\nSarah Storti\nAnnie Swafford\nand Dana Wheeles\n\nThanks for inspiring us all!\n"},{"id":"2011-10-11-riding-the-rails-and-learning-not-to-fall-off","title":"Riding the Rails (And Learning Not to Fall Off)","author":"annie-swafford","date":"2011-10-11 09:58:49 -0400","categories":["Grad Student Research"],"url":"riding-the-rails-and-learning-not-to-fall-off","content":"It’s hard to believe that only a few months ago, I thought the idea of using a web framework to generate a website seemed like taking the easy way out.  Although I’d heard of Django and Rails, I didn’t really see the point of them.  Apparently I had a lot to learn.  Last week was our first experience of Rails, and I think I’m being converted.  It’s nice to type a few lines of code into the terminal and to have a barebones working framework for a site. And although the syntax initially seemed as clear as gibberish incantations, I’m starting to see how to have it build what I want it to, even without the scaffolding.  I’m finally understanding MVC architecture patterns, and I think that many features of Rails, including partials, will be incredibly helpful.  I’m even thinking about rebuilding my digital project ([I blogged about it a few weeks ago](http://www.scholarslab.org/praxis-program/introducing-our-digital-work-%E2%80%9Csongs-of-the-victorians%E2%80%9D/)) in Rails.  I think that it might still take awhile before I feel comfortable creating my own site in it without step by step instructions, but I’m looking forward to trying it.  Maybe next time I’ll blog about transitioning my project from its current framework (html5, css, and javascript) to Rails!\n"},{"id":"2011-10-11-towards-a-geo-textual-humanities","title":"towards a geo-textual humanities","author":"alex-gil","date":"2011-10-11 06:32:28 -0400","categories":["Grad Student Research"],"url":"towards-a-geo-textual-humanities","content":"Maps are texts, and texts are maps.\n\nAt [the beginning of the movie _The English Patient_](http://www.youtube.com/watch?v=rAUJgjxNGd8), as Márta Sebestyén's \"Szerelem, szerelem\" overcomes our senses, a paintbrush traces the figure of human swimmers on a yellowing page. The black-ink soon gives way to a skin-colored desert landscape sifting beneath our aerial view, evoking hands moving over human curves. Skin, page, territory all united by the theme of lost love. I can't think of a better image to describe how we are wedded to the [n-dimensions](http://digitalhumanities.org/companion/view?docId=blackwell/9781405103213/9781405103213.xml&chunk.id=ss1-3-4) of the textual condition.\n\nAs we get ready to think about design I wanted to outline a few of the ways we can abstract the material reality of print to a totality of 1's and 0's. In my own work I have been trying to create a digital edition of Aimé Césaire's _Et les chiens se taisaient _that is both pleasant to read and that allows for some algorithmic manipulation of the textual territory. My goals lead me to seek the chimera of [html forgeries](http://www.elotroalex.com/workbench/dr_sample.html) as opposed to the classic images with texts beneath them. The experience taught me an enormous deal about the process of remediation.\n\nThere are many ways we can remap texts online. We can have a simple image. We can have text behind that image, like your typical PDF. We can map out the position of text and white space on that image by overlaying a basic Cartesian x and y grid on top (or is it below?). We can name areas on that grid like land-grabbers use contracts to justify their fences. We can query  the areas, we can query the points, we can query the text. We can overlap those areas, like the map of [Aztlán](http://en.wikipedia.org/wiki/Aztl%C3%A1n) tensely overlaps with the map of the United States, like our Prism diffracts difference. We can create replicas from scratch using HTML, using Canvas, and trade grain for the possibility of playful deformation and a digital audience born into cool media. We can standardize our geo-textual mark-up, make a TEI out of HTML/CSS, opening the door for large scale analysis  of page design in book-history. Heck, we can just put our UTF-8 txt's out there and just sit back and wait for our computer overlords to tell us that the eternal present of spotless text was all we ever needed. Lord knows, most literary scholars haven't done better than that. (I will rebel against that last possibility the way I rebel against propaganda, the way I rebel against the early Wittgenstein, who wanted to get rid of love because we couldn't fit it in just one map).\n\nLet us move towards a geo-textual humanities conscious there are swimmers in the desert of the page.\n\n(to be continued...)\n"},{"id":"2011-10-11-wayne-graham-leader-of-lemmings","title":"Wayne Graham: Leader of Lemmings","author":"ed-triplett","date":"2011-10-11 09:55:16 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"wayne-graham-leader-of-lemmings","content":"Last week from 2-4pm we jumped into a new field with the assistance of Wayne Graham. The same Wayne that has designed many of our exercises was stuffed into a corduroy jacket with leather elbow-patches and asked to do something very difficult: teach the Praxis group how to implement Rails. I want to take this opportunity to describe the challenges set before an instructor attempting to get our group up to speed.\n\n\n\n\nFirst, lets set up the immediate context before our last lesson. The time designated to teach something new is two hours per week. Building up what Wayne and others have called our “muscle memories” takes place during the hours before our next meeting, usually on Monday and Tuesday. As Praxis fellows come and go during the course of their other responsibilities, we eventually arrive at a semblance of even proficiency on the previous lesson about 15 minutes before we begin a new set of challenges at 4:00.\n\n\n\n\nNow eleven of us are in a room staring at Wayne’s first slide like it’s an RCA television. Professional DH programmers, researchers and developers, English graduate students and a token architectural historian each plug in their laptops and gear up for a lesson that is new for at least nine of us. Then the fun begins for Wayne.\n\n\n\n\nRoughly every seven words of Wayne’s lesson he has to stop for a question. Twice per slide, Wayne must ask “Is everyone here?” while holding up a hand to a line of code. The answer is very rarely affirmative.\nBeginning with the second slide, it becomes clear that the PC users, Mac users and Unix user have unique challenges that hamstring attempts to keep us all going at the same pace. As I sit on my Toshiba netbook, I have to admit this is not the best tool for the job. Most of us will be working with at least VIM and Git Bash open while programming in Ruby on Rails. Add to this several essential tabs open in a web browser to test our changes to the code online. For us PC users, we also had to have a command prompt open because there are a host of permission and directory issues that came up during this process. While performing all of these tasks on a netbook, I am reminded of these weirdos that attempt to write the declaration of independence on a grain of rice, or paint the Sistine chapel on a post-it note.\n\n\n\n\nSo while Wayne is walking us through the process of installing rails, setting up our servers, making changes in VIM, committing changes in GIT, and testing the changes in our browser, some are following like dutiful lemmings. We happily jump off the cliff thinking “Wayne is our leader, he obviously wants us at the bottom of this gorge for a reason... weeee!” Others of us feel like we wore the wrong shoes and become jealous of the other lemmings that seem better suited for the jump. Still others want to know where we are now, and how we define “cliff.” At various times, some of us are distractedly wondering if we left the server on. While Lemming-leader Wayne is herding these unruly lemmings, the remaining lemmings just keep trying to touch the third rail with our tongues.\n\n\n\n\nI don’t have to tell our group that this is difficult stuff to learn. We should also understand that it is equally difficult to convince a group of naturally inquisitive people to have the patience to “wait and see.” Hopefully we can make the next lesson a little easier on Wayne and ourselves by doing some preemptive strikes on our laptops before we start. Finally, we will have a lot better idea of what cliff diving feels like when we are in mid air, rather than asking Wayne all about it back in lemming-town.\n"},{"id":"2011-10-12-tim-powell-revitalizing-jeffersons-vision-for-preserving-native-american-languages","title":"Tim Powell, Revitalizing Jefferson's Vision for Preserving Native American Languages","author":"ronda-grizzle","date":"2011-10-12 08:35:34 -0400","categories":["Podcasts"],"url":"tim-powell-revitalizing-jeffersons-vision-for-preserving-native-american-languages","content":"**Revitalizing Thomas Jefferson's Vision for Preserving Native American Languages**\n\nOn September 28th, the Scholars' Lab welcomed Tim Powell, Director of Native American Projects at the American Philosophical Society where he oversees the Native American Endangered Languages Digital Archive. Dr. Powell is also a Senior Lecturer in the Department of Religious Studies at the University of Pennsylvania, and a Consulting Scholar for the Penn Museum. He has worked closely for the last ten years with the Eastern Band of the Cherokee and Ojibwe Bands in northern Minnesota. He has won three NEH grants to create [Gibagadinamaagoom: An Ojibwe Digital Archive](http://gibagadinamaagoom.info/).\n\nTalk Summary:\nThomas Jefferson began the Native American language preservation project while President of the American Philosophical Society (APS) from 1797-1815. The APS recently received two Mellon Foundation grants to digitize its entire Native American audio recordings collection, totaling more than 3000 hours. The collection contains invaluable linguistic and historical recordings that scholars and Native American communities are using to bring languages back from extinction. The project also raises important questions about the meaning of Digital Humanities in august archives. In his talk, Tim introduced us to these efforts and discussed how cross-institutional work in the world of cultural heritage organizations might serve as a model for academic digital humanities writ large.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://www.google.com/url?sa=t&source=web&cd=1&ved=0CBUQFjAA&url=http%3A%2F%2Fitunes.apple.com%2Fus%2Fitunes-u%2Fscholars-lab-speaker-series%2Fid401906619&rct=j&q=scholars%27%20lab%20itunes&ei=FI61TdiZNo-Dtge0g_3pDg&usg=AFQjCNGGTBvTY5QpL9aRCKh7rjEOtlLAUQ&sig2=KBrhIc1DK814RPqoAB85Tg&cad=rja).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.10550711093/enclosure.mp3\"]\n"},{"id":"2011-10-17-in-the-neutral-ground","title":"In the Neutral Ground","author":"lindsay-o’connor","date":"2011-10-17 18:35:19 -0400","categories":["Grad Student Research"],"url":"in-the-neutral-ground","content":"With all the coverage of Occupy Wall Street, I’m hearing a lot about urban space and how we live in it. When space arises as a site or source or critique, I always wonder about the ways DH could help analyze the spatial and make spatial thinking more available to humanities scholars, but it also makes me think about the spaces of academic work itself. I’ll leave the digital question until I have a little more technical skill to bring to it, and for now I’ll try to think through the space of collaboration. I want to do that through a story about another interesting experience of public space that seems just as relevant as the protests. I guess this is a “getting to know you” autobiographical blog post as well.\n\nI was recently in New Orleans for UVa’s fall break, and I participated in my first proper second line. My teacher and friend Joel Dinerstein is a member of the Prince of Wales Social Aid and Pleasure Club, and he proved a great tour guide for a surprised and amazed grad student who has been in prim and proper Charlottesville for too long. [As Joel will tell you](http://muse.jhu.edu/journals/american_quarterly/v061/61.3.dinerstein.html), a second line is no quaint neighborhood shindig; it instead involves dancing-walking-grooving through a winding 5 mile route that crosses racial and socioeconomic boundaries. When you’re with an American Studies mentor, it also involves waxing ethnographic about African American working class culture and marveling at how this experience can only be lived, never represented.  Buying bottles of beer from people wheeling coolers through the street helps with both the dancing and the analyzing, and there’s barbecue on truckbed grills if you get hungry. I danced and clapped my way down Magazine St., stopped streetcars full of amazed tourists on St. Charles, and ducked into the Sandpiper Lounge for a beer. Around the corner on LaSalle St., the Dew Drop Inn sign remains across the street from where the iconic Magnolia projects have become a mixed-income housing development featuring cookie-cutter townhomes and treeless courtyards.  This is central city, a neighborhood that white folks generally avoid and that few tourists know exists. I had driven through this area countless times before, but it’s a place you pass through on the way to somewhere else, not a destination or a place to linger. But last weekend I found myself buying lemon cake from The Pie Lady in the middle of Louisiana Avenue, meeting the King of the second line while we jostled toward the bar for a beer, and chatting with the hodgepodge of Tulane (my alma mater) English faculty, post-docs, and alumni who all came out for the afternoon.\n[![Prince of Wales Social Aid & Pleasure Club Second Line 2011 by Catherine King](http://farm7.static.flickr.com/6116/6238031552_35b232e8e6_m.jpg) ](http://www.flickr.com/photos/wwoz/6238031552/)\n\nIn New Orleans, roads don’t have medians; they have the neutral ground. There’s interesting geographic history to this term that work like [Rich Campanella’s](http://richcampanella.com/index.php) makes accessible, and just the name itself sheds some light on what makes this place so special. The Second Line made me slow down, made me stroll or dance through places I had never noticed as places. Dancing is a way to move through urban space that I had never quite thought about before. If you’re doing it right, it makes you more aware of your body even as you stop worrying about exactly what your body is doing, and it makes you more aware of the place where you are and where you’re going as you negotiate everything from potholes in the road to your previous abstract ideas about the space you’re in. It’s inspiring and it’s reparative—second lines derive from [jazz funeral](http://en.wikipedia.org/wiki/Jazz_funeral) traditions that turn loss into celebration—and it’s making meaning out of music and dance.  Now I understand the metaphor of a political “movement,” and I hadn’t even noticed that it was a metaphor before.\n\n[![Prince of Wales Social Aid & Pleasure Club Second Line 2011 by Catherine King](http://farm7.static.flickr.com/6031/6238028508_7f67d7cc2e_m.jpg)](http://www.flickr.com/photos/wwoz/6238028508/)\n\nThis second line brought people together who wouldn’t normally be together. Identity categories and the implicit rules that separate us are transgressed just like neighborhood boundaries and traffic laws, if only temporarily, and there was also me with all these Tulane English folks past and present.  I was less self-conscious than I’ve ever been at a faculty-student social event; my “elevator story” wasn’t a source of tongue-tying anxiety, and I felt OK offering half-baked ideas about literature and culture and second lining. As I ate red beans and rice and sweet potato pie, I felt that we were all equal.  This is something I rarely feel as a graduate student, and I’m sure that’s my fault as much as anyone’s, but it seems at least partially related to the spaces in which we live and work. In New Orleans, I felt a spirit of collaboration I didn’t know I had been looking for. But I think maybe you can’t actually look for it; it's improvised, and that might be where some of the frustration of my previous blog posts came from. The planned Praxis collaboration seemed less organic that I would have hoped. But I’ve been inspired by recent serendipity and am looking forward to working with the Praxis team even more. I hope we can remember the “healthy dose of play” that we mention in our charter and make Praxis even more of a space of encouraging and productive boundary-crossing.\n\nOn [NPR’s Studio 360](http://www.studio360.org/2011/oct/14/a-visit-to-occupied-wall-street/) this weekend, architecture critic Michael Kimmelman pointed to Aristotle’s _Politics _for evidence of what’s exciting about Occupy Wall Street: “[Aristotle] talks about a _polis_ shaped by the distance of a herald's cry — meaning a civic space, a city functioning in which people have the ability to meet face to face to speak with each other.\" He goes on to point out how Zuccotti park has been transformed from a place people hurry through to a place where people linger and connect with one another. This seems to me like one of Occupy Wall Street’s most exciting accomplishments, but that’s just any Sunday afternoon in New Orleans.\n"},{"id":"2011-10-17-praxis-program-charter-available","title":"Praxis Program Charter Available","author":"jeremy-boggs","date":"2011-10-17 06:51:45 -0400","categories":["Grad Student Research"],"url":"praxis-program-charter-available","content":"After a bit of time discussing and editing, we're happy to announce that the project charter for the 2011–2012 Praxis Program is [available on our site](http://praxis.scholarslab.org/charter.html). The charter outlines the team's broader goals for Prism and the program, as well as our approaches and responsibilities for achieving those goals collaboratively. Feel free to read it, and let us know what you think!\n"},{"id":"2011-10-18-charter-and-design","title":"Charter and Design","author":"annie-swafford","date":"2011-10-18 08:48:49 -0400","categories":["Grad Student Research"],"url":"charter-and-design","content":"This has been an eventful week in Praxis, since we both finished our charter and started trying to design Prism.  The [charter](http://praxis.scholarslab.org/charter.html) took longer than anticipated to produce, primarily because we had some difficulty figuring out when we needed to be incredibly specific and cover all possible scenarios, and when we needed to be very general and deal with problems in a case by case basis.  For example, we eventually decided that we didn’t need to include a policy covering credit for departing members because we thought our general statement about credit would cover it.  We also had a tough time deciding on the tone for the document; we didn’t want to sound paranoid about potential problems by assuming that we would have conflicts, but we also didn’t want to sound too naïve or optimistic and assume that it would all be smooth sailing.  Eventually we decided on language that seemed suitably neutral.  Additionally, we had tried to use language that covered all our job titles (graduate students, faculty, and staff) in an attempt to be as inclusive and respecting of difference as possible, but we decided that we wanted to emphasize our unity above all, since the charter applies to the whole team, so we made such to always say “all Praxis team members” instead.  It was an excellent exercise in team building as well as project management.\n\nOur attempts to start designing Prism have been both rewarding and frustrating for me.  It’s rewarding because we have started discussing features and the philosophy behind the possible visualizations for Prism: today in our morning meeting we talked about how to visualize overlap, whether a user can mark a word with two colors, and whether the visualization should be tied to an image of the text, or whether it should be colored lines by themselves without the text.  The frustration for me comes from my own negligible artistic skill; although I’m intimately involved in the other arts, I have never been adept at drawing.  Consequently, I end up having to describe the image of the project I have in my mind with words instead of letting a pencil do the explaining.  Now more than ever I am reminded of one of our charter’s goals: “We aim to recognize, respect, celebrate, and leverage the differences in our intellectual convictions, our academic backgrounds and experiences, and our talents and skills.”\n"},{"id":"2011-10-18-design-to-play","title":"design to play","author":"alex-gil","date":"2011-10-17 20:08:05 -0400","categories":["Grad Student Research"],"url":"design-to-play","content":"Full disclosure: I have not used coloring pencils in over a decade... that is, until we got down and dirty to brainstorm some possible designs for Prism last Tuesday. Word has it, there will be some transparencies and markers tomorrow. Funny thing is my first salaried job was as a caricaturist for the newspaper _El Caribe_ in Dominican Republic. I remember sitting down with coloring pencils every night and cranking out a few designs before the 2am deadline. That was around the time when I was supposed to be an architect. _El Caribe_ paid for my full tuition to go to architecture school. I know. Why did I drop? Short answer: I didn't want to draw for the rest of my life.\n\nThose pencils felt like old friends in my hands last Tuesday.\n\nFull disclosure: I don't highlight books. I use light pencil markings. Somebody must have instilled the fear of the biblio-gods in me early on because I even feel bad when I see someone else do it. I started highlighting for the first time with computers. Now there's a place where I don't feel like I'm defiling anything by coloring it up (I use Diigo, Word and Adobe Pro nowadays).\n\nUntil now, I never stopped to think about the kind of highlighting we do when we highlight web pages, doc's or pdf's. I realize right away that it is a very different activity from highlighting paper. For example, when was the last time you could erase highlighting from a printed article? Prism highlighters also seem different than your average random markup. I'm not only talking about the controlled vocabulary. Any organized researcher can device his or hers color system and use the tools available already to give meaning to their markup. What feels different about Prism is that you are being asked to play. Unless it is a reviewer trying to call attention to some text for you, highlighting is a very personal and solitary activity. Prism is nothing of the sort.\n\nIt is hard to say what the effect would be before actually doing it with the real thing when we roll it out, but I can already tell it will be a new kind of play. In a ludic spirit, I suggest we make the color _palette _big and bright, almost child-like: four circles below the text. The text should have nothing to the left, nothing to the right. The horizontal space of the text should be clear to keep the peripheral vision in check. When the pointer goes down to dip in the palette, I would like to feel the danger of wet ink over the text. Highlighting and painting combined into one? Me to play!\n"},{"id":"2011-10-18-highlighting-limitations","title":"Highlighting limitations","author":"brooke-lestock","date":"2011-10-18 12:27:37 -0400","categories":["Grad Student Research"],"url":"highlighting-limitations","content":"In [his blog post](http://www.scholarslab.org/praxis-program/design-to-play/), Alex mentioned that we'll soon (I think today) be coming face to face with the Patacritical Demon - that is, we'll be doing the exercise that awoke the demon - in all of its highlighted transparency glory. I have been desperate to do this with the group since Bethany first mentioned it. Partly for a reason Alex mentions in his post in that I never highlight literature as I read. I don't necessarily have the bibliographic prejudice against it that Alex does, but highlighting literature goes against all of my instincts. I highlighted in my science, math, and history textbooks because they offered rules, laws, facts, and dates to be memorized. I highlight criticism now because there's a similar feeling that you're being offered more straightforward information that you're not necessarily interpreting. The critic, of course, is interpreting a text and you interpret the critic's argument to determine its validity or usefulness for your own purposes, but I feel more comfortable highlighting criticism because there's a sustained, usually (hopefully) unified argument, and thus there's less of a chance that I will return to it later with a different reading or highlighting for different things. Also, in those cases, I only use one highlighter because I'm not necessarily categorically interpreting the information I'm reading.\n\nWhen I read literature I will obviously mark for certain things, only rather than highlighting, I create headings in a OneNote notebook page and then add quotes and page numbers for passages or lines that I feel the need to make note of or categorize. This way my notes are always in flux: I can add new categories as they strike me as necessary without the fixed mark of what I thought was noteworthy in my first reading, and without the worry of running out of highlighter colors.\n\nPrism addresses my highlighting fears in an interesting way. The highlights are fixed and preserved, yet also necessarily in flux because they're not being evaluated or examined on an individual level (and there's even the possibility of having users mark texts over time). The highlighter colors are limited, but only so as to allow for maximum \"aesthetic provocation,\" as Bethany once said, and interpretive possibility on the user end. Even as I write, there's a conversation happening in the grad lounge about whether or not people would be able to mark a piece of text with multiple highlighters, and how that could be visualized. I think one way of generalizing some of these debates  is a fear of limitation that ignores or undermines nuance, a practice that most humanities scholars are very uncomfortable with, even at the most basic level of highlighting a novel at home with the four or five highlighters that come in the pack. Alex uses light pencil-marks, I use OneNote, whatever the strategy there are many ways of noting  and interpreting the nuance and complexity of a literary text, but what I'm struggling with (and what the group seems to be working towards) is devising a way to productively limit the texts in question, and productively limit Prism itself, and be comfortable with it as a group. We'll see how it goes, but I'm hopeful.\n"},{"id":"2011-10-18-prism-images-and-binaries","title":"Prism, Images and Binaries","author":"ed-triplett","date":"2011-10-18 18:55:27 -0400","categories":["Digital Humanities","Grad Student Research","Visualization and Data Mining"],"url":"prism-images-and-binaries","content":"Several of us were recently asked to come up with sample texts to use for a simulated Prism experiment. As the token art historian of our group, I volunteered to find an example that included images as well as text. My initial efforts were spent imagining how I would use Prism as a teaching tool in an art history course. I thought that the clear cut nature of prism,  i.e. its requirement that the reader/viewer make a sharp distinction between ideas, would be a great method to teach students about their own preconceptions with regard to art. For example, I am very interested in what a crowd sourced application could tell us about what a group of students believe are the formal qualities that represent \"Islamic\" or \"Christian\" art or architecture. Another simple example might be for a group of students to mark up images that appear \"Eastern\" or \"Western,\" or more problematically, \"Oriental\" versus \"Occidental.\" How would the crowd mark up a series of deliberately multicultural images if a variety of the above terms were offered as markers?\n\nThere is of course a problem in asking students to apply a binary they may not agree exists. However, would allowing a \"combination\" marker defeat the purpose of the exercise? In a sense this is a problem that is inherent in any cultural binary, but I couldn't help wondering how this potentially useful application for Prism might work.\n\nIn the process of searching for a published example that might be applicable for one of the binaries stated above, I came to the conclusion that Prism might be as useful in a design context as an art-historical one. I looked through Print magazine's 2010 regional design annual and noticed that the editors' reasons for selecting the works in each regional collection were rarely specific or clearly observable. In many cases, three or four adjectives were deemed sufficient to loosely hold the collection together. These adjectives, when separated into Prism \"markers\" seemed to be excellent vehicles to analyze \"art speak,\" editing, and curatorship.\n\nI was then struck by the idea that we could also use Prism to ask a group of students which images in a group seem more \"Midwestern\" in style versus \"Far West,\" and \"Eastern\" by selecting images from across Print magazine's regional categories. For our exercise, we only had time too work with a single page, and I did not want to black out the studio locations on the captions, so I selected the Midwest section and asked the group to mark the works that exemplify what the editors of Print magazine called \"Narrative,\" \"Organization,\" and \"Viewer Interraction.\" The exercise went well, and without a lot of time to discuss the results here, I will wait until we have scans of our marked up texts and images.\n\nI was tempted to have a marker devoted to \"Art Speak\" but that might be a little too snarky for a marker.\n"},{"id":"2011-10-22-overlapping-anxieties","title":"overlapping anxieties","author":"alex-gil","date":"2011-10-22 11:45:39 -0400","categories":["Grad Student Research"],"url":"overlapping-anxieties","content":"This week has brought back the question of image/text to my thinking with a vengeance. First, as Lindsay points out in [her latest blogpost](http://www.scholarslab.org/praxis-program/the-transparent-crowd/), our group has been occupied with the question of overlapping markup; second, during our weekly meeting Annie asked \"When can we start building Prism,\" to which Wayne responded, and I paraphrase, \"You already have. Next, you need to choose what you will upload to Prism, and learn what it takes to do so\"; third, right after our meeting, Lev Manovich presented some of his work with [ImagePlot ](http://lab.softwarestudies.com/p/imageplot.html)and the question of \"what to do with a million images?\" In his talk he made what I think is a troubling distinction between image-as-continuous and text-as-categorical; fourth, at the [ADE 2011](http://documentaryediting.org/meeting/index.html) in Salt Lake City, where I write these lines, the old anxiety of crowds versus scholars crept in (as expected) on a session on the design of scholarly editions online.\n\nAs a result, I am torn between writing on the question \"what is a (digital) text?\" or the question of overlapping representation. Since jet-lag prevents me from making a choice, I will try to combine both instead.\n\n**What is a (digital) text?**\nA text is both matter and language. Leaving Braille and voice aside for now, let us concentrate on the visible side of textual matter. An image is textual in as much as a text can be an image. Something wicked happens to this binary when you move from analog to digital. We are under the illusion that a text becomes ASCII and an image a bitmap. We have naturalized these two formats (there are others) without taking into consideration the fact that a representation of ASCII on the screen is an image, or that the representation of a bitmap is textual. We're almost confusing the manuscript for the ink on the pen. I say almost because the way we use a digital text can be very different from the way we use analog text. Furthermore, there are different kinds of digital texts, each with different uses. A [close-enough HTML replica and a JPEG image of a text](http://www.elotroalex.com/workbench/dr_sample.html), for example, provide similar images and similar texts, but we use them in different ways... and we must make choices. Which brings me to,\n\n**the first Prism**\nIf indeed we want to have a working Prism model by the end of the Spring semester, we might have to separate bitmaps from ASCII. The problem of satisfactorily having an image/text combo that we can prism successfully seems insurmountable at this juncture and what I understand to be current web technologies. I propose we do one textualey side for ASCII (inside HTML) mapping and a imagey side with bitmaps (inside HTML). That would limit our uploads to HTML and CSS for now. Of course, the manipulation of those texts and images is a whole different matter. Nevertheless, if we know we are going to be working with simple HTML, we can start focusing our Ruby and JavaScript learning (self + slab) to work in this environment.\n\nIf you ask me, I would just start with the ASCII and postpone the bitmaps for later. If we do decide to do that, the question of overlapping colors becomes somewhat of a linear (and well-trodden) problem. I can't even begin to imagine how you could solve overlapping bitmap data. If all we have to do is 4! colors that would also help. I understand there is a question of intensities when we try the \"stacked\" visualization. I say chuck it. Let's start with the side-by-side visualization which to me is more interesting and what's more important, easier to set up. \n"},{"id":"2011-10-22-the-transparent-crowd","title":"the Transparent Crowd","author":"lindsay-o’connor","date":"2011-10-22 06:45:29 -0400","categories":["Grad Student Research"],"url":"the-transparent-crowd","content":"Ed, Annie, and Brooke have each mentioned our discussion of whether to allow users to mark parts of a text or picture in more than one color. In the grad lounge on Tuesday morning, Ed and I went back and forth about the implications for interpretation and for possible visualizations. He was concerned about preserving and representing the particular instances when a user marks something as more than one color while I suggested that this would not matter nearly as much as the aggregate markings in each color. He drew on the dry erase board. I raised questions about his ideas and then questioned my own questions and probably contradicted myself. We were each of course projecting assumptions about what kinds of texts we will have in Prism, what we might ask people to highlight, and how the resulting interpretation might be used. I don’t think we realized all that, though; in retrospect, I can see how conversations like this one put pressure on the ideas and images of Prism that I’ve been forming since August. The same goes for the transparency exercise (quite a metaphor!) we did in our full group meeting Tuesday afternoon. I hadn’t thought much about interpreting images (Ed’s example) or about highlighter categories created to emphasize overlapping interpretations (Alex’s example). And for the first time, I recognized that Prism could be used to interpret the highlighter categories as much as or even more than the text (“modernism” in Sarah’s example). This exercise made me feel a little foolish for having spent so much time on the issue of overlap without a fuller sense of what kinds of categories and texts might be possible, but it also contributed to my realization that exercises like this are what we need to remind us that we still don’t know what exactly Prism is, and maybe we never will. Fittingly, I think the Fight Club reference finally emerged last week (first rule of Prism…)\n\nTalking even hypothetically about multiple markings taught me something about crowdsourcing as well. Preserving the individual instances of multiple highlights does indeed seem like it could serve many intellectual interests, but it does not seem like “crowdsourced interpretation” as I understand it. If we privilege crowdsourcing as the method that Prism makes possible (or at least much, much easier), should we limit the questions researchers can ask Prism to those that a large number of non-individuated responses can answer? When Bethany piled all our transparencies on top of one another, no one’s individual transparency was easy to see. One person’s double-markings were indistinguishable from the markings of one transparency beneath another. Will Prism recreate this exercise on a larger scale, or will it allow for a kind of crowdsourcing that preserves individual interpretation somehow? In my mind, this question parallels the ideas I was struggling with recently about equality and collaboration versus the solitary, masterful scholar. I think I’m coming down on the side of the wisdom of the crowd instead of the nuance of the individual, so maybe we can call that progress.\n"},{"id":"2011-10-24-international-open-access-week-2011","title":"International Open Access Week 2011","author":"ronda-grizzle","date":"2011-10-24 08:28:36 -0400","categories":["Announcements","Digital Humanities"],"url":"international-open-access-week-2011","content":"[![Open Access Week poster](http://www.scholarslab.org/wp-content/uploads/2011/10/openaccessweek20118.5x11-164x300.png)](http://www.scholarslab.org/uncategorized/international-open-access-week-2011/attachment/openaccessweek20118-5x11/)\n\nOctober 24 - 30 is International Open Access Week 2011. Now in its fifth year, International Open Access Week, sponsored by [openaccessweek.org](http://openaccessweek.org/), is dedicated to educating scholars and university administrators about authors' rights, copyright, and the importance of creating and maintaining free access to scholarship.\n\n[Openaccessweek.org](http://openaccessweek.org/) encourages scholars to allow unfettered access to the products of their scholarship by publishing in open access journals and depositing their pre- and post-prints in open access institutional repositories. Additionally, openaccessweek.org encourages university and departmental administrators to update policies for tenure and promotion so that publication in peer-reviewed, digital, open access journals is given the same weight as publication in traditional print publications.\n\nIn support of open access here at UVa, the University has implemented an institutional repository called [Libra](http://libra.lib.virginia.edu/), maintained and administered by the Library, in which faculty members are encouraged to deposit their research. More information about Libra, open access at UVa, and instructions for depositing articles is available from the [Frequently Asked Questions page](http://libra.lib.virginia.edu/about/) on the Libra website.\n"},{"id":"2011-10-24-what-ive-learned-from-my-kindle-part-ii-and-other-thoughts-on-prism-and-markers","title":"What I've learned from my Kindle: part II, and other thoughts on Prism and markers","author":"sarah-storti","date":"2011-10-24 06:30:39 -0400","categories":["Grad Student Research"],"url":"what-ive-learned-from-my-kindle-part-ii-and-other-thoughts-on-prism-and-markers","content":"A while back I posted what I thought would be the first of two blog contributions about my (relatively) new Kindle—I figured that in the second of these I would address the way the “pages” of a Kindle book work and perhaps offer some thoughts on how I think Prism “pages” (or whatever we’re going to call them) should work. Since then, as will be made clear by the many posts immediately preceding this one, there has been a flurry of conversation about a cluster of topics all related to how the marked-up documents should appear (in theory) in Prism. Some of this began before our meeting last week, but doing the transparency exercise presented us with a whole new set of problems to consider. Though I think a few of my original reflections on the Kindle are still worth posting, I’d also like to (briefly) address some of the bigger questions I’ve had from the beginning of this project about what kind of marking our crowd-sourced crowd will be able to perform on Prismed documents.\n\n\n\n\nI left you, dear reader, with quite the cliffhanger in my[ last post](http://www.scholarslab.org/praxis-program/what-ive-learned-from-my-kindle-part-i/). I had downloaded a free Amazon Kindle edition of _Loss and Gain_, and was prepared, cup of tea in hand, to devour some John Henry Newman. What happened next?\n\n\n\n\nTo keep things simple, I’ll just say that I still prefer books, and perhaps even print-on-demand books, to the Kindle. In her comment on my first Kindle post, Brooke mentioned some of the same problems I encountered, the most frustrating being the difficulty not of making comments on the text but of using them later. The general discomfort I felt during our discussion group when I was not able to page through the physical book is, of course, something one must get used to if one wants to use e-texts, but I do second Brooke’s wish for page numbers at the very least (the book I was reading was organized by “locations,” whatever those are. I’m afraid I didn’t do much Kindle research before I began this adventure). But these objections mostly arise from my preference for the physical form of the codex. The codex is an excellent machine, not ever quite satisfactorily reproducible digital form. An e-reader is not, and cannot be, a codex.\n\n\n\n\nOf course, Prism is not concerned with the codex, or with written comments. The only way to “comment” on a document in Prism will be to highlight it: to color it. You aren’t _allowed_ to explain yourself. The project wouldn’t be half so exciting (or provocative) without this limitation.\n\n\n\n\n Strangely enough, there is something similar going on in Kindle e-texts.\n\n\n\n\n As I read _Loss and Gain_, I began noticing that some sections of the text appeared to be underlined, and at the beginning of such sections, a grey subscript note indicated the number of people who had previously highlighted that phrase or sentence or (in some cases) paragraph. I next discovered that through the menu I could easily access what Kindle calls the “popular highlights” in the novel all at once. There are no comments attached to these highlights: you merely know that some anonymous group of thirty-six people all found this or that sentence interesting enough to underline. Perhaps Newman has something to do with it, but I thought the vast majority of these obviously favorite segments would do well on a greeting card: “we must measure people by what they are, and not by what they are not;” “our strength in this world is, to be the subjects of the reason, and our liberty, to be captives of the truth;” “in the choice of friends, chance often does for us as much as the most careful selection could have effected;” etc. In any case, this did get me thinking a bit more about what I’d like to see from Prism.\n\n\n\n\nThe only way to “mark” a text on the Kindle is to put your cursor in front of a word and then “highlight” until you reach the end of your section-of-interest. I could be wrong, but I had the impression that many on the Praxis team envisioned Prism working in a similar (linear) fashion—that is, before we got out the markers last week. After all, how could one possibly “highlight” something like this and achieve a satisfactory result?\n\n\n\n\n[![](http://farm7.static.flickr.com/6220/6274984753_c1081a51b8_z.jpg)](http://www.flickr.com/photos/68990790@N02/6274984753/in/photostream)\n\n\n\n\nEd’s transparency exercise involved an even more complicated image than the one above. But even documents we consider to be more “text” based, or prose-y (such as pages from novels or poems), do not necessarily call for straightforward, linear highlighting. Consider the following excerpt from William Carlos Williams’s _Paterson_, which I've taken the liberty of marking up with my snipping tool:\n\n\n[![](http://farm7.static.flickr.com/6035/6275075215_96ce50cc1f_z.jpg)](http://www.flickr.com/photos/68990790@N02/6275075215/)\n\n[Alex’s post](http://www.scholarslab.org/praxis-program/overlapping-anxieties/) demonstrates his obviously more sophisticated understanding of the technical limitations we’re up against as we try to build a basic working version of Prism, but I figure that as long as Wayne keeps saying “right now, the sky’s the limit,” it can’t hurt to push things a little bit!\n"},{"id":"2011-10-26-facing-the-demon","title":"Facing the Demon","author":"brooke-lestock","date":"2011-10-26 17:46:52 -0400","categories":["Grad Student Research"],"url":"facing-the-demon","content":"I mentioned in [my last blog post](http://www.scholarslab.org/praxis-program/highlighting-limitations/) that many of our recent debates have been about how to productively limit Prism, but I don't think I realized how difficult that would be until I had the transparencies and highlighters in front of me last Tuesday. We started the exercise with Alex's selection from Plato's Allegory of the Cave and his parameters were something like, \"Mark for allegories of the self, politics, and economics.\" I completely blanked. I've read the Allegory of the Cave before, but I kept wanting to ask Alex to define the parameters more clearly or asking Bethany what we're allowed to do, for example: \"Can we highlight margins?\"; \"Can we highlight something twice?\"; Or in Ed's selection of images, \"Should we be looking at the captions under the images?\" All of these questions either went unanswered as part of the exercise or were promptly answered by a cryptic shrug from Bethany. That's when I fully understood how frighteningly open Prism is to interpretation, not only the crowdsourced interpretation of texts  uploaded to Prism, but our personal interpretation of what Prism is and should do. I think that's why I tend to lean towards limiting Prism as much as possible while we're still defining what it is. The sky _is_ the limit at this stage as we imagine what Prism can become (when it grows up), but we need one clearly defined use of Prism to begin to build it.\n\nMy technical knowledge is obviously limited, so I keep framing this issue of limitations in a context I can understand, that is, I think of developing Prism as writing a paper. Last week, I turned in a paper on _The Waste Land_ and, to be honest, I was unhappy with what I turned in. I decided to write on The Waste Land for a course primarily because it's so incredibly open to interpretation. Where I went horribly wrong was in allowing myself to get lost amidst all the interpretive possibilities of the poem and in the seemingly bottomless pit of criticism. I must've spent hours just book-hunting here in Alderman library and when I was finished taking notes, I had twenty single-spaced pages for a ten-page, double-spaced paper. I was paralyzed by possibility, and I was focusing on the shortest, most overlooked section of the poem (\"Death by Water\"). I spent so much time pursuing other critics' interpretations of the poem that I not only lost my own focus, but I couldn't bring myself to begin writing  and ended up producing something I didn't want to own.\n\nThat being said, I find myself needing to impose limits on Prism so it doesn't turn into some kind of shape-shifting, waste-landish, baby-faced bat monster (the _Bat_acritical demon, just in time for Halloween). Of course I trust that we wouldn't allow that to happen, but I just don't want to spend too much time parsing out Prism in all its possibilities, feeling the need to develop it to allow for anyone to use it any way. As I said in [my \"Transdisciplinary Ethics\" post](http://www.scholarslab.org/praxis-program/a-transdisciplinary-ethics/), the possibilities absolutely must be considered eventually, but we need to start somewhere specific or we'll get lost in the scope creep. We addressed this issue in the Scholars' Lab grad lounge today and it resurfaced at our meeting this afternoon, and I think we came to a consensus: start small and generic, then build on that. We're still figuring out exactly what that means, but it seemed to be that we were headed toward the initial goal of building Prism on the most basic level with one HTML text and one to four highlighters, and defining our audience as college instructors and students. Simple enough, right?\n"},{"id":"2011-11-01-building-prism-let-there-be-light","title":"Building Prism: Let There Be Light!","author":"annie-swafford","date":"2011-11-01 10:20:16 -0400","categories":["Grad Student Research"],"url":"building-prism-let-there-be-light","content":"On Friday, David, Eric, and I spent some time drafting the data model for Prism.  I had never done anything like this before, so it was quite the experience.  Since I’m new to this sort of work with databases, I had to get over my initial hurdle of thinking about the user interface in order to think about the individual components.  Once we figured out that the component parts were the tags, text, and the image and document classes, we had to figure out how they were related.  Alex did the first pass of constructing the data model in Rails, and today we met to polish it up.  We learned some interesting things about Rails in the process: for example, it seems as though Rails scaffolding sometimes changes the symbol names without warning, but now that we know, we can keep our eyes open for it.  We haven’t yet added anything to our database, but that might be the next step.\n\nWe’ve been cautioned against getting  too attached to our data model yet as we’ll probably have to change substantial portions of it, but at least we’ve finally started building!\n"},{"id":"2011-11-02-report-from-the-rails-trenches","title":"report from the rails trenches","author":"alex-gil","date":"2011-11-02 07:39:07 -0400","categories":["Grad Student Research"],"url":"report-from-the-rails-trenches","content":"As [Annie reports](http://www.scholarslab.org/praxis-program/building-prism-let-there-be-light/) we have begun hacking Prism. I am still surprised by the speed at which we are picking up the skills to build a web application. Our first model is a proof-of-concept, and as Eric Rochester pointed out in session yesterday, we will probably chuck the first model down the road. Part of re-wiring my brain is adapting to this trial and error model. The original model, as we have it now, already seems to me to be quite powerful. Here is what we have so far:\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/PrismDataModel-1-300x246.jpg)](http://www.scholarslab.org/praxis-program/report-from-the-rails-trenches/attachment/prismdatamodel-1/)\n\n\nI'm having difficulty imagining why we would need to discard it later, but I am starting to believe there is such a thing as Web Development Fate. In this strange deterministic universe you prepare for unknown unknowns as a matter of course.\n\nIn the meantime, I will be making these models as I learn Rails. The process goes something like this: Read a chapter in the [Rails Tutorial](http://ruby.railstutorial.org/ruby-on-rails-tutorial-book), implement something, read a section of [RailsGuides](http://guides.rubyonrails.org/), tweak the implementation, rinse-and-repeat. So far, I have not had a pull request accepted into our central GitHub repository, but if everything goes alright, I will have one by the end of this week. I'll keep you posted.\n"},{"id":"2011-11-04-keeping-it-real-clean","title":"Keeping it Real… Clean","author":"ed-triplett","date":"2011-11-04 19:00:59 -0400","categories":["Grad Student Research"],"url":"keeping-it-real-clean","content":"Now that we have moved forward with our Rails programming for Prism I have been thinking this week about how our group will collectively decide on an overall aesthetic for Prism. I have been scanning the web and taking screenshots of inspiring designs for a few days as Jeremy suggested, and in the process I noticed very few common principles among them. I collected sites that had elaborate splash pages, some with minimalist styles, a hand-drawn quality, photographic backgrounds, sharp vector graphics, muted palates, and bright primary colors. When I step back and look at what I have collected, it is difficult to come up with a common thread between the pages, much less any sense of my own personal taste. I also tend to browse the web completely differently when I flip the “aesthetic judgment” switch in my head. Content fades away and the pages become more abstract. I expect most of us approached our design research with a number of aesthetic filters in mind to help sift through the mass of possible sources of inspiration, with similar disparate results.\n\nI am thus brought back to an idea that Bethany initially brought up a few weeks ago. I think it would be helpful for us to focus on a few adjectives that we think should inspire Prism’s aesthetics. Design is rarely expressed successfully by using language alone, but we do have a common vocabulary that we can use to filter the myriad of design options for Prism. For instance, I am very interested in preserving a “tangible” quality for our texts in Prism. On the simplest level, using “tangible” as an instigator for design could mean that it is important to keep images of the original text that Prism users will highlight. Taken further, a “tangible” aesthetic might mean an emphasis on simulated three-dimensionality, a “grainy” quality to flat surfaces, or even a reaction against the hard lines of vector graphics.\n\nOne medium that I believe possesses an inherently “tangible” aesthetic is letterpress. The presence of the large metal printing press seems to hover behind the image of the completed print. This is especially apparent given the current trend to deepen the impression of the plate into the paper.\n\n![Ctrl-Z](http://farm7.static.flickr.com/6057/6313023717_2c54c4ab33.jpg)\n\n[http://pinterest.com/](http://pinterest.com/)\n\nI am eager to see how prism might embrace the sharp, “intangible” quality of the vectorized highlighters while also emphasizing a mechanized, physical style in the application’s frame, logos and scanned texts.\n\nAnother adjective that is often used as a superlative for good design is “clean.” By itself, the adjective is not particularly useful given that it can connote a utilitarian, functional, uncluttered, sharply-defined, clear, high-contrast or even deliberate design. Nonetheless, “clean” is still one of the most common words used to describe web design that “works” and communicates clearly. It is easy to set up \"clean\" design as an antithesis to more physical, tangible, or \"real\" imagery, but I hope we are able to avoid this kind of binary distinction. Keeping it real and keeping it clean are not opposing ideas.\n"},{"id":"2011-11-08-designed-to-touch","title":"Designed to touch","author":"brooke-lestock","date":"2011-11-07 19:43:03 -0500","categories":["Grad Student Research"],"url":"designed-to-touch","content":"I enthusiastically agree with [Ed's use of the word \"tangible\"](http://www.scholarslab.org/praxis-program/keeping-it-real%E2%80%A6-clean/) to describe Prism. I couldn't have chosen a better word myself. To get the clearest possible definition of the word, I went old school and looked it up in the Merriam-Webster online dictionary where it's defined as \"perceptible by touch.\" No surprise there, but the phrasing of the definition did remind me of the importance of touch to perception. From the origins of the 'Patacritical Demon in those SpecLab transparency exercises, Prism has been all about touch: touching the transparency to the text, touching the highlighter in your hand, touching the highlighter to the transparency, pressing the transparencies together onto the text and deriving some kind of information from the touching or non-touching of highlights. The physical acts of marking up a text and then seeing many markups laid on top of each other are integral to the user's perception and interpretation of the text in question. Prism's ultimate goal of \"aesthetic provocation\" requires the user's physical interaction with the text, so Prism can't be successful (can't \"touch\" its users) if it loses its tangibility.\n\nLike Ed, I'm not exactly sure what \"tangible\" as a design aesthetic looks like, but as we've been asked to look at CSS design galleries for Prism inspiration, I find myself bookmarking the sites that play with the boundaries of the page and implement texture in an interesting way. I also agree with Ed's emphasis on \"keeping it real...clean,\" and I would go further to say that \"clean\" for me means maintaining the primacy of the text on the page, whether it's a page image or a text transcribed in HTML. While I'm on the subject, Sarah and I briefly chatted in the grad lounge today about tangibility in another sense. There has been a fair amount of discussion as to whether we should use page images or transcribed text and the general consensus we've come to in the last few meetings is that we'll use both. I think having both options opens up some interesting possibilities for interpretation (allowing for an examination of how users mark the same text in each form, for example), but it seems absolutely crucial to the tangibility of Prism to have page images, however complicated that may turn out to be.\n\nIn my interview for the Praxis Program this summer, I hopped up on my soapbox and boldly claimed, \"The book is dead.\" Sheath your swords, bibliographers (looking at you, Sarah and Alex), I didn't quite mean what I said. I meant that paper books will eventually become artifacts, which isn't a far-fetched claim to make, but the problem with e-readers, in my opinion, is that they aren't enough like physical texts. They aren't tangible in the way that readers are comfortable with. Prism obviously isn't an e-reader, but my point is that translating the tangibility and physicality of the transparency exercise is a design task that is absolutely crucial to its success. We want Prism to make interpretive possibilities \"perceptible by touch.\"\n"},{"id":"2011-11-09-building-prism-the-darker-side-of-the-enlightenment-spectrum","title":"Building Prism: The Darker Side of the Enlightenment Spectrum","author":"annie-swafford","date":"2011-11-09 15:16:46 -0500","categories":["Grad Student Research"],"url":"building-prism-the-darker-side-of-the-enlightenment-spectrum","content":"It’s finally happened—we’ve had our first setback. We knew from the start that we would probably have to scratch much of our data model, but we didn’t entirely understand why. Now we do. Apparently our model doesn’t allow for saving a highlighting session; every new visitor’s data would replace the previous entries. Also, in order to enable any sort of interaction with the text (uploading, highlighting, etc.), we need to create a user model. These are only some of the reasons that our original data model needs to be trashed.\n\nI’ve always known that trial and error was an important part of coding, and I’ve had ample experience with this from my own digital project. Even so, I’m still surprised by how many mistakes we made. It’s amazing what you can overlook when you’re just starting out on a project and learning what data models are. It seems as though part of the problem was that we began to code our data model before we had done much wireframing, and therefore our model doesn’t accurately reflect the project as it is being designed. We’re planning to wait on rebuilding the model until after the wireframing portion is complete. Although it’s frustrating to start again, I’m hopeful that we’ll be able to produce our second version more quickly having learned more about the process.\n"},{"id":"2011-11-09-the-dirt-on-clean","title":"The Dirt on \"Clean\"","author":"lindsay-o’connor","date":"2011-11-09 15:15:33 -0500","categories":["Grad Student Research"],"url":"the-dirt-on-clean","content":"Last week, when I should have been finishing up a conference paper I gave on Sunday, I instead kept messing with the webpage that Jeremy is teaching us how to design. Coding left me confused and bewildered, but now I realize that it also pretty much left me cold. I did once announce that it was satisfying, but I think only in a “I’m glad I did that” kind of way. In contrast, the _process _of designing a webpage is fun. I like thinking about design as I go about my life both on the web and otherwise. I like making my apartment look nice in ways I can afford and maintain. I like creating and cooking meals that balance taste, texture, and color. I like bright and bold and playful fashion statements that don’t entirely forsake comfort. These are some of the things I do while procrastinating, while avoiding my academic work because I’m confused or bewildered, so I think designing for Prism will be similarly appealing and therapeutic for me.\n\nAs for design values that we should adopt, “clean” seems so common a metaphor that it may not actually carry much weight as a guiding principle. Does anyone aim to make things look “dirty” or “messy” when designing something for many different kinds of users? I don’t find it helpful to take anything as a goal or guideline if its opposite is obviously undesirable. But I also don’t know that we need to abandon “clean” entirely—[Ed](http://www.scholarslab.org/praxis-program/keeping-it-real%e2%80%a6-clean/) and [Brooke](http://www.scholarslab.org/praxis-program/designed-to-touch/) have explained a bit about what “clean” means to them, and I think that’s what we should pin down. Just now Brooke, Jeremy, Wayne, and I briefly discussed how different people use “clean” in design terms. It could mean ample white space or big buffers and borders or not too many fonts or all the important stuff near the top of the page. This is the kind of stuff I hope we can start working out pretty soon, and to get that going, I propose “visually logical” as a guiding principle that is (hopefully) at least slightly more specific than “clean.”\n"},{"id":"2011-11-09-the-hunchback-of-notre-prism","title":"the hunchback of notre-prism","author":"alex-gil","date":"2011-11-09 15:49:14 -0500","categories":["Grad Student Research"],"url":"the-hunchback-of-notre-prism","content":"I'm [with Annie](http://www.scholarslab.org/praxis-program/building-prism-the-darker-side-of-the-enlightenment-spectrum/). Trial and error has been part of my dissertation for a while, so there is no shattered heart at the prospect of chucking the first model. I pointed out on [my previous post](http://www.scholarslab.org/praxis-program/report-from-the-rails-trenches/) that one thing I'm getting tons of praxis on these days is working with the \"unknown unknowns\" hovering in the air. Throwing the first fork into the recycle bin doesn't mean it has lost its use, though. Knowing that the first model was bound to be dropped from the beginning anyway, I've used it to teach myself Rails.\n\nIf the past couple of weeks have been marked by a lack of demonstrable results on the front of the stage, I vouch for the intense hustle and bustle on the backstage. I have defeated [Rails for Zombies](http://railsforzombies.org/) already, have seen many a YouTube video on Rails while doing chores around the house, and I'm half way through the highly recommended [Ruby on Rails Tutorial](http://ruby.railstutorial.org/chapters/rails-flavored-ruby#sec:method_definitions). Even though I've mostly worked with the examples in those tutorials, I've kept a dirty fork of prism close at hand to prod and tweak and hit with a sledgehammer.\n\nMost tutorials give you very doable exercises in a logical order, which makes it hard to screw them up, but also to remember. Having something to destroy and deform, though, can be the best learning tool sometimes. Of course, the world will never know what I've done to prism [macabre music plays in the background], but IT, the crooked, hunchbacked prism lurking in the shadows will probably stay there while the Dorian Gray prism goes on to live a youthful life.\n\nWhile I praise and practice open-access and the public life of the mind in general, I write these few words in honour of the pedagogical freedom of the backstage.\n"},{"id":"2011-11-12-cross-posted-it-starts-on-day-one","title":"cross-posted: It Starts on Day One","author":"bethany-nowviskie","date":"2011-11-12 10:13:48 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"cross-posted-it-starts-on-day-one","content":"(This is a brief essay related to the Scholars' Lab's Praxis Program, which has been cross-posted from [nowviskie.org](http://nowviskie.org/2011/it-starts-on-day-one/).)\n\nHere's a modest proposal for reforming higher education in the humanities and creating a generation of knowledge workers prepared not only to teach, research, and communicate in 21st-century modes, but to govern 21st-century institutions.\n\nFirst, kill all the grad-level methods courses.\n\nKill them, that is, to clear room for something more highly evolved -- or simply more fruitful -- to take their place.  Think: asteroids clobbering dinosaurs.  Choking weeds ripped from vegetable gardens.  The fuzzy little nothings and spindly cultivars in this scenario, squinting cautious eyes or uncurling new leaves into the light, are:\n\n\n\n\n\n  * those research methodologies and corpora (often but not exclusively gathered under the banner of the \"digital humanities\") that address hitherto unanswerable questions about history, the arts, and the human condition; and\n\n\n\n  * the new-model scholarly communications platforms we can already recognize as promising replacements to our slow and moribund systems for credentialing, publishing, and archiving humanities scholarship and the cultural record on which it is based.\n\n\n\nWhat do these critters need to grow up? The same thing our colleges and universities so desperately need: a generation of faculty and [alternative-academic](http://mediacommons.futureofthebook.org/alt-ac/) scholar-practitioners who have been trained to work in interdisciplinary contexts and who can not only _take advantage of_ computational approaches to their own research, but who have been instilled with enough of a can-do, maker's ethos that they feel _empowered to build and re-build_ the systems in which they and future students will operate.\n\n<!-- more -->Although a small number of extra-curricular experiments (like the [Praxis Program](http://praxis.scholarslab.org)) and curricular interventions (like Michigan State's [Cultural Heritage Informatics Fieldschool](http://chi.matrix.msu.edu/)) offer new and concrete models for emulation, there's little hope for wholesale, bottom-up, grass-roots reform of methodological training in the humanities. With vanishingly few exceptions, required first-year graduate methods courses are dinosaurs and weeds. Some are an abbreviated introduction to journals databases and the mysteries of inter-library loan. Others have little to do with research and production \"methodologies\" at all, and are instead a crash course in the jargon and en-vogue theories of a given discipline. The intra-institutional level of coordination in developing and teaching these courses, even among closely-allied humanities departments, hovers around zero.  Within single departments, they are catch-as-catch-can, shaped almost wholly by the individual faculty who teach them (often as they themselves were taught a generation or two before) and sometimes vacillating wildly in content from year to year as instructors rotate to make more equitable the \"burden\" of a course generally construed as service. Is it any wonder they're a mess?\n\nAnd is it any wonder that we continue to produce graduate students unready to engage with new technologies and opportunities for interdisciplinary and computational work -- baffled and frustrated at the conditions of the academic job market and its underpinnings in a dying scholarly publishing industry -- and under-prepared for or uneducated about hybrid and non-traditional academic careers?\n\nHere comes the asteroid we require. (And in offering a trajectory for it, I want to acknowledge my debt to conversations with participants in the [Scholarly Communication Institutes](http://uvasci.org) held at UVa Library, with [Scholars' Lab](http://lib.virginia.edu/scholarslab/) faculty and staff, and with our Graduate Fellows in Digital Humanities and Praxis Program students.)\n\nFunding agencies, both private and public -- like Mellon, Sloan, and (in the US) the NEH and NSF -- should be approached by a respected humanities organization that itself possesses a mandate and a track record of inter-institutional and interdisciplinary collaboration.  I think here of groups like [CHCI](http://chcinetwork.org), the international Consortium of Humanities Centers and Institutes -- especially in partnership with [centerNet](http://digitalhumanities.org/centernet), its digital counterpart -- or the American Council of Learned Societies ([ACLS](http://acls.org)). The organization should offer, with sufficient funding, to serve as a broker for a prestigious and competitive RFP (request for proposals). The RFP would would be issued to universities with core strengths in the humanities, adequate support for digital scholarship, and a desire -- able to be expressed at the institutional level -- to create broad-scale curricular change in the way graduate students are inducted into and trained for 21st-century humanities.  Probably no more than 3 or 4 schools would win funding, which would be contingent on this:\n\n\n\n\n\n  * the planned, top-down, apocalyptic wiping-out -- one academic year from delivery of the award -- of existing graduate methods courses in (say) four to six core humanities departments;\n\n\n  * the formation of a small but representative, collaborative, and interdisciplinary team charged with creating the year-long _common_ methods course that will replace them;\n\n\n  * a commitment by participating academic departments, in the light of the new common course, to re-think the training that they consider to be _absolutely unique_ to their disciplines and to offer an avenue (1-credit classes? discussion groups? new approaches to departmental teaching or to comps and orals requirements?) for students to acquire it; and\n\n\n  * a rigorous program proposed for assessing and publicizing the successes, failures, and overall impact of the experiment, so that lessons may be learned across institutions and new programs inspired.\n\n\n\nThe common methods course would be required of all incoming graduate students in participating departments.  Grant funding could could support staffing of curriculum design and assessment phases, offer incentives (including course release or professional development) for faculty participation, or pay for teaching assistants. The program would be designed and team-taught by its planning group, which should include faculty from relevant departments, representatives of the offices of deans and provosts, and -- importantly -- local #alt-ac professionals, trained in the humanities, but working as scholar-practitioners in R&D; or academic support roles in libraries, labs, publishing units, and centers. It should also engage faculty from departments like CS and Architecture, whose students may not participate directly in the program, but who would have important lessons to share about research methods and collaborative practices.\n\nAs its primary focus, the course must cover current humanities research skills, corpora, and trends -- both digital and archival or material. But it should also address issues like: intellectual property and open access; the intersection of scholarship with the public humanities; publishing, preservation, and scholarly communication; funding and material support for research and teaching; interdisciplinary collaboration; matters of credentialing and assessment (peer review, tenure and promotion), faculty self-governance; and the under-interrogated policies that cover and shape the humanities in the modern college and university.\n\nThis is a tall order -- but we can no longer afford to produce humanities PhDs who have only a foggy notion of how universities work, and how they are impacted by external technological and social forces.  The first time a humanities scholar encounters a budget spreadsheet or performs a calculation should not be when he or she becomes department chair. And no new member of the professoriate should feel utterly out of depth in decision-making processes that impact the teaching, research, and service mission of his or her institution.  Likewise, the health of the humanities depends on our production of graduate students who do not simply replicate the faculty of yesteryear, but who are prepared to take uncharted paths in and around the academy, working together to fashion new systems and adapt the ones we treasure to altered conditions.\n\nGraduate training in the humanities starts anew every year, on Day One. How, at a moment when we feel so much is at stake, can we allow it to remain so purposeless?\n"},{"id":"2011-11-15-the-mappy-goodness-that-is-gis-day-in-the-scholars-lab","title":"The Mappy Goodness that is GIS Day in the Scholars' Lab","author":"kelly-johnston","date":"2011-11-15 07:46:16 -0500","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Grad Student Research","Visualization and Data Mining"],"url":"the-mappy-goodness-that-is-gis-day-in-the-scholars-lab","content":"Every November on the Wednesday of Geography Awareness Week the world celebrates GIS Day.  On that day in Charlottesville the geospatial community gathers in the Scholars' Lab for mappy goodness.\n\nAnd cake.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/cake-timeline.png)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/cake-timeline/)\n\nIn 2010 we threw open the Scholars' Lab doors for folks to present geospatial lightning talks.  We were impressed by the breadth of GIS work ongoing across our community.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/2010-Talks-768x1024.png)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/2010-talks/)\n\nAnd lots of people came to hear these mappy stories.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/gpspresentation1.jpg)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/gpspresentation-2/)\n\nAnd for cake.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/2010-cake.png)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/2010-cake/)\n\nFace it, you love maps.  We invite you to join our band of Virginia Mapheads as we celebrate World GIS day 2011 with a lightning-fast show of the world's coolest geowork in the Scholars' Lab on Wednesday, November 16 at 1:30pm.\n\nSadly, I know many of you don’t have the pleasure of working all day every day with maps and geodata, so treat yourself to a once-a-year map fix.  You know you deserve it!  And bring a friend.\n\nAgain, we have a compelling lightning talk lineup for 2011.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/2011-The-Lightning2-793x1024.png)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/2011-the-lightning-3/)\n\nYour reward?  Two slices of our soon to be legendary 2011 GIS Day geocake to be revealed on GIS day!\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/2011-GIS-day-info.png)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/2011-gis-day-info/)\n\n\n# 2011 GIS Day Update\n\n\nOver 70 folks enjoyed our 2011 GIS Day celebration in the Scholars' Lab.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/20111116-GIS-Day-audience-2-1024x518.jpg)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/20111116-gis-day-audience-2/)\n\nSpeakers ranged from wily GIS veterans to those who'd recently started using geospatial tools.   Check the speaker list above to see the wide range of topics.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/20111116-GIS-Day-057-1024x768.jpg)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/20111116-gis-day-057/)\n\nThe Charlottesville CBS station sent their GIS Day team to cover the event interviewing Eric Johnson, Scholars' Lab Head of Outreach and Consulting.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/20111116-GIS-Day-040-1024x677.jpg)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/20111116-gis-day-040/)\n\nAnd we feasted on the already legendary Virginia-shaped geocake decorated with flags marking unusual place names.  All this was followed by delicious hot mulled cider.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/20111116-GIS-Day-035-1024x768.jpg)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/20111116-gis-day-035/)\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/20111116-GIS-Day-033-1024x768.jpg)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/20111116-gis-day-033/)\n\nThanks to everyone who played a part in making GIS Day 2011 a mappy success!\n\n\n# 2012 GIS Day Update\n\n\nThe Scholars' Lab celebrated GIS Day 2012 with lightning talks on spatial topics, one-of-a-kind GIS Day buttons, cartographic cupcakes, and hot mulled cider.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/20121114-UVA-GIS-Day-crowd-cropped-1024x345.jpg)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/20121114-uva-gis-day-crowd-cropped/)\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/GISDay2012SpeakerList-792x1024.jpg)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/gisday2012speakerlist/)\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/IMG_1291-1024x895.jpg)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/img_1291/)\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/20121114-UVA-GIS-Day-cake-2-1024x768.jpg)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/20121114-uva-gis-day-cake-2/)\n\nWe highlighted our recent work with Do It Yourself Aerial Photography using balloons, kites, and copters.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/20121114-UVA-GIS-Day-balloon-kite-copter-1024x790.jpg)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/20121114-uva-gis-day-balloon-kite-copter/)\n\nSome folks didn't want GIS Day to end.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/GIS-Week-Button-1024x983.jpg)](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/attachment/gis-week-button/)\n\nThanks to all the friends of the Scholars' Lab who made GIS Day 2012 a mappy success!\n\nAnd thanks to Ronda Grizzle of the Scholars' Lab for producing this 2012 GIS Day lightning talk video.\n\nhttp://vimeo.com/56020168\n\n![](data:image/png;base64,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)\n"},{"id":"2011-11-16-the-force-of-ux-design","title":"The Force of UX Design","author":"brooke-lestock","date":"2011-11-16 18:30:26 -0500","categories":["Announcements","Grad Student Research"],"url":"the-force-of-ux-design","content":"I've had design on my mind for a couple of weeks now, mainly because thinking about how Prism is going to look is so much more pleasant to me than writing code (may Annie and Alex be praised for taking the lead on that). It's been easy for me to point to websites and say \"I like this,\" or \"I don't like that,\" but Jeremy wisely outlawed the phrase \"I like\" from our design meetings, forcing us to exercise the same critical muscles we exercise in graduate study; I would get pelted with rotten vegetables and shunned from academia for life if I turned in a paper that said (or even said in a discussion), \"I like Virginia Woolf because her books are good.\" Now we're all thinking critically about why certain sites appeal to us and why other sites make us shudder. Right on cue, the Scholars' Lab hosted a presentation last Thursday by [Joe Gilbert](http://www.scholarslab.org/contributors/jfg9x/) on the Elements of User Experience design, in which I frantically jotted down every word he said because it directly addressed the design questions we're learning to ask in Praxis. Joe established four elements that comprise UX design: user requirements, design and architecture, usability, and content strategy. We've already established our requirements for Prism, on both the user and organizational levels, and after many weeks I think we all finally agree on what we want it to do for us and the user. Now to design it.\n\nAfter last week's design meeting, we all agreed that we would have some sketches of the main pages of Prism to share, but once I turned to a fresh sheet on paper in my notebook, I couldn't sketch more than a monitor-shaped rectangle with the word \"Prism\" in the top left corner. So I flipped to my notes from Joe's presentation and remembered a phrase he used when he was stressing the importance of visual hierarchy on the page in presenting the user with \"obvious calls-to-action.\" Jeremy made a similar point in last week's meeting when he said that a site should follow a path in its interaction with the user: \"Entice --> Inform --> Invoke.\" It's all about narrative: get the users' attention, tell them how to use it, then (and I keep coming back to Bethany's phrase) \"aeshetically provoke\" them.\n\nWe had a really productive conversation in the grad lounge today in which we proposed and discarded some ideas (much like [Annie](http://www.scholarslab.org/praxis-program/building-prism-the-darker-side-of-the-enlightenment-spectrum/) and [Alex](http://www.scholarslab.org/praxis-program/the-hunchback-of-notre-prism/), I am learning how to kill my [design] darlings), and we made great headway because we kept reminding ourselves  that we need to narrate Prism to the user in order to meet our requirements. We spent the majority of time talking about the first part of the Prism narrative, how to entice our user in the home page and inform them how to use it. Do we put the sandbox and/or demo video on the home page, emphasizing Prism's interactivity by placing the user right in the midst of it? Or should we have a simple home page with large links to the sandbox, text editor, and visualizations pages? What about our navigation bar (the chapter titles of our narrative, to keep it literary) -  how do we organize a nav bar to offer \"obvious calls-to-action\"? We raised more questions than we resolved, as usual, but that's certainly better than settling on design we \"like\" without asking how and why it contributes to user experience.  In any event, we're getting somewhere, and I have to thank design Jedis Joe Gilbert and Jeremy Boggs for teaching us how to use the UX force.\n"},{"id":"2011-11-26-giving-thanks","title":"giving thanks","author":"alex-gil","date":"2011-11-26 06:39:22 -0500","categories":["Grad Student Research"],"url":"giving-thanks","content":"I love lists. Here's one I'm thankful for: The things I've learned since I started Prism (including those I learned indirectly):\n\n\n\n\t\n  * How to negotiate a common charter.\n\n\t\n  * How to navigate, edit in and configure Vim.\n\n\t\n  * How to write JavaScript to make a reading interface on the web.\n\n\t\n  * How to read Ruby.\n\n\t\n  * The key principles of programming: a) Assignments; b) Iteration; c) I/O; d) Conditionals; e) Boolean Operations; and d) Data Structures.\n\n\t\n  * The difference between an object and a method.\n\n\t\n  * How to feed hungry functions.\n\n\t\n  * The three parts of a Rails app: a) Views, b) Controller and c) Model.\n\n\t\n  * To keep a portfolio of favorite designs.\n\n        \n  * When a constrained vocabulary meets an unbound vocabulary, magic happens.\n\n\t\n  * How to test before you code... and watch Henry at the same time.\n\n\t\n  * The licenses for software are different than the licenses for other media (I know, I'm slow).\n\n\t\n  * How to use the terminal for almost everything (sorry @sramsay, still like my eye-candy).\n\n\t\n  * How to Git with my eyes closed.\n\n\t\n  * The basics of GIS (thanks, Kelly!)\n\n\t\n  * How to run an Omeka-driven classroom.\n\n\t\n  * How to run a LAMP test server on a virtual machine.\n\n        \n  * How it all ties together.\n\n        \n  * How greedy I am for more.\n\n\n  \n\n\n\nBonus: What I will learn next week:\n\n\n\n\n\n\n\n\n\t\n  * How to interact with an API (In my case, Zotero and Juxta will be my first dates)\n\n\n\n"},{"id":"2011-11-26-whiteboard-wireframing","title":"Whiteboard Wireframing","author":"ed-triplett","date":"2011-11-26 06:00:33 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"whiteboard-wireframing","content":"Over the past few weeks, I feel our program is moving toward my, and others’ comfort zone. We are beginning to wireframe Prism on the whiteboard, so we can each come back after the Thanksgiving break with a few images of each of our main pages. There are new challenges that come from the wireframing process, but unlike some of our other tasks, the group seems more confident when articulating their thoughts about design. The whiteboard work last week got us to make a few serious choices about what the home page will contain, and how the site will be navigated. Without getting too specific, we have come to some consensus that we’d like to keep a constantly visible set of tabs at the top of each page to organize the site. This is not to say that other design schemas will not be offered in the future, but it does help us decide which elements we want visible on the home page.\nOur Tuesday group leaned toward offering a stylized image of a marked up text on the home page to act as a link to a “sandbox” of Prism. Sarah and I both toyed with the idea of a home page with three objects centered on the page to act as links to either the “about” page, the “sandbox” or the “text markup” page. For me, this kind of a home page was inspired by a site like [https://github.com/](https://github.com/) (after login) with its four immediate navigation options executed through elaborate icons. The tabbed form is strong in terms of being easily navigable, yet tabs themselves can only communicate so much before becoming messy. In addition, once a user has learned how to use the tool and wants to return to mark up texts, it seems strange to immediately click a small tab – that is exactly the same size as other tabs for the “about” page or the sandbox – to navigate away from the home page. I am going to advocate using both large icons to attract attention to the main pages, and tabs to navigate between them once the user has selected a option in the center of the page.\nAs Brooke remarked in her post, it would be nice to create a narrative for how to use the site, beginning with a textual “how-to” and ending with the user marking up our texts/images… or down the road, their own. I agree that it would be helpful to visually direct the user through Prism. I am going to try to mock up some images for each of the icons before our next meeting, but right now I am not sure what form they will take. My only instinct is to combine some abstract element of the Prism logo with a representation of a text/image, a “how-to” filmstrip or some other element in the “use” narrative… although that might need some more thought. I also took a lot away from Joe Gilbert's talk at the Scholar's Lab, and I would like to see us all come up with visual examples of the Prism homepage that will call the user to action, yet offer a familiar method of navigation.\n"},{"id":"2011-11-29-testing-and-more-data-modelling","title":"Testing and More Data Modelling","author":"annie-swafford","date":"2011-11-29 08:01:30 -0500","categories":["Grad Student Research"],"url":"testing-and-more-data-modelling","content":"Now that we've spent a few weeks focusing on wireframing, we're back to working on the data model.  We haven't actually created the models in rails yet, but we have started redesigning it, and it makes much more sense the second time around.  Currently, we're adding a user model, renaming all the other models, combining the \"TextDoc\" and \"ImgDoc\" models into a new model called \"PrismObject,\" and adding a \"Page\" model.  We'll post our next version here when it's ready.\n\nWe've also been learning about the wonders of testing.  Of course, we've all had our own (frustrating) experiences with debugging, but it looks like Test-Driven-Development will help us minimize our errors, or at least catch them earlier.  We're now supposed to write the test before we write the code, and the test will tell us whether our code passed or failed.  I think it will take some practice to actually write the tests correctly, but I think it will pay off once I get used to it.\n"},{"id":"2011-12-01-projection-lessons-in-maps","title":"Projection Lessons in Maps","author":"chris-gist","date":"2011-12-01 10:41:26 -0500","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"projection-lessons-in-maps","content":"For many years, I have used the following map in my presentations.\n\n\n[![](http://www.fulltable.com/vts/f/ffa/01.jpg)](http://www.fulltable.com/vts/f/ffa/01.jpg)\n\n\n\n\nThis map is a great example of proportional symbology and is of an interesting subject, especially when juxtaposed with modern oil trading.  Of course, the cartographic style is great too.  I hadn't much thought of the cartographer or why the map was created until recently.  When I took a closer look, I found the cartographer's name, [Richard Edes Harrison (1901-1994)](http://www.legends.mapsofworld.com/modern/richard-edes-harrison.html), and that this was one in a series of maps he did for _Fortune Magazine_ mainly in the World War II era.  He was a wonderful cartographer and his style intrigues me.  There is a large amount of information available about Harrison and his maps.\n\n\n\n\nA website showing some of Harrison's work: [http://www.fulltable.com/vts/f/fortune/reh/mn.htm](http://www.fulltable.com/vts/f/fortune/reh/mn.htm). (Almost all maps in this post are linked from this site).\n\n\n\n\nAn article in Imago Mundi about Harrison's work: [http://www.jstor.org/stable/1151400](http://www.jstor.org/stable/1151400) (JSTOR access required).\n\n\n\n\nA book showing much of Harrison's work for Fortune:_[ Look at the World, The Fortune Atlas for World Strategy](http://books.google.com/books?id=Ftk9AAAAYAAJ&q=Look+at+the+World,+The+Fortune+Atlas+for+World+Strategy.&dq=Look+at+the+World,+The+Fortune+Atlas+for+World+Strategy.&hl=en&ei=XqfXTuqlGObf0QHCqsjyDQ&sa=X&oi=book_result&ct=result&resnum=1&ved=0CC8Q6AEwAA)._\n\n\n\n\nHarrison employed many interesting techniques.\n\n\n\n\n\n#### Cartograms\n\n\n[More information on cartograms](http://en.wikipedia.org/wiki/Cartogram)\n\n\n[![](http://www.fulltable.com/vts/f/fortune/aac/0d5.jpg)](http://www.fulltable.com/vts/f/fortune/aac/0d5.jpg)\n\n\n\n\n[![](http://www.fulltable.com/vts/f/fortune/ax/SH176.jpg)](http://www.fulltable.com/vts/f/fortune/ax/SH176.jpg)\n\n\n\n\n\n#### Interesting Extent Overviews\n\n\n\n\n[![](http://www.fulltable.com/vts/f/fortune/az/SH227.jpg)](http://www.fulltable.com/vts/f/fortune/az/SH227.jpg)\n\n\n\n\n[![](http://www.fulltable.com/vts/f/fortune/az/SH259.jpg)](http://www.fulltable.com/vts/f/fortune/az/SH259.jpg)\n\n\n\n\n\n#### Series of Small Multiples\n\n\nA [Tufte](http://www.edwardtufte.com/tufte/) favorite.  [More information on small multiples.](http://en.wikipedia.org/wiki/Small_multiple)\n\n\n[![](http://www.fulltable.com/vts/f/fortune/SH751.jpg)](http://www.fulltable.com/vts/f/fortune/SH751.jpg)\n\n\n\n\n[![](http://www.fulltable.com/vts/f/fortune/xa/47.jpg)](http://www.fulltable.com/vts/f/fortune/xa/47.jpg)\n\n\n\n\n\n#### Comprehensive Data Visualizations\n\n\n\n\n[![](http://www.fulltable.com/vts/f/fortune/xa/49.jpg)](http://www.fulltable.com/vts/f/fortune/xa/49.jpg)\n\n\n\n\n[![](http://www.fulltable.com/vts/f/fortune/reh/SH729.jpg)](http://www.fulltable.com/vts/f/fortune/reh/SH729.jpg)\n\n\n\n\n\n#### Interesting Projections\n\n\n[More information on projections](http://egsc.usgs.gov/isb/pubs/MapProjections/projections.html)\n\n\n[![](http://www.fulltable.com/vts/f/ffa/05.jpg)](http://www.fulltable.com/vts/f/ffa/05.jpg)\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/11/image7a-1024x629.jpg)](http://www.scholarslab.org/wp-content/uploads/2011/11/image7a-1024x629.jpg)\n[![](http://www.fulltable.com/vts/f/fortune/reh/SH512.jpg)](http://www.fulltable.com/vts/f/fortune/reh/SH512.jpg)\n\n\n\n\nThe last two maps have very interesting projection lessons embedded in them.\n\n\n\n\n\n#### Embedded Projection Descriptions\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/12/gnomonic-1024x549.jpg)](http://www.scholarslab.org/wp-content/uploads/2011/12/gnomonic-1024x549.jpg)\n\n\n\n\nHow nice is that?  Having a clear, concise description of projection and scale within the map is a great benefit to the reader.\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/12/longitude-1024x331.jpg)](http://www.scholarslab.org/wp-content/uploads/2011/12/longitude-1024x331.jpg)\n\n\n\n\nIn this map, Harrison used comparative scales to show changes in distance across map due to projection distortion.\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/12/centerfuge.jpg)](http://www.scholarslab.org/wp-content/uploads/2011/12/centerfuge.jpg)\n\n\n\n\nThis one is the best.  A clear description with a visual that anyone can understand.\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/12/simplifiedCenterfuge.jpg)](http://www.scholarslab.org/wp-content/uploads/2011/12/simplifiedCenterfuge.jpg)\n\n\n\n\nHere is simplified version of the \"world centrifuged\" in a map called [On Assignment](http://www.fulltable.com/vts/f/ffa/02.jpg).\n\n\n\n\nA few weeks ago, I came across the following map.\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/12/r001bbp6-1024x696.jpg)](http://www.scholarslab.org/wp-content/uploads/2011/12/r001bbp6-1024x696.jpg)\n\n\n\n\nWhat a beautiful projection!  It is know as the butterfly or Cahill projection (Named after the inventor.  Cahill even got a [patent](http://www.genekeyes.com/Cahill-globe-patent/Cahill-globe-patent.html) for a rubber ball that does the same thing.)  And as the embedded projection description describes, this is a great map for comparing circumnavigation routes because it does not distort distance, area or direction.  Not only that, it gives the \"orange peel\" figure to further give readers information on how this projection works.\n\n\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2011/12/butterfly.jpg)\n\n\n\n\nI only learned of the Cahill projection after reading this [xkcd cartoon](http://xkcd.com/977/).\n\n\n\n\n\n#### Call for Help\n\n\n\n\nIf anyone know of other maps with this sort if imaginative visual explanations of projection embedded in maps, please contact[ me](mailto:cgist@virginia.edu?subject=Embedded Projection Descriptions).\n"},{"id":"2011-12-02-acceptance-testing-for-omeka-plugins","title":"Acceptance Testing for Omeka Plugins","author":"eric-rochester","date":"2011-12-02 11:40:58 -0500","categories":["Research and Development"],"url":"acceptance-testing-for-omeka-plugins","content":"For the month of December, I'm going to be heads-down on NeatlineFeatures ([project page](http://neatline.scholarslab.org/plugins/neatline-features/); [Github](https://github.com/scholarslab/NeatlineFeatures)). This is an [Omeka](http://omeka.org/) plugin that lets people associate geo-spatial metadata with Omeka items by drawing on a map.\n\n\n\n\n\nBefore I started coding, I wanted to make sure I knew what I was doing, so I wrote a few [user stories](http://www.extremeprogramming.org/rules/userstories.html) and passed them around for comment. Part of the value of user stories is that they are expressed in short, natural language statements, but they can also be transitioned into [acceptance tests](http://en.wikipedia.org/wiki/Acceptance_testing) that everyone has had a voice in.\n\n\n\n\n\n(I should mention that I'm really just trying out user stories. They seem like a good idea in theory, but we'll have to see how it works in practice.)\n\n\n\n\n\nWhen testing Omeka plugins, I use [Omeka's unit testing framework (PHPUnit)](http://omeka.org/codex/Unit_Testing). I don't see a reason to change that. However, unit testing frameworks generally aren't a good fit for acceptance tests. They are both too focused and low-level and sometimes don't have a great way of acting like a browser. A bigger problem is that they shut non-coders out of the loop. Having an English-y way of expressing tests helps to involve everyone working on the project, not just the developers.\n\n\n\n\n\nFor the [Praxis Program](http://praxis.scholarslab.org/), we've been talking about using [Cucumber](http://cukes.info/) for testing. I thought this would be a good time to try it out.\n\n\n\n\n\nI expected this would be painful.\n\n\n\n\n\nFortunately, I was wrong.\n\n\n\n\n\n# About the System\n\n\n\n\n\nThe tests will be driven by Cucumber. Since this is in Ruby, they won't interact directly with Omeka (there is [cuke4php](https://github.com/olbrich/cuke4php/wiki), but I'm not going there). Instead, everything will take place through the browser. Here's how the system breaks down:\n\n\n\n\n\n**[Cucumber](http://cukes.info/)** will read in the tests and execute them. It provides the language that we use to write the tests and the code we use to implement them.\n\n\n\n\n\n**[Capybara](https://github.com/jnicklas/capybara)** defines a DSL for interacting with the browser. We'll use this when we define actions or steps for the tests.\n\n\n\n\n\n**[selenium-webdriver](http://rubygems.org/gems/selenium-webdriver)** allows Capybara, Cucumber, or any Ruby code to talk to the browser.\n\n\n\n\n\n**[Selenium](http://seleniumhq.org/)** is a system for writing tests that run in the browser. It has a Firefox plugin, which we'll use today for actually doing the tests.\n\n\n\n\n\n**[Omeka](http://omeka.org/), [PHP](http://www.php.net/), and [MySQL](http://www.mysql.com/)** are all running in a VM, managed by [Vagrant](http://vagrantup.com/). (See [this post](http://www.scholarslab.org/announcements/omeka-development-with-vagrant/) for how to set up the system.)\n\n\n\n\n\n**[Rake](http://rake.rubyforge.org/)** is used to control the system.\n\n\n\n\n\n# Adding Ruby\n\n\n\n\n\nMost of these tools are in [Ruby](http://www.ruby-lang.org/), so the first step is to mix Ruby into the Omeka/PHP plugin project. To manage Ruby, I use [RVM](https://rvm.beginrescueend.com/), so I added a `.rvmrc` file to switch to the right version of Ruby and make sure all the right gems are installed:\n\n\n\n[sourcecode language=\"bash\"]\nrvm use 1.9.3\nbundle install\n[/sourcecode]\n\n\n\nThe gems are listed in a `Gemfile`:\n\n\n\n[sourcecode language=\"ruby\"]\nsource :rubygems\ngem 'rake'\ngem 'cucumber'\ngem 'capybara'\ngem 'selenium-webdriver'\n[/sourcecode]\n\n\n\nThat's it. Now, when I change to the plugin directory, Ruby and the gems I need are available.\n\n\n\n\n\n# Setting up Cucumber\n\n\n\n\n\nCucumber expects a specific directory structure. I created that with these Bash commands:\n\n\n\n[sourcecode language=\"bash\"]\nmkdir features\nmkdir features/step_definitions\nmkdir features/support\n[/sourcecode]\n\n\n\nCucumber also needs a configuration file in `features/support/env.rb`:\n\n\n\n[sourcecode language=\"ruby\"]\nrequire 'selenium-webdriver'\nrequire 'capybara'\nrequire 'capybara/cucumber'\nrequire 'capybara/dsl'\nCapybara.app_host = 'http://features.dev'\nCapybara.run_server = false\nCapybara.default_wait_time = 15\nCapybara.default_driver = :selenium\n[/sourcecode]\n\n\n\n# Adding Features\n\n\n\n\n\nCucumber groups tests into [features](https://github.com/cucumber/cucumber/wiki/Feature-Introduction). Each feature is in a separate file, and each contains one or more scenarios.\n\n\n\n\n\nFor a contrived example, I tested logging into into the Omeka admin console.\n\n\n\n\n\n## Feature Files\n\n\n\n\n\nThe feature file for this is easy to read and understand, by design. I put this into `features/login.feature`:\n\n\n\n[sourcecode language=\"ruby\"]\nFeature: AdminLogin\n  In order to make changes to the site\n  As the site administrator\n  I want to be able to log into the admin console\n\n  Scenario: Login\n    Given I visit the admin page\n    And I enter \"features\" for the \"Username\"\n    And I enter \"features\" for the \"Password\"\n    When I press \"Log In\"\n    Then I should see a page title of \"Omeka Admin:\"\n    And I should see a header of \"Dashboard\"\n[/sourcecode]\n\n\n\nThat's it. I could have added a scenario for not authenticating correctly or other cases, but I won't worry about that right now.\n\n\n\n\n\n## Step Files\n\n\n\n\n\nFeature files are great for people, but Cucumber/Ruby still doesn't know what to do with it. To fill that gap, I defined what to do for each step in the scenario in a step file. I put this into `features/step_definitions/login_steps.rb`:\n\n\n\n[sourcecode language=\"ruby\"]\nGiven /^I visit the admin page$/ do\n  visit('/admin')\nend\n\nGiven /^I enter \"([^\"]*)\" for the \"([^\"]*)\"$/ do |value, label|\n  fill_in(label, :with => value)\nend\n\nWhen /^I press \"([^\"]*)\"$/ do |button|\n  click_on(button)\nend\n\nThen /^I should see a page title of \"([^\"]*)\"$/ do |page_title|\n  find(:xpath, '//title').has_content?(page_title)\nend\n\nThen /^I should see a header of \"([^\"]*)\"$/ do |header|\n  find(:xpath, '//h1').has_content?(header)\nend\n[/sourcecode]\n\n\n\n(It would be better to group the step definitions into files by task or domain or some other way than one-step-file-per-feature-file, but for this demonstration, this is fine.)\n\n\n\n\n\nStep definitions tell Cucumber what to do for each test. Inside each definition, I used Capybara commands that tell the browser what to do or check the page that the browser's looking at. These commands use Selenium to actually drive the action, but I don't have to worry about that.\n\n\n\n\n\n# Running Tests\n\n\n\n\n\nWe have a couple of steps left to see actual tests being run. First, we could use the `cucumber` command, but we'll probably have other things to automate (PHPUnit tests, I'm looking at you), so we'll go ahead and create a `Rakefile` that runs the Cucumber tests. This is easy to do, since Cucumber ships with some rake tasks.\n\n\n\n\n\nPut this into `Rakefile`:\n\n\n\n[sourcecode language=\"ruby\"]\nrequire 'cucumber/rake/task'\n\ntask :default => :cucumber\n\nCucumber::Rake::Task.new do |t|\n  t.cucumber_opts = %w{--format pretty}\nend\n[/sourcecode]\n\n\n\nNow, as I mentioned before, the Omeka site is running in a Vagrant-managed VM. Start it up:\n\n\n\n[sourcecode language=\"bash\"]\nvagrant up\n[/sourcecode]\n\n\n\nOnce the VM's up, run the tests by just calling `rake`. You'll see Firefox start up and close down, and at the end, you should see something like this (you may want to set the video to full screen):\n\n\n\n\n\n[Cucumber-Omeka](http://vimeo.com/32990350) from [Eric Rochester](http://vimeo.com/user2087066) on [Vimeo](http://vimeo.com).\n"},{"id":"2011-12-02-design-qa","title":"Design Q&A","author":"brooke-lestock","date":"2011-12-02 05:40:08 -0500","categories":["Grad Student Research"],"url":"design-qa","content":"Today in Praxis: more design. The design team had agreed that we would all bring in some wireframe sketches to chat about collectively and try to agree on a few wireframes to talk about at the group meeting today. We spent some time working on sketches two weeks ago and raised quite a few questions that were readdressed when we met today. For instance, do we want Prism users to be able to access visualizations of texts if they haven't already marked up that text? In other words, can someone who's coming to Prism for the first time be able to look at the already-highlighted version of a text (including the one they're working on)? We were divided on this. Some of us (myself included) were concerned that allowing users to visualize a text that is \"open\" for markup could influence the way they highlight. Doing so would downplay the interest we have in an individual's response to the constrained highlighter parameters. Thinking of a classroom scenario: If a student is asked to mark up _The Waste Land_ for Modernism, noise, and water, wouldn't being able to see her classmates' highlights affect/tempt her own interpretation? And how much does that matter?\n\nWe also answered some questions today, I think. Two weeks ago, we were going back and forth on how the home page should look, what should be included on it and how to format that information so that it maintains a clear narrative for how to use Prism. We looked at some other sites for inspiration, and what we liked about [For Better for Verse](http://prosody.lib.virginia.edu/) was that the home page puts you directly into the exercise and offers neat little tabs to help you navigate the site.  We wondered if Prism's home page should be a sandbox, where the user is immediately asked to try Prism out for herself (without saving highlights), but we also want Prism's homepage to offer some instruction, because using it is a bit more complicated than using For Better for Verse. We also were concerned  that the tabs wouldn't provide a distinct hierarchy or clear set of instructions for the user. Ed and I were trying to figure out how to clearly offer an instructive narrative on the home page, and we turned to [GitHub](https://github.com/) for inspiration since its home page features four distinct steps (complete with strangely adorable Octocat taking the user through each step). Over the break, Ed did a fantastic job of merging the For Better for Verse tabs with GitHub's narration-through-icons. Jeremy also drew up a wireframe of the home page that prioritizes instructions and a how-to video while also including prominent links to the sandbox and to the actual text mark-up page (since that's what it's all about, after all). In today's meeting, we'll collectively talk wireframes, so be on the lookout for more questions and questions answered.\n"},{"id":"2011-12-02-wireframing-and-foundations","title":"Wireframing and Foundations","author":"lindsay-o’connor","date":"2011-12-02 05:40:22 -0500","categories":["Grad Student Research"],"url":"wireframing-and-foundations","content":"Discussing design in the grad lounge continues to raise questions both programmatic and philosophical. Today we revisited the question of whether or not users should be able to see results or visualizations before they highlight a text themselves. Most of us grad students were concerned about this possibility, especially since we have agreed to make the first version of Prism for pedagogical use. We worried that students would either copy what others have marked, or that they would simply be influenced by others’ markings unintentionally. Jeremy and Wayne suggested that that might all be just fine; maybe what we want to find out is how a group interprets as a group, not as a collection of isolated individuals. This raises a major critical or methodological question for us. Are we “croudsourcing interpretation” by collecting many individual interpretations or by creating a space for collaborative interpretation? Do we want many separate interpretations that we can compare and contrast visually, or do we want one interpretation that is the work of many minds all together?\n\nI’m surprised we haven’t considered this question as a group before now. If we remain committed to the pedagogical use of the first version of Prism, I do think we should keep the results hidden from students before they have highlighted a text. But if we want to start thinking more broadly, I’m sure there will be use cases in which collaboration would be favored over the collection and collation of individual interpretations. Perhaps we can have it both ways further down the line, with researchers who upload texts given options about what is visible to which users. Our “original” idea of “crowdsourcing interpretation” seems to emphasize difference, but the collaborative version would seem to move toward a singular or unified or at least somehow standardized interpretation of an object. Wayne also pointed out how this issue has major implications for those writing the code, so here again what we thought might be a simple discussion of wireframing turned into a conversation about the methodological approach or assumptions of the tool we’re building. We still need a stronger foundation to hold up the frame.\n"},{"id":"2011-12-05-model-vocabularies","title":"Model vocabularies","author":"alex-gil","date":"2011-12-05 06:50:04 -0500","categories":["Grad Student Research"],"url":"model-vocabularies","content":"Annie drafted a new Model for Prism this week, and we had a chance to tweak it and refine it on Friday. It was our first time programming together. Heck, it was the first time I ever programmed with anyone, period. We eased into the 24-inch monitor with some takeout Chinese. Patiently maneuvering one hand on the keyboard and one on the combination fried rice, we were able to create something close to the following diagram:\n\n[![](http://www.scholarslab.org/wp-content/uploads/2011/12/PrismModel1-300x217.jpg)](http://www.scholarslab.org/praxis-program/model-vocabularies/attachment/prismmodel-2/)\n\nWhen we had more or less finished, we posted it on the IRC to discuss with whoever was online. We found Bethany, Eric R. and Jeremy. In the discussion, two suggestions came up which I think are important to keep in mind as Prism moves forward. Originally we had the model now named 'Prism' named 'Experiment.' The concern there was that the word was not catering to a humanist audience, or at least that part of it that understands itself outside the boundaries of science. In the same spirit, the field now named 'prompt' was originally named 'research_question'. The other concern was over the use of the word 'Tag'. That was the original name of the model now named 'Facet'. The concern there was that the word brings with it its own semantic baggage, which might confuse the user. The new choices, 'Facet' and 'Prism', have the added advantage that they bring the color of Prism into the data models.\n\nSomething happened in that conversation. I realized that in order to be descriptive with data or objects, I don't have to be clinical. I had realized the same for prose a while back, but it took this exercise to see what's good for literature is good for code. Now I am ready to understand the full extent of the claim that code is an expressive language. Even if we will only have a few opportunities to share the language of the model with a general audience (a paper or conference talk here and there), we should code whenever possible with a humanist's ear.\n\n\n\n"},{"id":"2011-12-13-humanities-in-a-digital-age-symposium-podcast","title":"Humanities in a Digital Age Symposium podcast","author":"ronda-grizzle","date":"2011-12-13 10:42:22 -0500","categories":["Podcasts"],"url":"humanities-in-a-digital-age-symposium-podcast","content":"**Institute of the Humanities and Global Cultures: Humanities in a Digital Age Symposium**\n\n\nOn November 11th, the University's new [Institute of the Humanities and Global Cultures](http://www.virginia.edu/humanities/) hosted a daylong symposium on \"The Humanities in a Digital Age.\" The symposium included two panels--one on Access & Ownership and the other on Research & Teaching--and two keynote talks.\n\n\n\n\n\nThe first keynote was given by [Stephen Ramsay](http://english.unl.edu/faculty/profs/sramsay.html), Associate Professor in the Department of English and Fellow in the Center for Digital Research in the Humanities at the University of Nebraska - Lincoln.\n\n\n\n\n\nThe second keynote was given by [Dan Cohen](http://www.dancohen.org/), Associate Professor in the Department of History and Director of the Roy Rosenzweig Center for History in New Media at George Mason University.\n\n\n\n\n#### Panel 1: Access and Ownership\n\n\n\n\n**Jeremy Boggs**, Humanities Design Architect, UVa Library Scholars' Lab  \n\n**Ann Houston**, Director of Humanities and Social Sciences, UVa Library\n\n\n\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/DownloadTrackPreview/virginia-public-dz.5154837759.05154837761.11631377447/enclosure.mp3\"]\n\n\n\n\n\n#### Keynote: Stephen Ramsay, \"Textual Behavior in the Human Male\"\n\n\n\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/DownloadTrackPreview/virginia-public-dz.5154837759.05154837761.11642505413/enclosure.mp3\"]\n\n\n\n\n\n#### Panel 2: Research and Teaching\n\n\n\n\n**Alison Booth**, Professor, Department of English  \n\n**Mitch Green**, Horace W. Goldsmith Distinguished Teaching Professor of Philosophy, Department of Philosophy\n\n\n\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/DownloadTrackPreview/virginia-public-dz.5154837759.05154837761.11755983498/enclosure.mp3\"]\n\n\n\n\n\n#### Keynote: Dan Cohen, \"Humanities Scholars and the Web: Past, Present, and Future\"\n\n\n\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/DownloadTrackPreview/virginia-public-dz.5154837759.05154837761.11643175927/enclosure.mp3\"]\n\n\n\n\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n"},{"id":"2011-12-13-johannes-kepper-julian-dabbert-mei-or-musical-editions-improved","title":"Johannes Kepper & Julian Dabbert: MEI, or Musical Editions Improved","author":"ronda-grizzle","date":"2011-12-13 10:42:10 -0500","categories":["Podcasts"],"url":"johannes-kepper-julian-dabbert-mei-or-musical-editions-improved","content":"**MEI, or Musical Editions Improved**\nOn November 4th, the UVa Music Library and the Scholars' Lab welcomed Dr. Johannes Kepper, Entwicklung/Betreuung Kooperationspartner at the Edirom Project, and Mr. Julian Dabbert, Wissenschaftlicher Mitarbeiter on Project TextGrid at the University of Paderborn.\n\nDr. Kepper and Mr. Debbert discussed the requirements, characteristics and benefits of digital editions based on the Music Encoding Initiative schema, as well as MEI-based applications such as the Edirom toolset and the MerMEId metadata editor. The whole group also discussed the impact of these technologies on scholarly editing in general.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/DownloadTrackPreview/virginia-public-dz.5154837759.05154837761.11632589066/enclosure.mp3\"]\n"},{"id":"2011-12-13-the-beautiful-truth-about-praxis","title":"The (beautiful) truth about Praxis","author":"sarah-storti","date":"2011-12-13 06:51:19 -0500","categories":["Grad Student Research"],"url":"the-beautiful-truth-about-praxis","content":"I thought that this week I’d give you a behind-the-scenes peek at what we really do in the [Praxis Program](http://praxis.scholarslab.org) at the [Scholars' Lab](http://lib.virginia.edu/scholarslab). Brooke and I both wrote posts at the beginning of the semester about how much pressure we felt to appear polished and professional 24/7, but the reality is that 1) this is a learning process and 2) we’re only human. I applied to be part of the Praxis team because I wanted to work on DH with other people (not an opportunity that presents itself every day!) and it really is as great as I thought it would be.  At the risk of making all of you hopelessly envious of our camaraderie, I’m pleased to share below a pictorial flowchart of sorts that illustrates the way things generally happen here at the Praxis Program during Fellows’ “office hours.”\n\nFirst, one of the extremely knowledgeable members of the Praxis team teaches the newbies about something. Here, that something is wireframing.\n[![Slab](http://farm8.staticflickr.com/7175/6505765533_5d61f95803.jpg)](http://www.flickr.com/photos/72018725@N07/6505765533/)\n\nNext, Praxis Fellows try out their own ideas on the new concept, using each other as sounding boards. We achieve various degrees of success depending on a number of factors, including that day’s caffeine intake level.\n[![Slab2](http://farm8.staticflickr.com/7150/6505765845_b34c006a54.jpg)](http://www.flickr.com/photos/72018725@N07/6505765845/)\n\nAt some point, the knowledgeable Praxis team member usually has to avert some kind of impending crisis: “I’m going to make an intervention, because you need one.” But these interventions are always performed in a very kindly fashion. We appreciate them. (This photo actually captures two such interventions occurring simultaneously... wireframing and programming CAN happen in the same room!)\n[![Slab3](http://farm8.staticflickr.com/7014/6505766157_f969eeb391.jpg)](http://www.flickr.com/photos/72018725@N07/6505766157/)\n\nFinally, the focus group (wireframers here again) agrees upon some kind of model to be used for sharing with the larger Praxis team at the weekly Tuesday afternoon meeting. We dissect, discuss, and suggest modifications as a group. Then it's usually back to the Fellows Office for revisions and a repeat of the process.\n[![slab5](http://farm8.staticflickr.com/7024/6505766459_2ba24ca35a.jpg)](http://www.flickr.com/photos/72018725@N07/6505766459/)\n\nDisclaimer: the above images are drafts and first-round workings-out of wireframing problems. They are not necessarily representative of where we are now with our wireframing. I don't think we're giving away anything too top-secret, and since one of the goals of this program is to be \"live and in public\" I thought it was about time you got to see something we've actually drawn up ourselves.\n\nAs a final touch to this essay on the joys of group work, I’m appending, for your delectation, a sampling of overheard office hour chat:\n\n--Discussing life’s most challenging questions, such as Why We’re Here:\n\nEd: I’m here to break your application with one capital letter. That’s why *I’m* here.\nWayne: You’re not gonna break MY application with one capital letter.\nEd: You just let me at your application…\n\n--And another vignette from today featuring Ed:\n\nEd: YESSSSSSSSSSS!!\nThe rest of us: [quizzical looks]\nEd: I just got Heroku to work on my application. And even better, Wayne was wrong.\n[laughter]\nEd: I’m gonna go tell him.\n\nTil next time!\n"},{"id":"2011-12-24-representative-and-abstract-prism-logos","title":"Representative and Abstract Prism Logos","author":"ed-triplett","date":"2011-12-24 05:16:04 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"representative-and-abstract-prism-logos","content":"The prism Logo now has four prototypes. I spent part of the Thanksgiving holiday a few weeks ago creating ten to fifteen options and presented them to the group at our Tuesday meeting two weeks ago. My initial intention was to focus on creating a cohesive, but abstract shape that could be repeated elsewhere on the site without the full text of the logo. The logo that elicited the most response from the group was a more representative option that I likely spent the least amount of time on. I was initially surprised at the group’s choice, but afterward I realized that this is pretty typical for this kind of work.\n\nIt can be easy to create an over-wrought logo when you devote a long section of time to it. As you push, pull and minutely alter an initial idea to make it different from a previous example, the result is often an amalgamation of too many ideas. Inversely, I have found that a logo I create quickly “works” best because the quickness of the stroke leads to a corresponding quickness of communication.\n\nThe group’s decision to pursue a logo that more clearly represented an actual prism gave the logo direction, but it also capped the logo’s level of abstraction. It was my hope however to see how far I could abstract the design without losing its representative quality. At its core, a prism is inherently a narrative object with an implied beginning, a transition, and a result. These three elements had to exist in the logo with a perceivable order. As for most things in a digital medium, the four possible changes could be shape, texture, scale and color. In the end, I could not use all of these elements as part of the transition without the narrative breaking down. The transition was therefore set for scale and color, and the “transition object” was simplified to contrast with the color and scale changes on either side of it.\n\nI admit that logos and font choice do not make for the most entertaining reading, especially without visual examples (You readers will have to wait until we have a prototype of the site to see the logo.) That said, I wanted to note that the representative prism “narrative” option ended up completely changing my approach to the font for the prism logo. Initially, I looked exclusively at more stripped-down, modern, sans-serif fonts with the vaguest hint at a meticulous human hand in order to connote a sense of precision for the web application. However, after selecting the representative design of the prism and its narrow beam motif, the width of the sans-serif fonts seemed to dwarf the rest of the logo. I then began to look at fonts with bracketed serifs and long flat bases that invoked the shape of the thin horizontal lines in the rest of the logo.\n\nToday’s meeting should offer us an opportunity to get all but the last details decided on for the logo. I am looking forward to hearing everyone’s reaction.\n"},{"id":"2011-12-24-to-scaffold-or-not-to-scaffold","title":"To Scaffold, or Not To Scaffold?","author":"annie-swafford","date":"2011-12-24 05:16:05 -0500","categories":["Grad Student Research"],"url":"to-scaffold-or-not-to-scaffold","content":"Alex and I started trying to build our new data model last Thursday, and we figured that it would go much more smoothly and quickly than the last time we tried.  Like the previous time, we decided to use Rails scaffolding, figuring it would be easier than generating the individual pieces by hand.  However, we ran into some unanticipated road blocks.  For example, we wanted a few different models to have IDs associated with them, so we added them into the model framework.  The scaffolding helpfully added them to our views page as well, so instead of having  IDs generated automatically, they appear on a form for the user to fill out, thus defeating the purpose of the ID field.  After lots of frustration, we decided to try to create a model without scaffolding when we get back from break.  We figure that it will take more time, but will involve less tweaking and will give us the opportunity of better understanding how the component parts of a rails application fit together.  We may decide that it's too much of a headache and that it would make more sense to use scaffolding and just to delete the unnecessary parts, but in the interests of experimentation, we'll give it a shot.\n"},{"id":"2012-01-09-generating-html-fixtures-using-zend-omeka-phpunit-and-jasmine","title":"Generating HTML fixtures using Zend, Omeka, PHPUnit, and Jasmine","author":"david-mcclure","date":"2012-01-09 03:49:35 -0500","categories":["Research and Development"],"url":"generating-html-fixtures-using-zend-omeka-phpunit-and-jasmine","content":"One of the reasons that testing JavaScript can be so pesky (and perhaps one of the reasons that so little JavaScript is tested...) is the fact that you have to maintain a library of HTML \"fixtures\" for the tests to run on. What's a fixture? Basically, just a little chunk of markup that provides a sandbox environment for a particular test, or a suite of tests. So, if you have a jQuery widget that adds some extra functionality to a form input, your fixture could be as simple as just a single input tag. And indeed, some of the time you can get away with just manually whipping up a chunk of ad-hoc HTML, dropping it directly into your suite, and testing into the sunset:\n\n[sourcecode language=\"html\"]\n<input class=\"simple-fixture\" type=\"text\" />\n[/sourcecode]\n\nThis works fine, provided that the HTML your code is working on is relatively simple. In practice, though, most front-end applications that grow beyond a certain critical mass of complexity end up with JavaScript code that makes DOM touches on large, complex markup structures that can't be so easily replicated independent of the application itself.\n\nThis issue started to rear its head as Neatline became more and more complex over the course of the last couple months. The Neatline front-end consists of well over 10,000 lines of JavaScript, which works on markup generated by about 20 templates and partials - it would be a frightful headache to have to manually create a library of testing fixtures for the whole application. And even if I took the time to build them all out by hand, I would be committing myself to a labor-intensive, open-ended maintenance task: Every time you make a change to a template, you have to remember to comb through all the files in the fixtures library and replicate the change. Over time - and especially as new developers start working on the project - there's a high probability that the \"real\" HTML generated by the live application will start to diverge from your fixtures.\n\nMy search for a solution led me to [this fantastic post from JB Steadman at Pivotal Labs](http://pivotallabs.com/users/jb/blog/articles/1152-javascripttests-bind-reality-). Basically, he describes a clever method for automatically generating a library of HTML fixtures that uses the server-side test suite as a staging environment that prepares, creates, and saves the markup emitted by the application. That way, your fixtures library can only ever be as old as the last time you ran your back-end test suite, which should be a many-times-daily affair. I was able to implement this pattern in the Omeka/Zend + PHPUnit ecosystem with little difficulty. (Details and code after the jump)\n<!-- more -->\nBasically, we do this:\n\n\n\n\t\n  1. Create a special controller in your application that exists solely for the purpose of rendering the templates (and combinations of templates) that are required by the JavaScript test suite;\n\n\t\n  2. Create a series of testing cases that issue requests to each of the actions in the fixtures controller, capture the responses, and write the generated HTML directly into the fixtures library.\n\n\nHow does this work in practice? Imagine you have a template called _records.php that looks like this:\n\n[sourcecode language=\"php\"]\n<div id=\"container\">\n  <?php foreach $records as $record: ?>\n    <h1><?php echo $record->title; ?></h1>\n    <div><?php echo $record->description; ?></div>\n  <?php endforeach; ?>\n</div>\n[/sourcecode]\n\nAnd when it's rendered in the application, the final markup looks like this:\n\n[sourcecode language=\"html\"]\n<div id=\"container\">\n\n    <h1>Record 1 Title</h1>\n    <div>Description for record 1.</div>\n\n    <h1>Record 2 Title</h1>\n    <div>Description for record 2.</div>\n\n</div>\n[/sourcecode]\n\nSo, the goal here is to create a controller action that populates the template with mock records objects and renders the markup, which can then be captured and saved by an integration \"test\" that we'll write in just a minute (test in quotes, since we're using PHPUnit not so much as a testing framework, but more just as a mechanism for automating requests). First, add a new controller class called FixturesController, and create an action that mocks any variables that need to get pushed into the template:\n\n[sourcecode language=\"php\"]\nclass YourPlugin_FixturesController extends Omeka_Controller_Action\n{\n\n    /**\n     * Generate fixture for _records.php.\n     *\n     * @return void\n     */\n    public function recordsAction()\n    {\n\n        // Turn off the default Zend layout-discovery functionality.\n        $this->_helper->viewRenderer->setNoRender(true);\n\n        $record1 = (object) array(\n          'title' => 'Record 1 Title',\n          'description' => 'A description for record 1.'\n        );\n\n        $record2 = (object) array(\n          'title' => 'Record 1 Title',\n          'description' => 'A description for record 1.'\n        );\n\n        $records = array($record1, $record2);\n\n        // Render.\n        echo $this->view->partial('public/_records.php', array(\n            'records' =>  $records\n        ));\n\n    }\n\n}\n[/sourcecode]\n\nBasically, we're just stubbing out two artificial record objects (for simplicity, we add only the attributes that are used in the template) and directly render the template file as a \"partial.\" Note the call to `setNoRender(true)` - by default, Zend will try to automagically discover a template file with the same name as the controller action, but we're just disabling that functionality since we want direct control over which templates get rendered and in what order.\n\nNext, add a directory called \"fixtures\" in the /tests directory, and create a file called \"FixtureBuilderTest.php\" to house the integration test that will do the work of requesting the new controlled action, capturing the generated markup, and saving the result to the fixtures library.\n\nThis should look like this:\n\n[sourcecode language=\"php\"]\nclass YourPlugin_FixtureBuilderTest extends YourPlugin_Test_AppTestCase\n{\n\n    private static $path_to_fixtures = '../spec/javascripts/fixtures/';\n\n    /**\n     * Instantiate the helper class, install the plugins, get the database.\n     *\n     * @return void.\n     */\n    public function setUp()\n    {\n\n        // Set up the testing environment and plugin.\n        parent::setUp();\n        $this->setUpPlugin();\n\n    }\n\n    /**\n     * Fixture builder for _records.php.\n     *\n     * @return void.\n     */\n    public function testBuildRecordsMarkup()\n    {\n\n        $fixture = fopen(self::$path_to_fixtures . '_records.html', 'w');\n\n        $this->dispatch('your-plugin/fixtures/records');\n        $response = $this->getResponse()->getBody('default');\n\n        fwrite($fixture, $response);\n        fclose($fixture);\n\n    }\n\n}\n[/sourcecode]\n\nNote that you need to specify the location in the project directory structure that you want to save the fixtures to. In this case, I'm saving to the default location used by Jasmine, but you could point to anywhere in the filesystem relative to the AllTests.php runner file in /tests.\n\nMake sure that the /fixtures directory is included in the test discoverer in AllTests.php, run phpunit, and your fresh-out-of-the-oven fixture should be saved off and ready for action! All that's left to do now is load the fixture in your JavaScript test, run your code on the HTML, and start enumerating test cases. We use a testing framework called [Jasmine](http://pivotal.github.com/jasmine/) in conjunction with a plugin called [jasmine-jquery](https://github.com/velesin/jasmine-jquery), which provides an easy way to load fixtures into the tests:\n\n[sourcecode language=\"javascript\"]\n/*\n * Unit tests for the records Javascript.\n */\n\ndescribe('Records', function() {\n\n    var recordsContainer;\n\n    beforeEach(function() {\n\n        // Get the records markup.\n        loadFixtures('_records.html');\n\n        // Select the container div.\n        browser = $('#container');\n\n        // Instantiate your code.\n        browser.recordsWidget();\n\n    });\n\n    // Now, the tests:\n\n    describe('some class of behavior', function() {\n\n        it('should do X', function() {\n            expect(true).toEqual(true);\n        });\n\n    });\n\n});\n[/sourcecode]\n"},{"id":"2012-01-09-looking-forward-to-prism","title":"Looking forward to Prism","author":"brooke-lestock","date":"2012-01-09 11:22:56 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"looking-forward-to-prism","content":"With the end of the semester and year, and all of the accompanying hullabaloo (to use a polite term for it), I wasn't able to write my final blog post of the semester, which was going to be a retrospective of my Praxis experience so far. But now it's the new year and the new semester is imminent, so it seems more appropriate to look ahead - and who wants to look like Janus, anyway?\n\nI think it's brilliant the way that Praxis has been structured, with one training-intensive semester and the next semester spent - I'm assuming - in executing what we've been theorizing. That's not to say that we haven't _done_ anything yet; we have produced the programming and design foundations for Prism that we will hammer down and build from in the months to come. There have been days in the grad lounge, though, when I've felt anxious about how much work there is left to do and how difficult it seems to reach concrete goals when our discussions usually raise more questions than they answer. But even if we spend our entire Tuesday mornings arguing/theorizing about what we think Prism should accomplish, our time isn't wasted if it means Prism will be (1) a clearly designed tool (2) with a distinguishable thought process that (3) makes a specific intervention.\n\nAll of our discussions have addressed at least one of those three points without fail, so our mission in the months ahead is to channel our humanities-inspired zeal for theorizing into reaching the goals we set for Prism in September - mainly, that we want to produce a working, 1.0 version of Prism by the end of the academic year. To reach that final, rather intimidating endpoint, I'd like to suggest that the Praxis team begins the new semester by establishing (collaboratively, of course) some real, manageable, short-term goals for the coming weeks.\n\nI'm beginning to recover from Winter Break Amnesia (I'm sure our first Praxis meeting tomorrow will quickly bring me back to reality), but I am looking forward to getting back to learning HTML and CSS, to our civilized theoretical arguments in the grad lounge, and to those glorious moments when we (with the help of the gurus) learn how to turn theory into praxis and bring Prism further into the light.\n"},{"id":"2012-01-13-mapping-the-catalogue-of-ships","title":"Mapping the Catalogue of Ships","author":"bethany-nowviskie","date":"2012-01-13 09:02:40 -0500","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Visualization and Data Mining"],"url":"mapping-the-catalogue-of-ships","content":"_I'm very pleased to share a guest post by UVa Classics professor [Jenny Strauss Clay](http://www.virginia.edu/classics/clay.html), describing a new project we've undertaken at the Scholars' Lab.  We're excited not only at the opportunity to use GIS techniques to test Professor Clay's theories about the relation of ancient geography to mnemonic devices and poetic form, but also at the possibility that this process might assist in the identification of lost archaeological sites. -- Bethany Nowviskie_\n\nBook Two of the _Iliad_ notoriously contains a list of nearly 190 place names and includes the 29 contingents and that make up the Greek expedition to Troy.  Before launching into an over 250-line catalogue of the leaders of the Greek forces and the number of their ships, Homer appeals to the Muses to aid him in this _tour-de-force _of memory.  Without their help, he says:\n\n\n\n<blockquote>I could not recount their numbers nor name them,\nNot if I had ten tongues and ten mouths,\nAnd an unbreakable voice and a brazen chest within,\nIf the Olympian Muses, daughters of aegis-bearing\nZeus, would remind me how many came under Ilium.</blockquote>\n\n\n\nThe Catalogue of Ships that follows this invocation can be mapped as an itinerary, or more precisely, three itineraries that traverse most of Greece.  The theoretical basis for the project I am undertaking with the Scholars' Lab at the University of Virginia Library is already complete. In my recent book, _[Homer’s Trojan Theater](http://books.google.com/books/about/Homer_s_Trojan_Theater.html?id=d8JTqjNWHOsC)_ (Cambridge University Press, 2011), I argue that Homer was able to recite the Catalogue by creating a mental journey that used the mnemonic techniques involving _loci_ or places, well known from ancient rhetorical writers.  By envisioning a series of places, Homer could mentally walk – or sail – through Greece and produce a detailed catalogue. Our project will reproduce that journey by showing that the itinerary described follows the natural contours of Greek geography and the patterns of early Greek urban organization.\n\nMapping the Catalogue of Ships involves several steps.  \"Least-cost path\" GIS analysis by the Scholars Lab is revealing the terrain that must naturally be followed when taking a walking tour of the Greek mainland.  We are creating an interactive map that follows that path.  The _Barrington Atlas of the Ancient World_ (2002) as well as the recent _Historischer Atlas der antiken Welt_ (2007), _The Homer Encyclopedia_ (2011) and the [Pleiades Project](http://pleiades.stoa.org/), a collaborative database for ancient sites, have pinpointed locations for which we have evidence.  We will attempt to link the sites mentioned in Homer with archaeological material and useful bibliographies.  Finally, we hope to do _in situ_ investigations by actually traversing the plotted itinerary at ground level to survey the terrain, and create extensive panoramic photography. Our main goal is to demonstrate that the arrangement of the Catalogue, far from a random list of place names, corresponds to the natural geography of Greece.  In cases where the position of a site is unknown or disputed, we hope that our analysis will provide plausible _geographical and literary evidence_ to help identify its location.\n\nCollaborators in this project include Ben Jasnow and Courtney Evans, two of my graduate students who worked with me on the _[Trojan Theater](http://www.homerstrojantheater.org)_ project and who are assisting with GIS analysis, under the guidance of Chris Gist and Kelly Johnston of the Scholars' Lab. Wayne Graham and other members of the Scholars' Lab R&D division are creating a presentational framework for our maps and text, and Jeremy Boggs is our lead designer.\n\nJenny Strauss Clay\nWilliam R. Kenan Jr. Professor of Classics\n"},{"id":"2012-01-17-praxis-mla-2012-and-timeliness","title":"Praxis, MLA 2012 and timeliness","author":"alex-gil","date":"2012-01-17 10:39:53 -0500","categories":["Grad Student Research"],"url":"praxis-mla-2012-and-timeliness","content":"I'm finally settling back into my C'ville routine. My last stop this winter break was the MLA convention in Seattle. Like many of my colleagues, I also felt that \"[the MLA’s heart (like a post-holiday Grinch) grew at least three sizes over the four days of the 2012 conference](http://publishing.umich.edu/2012/01/16/mpub-mla/).\" While last year echoed [a prominent informer](http://chronicle.com/blogs/brainstorm/pannapacker-at-mla-digital-humanities-triumphant/30915)'s assessment that DH was \"the next big thing\" with anxiety, this year felt more like \"Hey, I like that. How do I do it?\" This was especially a good year for those in the business of [rethinking the future of graduate methods training](http://www.uvasci.org/current-work/) (ahem, ahem) and of [graduate futures in general](http://mediacommons.futureofthebook.org/alt-ac/). Needless to say, I felt really great about being part of the first cohort of [Praxis](http://praxis.scholarslab.org).\n\nSaturday evening I had a chance to catch up with one of my early undergraduate mentors. He had questions. He wanted to know what I knew about the DH world. I'm sure half of his curiosity came out of an earnest desire to hear the tale of my travels. The other half was a shrewd (and responsible) move to build a vocabulary for conversations his department will inevitably have this year with the dean, other departments, the library, etc: Can an isolated DHer work well with limited resources? Do you need a center? How do you get graduate students involved? Our conversation went on for a good three hours and it was very rewarding to offer a candid assessment of the field from where I'm standing.\n\nI also realized that where I'm standing is what in battle we would call _higher ground_. I don't mean the privilege of hobnobbing with the enormous DH talent we have on grounds. Nothing, of the sort. Although projects are a whole different affair, you could develop decent DH _skills _and _ideas _were you connected from [Pie Town](http://freecabinporn.com/). I mean the privilege of seeing graduate methods transformation first-hand. I agree with [Brooke](http://www.scholarslab.org/digital-humanities/project-management-and-graduate-training/) that there is a continuum that links us to analog models in the department (at least at UVa). But the continuum does eventually lead to new ground.\n\nWhat I've seen, of course, has been well recorded here by all the Praxis bloggers. If this is your first time visiting our site and you are interested in the fresh air blowing our way, I encourage you to [read more...](http://www.scholarslab.org/blog/archives/)\n\n\n"},{"id":"2012-01-17-project-management-and-graduate-training","title":"Project Management and Graduate Training","author":"brooke-lestock","date":"2012-01-17 07:41:30 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"project-management-and-graduate-training","content":"As if on cue, right after[ I posted last week](http://www.scholarslab.org/digital-humanities/looking-forward-to-prism/) to call for clear, concrete goals for Prism this semester, Bethany began last week's meeting by asking for a Project Manager. Sarah Storti and I quickly volunteered for the job, probably because we share a love of deadlines, self-imposed or otherwise, and work at similar levels of anxiety without them. Bethany assigned us some weekend reading on Project Management and we convened for \"Projectmanageapalooza\" yesterday to discuss the material and devise a plan for managing this semester's hefty workload.\n\nThe readings were extremely helpful (see the [\"Intro to Project Management\" ](http://praxis.scholarslab.org/topics/project-management/)section of Praxis topics for our short list of most helpful resources), especially Brian Croxall's[ \"12 Basic Principles of Project Management\"](http://chronicle.com/blogs/profhacker/12-basic-principles-of-project-management/31421) and Sharon Leon's[ \"Project Management for Humanists.\"](http://mediacommons.futureofthebook.org/alt-ac/pieces/project-management-humanists) Both articles stress the PM's need to assess the viability/sustainability of a project before it's begun, the importance of a clear and flexible workplan that is derived collaboratively and realistically, and the PM's responsibility to manage and encourage frequent communication amongst team members and partners. Both articles also begin with a point that has been made quite frequently, but that has not necessarily been my experience as a graduate student so far: that humanities graduate students are not trained to work collaboratively.\n\nWhile I wholeheartedly agree with Leon, Croxall, and most of the DH community that graduate education must be transformed to_ formally, explicitly_ transmit this kind of training to humanities scholars, for the sake of the individual scholar and the profession as a whole, there are many opportunities to work collaboratively as a graduate student, but they must be sought out and are often extracurricular and small-scale. I recognize that it can be very easy to become the scholar/hermit in a graduate program, especially because programs have not yet adapted to encourage collaborative research in the traditional sense (like a tag-team digital dissertation), but collaboration on the most basic level has been ingrained in my daily experience in UVA's English department - from discussion in grad seminars that leads to new research, to collaboratively editing papers and personal statements with peers and faculty mentors alike, to extracurricular activities like the Graduate English Students Association and its conference committee (groups that require quite a bit of PM-type skills).\n\nThis is not the kind of training Leon and Croxall are calling for, but my graduate education (so far) has trained me to seek out opportunities to collaborate with others within the department and outside of it (in my work with Praxis and IATH, for example). I have to stress that I do not disagree with the inadequacy of graduate methodological training; if I did, I wouldn't be a Praxis Fellow. But I think we can find the basic principles for collaborative research happening already on a very small scale, and it's up to graduate students to make collaborative research a priority - that is, to find those opportunities, seize them, and ask their programs to support them. Pardon the lengthy post; I know I'm not saying anything new here and I may be totally off-base, but I thought I'd respond with my experience as a nascent scholar and even-more-nascent member of the DH community.\n"},{"id":"2012-01-23-why-i-love-project-management","title":"Why I love project management","author":"sarah-storti","date":"2012-01-23 09:51:32 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"why-i-love-project-management","content":"Last week Brooke and I celebrated our new roles as [co-project managers](http://praxis.scholarslab.org/topics/project-management/) by running our very first Praxis meeting. We had a fairly ambitious agenda, and I must admit that I was a little bit concerned about whether our enthusiastic (debate-loving) group would be able to get through everything we wanted to do, but thanks to Brooke's no-nonsense attitude, our pre-planned strategy, and the team's brilliant cooperation, I think we can call our first official act as co-project managers a decided success.\n\nThe most important product of our meeting was the project workplan timeline the team collectively created. I know that deadline-lovers Brooke and I feel six hundred times better about how the rest of this semester is going to proceed, but I think everybody is happier knowing exactly what needs to happen and when. I would only be exaggerating slightly to say that it was magical to watch the workplan take shape. Despite the fact that we've all been meeting weekly for an entire semester, as the project has progressed and the design team and programming team have been working more independently of one another, it has been difficult to see how everything fits together. But when we had to agree on deadlines for project milestones (when will we be able to display a text? when will user accounts be ready? when should the highlighting tool be functional?), designers and programmers had to engage in a dialogue about what each group would need from the other in order to meet these goals.\n\nThe meeting served as yet another reminder of how absolutely, impossibly lucky I am to be part of this program. As project managers, Brooke and I are ideally positioned to understand how the Praxis Program and our project Prism work as collaborative ventures. It is impossible for every member of the team to keep track of what each subgroup is doing at all times, but it is necessary that project managers do just that. I can't wait to get my hands dirty! I'm sure I speak for both of us when I say that we are thrilled and honored by the trust our team has placed in us. We won't let you down!\n\n[Brooke posted last week](http://www.scholarslab.org/digital-humanities/project-management-and-graduate-training/) about seeking out ways to engage in collaborative work, and I want to point very quickly here to the [Digital Humanities Summer Institute](http://www.dhsi.org/). I attended DHSI last summer, and it certainly didn't disappoint on the collaborative front. Brooke and I are both making the trip to Victoria this summer, and I'm sure she will only find more evidence there of what she's seen so far as a graduate student in the humanities: seek collaboration, and you shall find it. If you've never been but always wondered, I highly recommend that you make this the year you join the (collective) fun!\n"},{"id":"2012-01-24-building-and-texts","title":"Building and Texts","author":"annie-swafford","date":"2012-01-24 09:03:04 -0500","categories":["Grad Student Research"],"url":"building-and-texts","content":"Alex and I finally have something to show for all our work! We built the document model for Prism that allows us to add new texts to the database!  It even passes the tests we built!  We're putting the finishing touches on it now, but we should easily make our deadline of this afternoon, so we can successfully pass it on to the design team.  It's exciting that we're actually able to put together our rails and ruby knowledge and produce tangible results.  This gives us hope that we will be able to continue to meet our other deadlines and eventually end up with a working project.\n\nOur next programming steps include building a user model and incorporating authentication and authorization (probably using CanCan and Devise ruby gems).  This will be more complicated that what we've produced so far, but we think that with the help of lots of screencasts and ruby gem documentation, we will persevere.\n\nOn other prism news, we will officially decide on our texts for Prism today in our meeting!  It's all starting to take shape.\n"},{"id":"2012-01-25-sci-opening","title":"seeking a Senior Research Specialist for SCI","author":"bethany-nowviskie","date":"2012-01-25 06:19:34 -0500","categories":["Announcements"],"url":"sci-opening","content":"I'm pleased to share a formal job posting for an 18-month research-and-writing position with the Scholarly Communication Institute. The posting is limited to current employees of the Unversity of Virginia for four business days, after which it will open more broadly.*  It's a short-term position, but an excellent opportunity to work with [SCI](http://uvasci.org/) leadership and representatives of the following groups: the Scholars' Lab [Praxis Program](http://praxis.scholarslab.org/), [CHCI](http://chcinetwork.org/), [centerNet](http://digitalhumanities.org/centernet), [PressForward](http://pressforward.org/), the [Modern Language Association](http://mla.org), the [Alliance for Networking Visual Culture](http://scalar.usc.edu/anvc/?page_id=2), the [ACLS](http://www.acls.org/), [CLIR](http://clir.org), and more. The timeframe for our decision is very short, so interested applicants should not delay!\n\n***UPDATE: The position is now open to all qualified candidates.**\nPlease note new URL below!\n\n\n\n<blockquote>**Senior Research Specialist, SCI**\n\nThe [Scholars' Lab](http://lib.virginia.edu/scholarslab) at the University of Virginia Library [seeks a senior research specialist](http://jobs.virginia.edu/applicants/Central?quickFind=66407) for a full-time, 18-month position with the [Scholarly Communication Institute](http://uvasci.org/) (SCI).  The ideal candidate will have: excellent research, writing, and organizational skills; familiarity with humanities scholarship at the graduate level; and an interest in experimental approaches to digital authoring and publication or the education of emerging scholars and knowledge workers. Some travel is required, and there is a possibility of a work-from-home arrangement for a professional and thoroughly reliable candidate.\n\nReporting to UVa Library's [director of digital research and scholarship](http://www.uvasci.org/about-us/steering-committee/bethany-nowviskie/), the SCI research specialist will:\n\n    \n> \n> \n\t\n>   * help design, execute, and analyze a broad-based, anonymous survey of humanities professionals (together with the perceptions of their employers) to examine graduate-level preparation for so-called \"[alternative academic](http://mediacommons.futureofthebook.org/alt-ac/)\" careers (20%);\n> \n    \n>   * attend and provide research support for three meetings and regular conference calls on new-model publishing and authoring environments, in order to take notes and help compile proceedings and recommendations (15%);\n> \n    \n>   * attend and provide research support for two to three meetings centering on the reform of methodological training in the humanities, involving major humanities consortia, professional associations, and funders, to take notes and help compile proceedings and recommendations (15%);\n> \n    \n>   * assist in research and organizational tasks related to the development and day-to-day operations of the [Praxis Program](http://praxis.scholarslab.org/) at the University of Virginia Library, including the hosting of a gathering to foster inter-institutional collaboration on similar initiatives (20%);\n> \n    \n>   * and perform other research and writing tasks as needed by SCI principals (10%).\n> \n\nThe SCI research specialist will join a [vibrant and dedicated community](http://scholarslab.org) of faculty and staff at the Scholars' Lab, and as such will be eligible for the self-directed \"20% time\" that all team members are granted to pursue professional development and their own (often collaborative) R&D; projects.  This is a [grant-funded position](http://uvasci.org/current-work) with a salary of approximately $50k per annum and full benefits as a member of the managerial and professional staff of the University of Virginia. The position begins no later than March 1st, 2012 and extends no later than August 31st, 2013.\n\n**Experience and Education:** Master's degree or higher in fields related to humanities scholarship or information science. Demonstrated ability as a writer and researcher. Project management experience or experience with survey design and analysis desirable.</blockquote>\n\n\n\nPlease [APPLY ONLINE](http://jobs.virginia.edu/applicants/Central?quickFind=66487).\n\n"},{"id":"2012-01-26-final-prism-wireframes","title":"\"Final\" Prism Wireframes","author":"lindsay-o’connor","date":"2012-01-26 02:22:45 -0500","categories":["Grad Student Research","Research and Development"],"url":"final-prism-wireframes","content":"Despite whatever I might have said in previous blog posts, coding was quite a challenge for me and not something I could see myself devoting sustained attention to, so I was pleased to find some inkling of intuition about graphic design. My days of designing my high school newspaper on antiquated Adobe software suddenly became relevant, and all the time I waste perusing Fab.com and looking for affordable mid-century housewares on Craigslist can now be called “design research.” With lots of help from Jeremy and the internets, a Prism homepage is in development, complete with a header and footer and floated text and working links. Of course we’ll need to do a lot more than that to get this thing looking nearly as good as some of Ed’s designs in Adobe Illustrator, but I finally have some sense of what it will take, and with so many others on the Praxis team interested in design, it all seems possible.\n\nSince I’m so new to HTML and CSS, I have no shame about asking for help with the smallest issues (like italicizing text) or with far-reaching errors (the whole page is the body?!), but I also have no shame about celebrating the smallest successes (rounded corners!). In addition to a few meetings with Jeremy, Ed’s great ideas and Illustrator skills and Annie and David’s HTML knowledge have helped me make progress. People are probably going to start sending me “Let me Google that for You” links pretty soon, but at least right now, I’m feeling warm and fuzzy and collaborative. On Monday morning, Brooke, Sarah, Jeremy, Ed, and I worked through the wireframes for Prism one more time. We had some drafts from the end of last semester, but some decisions about how the tool will work have affected how the site will look, so we revisited our wireframes and got them in good enough shape to present to the whole team on Tuesday. This picture shows what we came up with. It doesn’t look like much, but I think we all feel pretty accomplished now that we have a “clean” and simple form to design on.\n\n![](https://lh5.googleusercontent.com/-DIJdjJ0oVO0/TyCxvmcebKI/AAAAAAAABLI/sO8a6Xs9bnA/s640/wireframes.jpg)\n"},{"id":"2012-01-26-teasing-the-blogosphere","title":"Teasing the Blogosphere","author":"ed-triplett","date":"2012-01-26 02:21:44 -0500","categories":["Digital Humanities","Grad Student Research","Research and Development"],"url":"teasing-the-blogosphere","content":"Describing visual work in a blog without being able to reveal the images is more than challenging: it's boring. After we discussed this issue in our most recent Praxis meeting, the group suggested I post a Prism striptease. Linked below is a teasingly cropped and subtly altered image of the logo we decided on last week.\n  \n\n[![logo-ish](http://farm8.staticflickr.com/7156/6761706485_4915ce8725.jpg)](http://www.flickr.com/photos/28122639@N05/6761706485/)\n  \n\nThe other images show some renders of a 3D model I created to use as inspiration for the site's color palette. The model is of a \"deck prism\" which was used to filter light from above the deck of a ship into the cabins below. The refracted colors that emerge when light passes through the prism can be seen in online images, but the digital model offered a lot more control.\n  \n\n[![deck+prism7](http://farm8.staticflickr.com/7008/6761706471_f431e1339f_m.jpg)](http://www.flickr.com/photos/28122639@N05/6761706471/)\n  \n\n[![deck+prism5](http://farm8.staticflickr.com/7016/6761706447_3e9824a05d_m.jpg)](http://www.flickr.com/photos/28122639@N05/6761706447/)\n  \n\nhttp://www.flickr.com/photos/28122639@N05/6761716871/\nThe above video shows the visual options offered by the model of the deck prism.\n  \n\nWhile the deck prism is recognizable as an object, its clarity and sharp lines make it a good source of abstract imagery for the site. The model also allowed me to experiment with reflections of the texts we chose as our three highlighting samples. The text was simply applied to a plane directly below the prism in the 3D scene, and the reflections changed on the fly based on the camera position. You can see part of the first line from \"The Raven\" reflected in one of the images below.\n  \n\n[![Highlighter Palette](http://farm8.staticflickr.com/7148/6762343843_545f6f5d52.jpg)](http://www.flickr.com/photos/28122639@N05/6762343843/)\n  \n\nFinally, the last image shows the highlighter \"palette\" that we used for our wireframe. The boxes are mostly just placeholders, but the colors represent some of the options that came out of the deck prism, and I think the categories we chose for Edgar Allan Poe''s \"The Raven\" could elicit a thought provoking response from a crowd.\n\n\n"},{"id":"2012-02-01-done-is-the-engine-of-more-2","title":"Done is the engine of more.","author":"sarah-storti","date":"2012-01-31 21:22:43 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"done-is-the-engine-of-more-2","content":"(My title is not mine! It is lovingly borrowed from Bre Pettis and Kio Stark's [\"Cult of Done Manifesto\"](http://www.brepettis.com/blog/2009/3/3/the-cult-of-done-manifesto.html))\n\nI love lists almost as much as I love agendas and program management in general. Here is a status update in list form for you, Dear Reader. And Team, please feel free to expand/clarify/correct the following:\n\n1. We have finalized the texts we're going to be uploading into Prism for first-round users. Today, some of us tested a few of the suggested \"highlighting\" categories with help from our low-tech friends: transparencies, markers, and photocopies. Ed, Brooke, Annie, and I will be reviewing and revising categories over the next few days, and the entire Praxis team is invited to test our new suggestions this coming week.\n\n\n[![slab6](http://farm8.staticflickr.com/7142/6798750351_8941df7f90.jpg)](http://www.flickr.com/photos/72018725@N07/6798750351/)\n\n\n2. The development team met their latest milestone goal: authentication and authorization for the Prism site is a go! User accounts! Ding!\n\n\n[![slab8](http://farm8.staticflickr.com/7027/6798750829_65e43bbd43.jpg)](http://www.flickr.com/photos/72018725@N07/6798750829/)\n\n\n3.[ As Lindsay notes](http://www.scholarslab.org/praxis-program/final-prism-wireframes/), the wireframes (and thus, for the most part, the user story), are finalized. Design wizzes (plural of wiz?) Ed and Lindsay have been rocking out on the front-end work: Ed continues to wow us with his aesthetic brilliance (see his[ \"striptease\" post](http://www.scholarslab.org/digital-humanities/teasing-the-blogosphere/) of Jan 26), while Lindsay works on making the dream a reality via CSS/HTML.\n\n4. We've been inspired by the recent trend toward internationalizing Ruby on Rails applications, and have nominated the most Continental of our Fellows, Alex, to the position of Internationalization Expert(-to-be). I'm sure you'll be hearing more from him about this exciting new development in the near future.\n\n5. Finally, on a more personal note: I continue to enjoy how much time I get to spend with my colleagues while we work toward a common goal. Collaborative ventures certainly pose some challenges that would be non-issues in individual project contexts, but I think we're all benefiting from learning how to work not only with digital tools but also with one another. The Fellows' lounge was busy busy busy today--and that's the way we like it!\n\n\n[![slab7](http://farm8.staticflickr.com/7170/6798750557_5c239e0802.jpg)](http://www.flickr.com/photos/72018725@N07/6798750557/)\n"},{"id":"2012-02-02-collaborative-mentoring-at-ut-and-uva-co-developing-an-updated-teidisplay-for-omeka","title":"Collaborative mentoring at UT & UVa: co-developing an updated TEIDisplay for Omeka","author":"tanya-clement","date":"2012-02-02 05:43:04 -0500","categories":["Announcements","Digital Humanities"],"url":"collaborative-mentoring-at-ut-and-uva-co-developing-an-updated-teidisplay-for-omeka","content":"In partial answer to [Bethany](http://www.scholarslab.org/author/bethany/)'s charge in her recent ProfHacker piece \"[it starts on day one](http://chronicle.com/blogs/profhacker/it-starts-on-day-one/37893),\" I'm very excited to introduce a cross-institutional effort  between the Scholars' Lab and the School of Information at UT-Austin to mentor two UT graduate students in the iSchool as they work to develop a DH tool for the DH community. The project will have two corresponding parts based on the background and interest of the students. [Zane Schwarzlose](http://www.scholarslab.org/author/zschwarzlose/), whose background includes extensive experience in developing with PhP and JavaScript will work to enhance [TEIDisplay](https://github.com/scholarslab/TeiDisplay), an Omeka plugin originally written by Ethan Gruber at the Scholars' Lab, that allows users to upload and display searchable [TEI texts](http://www.tei-c.org/) within the [Omeka](http://omeka.org) environment. [Carin Yavorcik](http://www.scholarslab.org/author/cyavorcik/), an emerging archivist, will create TEI templates as well as user documentation so that the new tool will be useful not only to the many cultural institutions that Omeka serves but also to instructors who are looking for an environment within which they can teach the integral ways in which a TEI text can function as a cross-platform representation of text.\n\nThe collaboration makes sense on many levels, but here are two that surface readily:\n\n\n\n\t\n  1. These are complex technologies that function in a complex social and cultural system. We can meet the development needs because we represent institutions with different institutional missions, different (though like-minded) communities, with different resources.\n\n\t\n  2.  Our students, who will seek jobs in which they work collaboratively in different institutional missions, from the perspective of different (though like-minded) communities, with different resources, must be prepared to meet these challenges within a network of a the wider DH community.\n\n\nIf we believe in a basic DH tenet that making is a theoretically framed activity that helps deepen our understanding of our cultural artifacts and our modes of knowledge production, we must instill, as Bethany so aptly articulates, \"a can-do, maker’s ethos\" in students who will feel \"_empowered to build and re-build_ the systems in which they and future students will operate.\" To further this cause, we must also instill a second basic DH tenet in our community of scholars, makers, and teachers: we must pool our resources, both technical and academic, and develop our technologies (such as the TEI and Omeka) and mentor our students, together.\n\nBoth Carin and Zane will blog regularly in this space as the project develops. Onward ho, ya'll.\n"},{"id":"2012-02-06-an-update-to-teidisplay-for-omeka","title":"An Update to TEIDisplay for Omeka","author":"carin-yavorcik","date":"2012-02-06 06:38:08 -0500","categories":["Digital Humanities"],"url":"an-update-to-teidisplay-for-omeka","content":"This spring, my colleague Zane Schwarzlose and I are working on an update to the TEIDisplay plugin for Omeka, developed by Ethan Gruber at the Scholars’ Lab. While it’s a great tool, it was developed as part of previous versions of Omeka. Even then, it was at times difficult to use, and some TEI elements did not render correctly. We’re hoping to update the plugin and iron out some of those bugs as we progress.\n\nMy first experience using the plugin was last year for an [Introduction to Digital Humanities](http://blogs.ischool.utexas.edu/f2011dh/) course at UT-Austin’s School of Information taught by Tanya Clement (also our advisor for this project -- she goes into more detail about the project background [here](https://www.scholarslab.org/announcements/collaborative-mentoring-at-ut-and-uva-co-developing-an-updated-teidisplay-for-omeka/)). Dr. Clement asked us to create an Omeka exhibit for a set of correspondences, including images of the original letters as well as TEI documents of marked-up content. It sounds simple enough, but there was a lot of frustrated graduate student Facebook posting going on in the days before the project was due as people ran into unexpected technical difficulties.\n\nOverall, there seem to be two main problems with the plugin. For the majority of the documents, we couldn’t get them to format at all – the entire body of text would wind up in one large paragraph in miniscule font at the top of the page. In a few other cases, the document would format enough to be readable, but it wouldn’t pick up the finer details of the TEI tags. For example, <p> tags would result in paragraph breaks, but <sic> and <corr> elements wouldn’t display– so something like “<choice><sic>sndstorms </sic><corr>sandstorms </corr></choice>” would show up as “sndstormssandstorms” in the body of the text. Though this seems like a relatively easy XSLT fix, we imagine other fixes (even possibly this one) may not be so straightforward.\n\nThroughout the project, Zane and I will be documenting our work here at the Scholars’ Lab blog. At the end of the semester, we hope to provide not only an update to the plugin, but also user documentation, including use case scenarios and templates.\n\nWe’re going to be seeking input over the next few weeks on how this plugin would work in an ideal world, and we need your help! Look for a more formal survey soon, but in the meantime, let us know your thoughts in the comments – what are you looking for in a simple, out-of-the-box TEI display tool?\n"},{"id":"2012-02-06-teaching-prism-how-to-speak-spanish-and-french","title":"teaching prism how to speak spanish and french","author":"alex-gil","date":"2012-02-06 05:48:10 -0500","categories":["Grad Student Research"],"url":"teaching-prism-how-to-speak-spanish-and-french","content":"Well, it looks like me and Annie will go on different development branches for a few weeks. I have been assigned the noble task of teaching Prism how to speak Spanish and French. The goal is to give the tool the largest possible reach on day one. Internationalization has always been one of my hot-button issues for a while, given that I hail from and engage with the global south, where [the digital divide and the digital humanities divide](http://www.insidehighered.com/blogs/globalisation-digital-humanities-uneven-promise) is more visible.\n\nSo far the tool I'm working with is the [I18n](http://guides.rubyonrails.org/i18n.html) gem which comes bundled with rails. The tool will make it easy for us to translate the interface. I18n so far seems very straightforward. I am creating YAML documents for each language. The YAML documents assign keys or placeholder names (kind of like variables) to all the strings that need to be translated in the views. Instead of writing strings directly in the views, we will write the placeholders. Depending on the _locale_ that is selected, the placeholders will fetch the appropriate strings from the YAML files. Pretty simple! [crosses fingers nervously]. Besides the ability to abstract strings from their context, I18n also takes care of date and currency formats. Not that we will need the latter, though it's nice to know we could make it all about the _pesos_ if we wanted to.\n\nAdding sample texts in French and Spanish remains a decision for the future, given our time constraints. I'm working as fast as I can to develop the i18n functionality, so that we do have time, but there is no guarantee. Stay tuned!\n\n\n"},{"id":"2012-02-15-all-hail-the-workplan-accountability-and-collaborative-research","title":"All Hail the Workplan!: Accountability and Collaborative Research","author":"brooke-lestock","date":"2012-02-15 08:22:59 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"all-hail-the-workplan-accountability-and-collaborative-research","content":"In our first official meeting as Project Managers, Sarah and I drew a line on the whiteboard that had two distinct endpoints - a start and a finish. The start point was that day's date, January 17th, and the endpoint would be whatever date we could get the group to agree on for Prism's launch. After briefing the group in [our project management crash course](http://praxis.scholarslab.org/topics/project-management/) on the elements of a good workplan, I grabbed a marker, Sarah opened a Google doc, and the great experiment began. We worked backwards from the official release of Prism into the world, setting hard dates for the feature freeze and soft launch, and then drew in key points for each aspect of the project (when the highlighting function will be coded, when the pages will be designed, etc.). And so the workplan was born. The process took something like 90 minutes, but it might have been the most productive 90 minutes I've experienced in Praxis so far.\n\nThough Sarah and I have both freely admitted to our fetishization of deadlines, hard-and-fast deadlines seem to be rare in graduate school. It is not uncommon to ask for, and usually receive, extensions on research papers, even theses (though that involves quite a bit more hoop-jumping), and everyone knows at least one perpetually-dissertating Ph.D. I also don't know many graduate students who can set rigid deadlines for themselves and adhere to them without fail.  I don't mean to speak judgmentally of my own ilk, but as a species, humanities graduate students seem to be more comfortable with deadlines of a very flexible nature, and for good reason. Our research is often difficult to demarcate into stages, and hours of work are not easy to estimate because the material we examine is rarely quantitative.\n\nI say all this to preface my surprise at the ease in which the Praxis team was collectively able to create and agree on a workplan, as well as at the relief most team members expressed once tasks were clearly assigned to dates on the whiteboard and to people in GitHub, which is what we'll be using for workflow management. The readings on project management repeatedly emphasized the value of allowing those responsible for certain aspects of the project to set their own deadlines, and so far Sarah and I have been the only ones to push back a deadline we've set (for writing the About page prose, no less, which you would think would be easy for English grad students). The team has consistently met their deadlines and each specialized group has met to set even more specific goals for their work. Prism is coming along, at warp speed!\n\nI'm beginning to see the efficacy of group workplans outside of Praxis, too, and this coincides with [my earlier post](http://www.scholarslab.org/digital-humanities/project-management-and-graduate-training/) about the possibilities for collaborative work available to graduate students who seek them. I am currently one of two representatives for Master's students on the Graduate English Students Association board, and we've organized a thesis work group in response to a request at last semester's department town hall meeting. I expected the group to be very lax, meeting biweekly to check in and sound off, but the group quickly decided to meet weekly, share work frequently, and set rigid deadlines; our task for this week's meeting is to devise a thesis syllabus, a workplan for thesis research for the semester. I was not expecting this level of enthusiasm for deadlines and accountability from a room full of second-year MAs -  many of whom are distracted by job-hunting or, like me, the medieval torture chamber that is Ph.D. program admissions -  but even outside of DH projects, it seems that graduate students _want_  to work collaboratively because being accountable to a group, the joys and shames of having to report your successes and your failures to peers, can be a very powerful motivator.\n"},{"id":"2012-02-19-commentary-on-migrating-an-omeka-site","title":"Commentary on Migrating an Omeka Site","author":"zane-schwarzlose","date":"2012-02-19 13:00:18 -0500","categories":["Digital Humanities","Research and Development"],"url":"commentary-on-migrating-an-omeka-site","content":"This week, for the TEIDisplay project, I migrated an existing Omeka installation to another server for Carin and me to use as our development sandbox. (We could have also locally installed an Omeka collection on our computers, but I wanted to make sure we were always wading through the same river.)\n\nI found some good documentation on [moving installations of Omeka](http://omeka.org/codex/Moving_to_Another_Server) between servers, but I wanted to add some commentary to these instructions. Institutions might use these instructions and commentary when they wish to upgrade servers or change hosts for their Omeka sites.\n\n**First, install a new version of Omeka on your new server.**\nCarin and I needed to complete this step so we were sure we were using the latest version of Omeka. The [steps for installing Omeka](http://omeka.org/codex/Installation) are well documented. I should note that several web hosting companies offer one-click installations of Omeka. This might a preferable option for information professionals lacking technical expertise.\n\n**Second, export your existing database file.**\nIn a moment, we're going to copy all of your files from your old server to your new server. Intuitively, you might think that the database file storing your collections would be moved along with those files. Unfortunately, that is not the case. [Export your database file using these instructions.](http://omeka.org/codex/Backing_up_an_Omeka_Database)\n\nPlease note: these instructions don't remind you that you need to update the host, user name, password, and database name fields in the database file before importing it into your new server. Make these changes.\n\n**Third, import your modified database into your new database on the new server.**\nYou can import these database commands from phpMyAdmin or the command line. It is important to note that if you don't change some of the information described above, your tables will still get created. Things will appear to be happening. This is not the case.Your database will not be able to talk to your new server.\n\n**Fourth, copy all of your files from your previous installation to a hard drive.**\nGet the FTP information for your old server (host name, user, and password.) Use an FTP program to copy these files. Preserve your file structure. Highlight your entire file directory for the site and copy it while preserving the file structure. Moving the files might take some time. (6 hours in my case!) Much of this time is taken by the archival quality image files. Don't worry.\n\nIf your FTP program stops working in the middle of the transfer, don't worry. You might have just disconnected from the Internet for a second. Alternately, some hosting companies throttle your connection if you're taking up too much bandwidth. Just reconnect to your FTP program and restart the transfer where you started.\n\n**Fifth, copy the files on your hard drive to your new server.**\nGet the FTP information for your old server (host name, user, and password.) Highlight the entire file directory for the old site (stored on your hard drive) and copy it to the new site. Be sure to overwrite existing files and folders.\n\nYou now have made a copy of an Omeka installation on a new server. Your user names and passwords will be the same as for the previous installation.\n\nAside from reactivating plug ins and themes, the most prevalent problem I saw in this process were servers write-protecting the database file. This means that the file db.ini in the root directory will remain the default file despite you trying to overwrite it. There are many ways to fix this problem, but you can just delete the default db.ini file from your root directory and then transfer the db.ini from your hard drive.\n"},{"id":"2012-02-21-teaching-coffeescript","title":"Teaching CoffeeScript","author":"wayne-graham","date":"2012-02-21 04:02:47 -0500","categories":["Grad Student Research","Research and Development"],"url":"teaching-coffeescript","content":"As the Prism project has progressed, one of the technologies we kept pushing off teaching was JavaScript. We knew this is one of the core languages that they would need to learn to actually work with the browser, but kept trying to determine the best way to actually introduce JavaScript in a way that would minimize bugs, and not discourage their development efforts. We had been working with the group as a whole introducing them to object oriented thinking with Ruby, but  recognized a huge context switch between writing in Ruby's (and Rails) [object oriented](http://en.wikipedia.org/wiki/Object-oriented_programming) programming style to JavaScript's [prototype-based](http://en.wikipedia.org/wiki/Prototype-based_programming) style.\n\nOne of the other issues we discussed was how to best teach \"good\" practices with JavaScript. The language constructs can get you in to a lot of hot water pretty quickly. You may have seen this, but a classic example of this is in the difference in the side Doug Crockford's _JavaScript: The Good Parts_ and David Flanagan's _JavaScript: The Definitive Guide_.\n\n![](http://img.anongallery.org/img/2/0/javascript-the-good-parts-the-definitive-guide.jpg)\n\nThis is a bit of an absurd comparison, but it does highlight that there is a big discrepancy in what JavaScript is capable of and what it is actually good at. There are just some things that JavaScript implements in a weird way. A crazy example of this is something like the following:\n\n[code lang=\"javascript\"]\n\nconsole.log(++[[]][+[]]+[+[]])\n\n[/code]\n\nIf you guessed this expression evaluates to 10, you are correct.\n\nThere are a variety of other pain points that are common in JavaScript applications; case sensitivity, hard line breaks (they are interpreted as as line-ending semi-colons), extra commas ([IE has fits](http://www.enterprisedojo.com/2010/12/19/beware-the-trailing-comma-of-death/)), global scope creep (e.g. [hoisting](http://www.adequatelygood.com/2010/2/JavaScript-Scoping-and-Hoisting)), and issues with [binding](http://www.alistapart.com/articles/getoutbindingsituations) (e.g. a missing the this modifier). If you have developed much with JavaScript, you have most likely been bitten by several of these issues (hopefully not at the same time).\n\nAs we started planning the project out, we noticed that [@dhh](https://twitter.com/#!/dhh) added support for [CoffeeScript](http://coffeescript.org/) in the version of Rails (3.1+) we were targeting for the Praxis program. As part of the [Asset Pipeline](http://guides.rubyonrails.org/asset_pipeline.html), Rails will automatically convert CoffeeScript to JavaScript, allowing you to write in a format that is closer to Ruby (with some pesky Python whitespace) that compiles to \"good\" JavaScript. Quite honestly, I wasn't sure what to make of CoffeeScript when I first started looking at it. Unlike [SASS](http://sass-lang.com/) which immediately changed the way I write CSS, I couldn't imagine taking nearly a decade's worth of knowledge for developing JavaScript for this crazy scripting language that just compiles down to what I've been writing, and I wasn't alone with the other developers in the R&D group.\n\nSo what exactly was the turning point? Well, quite honestly after working with it a bit, I have to admit it has some nice features, but perhaps the biggest draw is the fact that it doesn't require a big context switch in different modes of development. The other big draw is that the JavaScript that is generated is a better quality ([uses the module pattern](http://www.adequatelygood.com/2010/3/JavaScript-Module-Pattern-In-Depth)), has [splats](http://coffeescript.org/#splats), [comprehensions](http://coffeescript.org/#loops), takes care of scoping, the fat arrow (=>) can evaluate the '[this](http://www.digital-web.com/articles/scope_in_javascript/)' keyword properly for function binding, and the [existential operator](http://coffeescript.org/#operators). There has been a lot of hype behind this, and Sam Stephenson (37Signals) made a great argument for using the language at the Future of Web Apps in London (2011).\n\n\n\n[Better JS with CoffeeScript - Sam Stephenson (37signals)](http://vimeo.com/35258313) from [Louise Morgan](http://vimeo.com/user10059996) on [Vimeo](http://vimeo.com).\n\nEric has walked the group through an [introduction to JavaScript and CoffeeScript](http://praxis.scholarslab.org/topics/intro-to-javascript/), and they seemed receptive to the idea of a language that compiles to JavaScript. So far, nothing has been written for the project in CoffeeScript, but I'm cautiously optimistic that the JavaScript that will drive most of the interactions will be a bit less buggy (and faster to develop) than it would had we targeted JavaScript for this project.\n\nIf you need even more incentive to take a look at CoffeeScript, be sure to take a look at Gary Bernhardt's lightening talk from [CodeMash](http://codemash.org/) 2012 entitled [Wat](https://www.destroyallsoftware.com/talks/wat).\n"},{"id":"2012-02-21-the-models-are-done","title":"The Models are Done!","author":"annie-swafford","date":"2012-02-21 09:23:49 -0500","categories":["Grad Student Research"],"url":"the-models-are-done","content":"Great news! All parts of our data model are now in Rails!  We used the Ruby gem Devise for the user model, and Prism now has user account capabilities (and the links for \"sign in,\" \"sign out,\" and \"sign up\" on the homepage)! The documents are also in the system and each has its own page, and we have also created the Markings, Facets, and Prism models so we're now set up to start building the highlighting functionality.  All of our tests pass, and apparently 98% of our code has tests written for it (thanks to Wayne for incorporating the Ruby gem Simplecov to give us this information), so we're in good shape.  After Eric's [helpful Coffee Scripting session](http://praxis.scholarslab.org/topics/intro-to-javascript/), I've been watching lots of screencasts to make sure I feel comfortable writing Coffee Script myself. At this point, I know how to convert Javascript to Coffee Script, but I'm not adept at writing it myself from scratch.  I've been brainstorming algorithms for the highlighting recently, and I hope to start trying to program them over the next week.  I'll write about my foray into [Coffee Scripting land](http://www.scholarslab.org/slab-code/teaching-coffeescript/) next week!\n"},{"id":"2012-02-22-how-do-you-display-tei-documents-online","title":"How do you display TEI documents online?","author":"carin-yavorcik","date":"2012-02-22 10:05:23 -0500","categories":["Digital Humanities"],"url":"how-do-you-display-tei-documents-online","content":"As you can see from [Zane’s post a few days ago](http://www.scholarslab.org/tei/commentary-on-migrating-an-omeka-site/), we’ve been hard at work on our update to [TEIDisplay](http://omeka.org/codex/Plugins/TeiDisplay) here at UT-Austin. While he’s been working to jive the plugin with the newest version of Omeka, I’ve been thinking more about how this tool will be used. While I’ve studied TEI and used it in a few distinct scenarios, I’m still a newcomer to the world of digital humanities. In order to make the best product possible, we need more feedback from our potential users – you! I’ve put up a post over at [DH Answers](http://digitalhumanities.org/answers/) to learn more about what TEI display tools (if any) people are using, and what features are most important. [Please join the conversation by giving us your feedback!](http://digitalhumanities.org/answers/topic/how-do-you-display-tei-documents-online?replies=1#post-1525)\n"},{"id":"2012-02-23-slightly-better-than-brain-dead","title":"Slightly Better than Brain Dead  ","author":"ed-triplett","date":"2012-02-23 08:24:06 -0500","categories":["Digital Humanities","Grad Student Research","Research and Development"],"url":"slightly-better-than-brain-dead","content":"This post was inevitable, but I can’t say I was ready for it. In exactly one week, I will be leaving for Spain on a two-month dissertation research trip. In relation to this blog, this means that my time in the Praxis Program has all but come to a close. As such, this seems like the best time to mercilessly cram my scattered experiences into a box marked “conclusion” and hope it doesn’t explode before I can tell the police that “I was in Barcelona at the time.”  \n\n  \n\nI had a moment after the winter break to reflect on how bizarre the Fall semester had been for me. Between my Scholar’s Lab fellowship, the Praxis Program, and my time just 20 vertical feet below the Scholar’s Lab conference room at IATH, my mind was kidnapped by digital humanities. I can think of no semester where I was tossed out of my wheelhouse on such a regular basis.  \n\n  \n\nWhen I returned to my rotating office at Alderman Library after an equally bizarre, warm winter in Fargo, ND my previous schedule resumed... but everything felt different. Two hulking objects loomed out of the fog: Spain and Prism. Predictably, Spain got raucous at night and gave me heartburn thinking about it. Fortunately, I could still join in on the practical attack on Prism with my fellow Praxis-ers during the day while Spain was taking a siesta. Our meetings were held in the same place each Tuesday, but in the second semester, It felt more like a boardroom than a classroom. For everyone in the program, a lot of the intellectual debates that stymied earlier meetings were dropped in favor of getting something done.  \n\n  \n\nReal due-dates meant real design and programming assignments. Of course those dates had a slightly different effect on me because I knew I would be gone by Feb. 29th, and I would miss a lot of the process leading up to the release of Prism. Nonetheless, I immediately decided that I needed to at least put one foot back in my wheelhouse if I was going to leave some tangible imprint on Prism. I therefore devoted the last three weeks to creating detailed “full color” wire-frames of Prism’s website using Adobe Illustrator.   \n\n  \n\nLike with the logo, I had some missteps and experiments along the way that I would not repeat in the future. At one point - in an ill-conceived attempt to heed the many notes about the site’s color scheme - I dove head-first into “design by committee” by asking the group to offer color-scheme samples. On a related note, Brooke recently reminded me of a saying that my Dad is fond of: “A camel is a horse designed by committee.” At the suggestion of several of the SLab staff, I happily chucked the idea and simply presented a triadic color scheme that had sufficient tonal contrast for the different highlighters, yet would not cause us to imagine a poor kid’s box of crayons.  \n\n  \n\nLike most design processes, the five pages I presented to the group yesterday would change a good bit if I had more time. I am reasonably happy with the header, and I believe the rotating backgrounds will be distinctive, but I would love to spend some more time working on the site’s buttons and text boxes. I’d also love to help Lindsay and the others on the design team transform the static illustrator file into a functional page in HTML and CSS.  \n\n  \n\nThese reservations are true for the entire process. I am disappointed not to be able to see how the data will be visualized while I am gone, and I would have liked to see first hand how Ruby, Rails, CSS, HTML, Coffeescript, and YML fit together. I never even got a chance to sneak some Hungarian erotic poetry after the “en” in Alex’s YML file just to see what he would do with it. Oh well.  \n\n  \n\nIt is probably better this way because I think Spain is waking from its siesta. I’m still juggling several responsibilities right now, but I feel like one my four tennis balls has been replaced with an anvil stamped “Hecho en España.” Prism is in good hands, and I think my brain is beginning to show 505 errors anyway. As two members of Prism graciously tweeted yesterday, while trying to explain a visualization idea I had, I confidently uttered the phrase “That word will be the size of Yellow!” I think it is time for me to re-introduce both hemispheres of my brain to each other and focus on Spain. Finally, as some of you were around to hear, I am leaving on a high note - yesterday Bethany referred to my “size of yellow” idea as the “slightly less brain-dead” of three possible options! You hear that? Ba-BAM!  \n\n  \n\n- Incidentally, I am going to try to blog as I travel to 20 different fortresses, monasteries and medieval towns from Catalonia to Extremadura during March and April. The extremely humble URL is here: [http://www.edwardtriplett.com/](http://www.edwardtriplett.com/)\n"},{"id":"2012-02-28-hwaet","title":"Hwaet!","author":"eric-johnson","date":"2012-02-28 05:03:43 -0500","categories":["Announcements"],"url":"hwaet","content":"Researchers here at the Scholars' Lab recently unearthed a manuscript fragment, dating roughly to the 9th or 10 centuries CE, which we here publish for you in rough translation.  It seems to address preparation for some sort of epic conflict.\n\n\n<blockquote>Lo! By the bright-shining beach at Alderman-sea\nA clarion-call crested the coursing ocean-horses\nFrom Beþienni, duty-drover, DH-thane,\nTo defeat the doughty foe, the Day-table,\nAge-breaker, stress-maker, old-conductor.\nConstant companions cry-heeded and came,\nTroop-leaders the two, trusted and able,\nGold-bearded and glass-eyed, girded and battle-wise.\nFirst the sunset-settled, steel-structured\nWaenbatte, code-conqueror and calculator-kin.\nHis double, dawn-dweller Aeric,\nBlue-eyed bridge-builder and feeling-hugger,\nSchedule-sack on his shoulders not over-full\nWith kinder-kith of the calendar-foe:\nA mead-room of the mind, moving-chaired and floating-floored,\nA learning-place for library-kind.\nAlso a seeker-space to supply apt answers\nTo aid the askers of helping-queries.\nAlso the thinking-plan of the tinker-space,\nGrowing to a gregarious maker-hall\nSubject of singers’ songs and far-flung fame.\nAlso signal fires for SLab-friends,\nTaken with our teaching-times . . .</blockquote>\n\n\nAnd here the fragment ends.  But we continue to comb the shelves of SLab Special Collections to see if other pieces of the manuscript might yet be found.\n"},{"id":"2012-03-02-customizing-bash","title":"Customizing Bash","author":"wayne-graham","date":"2012-03-02 06:18:36 -0500","categories":["Research and Development"],"url":"customizing-bash","content":"I spend a lot of time every day looking at a terminal window, and over the last decade I had been tweaking my bash profile to make the terminal act, and look, the way I wanted it to. As a systems administrator in a former life, I had collected a bunch of \"useful\" scripts that would help me work on a variety of operating systems, from [Solaris](http://en.wikipedia.org/wiki/Solaris_(operating_system)), to [AIX](http://www-03.ibm.com/systems/power/software/aix/index.html), to [SGI](http://www.sgi.com/),  as well as various flavors of Linux ([CentOS](http://www.centos.org/), [Fedora](http://fedoraproject.org/), [Ubuntu](http://www.ubuntu.com/), [SUSE](http://www.suse.com/), [Gentoo](http://www.gentoo.org/), etc.). I had aliases to commands (and my common typos) to log on to servers, control various services in my local development environment, and override common commands I typed all the time (e.g. ss for rails server) in my .bashrc which was [symlinked](http://en.wikipedia.org/wiki/Symbolic_link) to my .bash_profile due to an OS X quirk. This has resulted in a really long bashrc file (nearly 2000 lines long), and I wanted to take some time to clean things and get rid of a lot of the legacy cruft that was in that file.\n\nIn my personal workflow, I try to stay in the terminal as much as I can, finding the cognitive shift from a text-based environment to a GUI rather jarring. Imagine that; I connect my MacBook Pro with its two graphics cards to a 24\" monitor, yet most of my \"work\" is done in a terminal (I do use the graphic card for its [GPU](http://en.wikipedia.org/wiki/Graphics_processing_unit) regularly though). Since I spend so much time in the terminal, I not only wanted to get rid of the cruft, I wanted to do a bit more to make the environment look as good as this computer performs.\n\nI had been intrigued by the [oh-my-zsh](https://github.com/robbyrussell/oh-my-zsh) approach of bundling some themes and plugins. While I have used [Zshell](http://www.zsh.org/) in the past, I never quite found the tab-completion, auto-correction, or the improvements to the scripting language compelling enough to make the move from [Bash](http://en.wikipedia.org/wiki/Bash_(Unix_shell)). However, being able to share a history across sessions and the built-in [pager and globbing features ](http://friedcpu.wordpress.com/2007/07/24/zsh-the-last-shell-youll-ever-need/)almost got me there!\n\nApparently other folks were in the same boat I was in. I ran across the [bash-it](https://github.com/revans/bash-it) project a few weeks ago and finally had an opportunity recently to try this out. Basically what this project does is provide a framework for you to build your own themes, plugins, and aliases for your own environment. The installation is straight-forward on OS X (and every Linux box I've tried this on):\n\n[code lang=\"bash\"]\n\ngit clone http://github.com/revans/bash-it.git ~/.bash_it\n~/.bash_it/install.sh\n\n[/code]\n\nThese two lines clone the bash-it repository to a hidden directory (.bash_it) for your user account (/Users/[your user name]/.bash_it) then launches the installer script. Using hidden directories (.bash_it) hides the directory from Finder, but still gives you access to the directory with the Terminal app. The installation script backs up your current ~/.bash_profile file (if you have made any adjustments to it), then prompts you for the features you want. I answered 'some' to most of the questions in the installation script to choose which plugins and aliases you would like to enable. I don't use [emacs](http://www.gnu.org/software/emacs/tour/) or [nginx](http://wiki.nginx.org/Main) on a regular basis, and use [rvm](http://beginrescueend.com/) over [rbenv](https://github.com/sstephenson/rbenv), and it turns out the [xterm](http://en.wikipedia.org/wiki/Xterm) plugin causes some issues on OS X, so I left those out.\n\n\n# Themes\n\n\nThe default theme is named Bobby and looks really nice. It uses [solarized](http://ethanschoonover.com/solarized) colors and had one feature I really like: multiline feedback. On one line, I know which Ruby version I have active in RVM, the server I'm on, and where I am on the system. On the second line, I know which git branch I'm on, and if there are uncommitted changes (red x if there are changes, green check if everything is committed).\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/02/screen-300x156.png)](http://www.scholarslab.org/dh-developer/customizing-bash/attachment/screen-2/)\n\n\n# Aliases\n\n\nI had quite a few aliases in my .bash_profile that I had created over the years. There were, however, some nice additional aliases included in the bash-it package to shorten some typing of commands. They included some common mispellings (yes, that was on purpose); most of the git aliases I had been using anyway, some useful aliases for heroku, homebrew (yeah bup), for opening various applications like Firefox, Photoshop, and Chromium, and some nice features for [todo-txt](http://todotxt.com/).\n\n\n# Fonts\n\n\nThis isn't something that the bash-it library deals with, but important in customizing the experience. The default font for the Terminal in OS X is [Menlo](http://9-bits.com/post/123940811/menlo-font-macosx). It's a fine system font, but can get a little difficult to read at the distance I sit from my monitor. A really popular font for developers is [Inconsolata](http://levien.com/type/myfonts/inconsolata.html). I changed my font to Inconsolata 14pt, and it has a very nice look to it.\n\n\n# Terminal\n\n\nIn Snow Leopard, the Terminal app doesn't support 256-bit colors. Apple has updated Terminal  in Lion to support this, but I have not yet upgraded to Lion. Quite honestly, the Lion machines I have dealt with have had 'issues' getting the tools I use on a regular basis installed, and I have just not had the time to deal with upgrading yet (most of the issues involve issues with XCode's removal of [gcc](http://en.wikipedia.org/wiki/GNU_Compiler_Collection) in favor of [llvm](http://llvm.org/). There is [a good work around](https://github.com/kennethreitz/osx-gcc-installer), and I believe the issue has been resolved in recent updates to XCode).\n\nThere is, however, an awesome Terminal app replacement named [iTerm2](http://www.iterm2.com/) that has a lot of the features I want; 256-bit color support, full-screen mode, ability to split the screens, and hot keys (please don't make me click when I can type). After installing iTerm2, I can now run the tests for [prism](https://github.com/scholarslab/prism) and get all the [NyanCat rainbow ](https://github.com/mattsears/nyan-cat-formatter)awesomeness.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/03/tests-300x147.png)](http://www.scholarslab.org/dh-developer/customizing-bash/attachment/tests/)\n\nAfter updating Terminal, installing the bash-it themes and plugins, and getting a 'better' font on my machine (and adding a few aliases back in to the ~/.bash_profile to log on to some servers), I used scp to push these files to the various server environments I work on (replace user and server):\n\n[code lang=\"bash\"]\n\nscp -r ~/.bash_it user@server:~/\n\nscp ~/.bash_profile user@server:~/.bash_profile\n\n[/code]\n\nOverall, I've been happy with the move to this setup. It plays nicely with some of the other cool things I use (e.g. [pianobar](http://6xq.net/projects/pianobar/), [tmux](http://tmux.sourceforge.net/), and [vim](http://www.vim.org/)). With some cleaver key bindings, and a transparent terminal, I can actually change something on my screen and see the update in the browser without changing programs,\n\nNext up? I think I may write either an [rsync](http://en.wikipedia.org/wiki/Rsync) script that will push any local changes to the various servers I use, or maybe even use the Dropbox client on Linux to symlink these files in, ensuring as soon as I make a change on my local development environment, they will be on the remote systems as well.\n"},{"id":"2012-03-08-scholars-lab-speaker-kathleen-fitzpatrick","title":"Scholars' Lab Speaker: Kathleen Fitzpatrick","author":"ronda-grizzle","date":"2012-03-08 11:04:30 -0500","categories":["Announcements"],"url":"scholars-lab-speaker-kathleen-fitzpatrick","content":"We are pleased to welcome Dr. Kathleen Fitzpatrick to the Scholars' Lab on Thursday, March 15 at 2:00 p.m. for the second event in our Spring 2012 Speaker Series.\n\n\n<blockquote>What if the academic monograph is a dying form? If scholarly communication is to have a future, it's clear that it lies online, and yet the most significant obstacles to such a transformation are not technological, but instead social and institutional. How must the academy and the scholars that comprise it change their ways of thinking in order for digital scholarly publishing to become a viable alternative to the university press book?</blockquote>\n\n\nDr. Fitzpatrick's talk, entitled \"Planned Obsolescence: Publishing, Technology, and the Future of the Academy,\" will explore some of those changes and their implications for our lives as scholars and our work within universities.\n\nA long time friend of the Scholars' Lab, Dr. Fitzpatrick is Director of Scholarly Communication of the Modern Language Association, and Professor of Media Studies (on leave), Pomona College. She is author of _Planned Obsolescence: Publishing, Technology, and the Future of the Academy_, published in 2011 by NYU Press and previously made available for open peer review online ([http://mediacommons.futureofthebook.org/mcpress/plannedobsolescence](http://mediacommons.futureofthebook.org/mcpress/plannedobsolescence)), and of _The Anxiety of Obsolescence: The American Novel in the Age of Television_, published in 2006 by Vanderbilt University Press. She is co-founder of the digital scholarly network MediaCommons ([http://mediacommons.futureofthebook.org](http://mediacommons.futureofthebook.org/)), and has published articles and notes in journals including the _Journal of Electronic Publishing_, _PMLA_, _Contemporary Literature_, and _Cinema Journal_.\n"},{"id":"2012-03-12-seeking-praxis-program-fellows","title":"Seeking new Praxis Program fellows!","author":"bethany-nowviskie","date":"2012-03-12 09:06:49 -0400","categories":["Announcements","Grad Student Research"],"url":"seeking-praxis-program-fellows","content":"Last August, [we announced](http://www.scholarslab.org/praxis-program/announcing-the-praxis-program/) a new, pilot initiative for the Scholars' Lab -- the creation of a [Praxis Program](http://praxis.scholarslab.org), which would offer hands-on training to six UVa graduate students in digital humanities project creation, taking them from conception to software and interface design and development, through to deployment, communications, and analysis. Praxis is a unique training program in the international digital humanities community. The most notable thing about it was our plan to involve students as a team -- a cohort, who would intern with Scholars' Lab faculty and staff, [blog about their experiences](http://www.scholarslab.org/category/praxis-program/), and work to develop the so-called \"softer skills\" of collaboration and project management even as they tackled (most for the first time) new programming languages, software tools, and digital methods.\n\nThe [2011-12 Praxis cohort](http://praxis.scholarslab.org/people.html) is in full swing, and -- thanks to the generosity of the [Andrew W. Mellon Foundation](http://mellon.org) and the support of UVa Library's [Scholarly Communication Institute](http://uvasci.org/current-work/) (SCI) -- we are able to undertake a second pilot year of this innovative program, even while we work to explore the creation of an international Praxis Network, involving like-minded initatives that address the changing needs of humanities graduate students in the digital age. (The program itself and its SCI context are are the subject of two recent _Inside UVa_ articles: [Praxis Program Gives Future Scholars Crash Course in Digital Humanities Skills](http://www.virginia.edu/uvatoday/newsRelease.php?id=16214) and [Mellon Grant Extension Boosts Digital Humanities Graduate Training Program](http://www.virginia.edu/uvatoday/newsRelease.php?id=17359).)\n\nSix new Praxis students will begin working with us in late August. Each will be awarded _$8000 in fellowship funds_ to offset the commitment of time (generally about 10 hours per week, through May 2013) they will make to living, learning, and building a collaborative digital humanities project in the Scholars' Lab.\n\nAll _University of Virginia graduate students_ working in or committed to humanities disciplines are eligible to apply to join the 2012-13 cohort. We particularly encourage applications from women, LGBT students, and people of color, and will be working to put together an interdisciplinary and intellectually diverse team.\n\n**_Luncheon Q&A_, Wednesday, March 21st**\nPlease join Scholars' Lab staff and current Praxis Fellows on Wednesday, March 21st for a Q&A session on the program, to be held in the Scholars' Lab at 12:00 p.m. Lunch will be provided, so RSVP to Ronda Grizzle, Outreach & Training Specialist (rag9b at virginia dot edu) by noon on Monday, March 19. If you're unable to join us for lunch but have questions about the program, don't hesitate to be in touch with Eric Johnson, Head of Outreach & Public Services (ej9k at virginia dot edu).\n\n**_Application deadline_: Sunday, March 25th**\nThe application process is simple: direct an email to Eric Johnson at the address above. Please indicate why you're interested in the [Praxis Program](http://praxis.scholarslab.org/), what you think you will gain from it, and what you feel you would bring to a collaborative digital humanities project. A small committee, consisting of Scholars' Lab faculty and staff and members of the 2012-13 Praxis cohort, will evaluate expressions of interest and schedule group or individual interviews with finalists.\n"},{"id":"2012-03-12-through-another-prism","title":"through another prism","author":"alex-gil","date":"2012-03-12 17:56:14 -0400","categories":["Grad Student Research","Research and Development"],"url":"through-another-prism","content":"A couple of weeks ago Suzanne Keen and Alison Booth offered [a workshop at the Scholars' Lab](http://www.iath.virginia.edu/news/news_2012_02_08_s53.html). The workshop was an introduction to BESS (Biographical Elements and Structure Schema), \"an XML standoff markup schema designed at IATH as part of Professor Booth’s IATH Fellowship to analyze narrative structure.\" If you recall from [Bethany's introduction](http://www.scholarslab.org/digital-humanities/crowdsourcing-interpretation/), the original idea for Prism was inspired in part by recent discussions with Alison about her project.\n\nThe workshop/talk was divided into two parts, each related to our Prism in one way or another. In the first part, we were broken up into four groups, each with a small biography of a woman from [Collective Biographies of Women](http://womensbios.lib.virginia.edu/). The biographies were printed on copy paper with each paragraph clearly enumerated. Our task was to work as a team to assign one or more of six basic narratological categories separating the Story from the Narrative to each paragraph. The categories roughly corresponded to how one would linearize a person's life: a)Before they lived, b) The beginning, c)The middle (career period), d) A climax (some kind of breakthrough), e) The end and, of course, d) The post-life. Me and Bethany ended up in the same team, the [Sister Dora](http://womensbios.lib.virginia.edu/featured?id=SISTER_DORA) team. (Sister Dora, if you haven't heard about her, was the nurse in whose arms you probably would've died in the 19th Century).  We were struck right away by the differences from our prism exercises. Not only were we assigning categories by paragraphs, we had to come to consensus as teams! The team consensus gives the exercise a much different dynamic as you can imagine.\n\nThe second part of the exercise was closer to our model. Each of us was given four paragraphs in a sheet of paper, a transparency and four markers. Sound familiar? We each could only pick one category out of four. I remember I chose _figures of speech_. I don't remember the other three categories, but I do remember being surprised to find only a very small number of figures of speech in the sample text. Again, this exercise was very different than our prism. While we give users access to all categories, in this case users are only allowed one category. After we were all done marking up the texts, Ms. Keen collected the transparencies.\n\nAt this point she started doing something really neat which I think we are going to emulate in our visualization. She laid one transparency on the projector. She then laid another one on top of the first, ever so slowly. The 'animation' effect was very cool to watch, as all the different transparencies started shifting the direction and overall effect of the whole.\n\nAfter the workshop we had time to talk about the goal of the BESS project and how it relates to the transparency and team exercises. BESS consists of a standard set of narratological 'tags' for marking up biographies of women. The list Prof. Booth showed us was very long. Teams of students are assigned the task of marking up the texts using these categories. Of course, there will be much disagreement, but the goal is to find those places were different encoders agree as a tool to help the editors decide what the 'final' markup should be. This is a world apart from our goals, but does provide another possible use case for the tool we are actually building.   \n"},{"id":"2012-03-12-who-says-i-like-right-angles","title":"Who says I like right angles?","author":"lindsay-o’connor","date":"2012-03-12 14:54:45 -0400","categories":["Grad Student Research","Research and Development"],"url":"who-says-i-like-right-angles","content":"Last week, I started tackling what I naively assume to be some CSS issues that “real” web designers might also see as challenges.  The “box model” in CSS allows for lots of “clean” designs, but it discriminates against non-quadrilateral polygons, and against angles other than 90 degrees, for that matter. When a prism is a guiding concept and image for your design, you probably don’t just want a bunch of rectangles, and accordingly, Ed’s designs for the Prism site involve a big “prism” in the background and a number of boxes and links that invoke the prism shape with their borderlines. So, with Ani DiFranco's voice in my head, I asked the internet what to do. A few online sources for web design advice suggested creating angles and arrow shapes with empty divs with thick borders and no width and height, so I gave that a try today. It seemed strange when I first looked at it, but it turned out to be simple enough.\n\n![](https://lh4.googleusercontent.com/-AdpkkUhIrhA/T15W6Yk43WI/AAAAAAAABLc/QFQV1zfBj0g/s312/arrows-preview.jpg)\n\nIn what appears above, I tinkered with the border size of two empty spans (I used spans instead of divs since they happen to fall inside of list items on this page) until they lined up with the borders on the “highlight” and “visualize” links. I assigned each span a class and styled them like this:\n\n\n.arrow-right {  \n\nfloat: left;  \n\nwidth: 0;  \n\nheight: 0;  \n\nborder-top: 11px solid transparent;  \n\nborder-bottom: 11px solid transparent;  \n\nborder-left: 22px solid #C0C0C0;  \n\n}  \n\n\n\narrow-inset {   \n\nfloat: left;  \n\nwidth: 0;  \n\nheight: 0;  \n\nborder-top: 11px solid #C0C0C0;  \n\nborder-bottom: 11px solid #C0C0C0;  \n\nborder-left: 22px solid transparent;  \n\n}\n\n\n\nOf course, there are a few big limitations here. For one, these spans have such a precise, absolute size that resizing the browser window even the slightest bit will reveal how messy of a solution this really is. And as Jeremy pointed out, the link ends at the edge of its rectangular container, so clicking in the “arrow” that seems attached to it won’t take you anywhere. To solve these problems, Jeremy suggested that I create a background image of an angle for each of the links. It will be a scalable part of the link rather than an object next to it, so the design will look better in different window-sizes and the interface will be more logical and easier to use. Figuring this out will also help me as I work on the large prism shape at the top of the page since it will also need an absolute position behind other elements.\n\nI’ve also been working on gradients in CSS, specifically on how best to layer gradients to produce the effects in Ed’s header design.  More on the header next time.\n"},{"id":"2012-03-13-gis-day-2011-lightning-rounds","title":"GIS Day 2011 Lightning Rounds","author":"ronda-grizzle","date":"2012-03-13 11:12:48 -0400","categories":["Podcasts"],"url":"gis-day-2011-lightning-rounds","content":"**GIS Day 2011 Lightning Rounds**\n\nOn November 16, 2011, the SLab welcomed a diverse group of speakers presenting lightning round talks as part of our [GIS Day](http://www.gisday.com/) celebrations. Eighteen of those speakers agreed to have their talks appear on our podcast, which we're pleased to present.\n\nSpeakers:\n**Kelly Johnston**, UVa Scholars' Lab\n**Bill Ferster**, UVa SHANTI\n**Simona Babiceanu**, UVa Civil & Environmental Engineering\n**Paolo Tovar**, Apex Wind Energy\n**Tim Morton**, UVa Library Government Documents\n**Randi Lewis**, UVa Department of History (and Scholar's Lab Graduate Fellow!)\n**David McClure**, UVa Scholars' Lab\n**Charles Kromkowski**, UVa Department of Politics\n**John Scrivani**, Virginia Geographic Information Network\n**Ed Triplett**, UVa Architectural History (and Scholars' Lab Graduate Fellow!)\n**Todd Wascher**, Applied Data Consultants, Inc.\n**Kelly Clifton and Larry Buckner**, UVa Department of Politics\n**Bill Palmer**, UVa Office of the Architect\n**Guoping Huang**, UVa Urban and Environmental Planning\n**Wayne Graham**, UVa Scholars' Lab\n**Pam DeGuzman**, UVa Nursing\n**Chris Gist**, UVa Scholars' Lab\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.13992627191/enclosure.mp3\"]\n"},{"id":"2012-03-13-lets-get-visual","title":"Let's get visual. ","author":"brooke-lestock","date":"2012-03-13 14:04:10 -0400","categories":["Digital Humanities","Grad Student Research","Research and Development","Visualization and Data Mining"],"url":"lets-get-visual","content":"I am aware of how ridiculous the title of this post is, but I'll gloss it by saying that visualizations have been a hot button issue in our recent Praxis talks, and in my opinion, they're by far the \"sexiest\" element of Prism. After all, the viz page is where the magic happens.\n\nThat being said, as our deadlines become more and more imminent and ominous, we are constantly running into that pesky thing called reality, and it recently even dared to interfere with our precious visualizations. At our 2/21 meeting, we brainstormed the kinds of visualizations we would like Prism to create and narrowed the list down to four possible types:\n\n\n\n\t\n  * A heat map, which would be closest to the transparency exercise in that it resembles the layering of many transparencies, but presents some problems, such as how to represent something legible and provocative with muddled colors.\n\n\t\n  * Quantitative visualizations, like bar graphs, charts, or tables, which we all agreed would be interesting, but there were some reservations about being that quantitative in our first visualization. We are humanists, after all.\n\n\t\n  * A zoomed-out, kind of macro view visualization, and here we used[ Ben Fry's \"On the Origin of Species: The Preservation of Favoured Traces\"](http://benfry.com/traces/) as a model. The most compelling aspects of this are its easy navigation of both \"distant\" and \"close\" readings, and [as Alex mentioned in his post](http://www.scholarslab.org/praxis-program/through-another-prism/), the possibility for animating the layering of readings so that it resembles the process of compiling transparencies one by one.\n\n\t\n  * The last is a kind of deformed reading resembling a word cloud, in which the font size of a word would change based on how often it was selected in a particular color, but the structure of the text would remain intact - that is, the words would not be processed into a cloud shape, but rather stay in the same order on the page. David McClure was the creative mind behind this idea and we collectively agreed that this  visualization was our first choice for Prism.\n\n\nThe original idea for the \"deformed reading\" visualization (we really need to come up with a better name for this...Prism-enstein?) involved layering each deformed reading in each color to represent all of the highlighter colors in one visualization while somehow being offset enough to remain legible and coherent. Then the reality check came, at last week's meeting. With the deadlines fast approaching, the visualization we chose was just too ambitious. So today we sat down and hashed it out - we revisited the workplan, which we established at the beginning of this semester, and the essential requirements, the goals we set for Prism all the way back in September, to determine what we can realistically accomplish in the time left that would meet our original requirements for a successful project. The group agreed that having Prism produce one visualization is necessary, so we came to a compromise with reality. We opted for a somewhat simpler version of the chosen visualization and discussed how we can gesture towards future possibilities for Prism visualization.\n\nI'm hoping our compromise is enough to appease reality for a little while, because when all's said and done, what's a prism without its rainbow? Just a clear pyramid...then again, I guess building a pyramid is pretty impressive, too.\n"},{"id":"2012-03-13-narrative-form-and-digital-tools","title":"Narrative Form and Digital Tools","author":"ronda-grizzle","date":"2012-03-13 11:27:22 -0400","categories":["Podcasts"],"url":"narrative-form-and-digital-tools","content":"**Suzanne Keen & Alison Booth: Narrative Form and Digital Tools Workshop**\n\nThe Scholars' Lab welcomed Dr. Suzanne Keen, Thomas H. Broadus Professor and Chair of English at Washington and Lee University, and Dr. Alison Booth, Professor of English at U.Va., on February 24, 2012 for the presentation of their workshop \"Narrative Form and Digital Tools.\"\n\nWorkshop Abstract:\n\"This workshop suggests that teams of humans can be trained to analyze narrative structure, rhetoric, and other genre conventions using controlled vocabularies—a longstanding dream of narratology. Drs. Keen and Booth will introduce BESS (Biographies Elements and Structure Schema), an XML standoff markup schema designed to analyze narrative structure in short biographies, and will offer examples of its application as a tool for interpreting narratives in large archives, within social networks, and beyond traditional formalism.\"\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.14016483491/enclosure.mp3\"]\n"},{"id":"2012-03-19-a-cukeybara-sandwich","title":"a cukeybara sandwich","author":"alex-gil","date":"2012-03-19 10:55:16 -0400","categories":["Grad Student Research"],"url":"a-cukeybara-sandwich","content":"During Spring Break I started working on integration tests for Prism. An [integration test](http://en.wikipedia.org/wiki/Integration_testing) is a bit more involved than the regular [unit tests](http://en.wikipedia.org/wiki/Unit_testing) we've been doing with the RSpec framework so far. An integration test seeks to test the software by imitating a typical user interacting with it. In particular, we wanted to make sure that all the different translations from the i18n process were working. In order to get it right, we needed to emulate a user visiting the homepage in different languages.\n\nFor Rails development, the tools of choice for this kind of testing are [Cucumber ](http://cukes.info/)and [Capybara](https://github.com/jnicklas/capybara). Cucumber is a language, a framework and a workflow if I understand it correctly. It is much more human readable than RSpec (even though it borrows some RSpec for some tasks), which makes ideal to share with non-developers in the team. Here, for example, is the .feature file I created to test i18n:\n\n\n    \n    \n    Feature: Check language\n      In order to test the selected language is right\n      As a cosmopolitan developer\n      I want to make sure the right words are present in the header\n    \n      Scenario: English pages\n        When I visit the \"English\" homepage\n        Then the header should have the words 'Prism is'\n    \n      Scenario: French pages\n        When I visit the \"French\" homepage\n        Then the header should have the words 'Prism est'\n    \n      Scenario: Spanish pages\n        When I visit the \"Spanish\" homepage\n        Then the header should have the words 'Prism es'\n    \n      Scenario: German pages\n        When I visit the \"German\" homepage\n        Then the header should have the words 'Prism ist'\n    \n\n\n\nOnce a feature file has been created, the next step is to 'translate' the natural language into 'step definitions.' Line by line, we convert the .feature file to Ruby... actually, to Capybara. Capybara is a Ruby gem that helps us imitate the behavior of users visiting a page by sending GET and POST requests if I understand it correctly. Below is our sample .rb 'step definition' file translating the original feature into ruby actions.\n\n    \n    \n    When /^I visit the \"([^\"]*)\" homepage$/ do |lang|\n      if lang == 'French'\n        locale = 'fr'\n      elsif lang == 'English'\n        locale = 'en'\n      elsif lang == 'Spanish'\n        locale = 'es'\n      elsif lang == 'German'\n        locale = 'de'\n      end\n    \n      visit('/' + \"?locale=\" + locale)\n    end\n    \n    Then /^the header should have the words ([a-z]*|[A-Z]*)/ do |desc|\n      page.should have_content(desc)\n    end\n    \n\n\n\nOnce the feature descriptions and the step definitions are in place, we run the test by simply invoking cucumber from the command line. I am happy to announce that after running the tests above, Prism is in the GREEN. I repeat, Prism is in the GREEN.\n"},{"id":"2012-03-19-wordpress-pharma-hack","title":"WordPress Pharma Hack","author":"wayne-graham","date":"2012-03-19 06:04:42 -0400","categories":["Research and Development"],"url":"wordpress-pharma-hack","content":"A few weeks ago I was walking to my car and was copied on an email from the director of the IT department for the UVa Library:\n\n\n<blockquote>It appears that website redacted has been commandeered…hacked.. Beware if you use windows as there is a malware service running….</blockquote>\n\n\nJust the kind of email you love to get! By the time I got home (takes me about an hour and a half), there had been a flurry of emails between the company that had a support contract for the site, and various people who were reacting. By the time I actually had a chance to sit down, the site URL had redirected the site to UVa's home page as a stop-gap measure to not expose the site's users to malware.\n\nI started taking a look at what had happened and apparently the hackers had exploited Wordpress by posting a comment in such a way that Wordpress wrote a file to the filesystem, when then injected code (in base64) everywhere it found the string '`<?php`'. Ah yes, pharma hack. It would be brilliant if it weren't so diabolical. Some clarification on what is actually going on here. There are a few levels of compromise that systems administrators worry about; web defacement and server compromise. The latter is really serious, but outward facing defacement is really really the Internet equivalent of someone tagging your fence. What these hacks are designed to do is abscond with your Internet search results. This is quite a different attack vector than say brute-forcing passwords on a Wordpress site. If you know a little about what you're doing, this is actually pretty straight forward. In fact, you can script these things pretty easily; this example was written by a hacker over a weekend:\n\n\n\nWith hacks like this, attackers usually have much darker intentions (e.g. there's a high probability you use the same password for Work/Facebook/Twitter/Google as you do for your Wordpress).\n\nLet me take a second here to briefly pause say that I'm not trying to scare anyone in to shutting down their WordPress site. I will, however, say that annoyances like these go along with the maintaining a WordPress site; sooner or later, you will have to deal with with the fact that some script kiddy found an attack vector to leverage your WordPress site for their purposes. WordPress a great tool, lots of people use it, but there are a lot of moving parts in the platform, and there are always 'issues' that pop up (issues in PHP itself, plugins, themes, etc.). If you are really concerned about this, you may like to take a look at an alternative like [Jekyll](http://jekyllrb.com/); it's much harder to inject content in to static files than it is a dynamic framework based on PHP (or Ruby, Python, etc.).\n\nGetting back to what was going on with our compromised site, I considered what I saw as a \"those damn kids\" attack, and not actually commandeering the site . I then set about cleaning up the Wordpress installation to move it to a server environment that I controlled and could make sure that the server had some elements to make this style of attack just a little harder to do.\n\nAs a former systems administrator who used to actually read the daily security bulletins put out by Windows, the various Linux distributions, my approach to these recovery is a little more dramatic than what [various blogs suggest](http://blog.sucuri.net/2010/05/simple-cleanup-solution-for-the-latest-wordpress-hack.html) to clean up. If we're using the tagging-the-fence metaphor, these posts show you how to paint over the graffiti. Since hackers can be clever little buggers, and I've found on at least one occasion that the hackers left something crazy and 'simple' to fix to mask what they were really doing (that time it was setting up a store-and-forward server for pornography), I adopted the equivalent of burning down the fence and building a new one when I see this happen.\n\nCaveat: I spend almost [all day in a terminal](http://www.scholarslab.org/dh-developer/customizing-bash/), so the code I show to back-up and massage the code is going to use the terminal. If this really isn't your cup-of-tea, there are generally [GUI](http://en.wikipedia.org/wiki/Graphical_user_interface) tools that will do the same (e.g. [PHPMyAdmin](http://www.phpmyadmin.net/home_page/index.php)) which, if you are on a hosted environment, most likely this tool is there. I just say this in case you are really not comfortable (or have shell access to the server) that you can use the same techniques, but use the tools available.\n\nThe basic workflow I was using was to get all the data out the MySQL server, and the WordPress theme that was 'infected' on to my local machine, download a fresh version of WordPress, import the WordPress data, reinstall any needed plugins, and the theme the site was using. For the particular hack in question, I actually had remote access (on a non-default port) to the MySQL server, so I could actually pull the data directly from the machine to my computer ([as a gist](https://gist.github.com/883062))\n\n[code lang=\"bash\"]\nmysqldump -h [mysql server] --port=[port number] -u [username] -p [database name] | gzip -c | cat ~/Desktop/`date +%Y-%m-%d-%T`.sql.gz\n[/code]\n\nIf you aren't comfortable with a Terminal commands, this can be a little scary. Basically this is a set of chained commands ([Pipes and Redirects](http://www.westwind.com/reference/os-x/commandline/pipes.html)) that dump out the entire database ([mysqldump](http://dev.mysql.com/doc/refman/5.1/en/mysqldump.html)), compress the output ([gzip](http://www.gzip.org/)), and write the compressed output to a file on my Desktop with a current timestamp ([cat](http://en.wikipedia.org/wiki/Cat_(Unix))). As an example, my output file with all of the WordPress data was named 2012-02-28:20:40:02.sql.gz.\n\nNow to download the latest version of Wordpress and set up a database named hack and inject the data from the remote server in to the newly created database.\n\n[code lang=\"bash\"]\n\ngunzip ~/Desktop/2012-02-28:20:40:02.sql.gz\n\nmkdir -p ~/public_html/hack && cd ~/public_html/hack\n\ncurl -O http://wordpress.org/latest.tar.gz\n\ntar xvf latest.tar.gz\n\nmysqladmin create hacked\n\nmysql -u [local mysql root] -p hacked < ~/Desktop/2012-02-28:20:40:02.sql\n\n[/code]\n\nThere is a lot here, and worth explaining briefly. These commands\n\n\n\n\t\n  1. Decompress the SQL file from the server that sits on the Desktop;\n\n\t\n  2. Create a new directory in the user's public_html directory named hack (creating public_html/hack if it does not already exist);\n\n\t\n  3. Changes the terminal in to the newly created directory (you can chain multiple commands together with the && operator)\n\n\t\n  4. Downloads the latest version of WordPress with [cURL](http://en.wikipedia.org/wiki/CURL);\n\n\t\n  5. Creates a new database on you local machine named \"hacked\" using the [mysqladmin](http://dev.mysql.com/doc/refman/5.5/en/mysqladmin.html) command;\n\n\t\n  6. Connects to the local MySQL server and inserts the contents of the SQL file generated earlier\n\n\nIf you are running some other type of system, you may need to change the location of where you put WordPress, but the rest should be pretty much the same. You can now log on to the WordPress system locally (e.g. http://localhost/~wsg4w/hacked/wordpress/wp-admin/) and reinstall your plugins. The one thing that will take a little bit of effort would be cleaning up the theme you are using if  you have made any significant changes to a theme (and not it in a [revision control system](http://en.wikipedia.org/wiki/Revision_control)), you will need to clean out the hack additions.\n\n[code lang=\"bash\"]\nfind ./ -name \"*.php\" -type f |  xargs sed -i 's#<?php /**/ eval(base64_decode(\"aWY.*?>##g' 2>&1\nfind ./ -name \"*.php\" -type f |  xargs sed -i '/./,$!d' 2>&1\n[/code]\n\nThese lines use the [find](http://en.wikipedia.org/wiki/Find) command to find PHP files and then use [sed](http://www.grymoire.com/Unix/Sed.html) to remove any of the crazy base64_decode from your files.  If you don't already have your theme in an SCM, this is a good time. [Github](https://github.com/) is a good option, but if you want a private repository, you can also check out [Bitbucket](https://bitbucket.org/). This will make things easier in the future...\n\nAfter you have everything cleaned up on your local machine, it's time to push it back up to the server. There are a few ways to go about doing this. If I'm doing this on a regular basis, I typically use [rsync](http://en.wikipedia.org/wiki/Rsync), but if it's a one time deal, I use [scp](http://en.wikipedia.org/wiki/Scp). If you only have FTP access, I highly recommend the [lftp](http://lftp.yar.ru/) utility.\n\n[code lang=\"bash\"]\n\ncd ~/public_html/\n\nscp -R hacked username@servername:~/path/to/wordpress/install\n\n[/code]\n\nThis should put things back on your server to a clean state for your WordPress site. You may want to also make a backup of the file contents occasionally too. This is a script I use to backup servers, modified to look at WordPress:\n\n[code lang=\"bash\"]\n\n#! /bin/bash\ncd path/to/wordpress\ntar cvpzf backup.tgz --exclude=/backup.tgz .\n\n[/code]\n\n\n## Postmortem\n\n\nHaving a WordPress site compromised can be kind of embarrassing. There are a lot of factors that can go in to the attack vectors that can affect your site. Even if  you regularly update the versions of WordPress, the plugins, etc., you can find yourself in a situation, especially on a shared server, where someone you don't even know about has left their site open to an attack that compromises your site. There really isn't a good way to keep this from happening, but what you can do is make it harder for these types of attacks to success. Here are some general guidelines to keeping your WordPress site Cialis free:\n\n\n\n\t\n  * Keep Wordpress updated;\n\n\t\n  * Disable plugins you aren't using; better yet, delete them off the server;\n\n\t\n  * Keep your theme in an SCM;\n\n\t\n  * Disable comments (this is one of the most common attack vectors); if you really want them, you can also use [IntenseDebates](http://intensedebate.com/) or [Disqus](http://disqus.com/)\n\n\t\n  * Set your file permissions properly;\n\n\nIf this is happening to you more than you'd like, I would also suggest taking a look at some of the static web generators. We use [jekyll](http://jekyllrb.com/) for the [Praxis webiste](http://praxis.scholarslab.org); [SecondCrack](https://github.com/marcoarment/secondcrack) by Maro Arment if you like PHP, and [Growl](https://github.com/xfire/growl/) if you prefer Python.\n"},{"id":"2012-03-20-we-have-highlighting","title":"We Have Highlighting!","author":"annie-swafford","date":"2012-03-20 11:09:09 -0400","categories":["Grad Student Research"],"url":"we-have-highlighting","content":"We have reached an important milestone in Prism development; the highlighting functionality is now complete! A user can now color a given text in accordance with a series of categories and then submit the markings to the database! The user clicks on a category on the right-hand side of the page to select that category, and can then mark up the text.  To mark the text, the user can either click on each word individually or drag over the text and all the words the cursor touches will be marked in that category. We also have an eraser function that lets the user delete any markings! Just click the eraser and then click on the word you want to unhighlight!\n\n\nBuilding this was quite an involved process; it took some ruby code to make a preprocessor that puts a span around each word and assignes each word a number, lots of coffeescript to control the marking functionality, and some css to control the colors of the markings and where they should appear on the text.  The next step for our talented design team is to figure out which colors we should use and whether we should attempt to layer colors and play with opacity, or whether some markings should appear above the text or below.  For example, should a phrase marked with both blue and red appear as purple, as red with a blue border, or should the words appear as blue with the red either above or below the words?  We've currently built it with the latter design in mind, as the image below demonstrates.\n\n\n\n\n\n![null](https://lh6.googleusercontent.com/-aowZ54KSrzk/T2ioVI6RCoI/AAAAAAAAAEA/4NL9XpK9BxQ/w711-h415-k/highlight.png)\n\n\n\n\nOur next step from the development side is figuring out how to visualize the data! We know what we want it to look like ultimately, and now we need to build it! I'm going to spend some time trying to make sense of d3.js to see if it will work for our purposes. More next week!\n"},{"id":"2012-03-22-kathleen-fitzpatrick-planned-obsolescence","title":"Kathleen Fitzpatrick: Planned Obsolescence","author":"ronda-grizzle","date":"2012-03-22 07:19:56 -0400","categories":["Podcasts"],"url":"kathleen-fitzpatrick-planned-obsolescence","content":"**Kathleen Fitzpatrick**\n**Planned Obsolescence: Publishing, Technology, and the Future of the Academy**\n\nDr. Kathleen Fitzpatrick, Director of Scholarly Communication for the Modern Language Association, joined us in the Scholars' Lab on March 15, 2012 for a talk as part of our spring speaker series.\n\nDr. Siva Vaidhyanathan, Robertson Professor in Media Studies and Chair of the Department of Media Studies at U.Va., served as the initial respondent for this talk.\n\nAbstract:\n\"What if the academic monograph is a dying form? If scholarly communication is to have a future, it’s clear that it lies online, and yet the most significant obstacles to such a transformation are social and institutional. How must the academy and the scholars that comprise it change their ways of thinking in order for digital scholarly publishing to become a viable alternative? This talk will explore some of those changes and their implications for our lives as scholars and our work within universities.\"\n\nDr. Fitzpatrick's Bio:\nKathleen Fitzpatrick is Director of Scholarly Communication of the Modern Language Association, and Professor of Media Studies (on leave), Pomona College. She is author of _Planned Obsolescence: Publishing, Technology, and the Future of the Academy_, published in 2011 by NYU Press and previously made available for open peer review [online on MediaCommons](http://mediacommons.futureofthebook.org/mcpress/plannedobsolescence), and of _The Anxiety of Obsolescence: The American Novel in the Age of Television_, published in 2006 by Vanderbilt University Press. She is co-founder of the digital scholarly network [MediaCommons](http://mediacommons.futureofthebook.org), and has published articles and notes in journals including the _Journal of Electronic Publishing_, _PMLA_, _Contemporary Literature_, and _Cinema Journal_.\n\nDr. Vaidhyanathan's Bio:\nSiva Vaidhyanathan is a cultural historian and media scholar, and is currently the Robertson Professor in Media Studies at the University of Virginia. Vaidhyanathan is a frequent contributor on media and cultural issues in various periodicals including the _Chronicle of Higher Education_, _New York Times Magazine_, _The Nation_, and Salon.com, and he maintains a blog, [www.googlizationofeverything.com](http://www.googlizationofeverything.com/). He is a frequent contributor to National Public Radio and to MSNBC.COM and has appeared in a segment of \"The Daily Show\" with Jon Stewart. Vaidhyanathan is a fellow of the New York Institute for the Humanities and the Institute for the Future of the Book. In 2011 he was appointed chair of UVA's Department of Media Studies. His book _The Googlization of Everything — and Why We Should Worry_ was published by University of California Press in 2011.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.14243384743/enclosure.mp3\"]\n"},{"id":"2012-03-23-shell-programming-in-haskell-converting-s5-slides-to-pdf","title":"Shell Programming in Haskell: Converting S5 Slides to PDF","author":"eric-rochester","date":"2012-03-23 05:09:09 -0400","categories":["Research and Development"],"url":"shell-programming-in-haskell-converting-s5-slides-to-pdf","content":"# Shell Programming in Haskell: Converting S5 Slides to PDF\n\n\n\n\nRecently, I gave an introduction to Python for Chris’ and Kelly’s [GIS Workshop](http://tinyurl.com/s12gis). It was a really great experience, and we had a lot of fun learning about Python and how to use it with ArcGIS.\n\n\n\n\nI did [my slides](http://people.virginia.edu/~err8n/pythongis/) for it in Markdown, using [S5](http://meyerweb.com/eric/tools/s5/). Others around the Scholars’ Lab have used [Show-off](https://github.com/schacon/showoff) to compose slide-shows in Markdown, but I wanted something a little simpler, and it had been a while since I’d looked at S5, so I used that instead.\n\n\n\n\nThen Kelly asked me for a PDF version of the slideshow. Heh.\n\n\n\n\nAt first I thought I might have to covert it to Showoff or (worse yet) PowerPoint. But I Googled around and found that converting it wouldn’t be too difficult. The process itself would be simple, and a small shell script would make it even easier.\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/03/philosoraptor.jpg)\n\n\nAnd then my infallible instinct to make any project ten times more interesting (i.e., _complicated_) kicked in.\n\n\n\n\nI remembered that I’d just read Greg Weber’s post about [Shelly](http://www.yesodweb.com/blog/2012/03/shelly-for-shell-scripts), a library to make shell scripting a bit easier in Haskell. I’ve been seriously playing with Haskell for almost a year now, using it for most of my side-projects and for anything that no one else will have to maintain. The thought of using Haskell for shell scripting was intriguing, just because it would be another way for me to wrap my head around this very different computer language.\n\n\n\n\nBut I was skeptical. At first glance, Haskell doesn’t seem like a good candidate for shell programming. Typically, these scripts are quick, one-off programs, often written in anger, that need to be created quickly and nimbly (dare I say, _agily_?). However, Haskell is statically-typed, and its type system is not given to making quick changes. (Well, I’ve found that not to be quite accurate, but it is the perception.) Generally, I think that languages like Haskell are more suited to larger systems, because their power and concision really only become apparent when working with large bodies of code.\n\n\n\n\nWhatever my reaction, though, a small script like this, with limited scope, seemed perfect.\n\n\n\n\n## The Process\n\n\n\n\nThe process I found to handle the conversion was fairly simple.\n\n\n\n\n\n\n  1. Get a PNG screenshot of each slide using [webkit2png](http://www.paulhammond.org/webkit2png/);\n\n\n  2. Concatenate all of the PNGs into a PDF using the [ImageMagick](http://www.imagemagick.org/script/index.php) tool `convert`;\n\n\n  3. Clean up the PNGs.\n\n\n\n\nWith that laid out, let’s jump in.\n\n\n\n\n## Preface\n\n\n\n\nFirst, some book-keeping: I have to let Haskell know that I’m going to use string literals in places that require [Data.Text.Text](http://hackage.haskell.org/package/text) instances:\n\n\n\n    \n    <code class=\"sourceCode haskell\"><span class=\"ot\">{-# LANGUAGE OverloadedStrings #-}</span></code>\n\n\n\n\nAlso, we have to import the [Shelly](http://www.yesodweb.com/blog/2012/03/shelly-for-shell-scripts) module.\n\n\n\n    \n    <code class=\"sourceCode haskell\"><span class=\"kw\">import</span>           <span class=\"dt\">Shelly</span></code>\n\n\n\n\nAnd we need some other modules for working with characters, text, and other things.\n\n\n\n    \n    <code class=\"sourceCode haskell\"><span class=\"kw\">import</span>           <span class=\"dt\">Control.Monad</span> (forM_)\n    <span class=\"kw\">import</span> <span class=\"kw\">qualified</span> <span class=\"dt\">Data.Char</span> <span class=\"kw\">as</span> <span class=\"dt\">C</span>\n    <span class=\"kw\">import</span> <span class=\"kw\">qualified</span> <span class=\"dt\">Data.Text.Lazy</span> <span class=\"kw\">as</span> <span class=\"dt\">T</span>\n    <span class=\"kw\">import</span>           <span class=\"dt\">Filesystem.Path</span>\n    <span class=\"kw\">import</span>           <span class=\"dt\">Prelude</span> <span class=\"kw\">hiding</span> (<span class=\"fu\">FilePath</span>)\n    <span class=\"kw\">import</span>           <span class=\"dt\">System.Environment</span></code>\n\n\n\n\n## Converting to PNGs\n\n\n\n\nThe first step is taking screenshots of each slide. To do that, I used the [webkit2png](http://www.paulhammond.org/webkit2png/) script.\n\n\n\n\nFor most things, I’m using Python 2.7, but I haven’t bothered installing `pyobjc` for it. `webkit2png` uses `pyobjc`, though, so I have to run that program with Python 2.6, which is the default Python shipped with Mac OS 10.6. I only generate the full-sized screenshot, and I output it to a filename that includes the slide number. In Bash, that would look like this:\n\n\n\n    \n    <code class=\"sourceCode bash\">python2.6 <span class=\"ot\">$(</span><span class=\"kw\">which</span> webkit2png<span class=\"ot\">)</span> <span class=\"kw\"></span>\n            --fullsize <span class=\"kw\"></span>\n            --filename pythongis-000 <span class=\"kw\"></span>\n            http://people.virginia.edu/~err8n/pythongis/#slide0</code>\n\n\n\n\nFirst, let’s create a generic function to run commands in Python 2.6. In Shelly, the convention is to add an underscore to functions that throw away their output:\n\n\n\n    \n    <code class=\"sourceCode haskell\">python26_ script args <span class=\"fu\">=</span> run_ <span class=\"st\">\"python2.6\"</span> (script<span class=\"fu\">:</span>args)</code>\n\n\n\n\nThis is kind of interesting because I wouldn’t abstract this out if I were writing this in Bash, Python, or Ruby. But adding this function felt quite natural in Haskell, which tends to encourage smaller, more generic, yet more focused, functions.\n\n\n\n\nNow I’ll build on that to create a command to look for the program `webkit2png`, and if it finds it, pass it to Python 2.6:\n\n\n\n    \n    <code class=\"sourceCode haskell\">webkit2png_ filename url <span class=\"fu\">=</span> <span class=\"kw\">do</span>\n        script <span class=\"ot\"><-</span> which <span class=\"st\">\"webkit2png\"</span>\n        <span class=\"kw\">case</span> script <span class=\"kw\">of</span>\n            <span class=\"kw\">Nothing</span>      <span class=\"ot\">-></span> echo <span class=\"st\">\"ERROR: webkit2png not installed.\"</span>\n            <span class=\"kw\">Just</span> script' <span class=\"ot\">-></span> <span class=\"kw\">do</span>\n                s <span class=\"ot\"><-</span> toTextWarn script'\n                python26_ s [ <span class=\"st\">\"--fullsize\"</span>\n                            , <span class=\"st\">\"--filename\"</span>, filename\n                            , url\n                            ]</code>\n\n\n\n\nThis could be better. For one thing, this command could print an error message if `webkit2png` isn’t available. If that happens, it should probably also short-circuit the rest of the script. The way to do this in Haskell would be to return a [Maybe](http://www.haskell.org/ghc/docs/latest/html/libraries/base/Data-Maybe.html) value, which is what the `which` function above does. In this case, I know that the program is installed and on the `PATH`, so I’m being a little sloppy.\n\n\n\n\n## Converting to PDF\n\n\n\n\nThe next step is to concatenate all the PNGs into one PDF. I’m using the `convert` program from [ImageMagick](http://www.imagemagick.org/script/index.php) to do this. This takes a list of PNG files to convert, the name of the PDF file, and generates the output.\n\n\n\n    \n    <code class=\"sourceCode haskell\"><span class=\"ot\">convert ::</span> <span class=\"fu\">FilePath</span> <span class=\"ot\">-></span> [<span class=\"fu\">FilePath</span>] <span class=\"ot\">-></span> <span class=\"dt\">ShIO</span> ()\n    convert pdf pngs <span class=\"fu\">=</span> run_ <span class=\"st\">\"convert\"</span> <span class=\"fu\">=<<</span> <span class=\"fu\">mapM</span> toTextWarn (pngs <span class=\"fu\">++</span> [pdf])</code>\n\n\n\n\n## Working on Multiple Files\n\n\n\n\nRight now, `webkit2png_` (the function to download the slides as PNGs) operates on a single slide. But we’ll need to do this for every slide in the show. `downloadSlides` takes the number of slides and the base URL, and it calls `webkit2png_` for each slide. It returns a list of file names for the downloaded PNGs.\n\n\n\n    \n    <code class=\"sourceCode haskell\"><span class=\"ot\">downloadSlides ::</span> <span class=\"dt\">Int</span> <span class=\"ot\">-></span> <span class=\"dt\">String</span> <span class=\"ot\">-></span> <span class=\"dt\">ShIO</span> [<span class=\"fu\">FilePath</span>]\n    downloadSlides slideCount baseUrl <span class=\"fu\">=</span> <span class=\"kw\">do</span>\n        forM_ inputs <span class=\"fu\">$</span> (url, file) <span class=\"ot\">-></span> webkit2png_ file url\n        <span class=\"fu\">return</span> files'\n        <span class=\"kw\">where</span>\n            baseUrl' <span class=\"fu\">=</span> T.pack <span class=\"fu\">$</span> baseUrl <span class=\"fu\">++</span> <span class=\"st\">\"#slide\"</span>\n            <span class=\"fu\">range</span>    <span class=\"fu\">=</span> <span class=\"fu\">map</span> (T.pack <span class=\"fu\">.</span> <span class=\"fu\">show</span>) [<span class=\"dv\">0</span><span class=\"fu\">..</span>slideCount]\n            urls     <span class=\"fu\">=</span> <span class=\"fu\">map</span> (T.append baseUrl') <span class=\"fu\">range</span>\n            files    <span class=\"fu\">=</span> <span class=\"fu\">map</span> (T.append <span class=\"st\">\"slide-\"</span>) <span class=\"fu\">range</span>\n            files'   <span class=\"fu\">=</span> <span class=\"fu\">map</span> (fromText <span class=\"fu\">.</span> <span class=\"fu\">flip</span> T.append <span class=\"st\">\"-full.png\"</span>) files\n            inputs   <span class=\"fu\">=</span> <span class=\"fu\">zip</span> urls files</code>\n\n\n\n\nThe only wrinkle here is that the file names that are passed to `webkit2png` aren’t the ones that are output. Instead, the program appends the size of the image (thumbnail, full, etc.) and the .png extension. Since I want to operate on those files later, I have to create both the file name prefix to pass to `webkit2png` and the full file name to process later. This is unfortunate and brittle, because if `webkit2png` ever changes how it names the output files, my script will break.\n\n\n\n\nThis is also shell-script sloppy in another way. I should really create a temporary directory and download the PNGs there. Maybe someday.\n\n\n\n\n## Putting it all Together and Getting the Inputs\n\n\n\n\nAll the pieces are in place. The only things left are to parse the command-line arguments, call `downloadSlides` and `convert`, and delete the downloaded PNGs.\n\n\n\n\nThe `main` function is the entry-point for the script. It picks three parameters from the command line and tries to make one a `Int`. If that can’t happen for any reason, it prints the usage message and exits. If the command-line is right, the script continues processing.\n\n\n\n    \n    <code class=\"sourceCode haskell\"><span class=\"ot\">main ::</span> <span class=\"dt\">IO</span> ()\n    main <span class=\"fu\">=</span> shelly <span class=\"fu\">$</span> verbosely <span class=\"fu\">$</span> <span class=\"kw\">do</span>\n        args <span class=\"ot\"><-</span> liftIO <span class=\"fu\">$</span> getArgs\n        <span class=\"kw\">case</span> args <span class=\"kw\">of</span>\n            [slides, url, pdf] <span class=\"fu\">|</span> <span class=\"fu\">all</span> C.isNumber slides <span class=\"ot\">-></span> <span class=\"kw\">do</span>\n                pngs <span class=\"ot\"><-</span> downloadSlides (<span class=\"fu\">read</span> slides) url\n                convert (fromText <span class=\"fu\">$</span> T.pack pdf) pngs\n                echo <span class=\"fu\">.</span> T.pack <span class=\"fu\">$</span> <span class=\"st\">\"Wrote PDF to \"</span> <span class=\"fu\">++</span> pdf\n                <span class=\"fu\">mapM_</span> rm_f pngs\n            <span class=\"fu\">otherwise</span> <span class=\"ot\">-></span> echo usage</code>\n\n\n\n\nThis is the usage/help message.\n\n\n\n    \n    <code class=\"sourceCode haskell\"><span class=\"ot\">usage ::</span> <span class=\"dt\">T.Text</span>\n    usage <span class=\"fu\">=</span> <span class=\"st\">\"</span>\n    <span class=\"st\">    usage: s5topdf.lhs [slides] [url] [output] n</span>\n    <span class=\"st\">     n</span>\n    <span class=\"st\">      slides is the number of slides in the slideshow.n</span>\n    <span class=\"st\">      url    is the URL to access the slideshow at.n</span>\n    <span class=\"st\">      output is the filename of the PDF file to create.n\"</span></code>\n\n\n\n\n## Running\n\n\n\n\nTo run this script, pass it to `runhaskell` with the right command-line arguments. For example, here’s a small [wrapper script](https://gist.github.com/2150126#file_s5topdf).\n\n\n\n\n## Conclusion\n\n\n\n\nUsing Haskell for shell programming hasn’t been bad, but it’s not as fast as shell programming usually is, either. This is still more verbose than the bash, Python, or Ruby versions would be, and it took me (a little) longer to write. (Of course, I was unfamiliar with several of these libraries, and that slowed me down.)\n\n\n\n\nHowever, I needed to do almost no debugging. Once I got the types to line up and `runghc` to stop complaining, it just worked. There were no bugs hiding in parts that hadn’t run yet. Based on experience with other languages, I’d expected to have to tweak the `convert` function (the second stage of processing) once I got the `webkit2png` part working (the first stage). But that wasn’t necessary. After I coaxed the complete script into printing the usage message, everything else worked flawlessly.\n\n\n\n\nThe bottom line: For very short one-off scripts, this seems like over-kill. For scripts that you expect to grow, Haskell plus Shelly might be more attractive.\n\n\n\n\n## Second Conclusion\n\n\n\n\nOne of the things that attracts me to Haskell is it’s history of using [literate programming](http://en.wikipedia.org/wiki/Literate_programming). In fact, I’m using it right now. This post was generated from the script itself. I’ve posted the raw version to a [gist](https://gist.github.com/2150126), so you can compare them.\n\n\n\n\nUsing literate Haskell was a success. I really liked being able to interleave extended commentary with the code and to have both be part of the final product. I think it changed the nature of both the script and the post. This might not work as well for larger projects with more lines of code and multiple modules, but for a small script, it was very comfortable. I can see doing this again for descriptions of small algorithms, projects, or demos.\n\n\n\n\nAlso, having this file double as a script _and_ the post is kind of neat, at least for the moment.\n\n\n\n    \n    <code class=\"sourceCode haskell\"><span class=\"co\">-- vim: set filetype=lhaskell:</span></code>\n\n\n\n"},{"id":"2012-03-23-welcoming-katina-rogers","title":"Welcoming Katina Rogers!","author":"bethany-nowviskie","date":"2012-03-23 06:39:16 -0400","categories":["Announcements","Grad Student Research"],"url":"welcoming-katina-rogers","content":"[![Katina Rogers](http://www.scholarslab.org/wp-content/uploads/2012/03/KR_photo-271x300.jpg)](http://www.scholarslab.org/announcements/welcoming-katina-rogers/attachment/kr_photo/)Today, we're very happy to announce that Dr. Katina Rogers is to join the [Scholarly Communication Institute](http://uvasci.org) and [Praxis Program](http://praxis.scholarslab.org) teams at the [Scholars' Lab](http://lib.virginia.edu/scholarslab).\n\nKatina comes to us from an appointment at the Alfred P. Sloan Foundation, where she has been responsible for a number of operational areas and has contributed to the strategic development of Sloan's [Digital Information Technology](http://www.sloan.org/program/28) program under the direction of Josh Greenberg. Her past work includes positions as Assistant Secretary General to the International Union of Geodesy and Geophysics, and as an instructor in language and literature courses at the University of Colorado, Boulder, where she completed her PhD in Comparative Literature in 2010.\n\nIn her new role as Senior Research Specialist for the Scholarly Communication Institute, Katina will help to support and share outcomes from a number of conversations SCI is convening: on emerging models for authoring and publication (with the [Alliance for Networking Visual Culture](http://scalar.usc.edu/anvc/), [PressForward](http://pressforward.org/), and the Modern Language Association’s [program in scholarly communication](http://www.mla.org/news_from_mla/news_topic&topic=303)); and on graduate education reform, with [CHCI](http://chcinetwork.org) and [centerNet](http://digitalhumanities.org/centernet). She will also plan and conduct a broad survey of humanities-trained respondents who self-identify as working in [alternative academic careers](http://mediacommons.futureofthebook.org/alt-ac/), to illuminate perceived gaps in graduate-level preparation and help move _alt-ac_ conversations beyond the anecdotal. Finally, she will play a role in the planned expansion of the [Praxis Program](http://praxis.scholarslab.org) to a multi-institutional and international effort, geared toward sharing model programs and experiments in humanities methodological training.\n\nKatina blogs at [Black Ink/White Page](http://katinarogers.com), and is [@katinalynn](https://twitter.com/#!/katinalynn) on Twitter. She will begin work with us in April and will be based in New York for the duration of this appointment. We're thrilled to welcome her to SCI and to the Scholars' Lab family!\n"},{"id":"2012-03-27-diy-aerial-photography","title":"DIY Aerial Photography","author":"chris-gist","date":"2012-03-27 11:00:24 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"diy-aerial-photography","content":"Here in the Scholars' Lab, we've been interested for some time in having the ability to take aerial photographs on a small scale.  Many great uses exist for such techniques here at UVa.  A landscape architect could use current, high resolution photos of a work site. An environmental science student may wish to see how things have changed along the shoreline over a short period of time.  For these researchers, paying for an aerial survey would be cost prohibitive and unlikely. Flying a kite or balloon with camera attached is relatively inexpensive and can create great results.\n\nHanging cameras from aerial platforms (kites, balloons and pigeons) has been used since the dawn of photography.\n\nBy combining readily available products and a little ingenuity, just about anyone can fly a camera.  We have taken our inspiration from [Grassroots Mapping](http://grassrootsmapping.org/), an initiative of [Public Laboratory](http://publiclaboratory.org/home), which is a organization dedicated to helping local activists leverage technology.\n\n\nThis morning, we made our maiden voyage using the balloon methods described on the Grassroots Mapping website.  With our weather balloon and [hacked Canon camera](http://chdk.wikia.com/wiki/CHDK), we headed out in front of the library and made two small flights.\n\n\n\n\n\n    ![](http://www.scholarslab.org/wp-content/uploads/2012/03/IMG_84711.jpg)\n\n\n[caption id=\"attachment_4052\" align=\"aligncenter\" width=\"664\"]![](http://www.scholarslab.org/wp-content/uploads/2012/03/IMG_2379.jpg) Clemons Library Terrace[/caption]\n\n[caption id=\"attachment_4057\" align=\"aligncenter\" width=\"664\"]![](http://www.scholarslab.org/wp-content/uploads/2012/03/IMG_2386.jpg) Louis S. on Clemons Terrace[/caption]\n\nAfter flying over Clemons, we moved to the front of Alderman Library.  The wind eddies were an issue this morning but we got some really nice shots.\n\n[caption id=\"attachment_4058\" align=\"aligncenter\" width=\"456\"]![](http://www.scholarslab.org/wp-content/uploads/2012/03/IMG_8472.jpg) Alderman Launch[/caption]\n\n[caption id=\"attachment_4059\" align=\"aligncenter\" width=\"456\"]![](http://www.scholarslab.org/wp-content/uploads/2012/03/IMG_8474.jpg) Balloon Over Harrison-Small[/caption]\n\n[caption id=\"attachment_4061\" align=\"aligncenter\" width=\"664\"]![](http://www.scholarslab.org/wp-content/uploads/2012/03/IMG_2423.jpg) Kelly Holding Balloon Line[/caption]\n\n[caption id=\"attachment_4062\" align=\"aligncenter\" width=\"664\"]![](http://www.scholarslab.org/wp-content/uploads/2012/03/IMG_2447.jpg) Skylights and Sidewalks[/caption]\n\n[caption id=\"attachment_4063\" align=\"aligncenter\" width=\"664\"]![](http://www.scholarslab.org/wp-content/uploads/2012/03/IMG_2453.jpg) Trees and Benches[/caption]\n\n[caption id=\"attachment_4064\" align=\"aligncenter\" width=\"497\"]![](http://www.scholarslab.org/wp-content/uploads/2012/03/IMG_2442.jpg) The Ground Crew In Front of Alderman[/caption]\n\nThe folks at the Public Laboratory also created an online georeferencing tool called [MapKnitter](http://mapknitter.org).  This tool may not be the best but it's fairly easy to understand and get some fairly decent results.  View our test site [here](http://mapknitter.org/maps/uva-library-test).\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/03/MapKnitter.png)\n\n\nThis is all in our preparation for our [DIY Aerials workshops](http://www.lib.virginia.edu/scholarslab/resources/class/Spring2012GIS/) (on April 18 & 19) and [THATCamp](http://virginia2012.thatcamp.org/) (April 20 & 21).\n\nGuess what else we can do?  We can make 3D models with our shots!  A site called Hypr3D lets you upload images and then calculates the 3D geometry for you.  It's magic.\n\n[iframe width=\"700\" height=\"600\" src=\"http://www.hypr3d.com/models/4f74a45308c2fa0001000056/embedded_viewer\"]\n\n[http://www.hypr3d.com/models/4f74a45308c2fa0001000056](http://www.hypr3d.com/models/4f74a45308c2fa0001000056)\n"},{"id":"2012-03-27-examining-tei-displays-across-the-web","title":"Examining TEI Displays Across the Web","author":"carin-yavorcik","date":"2012-03-27 10:00:13 -0400","categories":["Digital Humanities"],"url":"examining-tei-displays-across-the-web","content":"As part of my project with [Zane](http://www.scholarslab.org/author/zschwarzlose/) to update the [TEIDisplay plugin for Omeka](http://www.scholarslab.org/digital-humanities/an-update-to-teidisplay-for-omeka/), I have been examining ways that different digital collections present TEI-encoded texts. We hope that by looking at the ways that other display tools function, we’ll be able to gain more insight into what works well and what doesn’t.\n\nOne of the first sites I looked at was _[Thomas MacGreevy and George Yeats: A Friendship in Letters](http://www.macgreevy.org/collections/gyeats/index.html), _a collection of letters spanning 1922-1965. What first stands out about this collection is the two-paned display. On either side, users can choose from a set of tabs how they want to examine each letter – annotated text, scanned page images, bibliographic information from the letter’s teiHeader, and biographic information about people mentioned in the letter. This setup makes it easier to display necessary information depending on what a user’s goals are.\n\nThe display of the TEI text itself also includes some nice features. Most letters have scholarly annotations that default to displaying in superscript – readers click on the note indicator and get a pop up box displaying the note, or they can click a button to hide notes entirely and read the letter as is. The encoded texts also display some of the visual features of the original letters, such as noting deletions with a strikethrough and additions with superscript text. The effect is a nice example of how digital editions of scholarly texts can combine different editorial practices such as social or documentary editing, as the MacGreevy Archive editor, Susan Schreibman, notes in her article about the collection, “The Lives of Others: Editing Online Editions of Correspondence,” from _Digital Scholarship._\n\nThe setup of the [Victorian Women Writers Project](http://webapp1.dlib.indiana.edu/vwwp/welcome.do) at Indiana University is very different. The focus here is more specifically on the encoded text body of the materials, primarily prose and poetry written by British women of the 19th century. The display is simple but effective. A table of contents runs along the left hand side of the page, giving users a way to navigate through each document. When notes occur in the text, they are displayed at the bottom of each page as footnotes. Additionally, users can download the XML if they so choose.\n\nThough page images are not available for most of the materials in the Women Writers Project collection, the system does have the capability to display them, as is evidenced by one of the other Indiana University collections, the [Brevier Legislative Reports](http://webapp1.dlib.indiana.edu/brevier/welcome.do), which uses the same display tool. If a user would like to examine an image of a page, she can click the “view page” link next to the page number indicator, and a pop-up image appears. However, this image does in some cases cover the encoded text, which makes it a little harder to compare the two. Alternatively, where these page images are available, a user can switch directly to “image mode” and read the page images without the encoded text, or download the whole document as a PDF.\n\nI also explored the classics texts at the [Perseus Digital Library](http://www.perseus.tufts.edu/hopper/) at Tufts University, which offers a lot of interesting interactive features with the display of its TEI text. Like the Victorian Women Writers site, there is a table of contents on the left-hand side to facilitate navigation. Through a series of tools on the right-hand side of the page, users can choose to display different editions and/or translations of a work, including editions of scholarly annotations. Additionally, place names are automatically extracted from the TEI text and can be searched or visualized in a network or on a map. Users can also jump back and forth between commentary or cross-references to the work, and the site also offers the ability to search within a specific document or discover how words are used throughout a work.\n\nThere were two main features that really stood out to me when evaluating these projects. One was that for the longer texts, it was really helpful to have the table of contents feature that allowed for easier browsing. The original version of TEIDisplay did have an option to render documents as “segmental” with a similar table of contents – this is definitely a functionality we’d like to keep, so we’ll be experimenting with it as we update the plugin.\n\nAnother feature I really liked in exploring the projects above was flexibility. It was nice to have the opportunity to customize the view you want to what your needs are – whether it’s hiding scholarly annotations, pulling up page images, or comparing editions. We are currently working on one viewing option that is progressing well – the ability to associate page images with encoded text. Overall, though, it may harder for us to implement something really flexible on the user end out of the box, given that we are not designing for a specific collection with specific needs. Rather, we are trying to make a general tool that works in many situations, which can be easily customized on the designer end to fit different collection needs as necessary. With something so general, it’s impossible to plan for all flexibility options that users may desire. But we hope that with really robust documentation about how everything works in the plugin, the creators of digital collections may be able to get in and tinker “under the hood” to customize the display as necessary.\n"},{"id":"2012-03-27-seeing-the-prism-we-have-visualizations","title":"Seeing the Prism: We Have Visualizations!!","author":"annie-swafford","date":"2012-03-27 09:54:05 -0400","categories":["Grad Student Research","Visualization and Data Mining"],"url":"seeing-the-prism-we-have-visualizations","content":"I am happy to report that we have successfully build visualization capabilities into Prism!  Once users have highlighted the text according to the set categories, the users click on the submit button, which takes them to the visualization page! The users can then click on the categories at the right-hand side of the page to see how people have marked up the text, one category at a time.  The words will all turn the color that corresponds to the category, and the words will increase in size depending on how frequently they have been marked for that category.  Here's how Jefferson's \"Notes of the State of Virginia\" has been marked up for \"rhetoric\":\n![rhetoric](https://lh6.googleusercontent.com/-ufIVzWF9ark/T3H5gvdtA9I/AAAAAAAAAE0/ssNiTVnUBfM/s645/blue.png)\nHere's how the same text was marked for \"orientalism\":\n![orientalism](https://lh5.googleusercontent.com/-ajSRqa2nNqA/T3H5aWSm9JI/AAAAAAAAAEk/h0m_8EuGk8Q/s787/red.png)\nHere's how the same text was marked for \"social Darwinism\":\n![social Darwinism](https://lh4.googleusercontent.com/-rrUyjPYyAUg/T3H5eId9Q_I/AAAAAAAAAEs/w-r6BX06D_o/s645/green.png)\nAs you can probably guess from the results, I didn't actually spend time making sure that I've highlighted in accordance with the categories; this is merely a proof of concept.\nThis visualization functionality is built with the help of the d3.js library; it's amazingly powerful for all visualization needs!\nThe design team will now work on making it look even more attractive, but at least we know that we'll have visualizations to show.\nOur next task is to make sure that only users with accounts can highlight texts and see the resulting visualizations. More info next week!\n"},{"id":"2012-03-28-day-of-dh","title":"Day of DH","author":"brooke-lestock","date":"2012-03-28 07:45:04 -0400","categories":["Digital Humanities","Grad Student Research","Research and Development"],"url":"day-of-dh","content":"Yesterday, as I'm sure you all know, was [Day of DH](http://dayofdh2012.artsrn.ualberta.ca/), and it was my first year participating. I was only able to blog twice because as soon as I sat down to work on some design tasks for Praxis (choosing a font for the Prism site), I had some server troubles that kept me busy \"raking,\" dropping, migrating, importing, etc., and then it was time for our weekly meeting. In any event, visit [my Day of DH page](http://dayofdh2012.artsrn.ualberta.ca/members/brookelestock/) to read about how I got my feet wet in DH and what I was up to yesterday.\n\nTo add to that, because I didn't get a chance to update on the Day of DH blog yet, Bethany spoke to the group in our weekly meeting about budgets and grant-writing. She offered some valuable insight into the financial processes that pay for us and our meetings (we should probably watch less cat videos in there), the larger systems that affect budgeting within a university setting, and the small- and large-scale questions a financial manager needs to ask. She also provided us with a plethora of helpful tips on writing grants and devising budgets (like, \"Soft money is the devil!\"), which cannot be valued enough coming from someone with Bethany's success rate!\n\nNow I need to get back to picking a font. You wouldn't think it would be this difficult to choose one serif and one sans-serif font, but this Praxis group is chock-full of typography and web design street cred, so I'm feeling the pressure.\n\nI'll end by saying that it was a productive Day of DH for me in the Scholars' Lab and for the international DH community-at-large, and I'm looking forward to participating in many more!\n"},{"id":"2012-03-28-die-praxis-programm","title":"Die Praxis-Programm","author":"alex-gil","date":"2012-03-28 05:18:50 -0400","categories":["Grad Student Research"],"url":"die-praxis-programm","content":"[Today was the Day of DH. This post was [originally written for that crowd](http://dayofdh2012.artsrn.ualberta.ca/elotroalex/2012/03/28/die-praxis-programm/), hence some of the introductory material.]\n\nBack at the Scholar's Lab, the whole gang was hanging out at the graduate lounge waiting for our long-awaited session on grant writing and budgeting. Most of us had never seen a budget sheet until today. As we are wont to do, when the meeting started we went over our progress for the week. [Annie Swafford](https://twitter.com/#!/annieswafford) had really good news for everyone. Solidly ahead of schedule, she demo'd [Prism's first visualization](http://www.scholarslab.org/visualization-and-data-mining/seeing-the-prism-we-have-visualizations/). [Cue applause]. Unspoken fears that we would not achieve visualization were laid to rest and a year's worth of work was validated in the smiles of everyone around our camelot table.\n\nWe had other minutia to report before moving on to Bethany Nowviskie's presentation on the cash-money. Jeremy (read my [third post of the day](http://dayofdh2012.artsrn.ualberta.ca/elotroalex/2012/03/28/introduction-to-omeka/)) and Lindsay reported on their work with the left column of Prism (read my [second post of the day](http://dayofdh2012.artsrn.ualberta.ca/elotroalex/2012/03/27/achieve-office/)). I reported on my <q>progress</q> translating the about page to German. _Ja, ich weiß. Geil_. I have been working on both the [i18n framework](http://guides.rubyonrails.org/i18n.html) for our rails application and the translations themselves. This week I will close that branch with the incorporation of links on the footer to allow users to select the language.\n\n[caption id=\"attachment_4093\" align=\"aligncenter\" width=\"300\" caption=\"Doing our Praxis thing\"][![The Praxis crew](http://www.scholarslab.org/wp-content/uploads/2012/03/IMG_1706-300x224.png)](http://www.scholarslab.org/praxis-program/die-praxis-programm/attachment/img_1706/)[/caption]\n\nBethany's presentation was one of the best hours I've spent so far this year. It was both full of general advice and very specific detail. Ahead of our times, we were made privy to [the contents of the briefcase](http://www.youtube.com/watch?v=PEORpjVNJQk). We went over budget spreadsheets, grant applications (join a grant review committee if you haven't done so), hourly labor calculations, cost sharing, program officers, F&A (Facilities & Administrative Costs), faculty time buyouts, private funders and. public funders, looking for change under the couch, equipment, weathering budget cuts, _und so weiter_.  Once our time was up, I was reminded of the many ways in which our [Praxis Program](http://praxis.scholarslab.org/) had succeeded in making me a different kind of professional.\n\n[meditation ensued]\n"},{"id":"2012-03-28-the-end-of-the-beginning","title":"The end of the beginning","author":"sarah-storti","date":"2012-03-28 19:25:45 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"the-end-of-the-beginning","content":"Graduate study in the humanities can be a lonely business. Enter my knight in shining armor, commonly referred to around these parts as the Praxis Program.\n\nI think I've shared with you all before that I jumped at the call for applicants to the soon-to-become Praxis Program last summer specifically because the invitation promised that the Scholars' Lab \"apprenticeship\" would offer ample opportunities for collaboration with not only Scholars' Lab staff but also with other grad students. Fittingly, I began writing my application essay in the Victoria, BC airport, following my first experience at the ultra-collaborative and congenial [DHSI](http://www.dhsi.org/index.php). This summer, along with my gorgeous and talented co-pilot Brooke Lestock, I'll be presenting the Praxis Program and our crowdsourcing interpretation tool Prism to DHSI 2012 participants during one of the Institute's [colloquium sessions](http://www.dhsi.org/events.php). Funny how things come full circle, isn't it?\n\nSpeaking of full circle: last Wednesday the Program held a lunchtime information session for potential 2012-2013 Praxis Fellows. Sitting up at the front of the room with my cohort, flanked by our fearless and supportive SLab team, I couldn't help but feel ridiculously proud of what we, as a discrete group, have achieved so far this year. Now people want to_ be us_. During the Q&A session, with the help of Bethany and company, we successfully fielded all kinds of questions about how we value the skills we've developed and the experiences we've shared over the past semester and a half. We were articulate. We sounded like we knew what we were talking about. Because we _did_ know what we were talking about! Finally. Thank you, Praxis Program.\n\nThis past weekend marked the deadline for submissions to the 2012-13 Praxis Fellowship. Obviously we have to keep details confidential, but it can't hurt to share that we received what I thought was a shockingly large number of applicants. And the applicants are stellar. It's going to be terribly difficult to make decisions, but I'm excited about the process (as always!) and can't express adequately how incredible it feels to know that I'll have a hand in shaping the next generation of Praxis.\n\nI envy the new cohort of Fellows, whoever they may be. To you, future Fellows, I offer the following sage (because I'm on the way out) advice: make the most of your time. I know I'll miss it.\n\nI leave you with my proposal for the 2011-12 Praxis team tshirt design, because I'm too happy about it to keep it to myself (n.b.: 99% of the credit for this goes to Brooke). Jeremy, is there any way we can get a higher-resolution logo? I had to steal this one from the blog.\n\n![shirt!](http://farm8.staticflickr.com/7062/7025666789_799d382766.jpg)\n\nCheers!\n"},{"id":"2012-03-29-rambles-of-a-runaway-from-southern-slavery","title":"Rambles of a Runaway from Southern Slavery","author":"chris-gist","date":"2012-03-29 07:06:19 -0400","categories":["Digital Humanities","Geospatial and Temporal"],"url":"rambles-of-a-runaway-from-southern-slavery","content":"Henry Goings was a slave born ca. 1810 on a plantation on the James River between Richmond, VA and Williamsburg, VA.  His interesting history of travel with his owners, escape, and eventual settlement in Canada were chronicled in an until recently unknown book.\n\nThe UVa Library acquired the[ book](http://search.lib.virginia.edu/catalog/u4391170) in 2007.  Researches Calvin Schermerhord of Arizona State University, Michael Plunkett, and Edward Gaynor both from UVa Small Special Collections Library were able to find other source material about/from Mr. Goings.  Their research is published in a newly edited version of Mr. Goings's _[Rambles of a Runaway from Southern Slavery](http://www.amazon.com/Rambles-Runaway-Southern-Slavery-Institute/dp/0813932386/ref=ntt_at_ep_dpt_1)_.\n\nThe Scholars' Lab was asked to contribute maps showing the general movement of Mr. Goings and places mentioned in the text.  We gladly obliged and through an iterative process came up with the four following maps.\n\n[caption id=\"attachment_4115\" align=\"aligncenter\" width=\"1024\" caption=\"Map 1 - Shows Goings's General Movement\"]![](http://www.scholarslab.org/wp-content/uploads/2012/03/Map1-1024x791.jpg)[/caption]\n\n[caption id=\"attachment_4128\" align=\"aligncenter\" width=\"1024\" caption=\"Map 2 - Shows Southern Locations with James River Plantations Inset\"]![](http://www.scholarslab.org/wp-content/uploads/2012/03/Map21-1024x791.jpg)[/caption]\n\n[caption id=\"attachment_4117\" align=\"aligncenter\" width=\"1024\" caption=\"Map 3 - Shows Places From Tennessee to Canada\"]![](http://www.scholarslab.org/wp-content/uploads/2012/03/Map3-1024x791.jpg)[/caption]\n\n[caption id=\"attachment_4118\" align=\"aligncenter\" width=\"1024\" caption=\"Map 4 - Shows Great Lakes Locations\"]![](http://www.scholarslab.org/wp-content/uploads/2012/03/Map4-1024x791.jpg)[/caption]\n\nThe UVa Press did make some minor changes to the maps (mostly legends and scalebars) for the book.  However, the extents and all the data in the maps are the same as those shown here.\n\nI have read the preface and and first chapter.  It is a fascinating story and the footnotes are incredibly informative.  Kudos to the editors!\n"},{"id":"2012-03-30-welcoming-our-201213-graduate-fellows","title":"Welcoming our 2012/13 Graduate Fellows","author":"eric-johnson","date":"2012-03-30 05:11:08 -0400","categories":["Announcements"],"url":"welcoming-our-201213-graduate-fellows","content":"The Scholars' Lab is very excited to announce the recipients of the 2012-13 [UVA Library Graduate Fellowships in the Digital Humanities](http://www2.lib.virginia.edu/scholarslab/about/fellowship-apply.html):\n\n**David Flaherty**\n_Corcoran Department of History_\n\nDavid's project, part of his dissertation _'Improving and Enlarging Your Majesty’s Dominions in America:' The Board of Trade’s Vision for a British Atlantic Empire, 1713-63_, will use the entries of the British Board of Trade's Journal to create a database and maps \"that will reveal the Board’s network of correspondents and the widespread places that drew their attention.\"  The visualizations will show \"precisely when and where the [Board's] Commissioners directed their energies to gather more knowledge about the colonies and to pursue the implementation of part of their vision of empire,\" allowing David \"to recreate, literally, the Commissioners’ geographical 'vision.'\"\n\n**Lydia Rodriguez**\n_Department of Anthropology_\n\nLydia's project, part of her dissertation _Thinking Gesture: The Dialectics of Language, Gesture, and Thought in Chol Maya_, focuses on \"the analysis of the gestures that co-occur with temporal utterances\" among Chol Maya speakers of Northern Chiapas, Mexico. She intends to create a database of Chol Maya temporal gestures \"that will allow me to perform quantitative and qualitative analysis on the frequency and types of gestures co-occurring with temporal utterances\" and a website that will allow her to share this data with colleagues working in Maya languages or in gesture research.\n\n**Annie Swafford**\n_Department of English_\n\nAnnie's project, \"Songs of the Victorians,\" is a digital scholarly tool facilitating \"interdisciplinary discussions of Victorian poetry and corresponding Victorian parlor song musical settings.\"  Part of her dissertation, the project will achieve this by \"combin[ing] close readings of both Victorian poems and musical settings of them in an interactive environment\" based on audio files, visual representations of musical scores, and presentations of poetic texts.  Annie has also held fellowships in the [NINES](http://www.nines.org/) program over the past two years and in the Scholars' Lab's [Praxis Program](http://praxis.scholarslab.org/) in 2011-2012.\n\nThe Scholars' Lab will schedule a luncheon in September at which our new Fellows will be introduced and will in turn be given the opportunity to introduce their projects.  We're delighted to have them as part of the SLab community!\n"},{"id":"2012-04-03-updating-visualizations-and-users","title":"Updating Visualizations and Users","author":"annie-swafford","date":"2012-04-03 09:30:40 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"updating-visualizations-and-users","content":"We have three new updates for Prism!  First, we slightly changed the html markup for our document files, so our James Joyce excerpt from _Portrait of the Artist as a Young Man_ is now formatted correctly!  Second, we decided to tweak our visualizations.  Originally, once users select a category for visualizations,  the entire text would turn that color and the words that had been highlighted multiple times for that category would grow in size.  We decided that it looked odd and misleading if the entire text for each category changed color, regardless of whether those words had been selected, so we changed it so that only words that have been highlighted for a category change color.\n\nHere's an example of our new visualization plan (complete with our newly complete Joyce text):\n\n![new visualization](https://lh4.googleusercontent.com/-C1_mTvO4cmg/T3sxVX1v2oI/AAAAAAAAAFc/mtk72WMuVzg/h301/Screen%2BShot%2B2012-04-03%2Bat%2B1.10.05%2BPM.png)\n\nFinally, we fixed our user functionality! Now, users need to sign in to their accounts in order to highlight texts or view visualizations!\n\nOur next plan is to see if it's possible to change the cursor to look like a highlighter after users click on a category to markup the text. Stay tuned for more updates!\n"},{"id":"2012-04-03-wrapping-up-i18n","title":"Wrapping up i18n","author":"alex-gil","date":"2012-04-03 11:07:38 -0400","categories":["Grad Student Research"],"url":"wrapping-up-i18n","content":"Yesterday I was able to wrap up the internationalization of the site. It took us a while to figure out how we wanted the user to choose languages. We have several options: A setting on the browser can decide; users can indicate their preferences in the user account; we can have the location of your IP address decide for you. In the end we opted for the language link approach. No flags in our case, just links with the name.\n\nOnce the links were made, the next challenge was to make the language choices stick. In order to accomplish this, I had to set all link methods in our rails application attach the ?locale parameter automatically. In practice this meant that every link helper in any of our pages would generate the right language after we had already set the variable for locale at least once. If you didn't do anything the locale would default to English.\n\nHard links were a completely different matter. Thus far, I was only able to find one hard-link to worry about: the link to the about page on the header section. In order to deal with hard links we had to come up with a little rails hack. We (I had help from Eric Rochester figuring this one out) sent a variable from the actual header tag back into the yml file to have the yml file generate the links in the right language. The code looks something like this:\n  \n\n## on app/views/layouts/_header.html.erb\n[html gutter=\"false\"]\n&lt;p class=&quot;description&quot;&gt;&lt;%= t 'header.description_html',\n:about_url =&gt; url_for(:controller =&gt; 'pages', :action =&gt;\n'about') %&gt;&lt;/p&gt;[/html]\n  \n\n## on the config/locales yml files\n[html gutter=\"false\"]\n   description: &gt;\n      Prism es un conjunto de herramientas que sirven\n      para &lt;a href='%{about_url}'&gt;colectivizar la\n      interpretación &lt;/a&gt;de textos... [/html]\n"},{"id":"2012-04-04-cross-posted-on-a-definition-of-open-humanities","title":"cross-posted: On a definition of \"open humanities\"","author":"eric-johnson","date":"2012-04-04 12:09:25 -0400","categories":["Digital Humanities"],"url":"cross-posted-on-a-definition-of-open-humanities","content":"_Cross-posted from [Eric's blog](http://cybernetickinkwell.com/2012/04/02/on-a-definition-of-open-humanities/):\n_\n\nIn considering [recent d](http://dayofdh2012.artsrn.ualberta.ca/dh/)[e](http://dayofdh2012.artsrn.ualberta.ca/dh/)[finitions of digital humanities](http://dayofdh2012.artsrn.ualberta.ca/dh/), I’m often struck by some common aspects of many of them—the ones that have nothing to do with the digital:\n\n\n<blockquote>\". . . essentially collaborative . . .\"\n\n\". . . a commitment to the openness of knowledge.\"\n\n\" . . . interdisciplinary; by necessity it breaks down boundaries between disciplines . . .\"\n\n\"Making stuff, and using it to collaborate and connect with the public . . .\"\n\n\". . . genuine interdisciplinarity . . . open communication.\"\n\n\". . . an inclusive, open community . . .\"\n\n\". . . using interdisciplinary approaches that may go beyond the comfort level of traditional scholars . . .\"</blockquote>\n\n\nThey speak to me in large part because I really come from the museum (specifically public history) and library worlds, and many of those celebrated features of DH are fundamental aspects and approaches of those other fields as well—and have been for a long time. The practitioners in those fields? Well, they're my people.\n\nLibraries, museums, the digital humanities, and other public-facing humanities efforts share some or all of the following values:\n\n\n\n\n\t\n    * Collaboration\n\n\t\n    * Inclusivity\n\n\t\n    * Interdisciplinarity\n\n\t\n    * Open Access\n\n\t\n    * Open Process\n\n\t\n    * Open Source\n\n\t\n    * Involvement of the public and/or public \"communities of passion\"\n\n\n\nI've [started referring](http://twitter.com/#!/ericdmj/status/178195525995212801) to this larger perspective as the **open humanities**. It's a broad term that encompasses those values outlined above, values shared by many libraries, museums, public humanities projects and practitioners of all kinds and standing in opposition to much of the traditional approach of \"solo scholar\" research and closed publication.\n\nIf I were to offer a first-pass attempt at capturing these elements in a definition, I might say something like: **the open humanities are those aspects of the humanities aimed at democratizing production and consumption of humanities research**.  It makes no judgment on the precise manner or praxis in which this work is done or delivered.\n\nAnd because I work in a digital humanities shop, I wanted to show my take on the relationship between digital humanities and the wider open humanities:\n\n[![](http://cybernetickinkwell.com/wp-content/uploads/2012/04/openhumanities21.jpg)](http://cybernetickinkwell.com/wp-content/uploads/2012/04/openhumanities21.jpg)\n\nThe digital humanities are a part of the open humanities to the extent that those same values are held, though of course the purely digital elements (the code, the markup, the hardware) are unique to the digital humanities and live largely outside of OH. That being said, much of DH—the commitment to open source, the collaborative nature of the field, the interdisciplinarity—is open.\n\nAs a comparison, the public humanities are <del>virtually entirely open by my definition, so they are included entirely within the circle of OH</del> _almost_ entirely open by my definition, but as [Sheila](https://twitter.com/#!/sherah1918/statuses/186901407658541056) [Brennan](https://twitter.com/#!/sherah1918/statuses/186901686269378561) [points out](https://twitter.com/#!/sherah1918/statuses/186940805565526016) there are still elements even within those institutions that don't support openness at every turn. (See my original diagram [here](http://cybernetickinkwell.com/wp-content/uploads/2012/04/openhumanities2.jpg).) There is also [some overlap](https://twitter.com/#!/ericdmj/status/182536251876966400) between public humanities and the digital humanities, as some—but not all—public humanities projects are also digital.  You see how this starts getting a little [overly-Venn-diagrammatical](http://venndiagrams.tumblr.com/), but you can probably see where I'm going.\n\nThose who know me know that I'm no coder—some might say that means I’m not much of a digital humanist. I'd say I'm a proud _open_ humanist with one foot solidly in the digital. That counts as DH for me.\n"},{"id":"2012-04-17-an-intro-to-my-work-with-sci","title":"An intro to my work with the Scholarly Communication Institute (SCI)","author":"katina-rogers","date":"2012-04-17 05:52:52 -0400","categories":["Announcements"],"url":"an-intro-to-my-work-with-sci","content":"Hi, readers! It's such a pleasure to be writing in this space. [As you may know](http://www.scholarslab.org/announcements/welcoming-katina-rogers/), I came onboard as Senior Research Specialist for the [Scholarly Communication Institute](http://uvasci.org/current-work/) just a couple of weeks ago. SCI is housed within the Scholars' Lab (which means I get to be a part of this stellar group that, if you're reading here, you already know well), and over the past nine years has convened discussion-based summer institutes on topics related to all aspects of scholarly communication. I'm still green as can be, but the shapes of my projects are becoming clearer, and as I plan them out I'm getting a sense of just how quickly these eighteen months will go.\n\nSo, what is it exactly that I'll be doing? At this point I'm thinking of my role as having two main streams. The first stream focuses on understanding and reforming **methodological training in humanities graduate programs**, and consists of two related projects: a census and a survey. Through the census, which will become a part of the [#Alt-Academy ](http://mediacommons.futureofthebook.org/alt-ac/)website, I will seek out humanities-trained scholars who self-identify as working in alt-ac roles; the results will be displayed as a dynamic and searchable directory on #Alt-Academy. It's important to me that the census cast a wide net in order to get as complete a picture as possible of the constellations of people working in alt-ac positions.\n\nThe census, useful in its own right, will also be a stepping stone leading towards another goal: working from the census, I will be administering a survey of alt-academics in order to determine opportunities for improved career preparation and refined methodological training in humanities programs. The survey responses, which will be strictly confidential, will help us to move beyond the anecdotal and gain concrete understanding of the skills that advanced humanities programs currently provide relative to the needs that graduates of the programs experience in their careers. Once it's complete, a report on the survey results will be published by [CLIR](http://www.clir.org/).\n\nThe second work stream involves supporting and sharing outcomes from a series of meetings that SCI will be convening on **reforming humanities graduate education** and on the **continued development of new models of scholarly publishing and authoring**. The meetings focusing on graduate education reform will be held in partnership with [CHCI](http://chcinetwork.org/) and [centerNet](http://digitalhumanities.org/centernet), and will probe more deeply into questions raised by the results of the survey mentioned above, as well as many other issues surrounding graduate education. The meetings on scholarly publishing and authoring are in partnership with three outstanding projects from the [Alliance for Networking Visual Culture](http://scalar.usc.edu/anvc/), [PressForward](http://pressforward.org/), and the Modern Language Association’s [program in scholarly communication](http://www.mla.org/news_from_mla/news_topic&topic=303). For each of these meetings (the first of which takes place in early May), I'll be providing research support and documenting the substance of the conversations. Needless to say, I'm very much looking forward to taking part in both sets of meetings, which include extraordinary people.\n\nI'll post updates here from time to time, so watch for more as the weeks go by! I'll also continue posting to my [personal blog](http://blackinkwhitepage.wordpress.com/) and my [Twitter feed](https://twitter.com/#!/katinalynn), so if you're interested in the topics I'll be working on, you may enjoy keeping an eye on those unofficial channels as well. Finally, I'll be seeking input for the census and survey over the coming weeks (both in terms of who to include and what questions to ask), so if you have thoughts or questions, please feel free to connect with me directly.\n"},{"id":"2012-04-17-spreading-the-light-prism-development-is-almost-done","title":"Spreading the Light: Prism Development is Almost DONE!","author":"annie-swafford","date":"2012-04-17 06:50:29 -0400","categories":["Grad Student Research"],"url":"spreading-the-light-prism-development-is-almost-done","content":"It's been a busy few weeks in Prism, which means that we have some exciting updates for highlighting, visualizations, and the sandbox!\n\nOur visualizations now display with the correct colors! This change was surprisingly complicated, since it involved adding yet another function in our d3.js algorithm, but it made a huge difference.\n\nI also made another important change that will improve the user experience.  On the right-hand side of the page, we have a key that shows which categories correspond to which colors: each category appears to the right of a little colored box.  I wrote javascript that adds a black border around the correct colored box when a user selects a category to make it clearer which category has been selected. We decided to make this change instead of changing the cursor to look like a highlighter.\n\nOur final update involves user accounts: originally, users had to log in to access the sandbox (a sample text that users can highlight as a way of practicing before they highlight our three main texts), but I was able to find a way to bypass the login system, so now everyone can use the sandbox without logging in.\n\nThe next step is to write tests for my javascript with Jasmine, which I will learn how to do today!  Once we have this task completed, I think we'll just have design work left, since we have all the functionality we want for this version of our tool.  We've almost finished building Prism! \n"},{"id":"2012-04-23-and-then-the-light-bulb-blew","title":"...and then the Herokulypse","author":"alex-gil","date":"2012-04-23 12:21:10 -0400","categories":["Grad Student Research"],"url":"and-then-the-light-bulb-blew","content":"After two and some years hanging around the Scholars' Lab and earning my badges in the DH community, I finally learned a lesson that should be required learning for all new-comers: plumbing is real. I mean, I was more or less aware of its existence, brief-sightings, a shudder here and there from a ghostly presence. Problem is, I've been focusing on the flashy, large, important, big, fancy, loud, loud, loud uses of already-made tools or those tools I dream of, five-million dollars and the-rest-of-your-life tools. You know: The shiny stuff.\n\nFor the past couple of weeks, I have been working instead on the small stuff that needed to be done to roll [Prism](http://praxis.scholarslab.org/) into production. Enter the plumbing. What I thought would be a series of small tasks turned out to be a major time vacuum. At issue was getting Heroku to play nice with what we had built in the development branch. The first two weeks, Heroku would not even display our site. A series of 'Application Error' messages was all I got. The culprits, in no particular order: the [Asset Pipeline](http://guides.rubyonrails.org/asset_pipeline.html), [Devise](https://github.com/plataformatec/devise) and [Jasmine](https://github.com/pivotal/jasmine). Eventually, with help from above (i.e. E. Rochester and W. Graham), we got the site running ...and then the [Herokulypse](https://github.com/scholarslab/prism/issues/73).\n\nOnce in a while a bug comes, so uncanny, so daunting, that it makes you want to become a novelist. That was the Herokulypse. I obsessed about it for three days at the expense of my dissertation and everything else, with no results. The great obi-wayne-kenobot finally found the problem. To my relief I was on the right track trying to solve it. I just didn't figure out the part about [disabling page caching on the pages controller](https://github.com/scholarslab/prism/commit/5251c6d4d1e50b0b39c418c1764843c4937812f8). Live and learn, and learn I did: Plumbing is real.\n\nI found the lesson timely at a moment when we are debating the [obstacles](http://miriamposner.com/blog/?p=1141) [and](http://byzantini.st/2012/04/coding-and-collaboration.html) [affordances](http://librarian.newjackalmanac.ca/2012/03/gender-coding-libraries-digital.html) [of](http://philomousos.blogspot.com/2012/03/spot-of-mansplaining.html) [coding](http://nowviskie.org/2012/dont-circle-the-wagons/) for digital humanities. The experience with the Herokulypse really brought home for me the idea that code is labor, and that the digital humanities really puts pressure on our notions of leisure, labor and power. I am still working out these issues --issues which all my predecessors seem to have encountered in one way or another-- and will be sure to report back to the public when I have more insights.\n\nIn the meantime, I won't ask you to be careful of what you wish for. On the contrary, I will encourage you to scurry down the rabbit hole of code, that you may never think yourself superior to anyone who leans on the side of hack over yack.\n"},{"id":"2012-04-29-praxis-through-prisms","title":"Praxis, Through Prisms","author":"bethany-nowviskie","date":"2012-04-29 15:48:10 -0400","categories":["Announcements","Grad Student Research"],"url":"praxis-through-prisms","content":"_[Cross-posted from [nowviskie.org](http://nowviskie.org).]_\n\nThis is just a quick post to share two bits of news about our [Praxis Program](http://praxis.scholarslab.org) at the [Scholars' Lab](http://lib.virginia.edu/scholarslab). The first is that I've written an op-ed on Praxis and our Fellows' practicum project for this year's [Digital Campus special issue](http://chronicle.com/section/The-Digital-Campus/491/) of the _Chronicle of Higher Education_.\n\nThe piece was originally titled \"Praxis, Through Prisms\" -- now \"[A Digital Boot Camp for Grad Students in the Humanities](http://chronicle.com/article/A-Digital-Boot-Camp-for-Grad/131665/).\" It's pay-walled, for now, but I'll re-publish it in open access format in 30 days.[caption id=\"attachment_1547\" align=\"alignright\" width=\"120\" caption=\"by Chad Hagen for The Chronicle\"]![prismatic badge](http://nowviskie.org/wp-content/uploads/2012/04/photo_20266_portrait_wide-e1335738451834-214x300.jpg)[/caption]\n\nCheck it out to learn more about the program, get a sneak peek at Prism (launching this Tuesday, which is the second newsflash! congrats, team!) and find out what I see as _the great project_ of humanities computing / digital humanities. Spoiler: it's \"the development of a hermeneutic--a concept and practice of interpretation--parallel to that of the dominant, postwar, theory-driven humanities: a way of performing cultural and aesthetic criticism less through solitary points of view expressed in language, and more in team-based acts of building.\"\n\nOr, in other words, the kind of thing our amazing grad students and diverse crew of scholar-practitioners are working on at [Praxis](http://praxis.scholarslab.org). Through Prism(s).\n\n<!-- more -->I'm incredibly proud of the UVa Library staff who have devoted so much energy to teaching and mentoring Praxis Fellows this year (Wayne Graham, Jeremy Boggs, Eric Rochester, David McClure, and Eric Johnson) -- and _even more proud_ of our first six Fellows themselves, who have built Prism independently. These are Sarah Storti, Brooke Lestock, Annie Swafford, Lindsay O'Connor, Alex Gil, and Ed Triplett. And in fact, they've built Prism from scratch, on time, in public (perhaps the scariest part), with great good humor, and having started with very little practical experience in digital humanities design and development. Lately, I haven't been able to stop myself from interrupting everything in our weekly Praxis meetings to make exclamations like, \"Look at you guys! Look what you can do!\"\n\nSo I hope you'll stay tuned through this week to the [Scholars' Lab blog](http://scholarslab.org/), the Praxis site, and to our [@PraxisProgram](http://twitter.com/PraxisProgram) and [@ScholarsLab](http://twitter.com/ScholarsLab) Twitter feeds, for posts on the launch of the Prism beta, an announcement of our 2012-13 Praxis Fellows, and reflections by current Praxis grad students and the rest of the team.\n\n"},{"id":"2012-04-29-the-last-days-of-development-jasmine-devise-and-visualization-tweaks","title":"The Last Days of Development: Jasmine, Devise, and Visualization Tweaks!","author":"annie-swafford","date":"2012-04-29 12:09:09 -0400","categories":["Grad Student Research"],"url":"the-last-days-of-development-jasmine-devise-and-visualization-tweaks","content":"We're getting close to deploying, so we're making all the necessary tweaks to having Prism ready to go! For this past week, that meant writing Jasmine tests, creating error messages for Devise, and tweaking the functionality of the visualization page.\n\n[Jasmine](http://pivotal.github.com/jasmine/) is a BDD framework for testing javascript.  It basically does for javascript what [RSpec](http://rspec.info/) does for Ruby.  It took over a week to get it set up, and we had to also learn to use a few more ruby gems ([jasmine-jquery](https://github.com/velesin/jasmine-jquery) and  [Guard](https://github.com/guard/guard)), but we finally got it to test our javascript.  It's incredibly useful, not only because it gives an error message when something fails (which is how I learned that our eraser functionality wasn't working properly), but also because it encouraged me to rewrite some of my javascript.  I discovered that I needed to do some refactoring to make the tests easier, and now the code is much clearer.\n\nThe next project was tweaking the visualization page: I had built it so that users had to click on the categories to see the visualizations, but the team  pointed out that users would probably rather see a visualization from the start.  I was able to change our code so that users will always see a visualization for the first category of any given text as soon as the page loads.\n\nFinally, we needed to include error messages for [Devise](https://github.com/plataformatec/devise), our ruby gem that handles user authentication.  Although we had already written a bunch of error and notification messages (ie. \"Invalid Password\" or \"Your password has been emailed to your account\"), we hadn't added the necessary code to make it work.  After adding a partial file and rendering it on the main application page, and then styling it with CSS, I was able to make all our error and notification messages show up!   Now, the design team will make them look attractive and Alex will add internationalization for the messages, and we'll be all set!\n\nIt's been a busy week, but it looks like we'll be able to make our deadline!\n"},{"id":"2012-04-30-welcoming-our-2012-13-praxis-fellows","title":"Welcoming our 2012/13 Praxis Fellows","author":"eric-johnson","date":"2012-04-30 10:18:01 -0400","categories":["Announcements","Grad Student Research"],"url":"welcoming-our-2012-13-praxis-fellows","content":"Before things get too hectic here this week with [#prismlaunch](http://www.scholarslab.org/announcements/praxis-through-prisms/), we wanted to briefly turn our attention to next year to say how excited we are to **announce and welcome the 2012/13 Praxis Fellows** to the second year of the program! They are:\n\n\n\n\t\n  * **Cecilia Márquez**, History\n\n\t\n  * **Chris Peck**, Music\n\n\t\n  * **Claire Maiers**, Sociology\n\n\t\n  * **Gwen Nally**, Philosophy\n\n\t\n  * **Shane Lin**, History\n\n\t\n  * **Sophia Gu**, English\n\n\nWe're particularly excited about the diversity of backgrounds and experiences of this new cohort.  While they may well end up extending the Prism tool developed by this year's project participants, they will bring their own unique combination of disciplinary perspective and technical and social know-how to the program and we look forward to seeing where we end up going together.  We can't wait to get started with them in the fall.\n\nAs always, Praxis 2012/13 will be a journey shared in the open: follow developments at [praxis.scholarslab.org](http://praxis.scholarslab.org/).\n"},{"id":"2012-05-01-announcing-prism","title":"Announcing Prism!","author":"brooke-lestock","date":"2012-05-01 05:07:44 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"announcing-prism","content":"We are pleased to announce the [official beta release](http://prism.scholarslab.org/) of Prism, a tool for collecting and visualizing crowd-sourced interpretations of texts.\n\nIn case you are new to this blog, you should know that Prism is the practicum project of the first cohort of[ Praxis Program](http://praxis.scholarslab.org/) Fellows at the University of Virginia Scholars’ Lab. Six of us have been interning with Scholars’ Lab faculty and staff for the 2011-2012 academic year, as part of a pilot project in team-based graduate methodological training. Praxis aims to produce humanities scholars with practical experience in the work that will underlie theoretical advances in the digital age: the formal representation of knowledge, the design of software and user interfaces, and the management of collaborative teams and complex projects. Over the course of the year, we have been meeting weekly to learn what it takes to theorize and to build a DH project, with the ultimate goal of releasing our own--Prism.\n\nPrism is an experiment in visualizing many readings of a common set of texts, using concepts shared by its users--“the crowd.”  While the Praxis Program itself makes an intervention in graduate training, Prism is an intervention in the [concept of crowd-sourcing](http://chronicle.com/article/Breaking-Down-Menus-Digitally/131658/), which until now has mostly made fact-checkers and copy editors of the crowd.  One of the fundamental questions behind Prism is: what happens when the crowd is asked to imagine and interpret, rather than merely transcribe? The goal of Prism is not to replace individual interpretations, but to produce aesthetic provocations, that is, collective visualizations that incite and encourage conversation.\n\nNow that you know what Prism is, who made it, and why, we invite you to[ go use it](http://prism.scholarslab.org/)! We welcome conversation (we are humanities grad students, after all), so feel free to share your thoughts, feedback, or suggestions in the comment field below. If you encounter a bug, please report it on our[ GitHub page](https://github.com/scholarslab/prism/issues?direction=desc&sort=created&state=open) where you can also find our open source code and documentation.\n\nAnd stay tuned for further blog posts from Praxis Program team members this week, reflecting on what we’ve learned and looking forward to what’s next.\n\n--Sarah Storti and Brooke Lestock, for the 2011-12 Praxis Team\n"},{"id":"2012-05-02-prism-and-praxis-reflections","title":"Prism and Praxis Reflections","author":"annie-swafford","date":"2012-05-02 05:31:17 -0400","categories":["Grad Student Research","Visualization and Data Mining"],"url":"prism-and-praxis-reflections","content":"It's been a whirlwind of a year, but we finally made it! We have a fully functional tool, we've fulfilled our goals from our charter, and we can all look back on everything we've learned this year and be astonished by how far we've come.\n\nAlthough I wasn't an absolute newbie to the DH and programming world when Praxis began, I'm still amazed by how much more I know now.  When I began, I knew HTML, CSS, and a smattering of javascript, and now I also know Ruby, Rails, coffeescript, much more javascript, and how to write tests in RSPEC and Jasmine.  I've also become better at thinking like a programmer; when I started, it took forever to figure out how to do basic programming tasks, like creating loops or methods, but now it feels straightforward.  I've also become much better at trouble-shooting.  I remember when we were all having trouble installing everything we needed for Rails and Ruby, I had no idea what the lines I typed into the terminal meant, and I had no idea what to do when it gave me error messages, but now I understand the error message and know how to fix the problem, or know where to look to find the answers, and I feel much more confident working from the command line instead of the GUI.  The most gratifying experience for me as the developer came from building the highlighting and visualization functionality; it was incredibly empowering to go from having no idea of how to split the task into its component parts or how to then reconnect those parts, to understanding it in the abstract, to actually building the working code.  I especially loved learning how d3.js worked and writing one short algorithm that fundamentally changed the appearance of our text.\n\nI also had a fantastic time working with the rest of the team.  We all learned to work together, to help each other solve problems and design Prism, and to laugh together when things didn't quite go according to plan.  It's been a pleasure and an honor to work alongside them, and I'll definitely miss the group next year.\n\nAlthough we've worked hard and had our stressful times, Praxis has been an incredibly rewarding experience, and I'm grateful to the Scholars' Lab for giving me the opportunity to be a part of such a fantastic program.\n"},{"id":"2012-05-03-insert-bad-prism-pun-here","title":"[Insert Bad Prism Pun Here]","author":"eric-rochester","date":"2012-05-03 05:05:40 -0400","categories":["Grad Student Research"],"url":"insert-bad-prism-pun-here","content":"[caption id=\"\" align=\"alignright\" width=\"180\" caption=\"Image courtesy Bethany Nowviskie\"][![Praxis Program](http://dayofdh2012.artsrn.ualberta.ca/nowviskie/files/2012/03/IMG_2717.jpg)](http://dayofdh2012.artsrn.ualberta.ca/nowviskie/2012/03/28/afternoon/)[/caption]\n\nThis is a blog post (one of many) celebrating the release of [Prism](http://prism.scholarslab.org/). This project and web site are the outcome of the [Praxis Program](http://praxis.scholarslab.org/), an experiment in graduate methodology training that we've been conducting at the Scholars' Lab.\n\nWell, Prism is one outcome of the Praxis Program. The most visible, at the moment. But also the least important.\n\nFor one thing, post-launch can be one of the most stressful phases of a project. Surprise! Everyone hits your shiny new site, and it falls over like a house of cards. Then the world watches as you panic and scramble to put it back together. It's good for character and a good learning experience. But not much fun.\n\nAlso, the senior members of this collaboration—the fulltime faculty and staff of the Scholars' Lab—have always had a different goal for this project: the students themselves. Over the course of the last year, they've received learning opportunities that I wish I'd had when I was a graduate student. We've tried to equip them to conceive of and plan large, interesting research projects; to evaluate and learn the technologies required to implement those projects; and to administer and manage them from inception to completion. Unfortunately, the most we can do is point out the outlines of the task before them and point them in (hopefully) the right direction. This is a journey they'll have to travel for many years. But whatever their career paths—whether tenure track faculty, adjunct faculty, alt-ac, or private sector—these skills will be needed.\n\nAnd in this sense, both Praxis and Prism are just getting started.\n"},{"id":"2012-05-03-on-collaboration","title":"On Collaboration","author":"jeremy-boggs","date":"2012-05-03 11:10:59 -0400","categories":["Grad Student Research"],"url":"on-collaboration","content":"Before I started making and sharing stuff, I always thought it was the maker who had power and authority to dispense knowledge. After I started making and sharing stuff, I began to understand that readers and users, not makers, had far more power, to take in that knowledge, critique it, and use it in new ways. At some point, though, if you're as fortunate as I have been, the lines between makers and users, teachers and students, become so porous that you just have_ collaborators._\n\nIt's kind of frightening to realize how much you don't know, and once you realize this, it then becomes frightening to make and share something. But I'm starting to understand, more than I ever have, that this is all part of the process of collaboration, that it's good to feel this way, and that it shouldn't debilitate us from learning and doing. Collaboration should provide some relief, even confidence, but it should not abdicate us from learning how to do new things. It demands that we pay attention when we don't know something, and obligates us to look for further clarification. It requires us to respect the skills and perspectives our collaborators bring to the table. It doesn't mean you have to learn everything. That's impossible and silly. But it also doesn't mean you should avoid learning _something_. Collaboration is yaking and hacking, then yaking and hacking some more, all the way down.\n\nI'm not exaggerating when I say I've never been more uncomfortable about things, and more excited by that feeling, than I have the last few months working in the [Praxis Program](http://praxis.scholarslab.org). Most of the stuff we covered was brand new to me, too, so I was every bit as much a student during the process. My lack of comfort with many of the technical topics was completely countered by the studio-like atmosphere the Praxis Program team managed to create by simply working together.\n\nWe're all collaborating, all the time; if you don't think you're collaborating, you're probably not paying attention. In the spirit of that, the folks in the Praxis program have [made a thing](http://github.com/scholarslab/prism) and [shared it](http://prism.scholarslab.org). I hope I speak for everyone when I say, I hope you'll collaborate with us.\n"},{"id":"2012-05-03-update-diy-aerial-photography","title":"Update: DIY Aerial Photography","author":"chris-gist","date":"2012-05-03 07:09:23 -0400","categories":["Geospatial and Temporal"],"url":"update-diy-aerial-photography","content":"It's been over a month since [our last post](https://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography/) re DIY aerial photography. Since then, we hosted two [GIS workshops](http://www.lib.virginia.edu/scholarslab/resources/class/Spring2012GIS/), a [THATCamp Virginia](http://virginia2012.thatcamp.org/) session, and our first real \"job\" flight for an [art installation](http://www.virginia.edu/uvatoday/newsRelease.php?id=17953).\n\nOur workshops were scheduled for Wednesday morning, April 18 and Thursday afternoon, April 19.  Unfortunately, it started raining hard just before our Wednesday session and we were forced to go to Plan B. We decided to fly the balloon in the mural hall located inside Clark Hall.  The ceiling there is two stories tall.  We did get some interesting images from that \"flight.\"\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/05/IMG_3010-1024x768.jpg)\n\nAlthough we think we made the point with the flight inside the building, we hoped for much better conditions the next day.\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/05/6948722440_6a3025776c_b-300x225.jpg)\n\nOn Thursday, the weather was much better but there was more wind than we hoped.  We knew the images wouldn't be great but decided to fly anyway.  It was quite an event, [NBC 29](http://www.nbc29.com/story/17590268/uva-students-take-photographic-balloon-to-the-sky) (news video) and [UVa Today](http://www.virginia.edu/uvatoday/newsRelease.php?id=18225) (lots more pictures and story) came.  We did two new things for this flight.  First, John Porter lent us a GPS to put on the balloon rig.  Second, we inadvertently switched the camera to video mode when loading into the balloon rig.\n\nThe GPS addition turned out to be a great thing.  It gave us a 3D profile of our flight which we converted to KML for viewing in Google Earth.  Please feel free to download it [here](http://lat.lib.virginia.edu/tmp/20120419 DIY Balloon Aerial 3D.kmz).\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/05/GEscreenShot1.jpg)\n\n\n\n\nThe mistakenly-created video is interesting in places but is not good for those among us who may have motion sickness issues.\n\n\n\n\n[iframe width=\"600\" height=\"450\" src=\"http://www.youtube.com/embed/KYilJ3kDLuI\"]\n\n\n\n\n[caption id=\"attachment_4478\" align=\"aligncenter\" width=\"1024\"]![](http://www.scholarslab.org/wp-content/uploads/2012/05/6948719170_bfa9d9dbd7_b.jpg) The Space Stare[/caption]\n\n\nNext up was the THATCamp Virginia Workshop.\n\n\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/05/20120420-UVA-Alderman-THATCampVA-DIY-Aerial-Workshop-group-1024x768.jpg)\n\n\n\n\nWe had about ten THATCampers and decided to fly over Nameless Field.  And since our GPS experiment went so well, it now has become a permanent fixture on our flights.  We got a lot of great shots of the tennis courts.  If you look closely, you can see the crew working to prep the surfaces for new paint.\n\n\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/05/IMG_3068-1024x768.jpg)\n\n\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/05/IMG_3106-1024x768.jpg)\n\n\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/05/IMG_3120-1024x768.jpg)\n\n\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/05/IMG_3137-1024x768.jpg)\n\n\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/05/20120420-UVA-THATCampVA-John-Porter-balloon-768x1024.jpg)\n\n\n\n\nWe have a KMZ of the flight available [here](http://people.virginia.edu/~dcg6b/THATCampVA.kmz).\n\n\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/05/THATCamp.jpg)\n\n\n\n\nNext was the art installation being created on UVa Foundation property on 29 North.  A group of students working with Prof. Megan Marlatt are painting the cat shown below in the parking lot.  They are filling in the cat with stencils of a large number of other cats.\n\n\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/05/6976145902_d810d3eaf5_b.jpg)\n\n\n\n\nHere is our documentation of their work.\n\n\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/05/6976147874_138445fffa_b.jpg)\n\n\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2012/05/7122232509_3d21486fda_b.jpg)\n\n\n\n\nOf course, we have a KML of the whole mission (two flights) and some images and the original conceptual plan.  Please download it [here](http://people.virginia.edu/~dcg6b/Studio%20Art.kmz).\n\n\n\n\nOur next big balloon event will be teaching some DIY aerial photography at the [STEM conference for k-14 educators](http://www.isat.jmu.edu/stem/workshop12.html) in June.\n\n\n\n\nOh and by the way, someone asked how we get that large balloon though rather small doorways.\n\n\n\n\n[iframe width=\"600\" height=\"450\" src=\"http://player.vimeo.com/video/41862950\" ]\n"},{"id":"2012-05-04-kenneth-dean-ritual-revolutions","title":"Kenneth Dean: Ritual Revolutions","author":"ronda-grizzle","date":"2012-05-04 07:57:05 -0400","categories":["Podcasts"],"url":"kenneth-dean-ritual-revolutions","content":"**Kenneth Dean**\n**Ritual Revolutions: Temple Networks Linking South China to Southeast Asia**\n\nDr. Kenneth Dean, Lee Chair in Chinese Cultural Studies in the Department of East Asian Studies at McGill University, spoke as part of the Digital Humanities Speaker Series on March 29.\n\nThe Digital Humanities Speaker Series is co-sponsored by IATH, SHANTI, and the Scholars' Lab\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.15310479307/enclosure.mp3\"]\n"},{"id":"2012-05-06-future-possibilities-for-prism","title":"Future possibilities for Prism","author":"david-mcclure","date":"2012-05-06 07:11:35 -0400","categories":["Grad Student Research","Research and Development"],"url":"future-possibilities-for-prism","content":"It's been incredibly exciting to watch Annie, Alex, Lindsay, Brooke, Sarah, and Ed work together over the course of the last two semesters to take Prism from idea to [working software](http://prism.scholarslab.org). Considering the fact that most of them hadn't ever written a line of Ruby, Javascript, or CSS when they started last semester, the end result is pretty remarkable.\n\nOne of the reasons that programming is so invigorating is that software is constantly leaning forward into further elaboration and complexity. Every feature is the precursor of a hundred possible new ones. Code is always the double-delight of what it is and what it could become.\n\nPrism currently occupies the place in the evolution of a software project where there's enough functionality in place to really engage with the shape of the idea, but still enough unfinished that there's space for broad, exploratory thought about the future direction of things. Annie did a fantastic job implementing a core interface that allows users to apply concept verticals (\"highlighters\") to texts. Looking forward, the motivating question is how this capability can be leveraged to produce concrete scholarly outcomes - ideally, new understandings of texts.\n\nThe first public release seems like an excellent opportunity to start trying to whittle down this question to a set of specific research goals to guide future development. To me, Prism points towards two general lines of inquiry:\n\n\n\n\t\n  1. How can experiments in collaborative markup capture **uncommon** or **dissenting **readings? The concept of crowdsourcing - and, really, the social internet in general - has proven highly adept at extracting majority opinions, at taking the pulse of a group of people. What is \"liked\" by the community of participants? Where is there agreement? Always implicitly contained in the data that yields these insights, though, is information about how individuals and dissenting groups diverge from the majority consensus.\n\nUsually, in the context of the consumer web, these oppositions are flattened out into monolithic \"like\" or \"dislike\" dichotomies. Tools like Prism, though, capture structurally agnostic and highly granular information about how users react to complex artifacts (texts - the most complex of things). I think it would be fascinating to try to find ways of analyzing the data produced by Prism that would illuminate places where the experimental cohort profoundly disagrees about things. These disagreements could be interesting irritations into criticism. _Why_ the disagreement? What's the implicit interpretive split that produced the non-consensus?\n\n\t\n  2. Continuing on the concept of the \"experiment.\" Prism points at the provocative possibility that literary study could literally take the form of experiments, similar in structure to the \"studies\" conducted in disciplines like research psychology, sociology, and experimental philosophy. Literary criticism generally asks questions about how texts _can _be read. The critic conjures highly creative statements of meaning that often stake their value claim on the extent to which they are unexpected, unanticipated, not obvious, or atypical.\n\nPrism, meanwhile, provides information about how texts _just are_ read. I think it would be fascinating to take this to the next level and stage formal experiments in which subjects are presented with a text and asked to mark it up with a small number (even just one) of carefully-selected, highly-controlled terms. Done responsibly, and with a healthy aversion to the sugary siren call of Data in a field that's fundamentally in the business of studying art, I think that this could provide fascinating insights about everything from the concept of Kantian \"expertise\" in the formation of aesthetic judgments to the questions about how people of different ages, ethnicities, genders, and disciplinary affiliations engage with texts. How does a college freshman read differently than a 6th year English graduate student? How do physicists read differently from philosophers?\n\n\nEither way, I can't wait to see [where it all goes](http://praxis.scholarslab.org).\n"},{"id":"2012-05-07-no-famous-last-words","title":"Got Prism?","author":"alex-gil","date":"2012-05-07 10:47:07 -0400","categories":["Grad Student Research"],"url":"no-famous-last-words","content":"Funny thing about collaborative projects in the (digital) humanities: No famous last words. A nun scholar lives and dies by her last words, but around here we're just part of an [incredibox](http://www.incredibox.com/en/#/application). On the week when we release [Prism](http://prism.scholarslab.org/), I could regale you with all the lessons I learned this year (right about doubled the stock), how wonderful this whole experience has been (I wouldn't trade your post-doc for my praxis), what exciting times we live in (don't look down), but after perusing the drafts queued up on the wp-admin, it sounds like those tunes will be sung to perfection by my fellow praxers. Let me instead play a bit.\n\nI'm sure it has not escaped your attention that Prism plays a sweet prank on the discursive class. The premise: graduate students collaborate to build a hermeneutic tool while learning much-needed technical and managerial skills their hermeneutic training made invisible. The punchline: Except for the cast of characters witnessing its birth, the tool the students make renders the hermeneut invisible.\n\nPrism yanks the out-of-line hero in Aeschylus right back into the incredibox where he came from. Your username is only visible to you. Once you register for the first time, it descends to the MySQL underworld, effectively re-enacting the Eleusinian Mysteries that preceded the birth of Greek tragedy. \"You can check out anytime you like/But you can never leave.\" Brilliant!\n\nBUT, the playful prank is not without use or merit, and we are laughing with you, not at you. Heck, I AM one of you. Yes, Prism abstracts interpretative labor (c.f. Baudrillard on Symbolic Exchange and Death), but it does so without foul. I'm guessing most folks would find the momentary anonymity quite refreshing. Truth be said, we don't need to doodle our signatures on every wall. (c.f. Derrida... No? You don't have to).\n\nFurthermore,  and as David shows by example on [his recent post](http://www.scholarslab.org/?p=4279), many new avenues of research open up with Prism. The tool does not replace, but sits to the left of other hermeneutic approaches. To pick one question of the many new questions that Prism opens up:\n\n\n<blockquote>_Given that anonymity changes the way we enact readings; _and_, given that the ruling ideology in Amerika would have us believe that racial identity is a thing of the past: How does the invisible subject react to symbolic Law__?_</blockquote>\n\n\nObviously, that is not the only new question Prism can ask, nor the first to come to mind for most users. We worked hard to make it generic enough that many disciplines could find plug-n-play uses for it. And then again, what else could Prism lead to, but renewed interest in interpretation as a microscopic-practice?  Yes, the lovely texts! Don't be fooled by the promise of macroscopic understandings of difference. That is just a (mc)lure.\n\n**\n**\n"},{"id":"2012-05-09-calling-all-alt-academics","title":"Calling All #Alt-Academics!","author":"katina-rogers","date":"2012-05-09 09:35:19 -0400","categories":["Announcements"],"url":"calling-all-alt-academics","content":"I'm happy to announce that [a census of alternate academics](http://mediacommons.futureofthebook.org/alt-ac/who-we-are), the first public-facing component of my work with the Scholarly Communication Institute, is now open to contributions. If you have graduate training in the humanities and work outside of the tenure track, I'd like to warmly invite you to [add your information](http://altacademy.wufoo.com/forms/who-we-are/) to the growing database. Not #alt-ac? [Check out the report](http://altacademy.wufoo.com/reports/who-we-are/) to learn more about who we are and what we do.\n\nAs I discussed in [an earlier post](http://www.scholarslab.org/scholarly-communication-institute/an-intro-to-my-work-with-sci/), the census has a dual purpose: First, it will serve the many individuals who are employed in (or considering) alternate academic roles by showing the breadth and depth of career trajectories that can follow graduate work in the humanities. The resulting database may help people to discover others with shared interests, find potential project collaborators, or open up new lines of inquiry. Second, it serves as an important first step towards the survey that SCI will conduct, which aims at better understanding career preparation and #alt-ac employment in relation to humanities graduate programs.\n\nI'd like the database to be as broad and truly representative as possible, which means I'll need help in extending its reach. Please forward the link widely and encourage the #alt-academics you know to contribute--the database becomes more useful as more people join in.\n\nThis census is part of a suite of new content and features at #Alt-Academy; the [announcement](http://mediacommons.futureofthebook.org/alt-ac/pieces/cfps-new-features) is restated below. Please read, contribute, and circulate!\n\n---\n\nWe are very happy to announce a new phase of publication at [#Alt-Academy](http://mediacommons.futureofthebook.org/alt-ac/), an open-access online project at MediaCommons. #Alt-Academy was launched last summer with 24 essays by 33 authors, highlighting the role of \"alternative\" academic professionals in the humanities and related fields. The four projects joining #Alt-Academy today promise to open the publication to an even richer and more diverse set of voices.\n\n\n\n\n\nPlease consider contributing to:\n\n\n\n\n\n\n\n\n\n\n\n\t\n  * \"Who We Are,\" a census of the community, led by Dr. Katina Rogers, who is also (with the Scholarly Communication Institute) conducting a survey of graduate preparation for alternative academic careers: [http://mediacommons.futureofthebook.org/alt-ac/who-we-are](http://mediacommons.futureofthebook.org/alt-ac/who-we-are)\n\n\n\n\n\n\n\n\n\n\n\n\t\n  * \"Visible Margin,\" a forthcoming regular publication of the site, edited by Drs. Polina Kroik and S. Miller.  Visible Margin will feature creative and critical work by PhDs, graduate students, and alternative academics: [http://mediacommons.futureofthebook.org/alt-ac/visible-margin](http://mediacommons.futureofthebook.org/alt-ac/visible-margin)\n\n\n\n\n\n\n\n\n\n\n\n\t\n  * \"Getting There 2,\" a second \"Getting There\" cluster for #Alt-Academy, offering practical pathways, signposts, and advice for people considering alternative academic careers. This cluster will be edited by Dr. Brian Croxall: [http://mediacommons.futureofthebook.org/alt-ac/pieces/cfp-getting-there-2](http://mediacommons.futureofthebook.org/alt-ac/pieces/cfp-getting-there-2)\n\n\n\n\n\n\n\n\n\n\n\n\t\n  * and \"Alt-Ac Goes Entrepreneur,\" a new cluster to be edited by Dr. Daveena Tauber, examining the role of entrepreneurialism in academic training, the knowledge economy, and the alternative academic community: [http://mediacommons.futureofthebook.org/alt-ac/pieces/cfp-alt-ac-goes-entrepreneur](http://mediacommons.futureofthebook.org/alt-ac/pieces/cfp-alt-ac-goes-entrepreneur)\n\n\n\n\n\n\n\n#Alt-Academy also welcomes proposals for further new clusters and features.  For more information, see \"How It Works\" on our MediaCommons site.\n\n\n\n"},{"id":"2012-05-11-wrapping-it-up-teidisplay-and-collaborative-mentoring-with-utuva","title":"Wrapping it Up: TeiDisplay and Collaborative Mentoring with UT/UVa","author":"carin-yavorcik","date":"2012-05-11 09:51:32 -0400","categories":["Digital Humanities"],"url":"wrapping-it-up-teidisplay-and-collaborative-mentoring-with-utuva","content":"I'm very excited to announce that my colleague [Zane](http://www.scholarslab.org/author/zschwarzlose/) and I have completed our work on the TeiDisplay plugin for Omeka! It's been a little while since I last posted, so if you'd like some background, check out our [previous posts](http://www.scholarslab.org/category/tei/) on this collaboration between the Scholars' Lab and the University of Texas School of Information. We have made several changes/additions, though the final product isn’t quite ready for release yet.\n\n- We fixed a few bugs in the import process from the TEI Header to Omeka's Dublin Core fields. Additionally, we created a map detailing which fields correspond to which elements. This map also explains how users can customize the import process if the current setup is not optimized for their documents.\n\n- The main XSLT stylesheet now supports every element in the [TEI Lite customization](http://www.tei-c.org/release/doc/tei-p5-exemplars/html/teilite.doc.html). We also reorganized it, adding headers to correspond with the headers in the TEI Lite documentation where particular elements are discussed, so that those who are familiar with XSLT can find and edit elements easily. At a minimum, in the transformation process each tag is given a <span class=\"TagName\">, and many have additional styles as well. This way, we guarantee that a defined set of elements will render in the document. Users unfamiliar with XSLT can also use these span classes to customize the style of their documents using CSS.\n\n- Some of the elements that have been styled for more than just a span class may special requirements in order to render, such as specific attributes that need to be included in the TEI text. We have written a series of formatting tips to help users understand what the XSLT is looking for when it attempts to render the documents.\n\n- The plugin now has functionality to link TEI text to corresponding page images. This is done using the @facs attribute with the <pb> element. Further details on how to use this feature are provided in the documentation.\n\n- We outlined a number of small changes that a user can make outside of the plugin code itself in order to improve functionality. For example, when displaying longer documents, users might wish to create an anchor that allows readers to jump directly from the top of the page to view the item metadata, which is usually at the bottom. This is done on one of the Omeka PHP documents, so it isn’t officially part of the TeiDisplay package, but we include the appropriate code in our documentation and describe where to put it.\n\n- We've written a number of troubleshooting tips, as well as a more detailed guide to installing the plugin and updating documents.\n\nI'm very happy with the changes we made to the plugin, but I think that overall, they were not as extensive as I expected going in. I think the real value added with this project was the extensive documentation we've created, as well as a [demo site](http://tclement.ischool.utexas.edu/teidisplay) that gives examples of the plugin in action. Armed with this documentation, users will not only be able to better understand how the plugin works with their documents out of the box, but they will also be better able to customize it to fit their needs.\n\nOver the coming months the Scholars' Lab will be conducting more rigorous testing of the plugin as well as making some other additions. Hopefully, the final product will be ready late this summer.\n\nThanks very much to Wayne and Bethany at the Scholars’ Lab, and to our adviser Tanya Clement at UT, for giving us the opportunity to be a part of this project!\n"},{"id":"2012-05-14-on-complexity","title":"On Complexity","author":"wayne-graham","date":"2012-05-14 12:54:21 -0400","categories":["Grad Student Research"],"url":"on-complexity","content":"When I wrote my first \"real\" code for a website, things were a lot simpler. I was taking SGML TEI files and running them through a DSSSL generator to create static HTML files. It was pretty straight-forward: I would tag a document, cross my fingers that it would validate, then run a script that would regenerate the entire site. CSS hadn't been invented yet, so the site layout was handled in a table, and really the only way 'discover' the content in the site was to actually read everything that was contained in the 'site.' There was a team of five of us attempting to figure all of this, and generally the feature that won was the one we figured out first.\n\nSkip forward a decade, and I now use a dizzying array of frameworks, tools, occasional tricks for any given project. As a case in point, look at any of the plugins our group have been developing for Omeka: we write the code in PHP, then use tooling written in Ruby, Java, and nodeJS.  The real question of how to explain exactly what goes on in a modern web-based application is one I, personally, struggled with over the last year. There's typically a database tier, a presentation tier, business logic, external libraries, and framework idioms and vernacular to explain. Even once you get an OK grasp of how a programming language functions, the jump from making a simple loop to a building a web application that 'works' is quite a feat. On top of this, add the softer skills of translating the intent of a feature into actual code and coordinating changes with multiple people working in the same places in the code base, and even what it means to work together in a productive way can be challenging.\n\nJust to give you an idea of the complexity of the things swimming around in the various heads of the team that developed [Prism](http://prism.scholarslab.org), this is a visualization of the library dependencies in the project:\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/05/gem_graph1-300x30.png)](http://www.scholarslab.org/wp-content/uploads/2012/05/gem_graph1.png)\n\nWith a system this complex, true collaboration is needed, with different people taking on different tasks and responsibility for sections of the code, and working together.\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/05/impact-300x176.png)](http://www.scholarslab.org/wp-content/uploads/2012/05/impact.png)\n\nYou can see in this [impact graph](https://github.com/scholarslab/prism/graphs/impact) that there isn't just one person making all the changes. You also see experiments that were started (and ended). Not only are experiments waxing and waning, there is also no single contributor who dominates the code base. I feel this is an exceptionally important take away from this last year; no one collaborator on [Prism](http://prism.scholarslab.org) would have been able to execute this project alone.\n\nWhen I reflect on the changes in how web applications are developed since I started in this kind of work, I see no real method to the madness. It was a wild-wild west, and we figured out what we needed to (often making what in retrospect were horrible technical decisions) in order to ship our sites. But things were much simpler: browsers were much simpler, almost everyone had a dial-up connection, and you could really figure out how someone implemented a cool feature (like an 'under construction' gif) by looking at the source code. Fast-forward a decade, and there are many more balls to juggle. Viewing the source code for a web application probably won't get you very far.\n\nAlong with understanding how software is put together, there are the more frustrating elements of development. Sometimes approaches just don't work, or they work in one environment, but not another. Dealing with these issues, and being able to adapt or completely rethink a plan of attack, is a skill in itself. Being able to pivot technically, and rethink a given approach (or even better, knowing where that line is) is one of the more difficult skills to learn. As I've watched the graduate students in our [Praxis Program](http://praxis.scholarslab.org) over the last year, I've seen them transform how they think about the problems they encounter, applying the same analytical skills they use in their scholarship in their coding practices. My hope is that the practice in critical thinking and scholarly argument with which they learned to imbue the actual code base of Prism will help them as they go out in to the world as scholars -- and will continue to serve them in the future.\n\n\n"},{"id":"2012-05-14-spatial-in-the-scholars-lab-spring-2012","title":"Spatial In the Scholars’ Lab: Spring 2012","author":"kelly-johnston","date":"2012-05-14 05:17:47 -0400","categories":["Digital Humanities","Geospatial and Temporal","Grad Student Research"],"url":"spatial-in-the-scholars-lab-spring-2012","content":"With classes over and finals behind us, let's look back on the Spring 2012 semester with a spatial eye.  Yes, January-May was a very mappy time in the Scholars’ Lab!\n\n[caption id=\"attachment_4570\" align=\"alignnone\" width=\"343\" caption=\"University of Virginia Library Scholars' Lab\"][![](http://www.scholarslab.org/wp-content/uploads/2012/05/P1010036-766x1024.jpg)](http://www.scholarslab.org/digital-humanities/spatial-in-the-scholars-lab-spring-2012/attachment/olympus-digital-camera/)[/caption]\n\n\n## Workshops\n\n\nFrom January through April twice every week my colleague Chris Gist and I welcomed faculty, staff, and students to our free “no experience required” hands-on spatial workshops. The picture below gives you a glimpse behind the curtain at our master workshop list on the left.   In case you missed one, our handouts, datasets, and workshop presentations are all available here: [http://www.lib.virginia.edu/scholarslab/resources/class/Spring2012GIS/ ](http://www.lib.virginia.edu/scholarslab/resources/class/Spring2012GIS/)\n\nThough our workshops have been popular for many semesters, we shook it up this spring with new sessions based on questions we’ve been asked, trends we’ve spotted, or mappy topics we wanted to get to know better.  And what a treat it was to collaborate with friends and colleagues from the Scholars’ Lab  Research and Development team (thanks Eric Rochester), UVA Facilities Management (thanks Drew MacQueen and Artie McDonough), and the UVA Library (thanks Tim Morton).\n\nFor all 24 workshops, folks turned out from across disciplines, asked lots of questions, shared their mappy stories, and learned new skills after a hands-on hour.  Later this summer we'll announce our Fall 2012 workshop series.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/05/IMG_9164-1024x768.jpg)](http://www.scholarslab.org/digital-humanities/spatial-in-the-scholars-lab-spring-2012/attachment/img_9164/)\n\n\n## Classes\n\n\nLike us, students get excited when they see their datasets come alive on a map.  Some of our best experiences working with geospatial resources in the Scholars’ Lab come from collaborations with faculty to develop specialized training for their courses.  American Studies, Architecture, English Criticism, Urban Planning, Environmental Science…we worked with them all this spring.  Check our whiteboard again and on the right side you’ll see some of our Spring 2012 faculty partnerships.  Now we’re meeting with faculty to prepare for Fall 2012.\n\n[caption id=\"attachment_4561\" align=\"alignnone\" width=\"717\" caption=\"Spring 2012 American Studies 3559 - Mapping Shantytowns\"][![](http://www.scholarslab.org/wp-content/uploads/2012/05/amst-1024x702.jpg)](http://www.scholarslab.org/digital-humanities/spatial-in-the-scholars-lab-spring-2012/attachment/amst/)[/caption]\n\n\n## Support in the Scholars’ Lab\n\n\nIn the Scholars’ Lab, we relish a “GIS Emergency”.   Typically a student visits the lab to work on a project, hits a roadblock, and contacts us for help.  So we drop what we’re doing and spring into action.\n\n\n<blockquote>“How do I use this cool historic map in my GIS project?\"\n\n“I don't know exactly where it happened, but I need to show it on a map”\n\n\"I have a Mac.  I want to run ArcGIS.  Help.\"\n\n\"We're leaving for Guatemala tomorrow. May I borrow a GPS?\"\n\n“US Census data from 1950…where do I find it?”\n\n\"How can I compare solar exposure for my study sites in the French Alps?\"</blockquote>\n\n\nCombining drop-in support with email queries, our official tracking system, and pre-arranged meetings gives us a great view of how spatial topics are increasingly visible across academic disciplines.  And it means I have 1,495 emails in my GIS Support folder since Jan 1, 2012.\n\n\n## On The Horizon\n\n\nSoon we’ll end the spring semester by hosting Scholars' Lab grad fellow Ed Triplett for our [last Scholars’ Lab mappy event of the Spring](http://www2.lib.virginia.edu/scholarslab/about/events.html).  And we'll start the summer with [Mapping Yourself](http://alumni.virginia.edu/events-reunions/reunions/class-reunions/events/) for Reunions Weekend.   Though our two hard-working GIS students will be gone for the summer, one having graduated and one home to China, we'll continue our spatial work at James Madison University where we'll lead two sessions at the [4th National Summit on Geospatial Technologies in K-14 Education](http://www.isat.jmu.edu/stem/workshop12.html).  And our online work continues with contributors and reviewers as we add new content to [Spatial Humanities Step By Step](http://spatial.scholarslab.org/), a peer-reviewed series of tutorials and guides to getting things done in teaching and research with spatial tools and resources.  We’ll train a new group of students for summer fieldwork at [Mountain Lake Biological Station](http://www.mlbs.virginia.edu/) and [keep an eye on the sky ](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography/)as we pursue [do-it-yourself remote sensing](http://www.scholarslab.org/geospatial-and-temporal/update-diy-aerial-photography/).\n\nHave a very mappy summer!\n"},{"id":"2012-05-15-praxis-doing-is-thinking","title":"Praxis: Doing is Thinking?","author":"lindsay-o’connor","date":"2012-05-15 06:52:41 -0400","categories":["Grad Student Research","Research and Development"],"url":"praxis-doing-is-thinking","content":"Speaking off the cuff to a group of prospective Praxis Program applicants in March, I found myself explaining how “aesthetic provocation” isn’t the same as argument, that Prism isn’t a tool that produces criticism as much as it is a tool that might prod us to see, read, or critique in new ways. I didn’t know I knew that until I said it in front of 30 people, and that experience might serve as a nice synecdoche for what Praxis is all about. I calmed down about all those big ideas I blogged about in the fall when I started to have design deadlines. I quickly remembered my pre-graduate school ambivalence about being a manager when trying to coordinate design team tasks. I learned CSS tricks when I had things I needed to style in certain ways. Sometimes I worry that I just gave up on all the big ideas in favor of shorter-term, more immediately satisfying web design lessons and goals, that I somehow betrayed my disciplinary yack in favor of less heady but more hands-on hack. But maybe we’ve just been too busy with the latter lately, and these final blogs provide a place for the former now that Prism has been released.\n\nI know, as Bethany eloquently explains in her [Chronicle article](http://chronicle.com/article/A-Digital-Boot-Camp-for-Grad/131665/) and [her blog](http://nowviskie.org/2012/praxis-through-prisms/) that Prism and much of DH is focused on creating and enabling new hermeneutics rather than on advancing critique. For that reason, I’m not yet imagining a digital component to my dissertation, but that doesn’t mean I’m not a digital humanist. Praxis has shown me a different, collaborative model of scholarship. Prism as it exists now could use some supporting research and theorization, especially about what kind of hermeneutics our working visualization instantiates or suggests, but it’s still a starting point for many possible readings or arguments to which many people can contribute.\n\nI may not be creating a digital dissertation chapter like Annie or a whole digital edition like Alex, but I did manage to learn enough about HTML and CSS this year to style a decent looking website, and I think I can figure out how to style one that looks even better. I could have kept tinkering with Prism for much longer and I know it's not the most sophisticated or aesthetically pleasing site you've ever seen, but I'm still proud to have been part of making it look how it looks. I will continue to seek out opportunities to become a better designer and to work on projects that interest me, and I’ll soon be starting design work with the wonderful UVa English department folks behind redschoolhouse.org. And that means I’ll probably stop into the Scholars’ Lab every so often for design advice from Jeremy and technical help from Wayne and Eric. I know I’ll see Sarah and Annie in the English department, and I hope to run into Ed in line at the dumpling truck. I wish Brooke and Alex all the best next year wherever they find themselves. I have learned so much from and with all you wonderful people, and the technical stuff is the least of it.\n"},{"id":"2012-05-16-the-sidlers-guide-just-smile-and-hit-your-mark","title":"The Sidler's Guide: Just Smile and Hit Your Mark","author":"ed-triplett","date":"2012-05-16 07:08:37 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"the-sidlers-guide-just-smile-and-hit-your-mark","content":"I can't help feeling like a Seinfeldian sidler. [Prism](http://prism.scholarslab.org/) is up. The code has been written, the pages designed, and the tests passed. Everyone is gathering together getting ready for the post-prism photo-op when out of nowhere some bearded weirdo who smells like tapas and olive oil rides into the frame on a tiny horse just before the flash goes off.\n\n\"Remember me amigos? I was here when Prism was static!\"\n\nHonestly it has been very nice to see Prism come together while I was in Spain. I am also proud to see all my Praxis fellows transform into real code monkeys. I had a very quick and grainy glimpse of a functional Prism page when I attended a Praxis meeting about a month ago via webcam, but last week I got the full tour. In a lot of ways I am in a unique position to appreciate how far Prism has come because the software was a squalling infant when I left. Prism is all grown up now, and it is time to send it into the world to socialize with others so it doesn't end up too \"home-schooled.\"\n\nWhile my contribution to Prism was almost entirely reserved to designing static templates, I can confidently say I feel very connected to the final product that the group executed. I spent a lot of time in the the [Scholars' Lab](http://lib.virginia.edu/scholarslab) during the Fall and early part of the Spring semester, and I hope to be around just as much in the future. Leading up to this Fall, I was far from a rookie when it came to applying graphics solutions to humanities subjects, but until this year, I was entirely at the mercy of a mouse. I am not going to claim I am a huge fan of VIM, but programming is no longer something I try to avoid when I consider how to solve a DH problem.\n\nFinally, I'd like to thank the entire Scholar's Lab staff for their help and patience. I'm sure the first few weeks of Praxis seemed like you adopted too many puppies, but we are house trained now and github is always willing to play fetch with us.\n"},{"id":"2012-05-23-thatcampva-2012-neatline-workshop","title":"THATCampVA 2012: Neatline Workshop","author":"ronda-grizzle","date":"2012-05-23 07:26:02 -0400","categories":["Podcasts"],"url":"thatcampva-2012-neatline-workshop","content":"**David McClure & Eric Rochester\nTHATCampVA 2012 Neatline workshop**\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/05/thatcamp-va-2012-logo-300x113.png)](http://www.scholarslab.org/podcasts/thatcampva-2012-neatline-workshop/attachment/thatcamp-va-2012-logo/)\n\nOn April 20, 2012, Scholars' Lab developers David McClure and Eric Rochester gave a talk for interested attendees of THATCAmpVA 2012 introducing  Neatline + Omeka and updating us on the latest developments in the project. Enjoy!\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.15640754273/enclosure.mp3\"]\n"},{"id":"2012-05-29-gff-hankins","title":"Graduate Fellows Forum: Gabriel Hankins","author":"ronda-grizzle","date":"2012-05-29 09:56:55 -0400","categories":["Podcasts"],"url":"gff-hankins","content":"**Gabriel Hankins**\n**The Map of an Argument: the Spatial Humanities, Modernism, and the League Moment, 1914-1921**\n\nOn April 11, 2012, Scholars' Lab Fellow Gabriel Hankins spoke about his work with the Scholars' Lab during his fellowship in a talk entitled _The Map of an Argument: the Spatial Humanities, Modernism, and the League Moment, 1914-1921_.\n\nDr. Jennifer Wicke, Professor, UVa Department of English, acted as respondent for Gabriel's talk.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.15515954661/enclosure.mp3\"]\n"},{"id":"2012-05-29-gff-lewis","title":"Graduate Fellows Forum: Randi Lewis","author":"ronda-grizzle","date":"2012-05-29 10:05:58 -0400","categories":["Podcasts"],"url":"gff-lewis","content":"**Randi Lewis**\n**Mapping the Merchant World: Early American Commerce in Salem, Massachusetts, 1763-1819**\n\nOn April 26, 2012, Scholars' Lab Fellow Randi Lewis spoke about her work with the Scholars' Lab during her fellowship in a talk entitled _Mapping the Merchant World: Early American Commerce in Salem, Massachusetts, 1763-1819_.\n\nDr. Max Edelson, Associate Professor, Corcoran Department of History, acted as respondent for Randi's talk.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.15636184338/enclosure.mp3\"]\n"},{"id":"2012-05-29-gff-triplett","title":"Graduate Fellows Forum: Edward Triplett","author":"ronda-grizzle","date":"2012-05-29 10:14:57 -0400","categories":["Podcasts"],"url":"gff-triplett","content":"**Edward Triplett**\n**Military-Religious Orders on the Border with Islam: Mapping the Architectural Progression of the Iberian Reconquest**\n\nOn May 16, 2012, Scholars' Lab Fellow Edward Triplett spoke about his work with the Scholars' Lab during his fellowship in a talk entitled _Military-Religious Orders on the Border with Islam: Mapping the Architectural Progression of the Iberian Reconquest_.\n\nDr. Eric Ramírez-Weaver, Assistant Professor, McIntire Department of Art, acted as respondent for Edward's talk.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.15983817764/enclosure.mp3\"]\n"},{"id":"2012-05-31-learning-and-improving","title":"Learning and Improving","author":"eric-rochester","date":"2012-05-31 10:23:08 -0400","categories":["Research and Development"],"url":"learning-and-improving","content":"Mentoring and training have always been a big part of the Scholars' Lab's mission. With the [Praxis Program](http://praxis.scholarslab.org/), that focus has only intensified over the last year. All this emphasis on growth has me thinking about how I try to improve professionally myself.\n\nFor the last few years, I've been delving into [functional programming](http://en.wikipedia.org/wiki/Functional_programming). This started by working first with [Clojure](http://clojure.org/) and more recently with [Haskell](http://www.haskell.org/haskellwiki/Haskell). In 2008 I wrote an extended tutorial introduction to Clojure and natural language processing as a [series of blog posts](http://writingcoding.blogspot.com/2008/06/clojure-series-table-of-contents.html). Although they're a little out of date, they still see some traffic.\n\nNot long ago, [Manuel Kiessling](http://manuel.kiessling.net/) commented on the page describing [some functions for the stemming algorithm](http://writingcoding.blogspot.com/2008/07/stemming-part-7-more-functions.html), suggesting an improvement to the `m` function.\n\nThis function takes the word currently being stemmed, along with some other data, and counts the number of consonant sequences between the beginning of the word and the end (not counting the initial consonant cluster, if there was one). The [description of the algorithm](http://tartarus.org/~martin/PorterStemmer/def.txt) gives these examples:\n\n<table >\n  <tr >Inputs Consonant Sequence Count</tr>\n  <tbody >\n    <tr >\n<td >tr, ee, tree, y, by\n</td>\n<td >0\n</td></tr>\n    <tr >\n<td >trouble, oats, trees, ivy\n</td>\n<td >1\n</td></tr>\n    <tr >\n<td >troubles, private, oaten, orrery\n</td>\n<td >2\n</td></tr>\n  </tbody>\n</table>\n\nFor reference, here was what I originally wrote:\n\n[sourcecode language=\"clojure\"]\n(defn m\n  \"Measures the number of consonant sequences between\n  the start of word and position j. If c is a consonant\n  sequence and v a vowel sequence, and <...> indicates\n  arbitrary presence,\n    <c><v>       -> 0\n    <c>vc<v>     -> 1\n    <c>vcvc<v>   -> 2\n    <c>vcvcvc<v> -> 3\n    ...\n  \"\n  [stemmer]\n  (let [\n        j (get-index stemmer)\n        count-v (fn [n i]\n                  (cond (> i j) [:return n i]\n                        (vowel? stemmer i) [:break n i]\n                        :else (recur n (inc i))))\n        count-c (fn [n i]\n                  (cond (> i j) [:return n i]\n                        (consonant? stemmer i) [:break n i]\n                        :else (recur n (inc i))))\n        count-cluster (fn [n i]\n                        (let [[stage1 n1 i1] (count-c n i)]\n                          (if (= stage1 :return)\n                            n1\n                            (let [[stage2 n2 i2] (count-v (inc n1) (inc i1))]\n                              (if (= stage2 :return)\n                                n2\n                                (recur n2 (inc i2)))))))\n        [stage n i] (count-v 0 0)\n        ]\n    (if (= stage :return)\n      n\n      (count-cluster n (inc i)))))\n[/sourcecode]\n\nWow. That's a mess.\n\n\n\n<blockquote>\n\n> \n> _(For better syntax color highlighting, see [this Gist](https://gist.github.com/2628865).)_\n> \n> \n</blockquote>\n\n\n\nEssentially, this is a [state machine](http://en.wikipedia.org/wiki/State_machine) that kind of [trampolines](http://en.wikipedia.org/wiki/Trampoline_(computers)) through the characters in a word. `count-v` walks through a vowel cluster; `count-c` walks through a consonant cluster; and `count-cluster` controls the process of alternating between these two functions. The first result passed out of each of these (usually assigned to a variable named `stage`) is what needs to happen next: either `:return` from `m` with the current count or `:break` from counting the current sequence (either vowel or consonant). The other two outputs are the character currently being considered and the number of consonant sequences seen so far.\n\nDon't worry if that doesn't make sense. The code up there is actively obtuse. It's trying to make you dumber.\n\nManuel suggested merging the three functions together to get this:\n\n[sourcecode language=\"clojure\"]\n(defn m\n  \"Measures the number of consonant sequences between\n  the start of word and position j. If c is a consonant\n  sequence and v a vowel sequence, and <...> indicates\n  arbitrary presence,\n    <c><v>       -> 0\n    <c>vc<v>     -> 1\n    <c>vcvc<v>   -> 2\n    <c>vcvcvc<v> -> 3\n    ...\n  \"\n  ([stemmer]\n    (m stemmer 0 0))\n\n  ([stemmer num-c num-cs]\n    (if (not (seq (:word stemmer))) ; Is the word empty? Then we reached the beginning of the stemmer\n      (if (> num-c 1)               ; THEN1: More than 2 consonants in current counting?\n        (inc num-cs)                ; THEN2: we have one more consonant sequence, and we return the number of sequences found plus 1\n        num-cs)                     ; ELSE2: Return the number of sequences found\n      (if (consonant? stemmer)                        ; ELSE1: Is there a consonant at the current index?\n        (recur (pop-word stemmer) (inc num-c) num-cs) ; THEN3: increase the number of currently consecutice consonants, recur\n        (if (> num-c 1)                               ; ELSE3: If not, check if we found more than 1 consecutive consonants\n          (recur (pop-word stemmer) 0 (inc num-cs))   ; THEN4: If yes, we found one more sequence\n          (recur (pop-word stemmer) 0 num-cs))))))    ; ELSE4: If not, then we found only one, and start anew\n[/sourcecode]\n\nIt's better, but still not great. (It also doesn't work, the way it's presented here, but fixing that wouldn't be hard, so I'm neither going to make an issue of it nor fix it.)\n\nSo what's up? I thought that functional programming was supposed to be concise yet readable.\n\nWell, when I wrote the first version, I was still learning to think functionally. I ended up with an imperative algorithm, without mutable state.\n\nIn other words, I was enjoying the worst of both worlds.\n\nI decided to refactor again, this time being more functional. Here's what I ended up with:\n\n[sourcecode language=\"clojure\"]\n(defn m\n  \"Measures the number of consonant sequences between the start of a word an\n  position j. If c is a consonant sequence and v a vowel sequence, and <...>\n  indicates arbitrary presence,\n  <c><v>       -> 0\n  <c>vc<v>     -> 1\n  <c>vcvc<v>   -> 2\n  <c>vcvcvc<v> -> 3\n  ...\n  \"\n  [stemmer]\n  (let [consonant-group? (fn [ws] (consonant? (first ws)))]\n    (->> stemmer\n      (iterate pop-word)                    ; a sequence of all the parts of the word, from the whole word to the first letter.\n      (take-while (fn [w] (seq (:word w)))) ; stop when done with the word\n      (partition-by consonant?)             ; break it into vowel/consonant groups\n      (reverse)                             ; reverse so the next step works\n      (drop-while consonant-group?)         ; remove the first (was at the end of the sequence) constant group\n      (filter consonant-group?)             ; filter out vowel groups\n      count)))                              ; finally, count the remaining consonant groups\n[/sourcecode]\n\nBetter (although I already see some places that could be improved). It really doesn't need the comments I added, and if this weren't pedagogical code, I would have left them out.\n\nSo what made the difference?\n\nPrimarily functional programming tries to abstract control flow out into [higher-order functions](http://en.wikipedia.org/wiki/Higher_order_functions) like `map`, `filter`, and `reduce`. (Or in this case, `iterate`, `take-while`, `partition-by`, `drop-while`, and `filter`.) In other words, the code to walk over the items in a sequence (`map`) is separated from what you do with those items, or the code to remove items from a list (`filter`) is separated from the predicate that determines which items to keep. This makes the data flow very clear.\n\nBut what is more interesting about this exercise is reflecting on my own growth as a programmer. As I mentor others, I often want to save them the pain and ugliness I've been through. But I need to remember that the only way to learn to write clear code is to write spaghetti code first and live through the fall-out.\n"},{"id":"2012-06-12-neatline-sneak-peek","title":"Neatline Sneak-Peek","author":"david-mcclure","date":"2012-06-12 08:01:59 -0400","categories":["Geospatial and Temporal","Research and Development"],"url":"neatline-sneak-peek","content":"The R&D; team here at the lab has been quiet over the course of the last couple weeks, but there's been a flurry of activity under the surface - we've been hard at work putting the finishing touches on Neatline, a geotemporal exhibit-building framework that makes it possible to plot archival collections, narratives, texts, and concepts on interactive maps and timelines.\n\nNeatline is built as a collection of plugins for [Omeka](http://omeka.org/), a digital archive-building framework developed by our partners at the [Roy Rosenzweig Center for History and New Media](http://chnm.gmu.edu/) at George Mason University. If you already have an Omeka collection, Neatline provides a deeply integrated, plug-and-play mapping solution that lets you create interpretive views on your archive. If you don't have an Omeka collection, though (or if it doesn't make sense represent your material as a collection of archival objects), Neatline can also be used as an effectively standalone application from within the Omeka administrative interface.\n\nIf you haven't been following the project, check out the [podcast](http://www.scholarslab.org/podcasts/thatcampva-2012-neatline-workshop/) of the workshop that Eric Rochester and I gave at THATCamp Virginia 2012 and read the [announcement](http://www.scholarslab.org/announcements/scholars-lab-and-chnm-partner-on-omeka-neatline/) about our partnership with RRCHNM.\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/editor-layout-editor-300x187.jpg)](http://www.scholarslab.org/geospatial-and-temporal/neatline-sneak-peek/attachment/editor-layout-editor/)\n\n\n\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/editor-map-styles-300x187.jpg)](http://www.scholarslab.org/geospatial-and-temporal/neatline-sneak-peek/attachment/editor-map-styles/)\n\n\n\n\nSo, what kinds of things can you do with Neatline? Here are a few:\n\n\n\n\n\t\n  * Create records and plot them on interlinked maps and timelines with complex vector drawings, points, and spans. Set colors, opacities, line thicknesses, point radii, and gradients.\n\n\t\n  * Add popup bubbles and define interactions among the map, timeline, and a record-browser viewport, which can display everything from short snippets and captions to long-format interpretive prose.\n\n\t\n  * Connect your exhibits with web map services delivered by [Geoserver](http://geoserver.org/display/GEOS/Welcome), which makes it possible to create rich displays of historical maps.\n\n\t\n  * Drag the viewports around to create custom layouts.\n\n\t\n  * Set visibility intervals on a per-item basis, making it possible to create complex time-series animations.\n\n\t\n  * Create hierarchical relationships among items, making it possible to curate \"batches\" of elements in an exhibit that can be manipulated as a group.\n\n\t\n  * (Using the Neatline Editions plugin, which is still in alpha and won't be ready until later in the summer) Create interactive editions of texts by connecting individual paragraphs, sentences, or words with locations on maps and timelines.\n\n\n\nWatch this space in the first week of July for the full public release with the dedicated website, code, documentation, and a hosted \"sandbox\" version of the application that will let you register and experiment with creating exhibits before downloading the software.\n\nUntil then, here are a handful of screenshots from some of the demo exhibits we're working on:\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/chancellorsville-1-1024x627.jpg)](http://www.scholarslab.org/geospatial-and-temporal/neatline-sneak-peek/attachment/chancellorsville-1/)\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/chancellorsville-2-1024x626.jpg)](http://www.scholarslab.org/geospatial-and-temporal/neatline-sneak-peek/attachment/chancellorsville-2/)\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/chancellorsville-3-1024x629.jpg)](http://www.scholarslab.org/geospatial-and-temporal/neatline-sneak-peek/attachment/chancellorsville-3/)\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/chancellorsville-4-1024x628.jpg)](http://www.scholarslab.org/geospatial-and-temporal/neatline-sneak-peek/attachment/chancellorsville-4/)\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/editor-layout-editor-1024x638.jpg)](http://www.scholarslab.org/geospatial-and-temporal/neatline-sneak-peek/attachment/editor-layout-editor/)\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/editor-map-styles-1024x638.jpg)](http://www.scholarslab.org/geospatial-and-temporal/neatline-sneak-peek/attachment/editor-map-styles/)\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/fredericksburg-1-1024x622.jpg)](http://www.scholarslab.org/geospatial-and-temporal/neatline-sneak-peek/attachment/fredericksburg-1/)\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/fredericksburg-2-1024x640.jpg)](http://www.scholarslab.org/geospatial-and-temporal/neatline-sneak-peek/attachment/fredericksburg-2/)\n"},{"id":"2012-06-25-diy-aerial-photography-in-a-crowd","title":"DIY Aerial Photography In A Crowd","author":"kelly-johnston","date":"2012-06-25 11:28:58 -0400","categories":["Geospatial and Temporal"],"url":"diy-aerial-photography-in-a-crowd","content":"The Lawn is the center of Mr. Jefferson's historic [Academical Village](http://www.virginia.edu/academicalvillage/) at the [University of Virginia](http://www.virginia.edu).  We took our Scholars' Lab DIY aerial photography equipment to the Lawn on Sunday afternoon as part of the [University Library's effort to document](http://sullivan.lib.virginia.edu/) the [events surrounding President Sullivan's resignation.](http://www.virginia.edu/keyissues/presidential-transition/)\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/Chris-Kelly-Rotunda-1024x768.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-in-a-crowd/attachment/chris-kelly-rotunda/)\n\nSince our [last flights](https://www.scholarslab.org/geospatial-and-temporal/update-diy-aerial-photography/), we've upgraded our camera to a [GoPro HD Hero2](http://gopro.com/hd-hero2-cameras/) and kept the same balloon [aerial photography rig](https://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography/).\n\nWe've learned this kind of aerial photography is like farming, completely dependent on the weather. So we kept one eye on the sky as a strong summer [thunderstorm cell loomed](http://twitpic.com/a04ayd).  We've [dealt with wind](http://youtu.be/KYilJ3kDLuI) before, but keeping the balloon out of the trees while walking through a crowd of over 1000 people with winds ranging 0-4 the [Beaufort Scale](http://www.spc.noaa.gov/faq/tornado/beaufort.html) was our challenge.\n\nCalm winds yield a long straight string and a straight down view.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/GOPR0767-1024x768.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-in-a-crowd/attachment/dcim100gopro-5/)\n\nGusty winds cause the camera to swing on its pendulum and we get a beautiful oblique view of lush green summer in Virginia.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/GOPR0332-1024x768.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-in-a-crowd/attachment/dcim100gopro-6/)\n\nThe crowd gathers before the event.  The [Rotunda roof project ](http://uvamagazine.org/top_university_news/article/rotunda_roof_to_be_repaired_not_replaced/)accounts for the elaborate curved scaffolding.\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/GOPR0270-1024x768.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-in-a-crowd/attachment/dcim100gopro/)\n\nChris was hoping for calm when he walked toward the front of the crowd.  Can you find him in the image below?[![](http://www.scholarslab.org/wp-content/uploads/2012/06/GOPR0447-1024x768.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-in-a-crowd/attachment/dcim100gopro-3/)\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/GOPR0447-Copy-1024x768.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-in-a-crowd/attachment/dcim100gopro-4/)\n\n[caption id=\"\" align=\"alignnone\" width=\"571\"][![](http://www.scholarslab.org/wp-content/uploads/2012/06/JoeyTombsRotunda-571x1024.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-in-a-crowd/attachment/joeytombsrotunda/) via @JoeyTombs[/caption]\n\nViews from lower altitudes compress distances and make the crowd appear more dense.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/GOPR0362-Copy-1024x509.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-in-a-crowd/attachment/dcim100gopro-2/)\n\nWhile an overhead view shows people tend to stand in remarkably straight lines with impressively uniform spacing.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/GOPR0465-1024x768.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-in-a-crowd/attachment/dcim100gopro-9/)\n\nBlue tarps cover West Lawn roofs where [chimney repairs ](http://www.virginia.edu/uvatoday/newsRelease.php?id=16387)are ongoing.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/GOPR0307-1024x768.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-in-a-crowd/attachment/dcim100gopro-7/)\n\nA gust of wind gave us a view of more blue tarps covering East Lawn chimneys and our first glimpse of [Old Cabell Hall](http://www.virginia.edu/webmap/popPages/67-CabellOld.html), and the [Health Systems complex ](http://uvahealth.com/)\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/GOPR0316-1024x768.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-in-a-crowd/attachment/dcim100gopro-8/)\n\nExistential Threats (and storms and wind and crowds) Don't Scare Us, We're Librarians!\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/06/Group-Pix.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-in-a-crowd/attachment/group-pix/)\n"},{"id":"2012-07-02-announcing-neatline","title":"Announcing Neatline!","author":"bethany-nowviskie","date":"2012-07-02 11:01:04 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"announcing-neatline","content":"**_What do you get when you cross archives and artifacts with timelines, modern and historical maps, and an appreciation for the interpretive aims of humanities scholarship?_**\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/Screen-shot-2012-07-02-at-10.36.28-AM1.png)](http://www.neatline.org/)\n\nToday, the [Scholars' Lab](http://scholarslab.org) is proud to announce the launch of [Neatline](http://neatline.org/), our set of [Omeka](http://omeka.org) plugins for hand-crafted geo-temporal visualization and interpretation. You can head right over to [http://neatline.org/](http://neatline.org/) to download the [1.0 software](http://neatline.org/plugins/), see [sample exhibits](http://neatline.org/neatline-in-action/) or play in the [sandbox](http://sandbox.neatline.org/), and read more [about the project](http://neatline.org/about/), including [news](http://neatline.org/news/) and [history](http://neatline.org/about/credits-and-history/).\n\n[Neatline](http://neatline.org/) is a geotemporal exhibit-builder that allows you to create beautiful, complex maps and narrative sequences from collections of archives and artifacts, and to connect your maps and narratives with timelines that are more-than-usually sensitive to ambiguity and nuance. In other words, Neatline lets you make hand-crafted, interactive stories as interpretive expressions of an archival or cultural heritage collection.\n\nEvery Neatline exhibit can be your own close reading of a humanities collection -- expressed in the visual vernacular.  Ours is a small-data approach in a \"big data\" world.\n\nStay tuned to the [Scholars' Lab blog](http://scholarslab.org/) and to [Neatline.org](http://neatline.org/) for a series of posts and screencasts to be shared over the course of the next two weeks. We'll be providing support for this [open-source software](https://github.com/scholarslab/) on the [Omeka forums](http://omeka.org/forums/) and [dev list](https://groups.google.com/forum/#!forum/omeka-dev).\n"},{"id":"2012-07-10-announcing-a-new-sci-study-on-alternative-academic-career-paths","title":"Announcing a new SCI study on alternative academic career paths","author":"katina-rogers","date":"2012-07-10 06:02:46 -0400","categories":["Announcements"],"url":"announcing-a-new-sci-study-on-alternative-academic-career-paths","content":"I'm pleased to announce that the [Scholarly Communication Institute](http://uvasci.org/current-work/) is conducting **a study on career preparation in humanities graduate programs**. As part of this study, we have launched two confidential surveys: the [first is for people on alternative academic career paths](http://alt-academy.questionpro.com/) (that is, people with graduate training in the humanities and allied fields working beyond the professoriate); the [second survey is for their employers](http://alt-academy.employers.questionpro.com/).  The surveys will be open until October 1, 2012.\n\nHumanities scholars **come from a wide array of backgrounds and embark on a variety of careers** in areas like libraries, museums, archives, higher education and humanities administration, publishing, research and technology, and more. SCI anticipates that data collected during the study will contribute to a **deeper understanding of the diversity of career paths **we pursue after our graduate studies, while also highlighting **opportunities to better prepare students for a range of careers** beyond the tenure track.\n\nThe surveys **complement the** [**public database**](http://altacademy.wufoo.com/reports/who-we-are/) that we recently created as a way to clarify the breadth of the field, and to foster community among a diverse group. If your work represents the diversity of the broad #alt-ac community, it's not too late to [tell us about yourself!](http://altacademy.wufoo.com/forms/who-we-are/)\n\nThe surveys and directory are being administered as part of the Scholarly Communication Institute's [current phase of work](http://uvasci.org/current-work/graduate-education/) -- which includes a close concentration on graduate education reform (largely in the North American context) and the preparation of future knowledge workers, educators, and cultural heritage and scholarly communications professionals.\n\nThe survey results will help us to make curriculum recommendations so that graduate programs may better serve future students, and anonymized or summarized data will be made available at [#alt-academy](http://mediacommons.futureofthebook.org/alt-ac/who-we-are) at a later date. Please [contact me ](http://mediacommons.futureofthebook.org/alt-ac/users/katinalynn)if you’d like to know more.\n\n\n\n\t\n  * **[Complete the main #alt-ac survey](http://alt-academy.questionpro.com/)**\n\n\t\n  * ****[Complete the employer survey](http://alt-academy.employers.questionpro.com/)****\n\n\n\n\t\n  * **[Add your information to the database](http://altacademy.wufoo.com/forms/who-we-are/)**\n\n\t\n  * ****[View the database report](http://altacademy.wufoo.com/reports/who-we-are/)****\n\n\n\n\t\n  * [**Other ways to get involved**](http://mediacommons.futureofthebook.org/alt-ac/how-it-works)\n\n\n"},{"id":"2012-07-17-neatline-and-the-framework-challenge","title":"Neatline and the framework challenge","author":"david-mcclure","date":"2012-07-16 22:55:19 -0400","categories":["Geospatial and Temporal"],"url":"neatline-and-the-framework-challenge","content":"With the first public release of [Neatline](http://neatline.org/) out the door, I've had the chance to take a short break from programming and think back over the nine-month development cycle that led up to the launch. In retrospect, I think that a lot of the exciting challenges we ran up against - the big, difficult questions about _what_ to program, as opposed to _how_ to program - emerged from tensions that are inherent in the task of creating frameworks as opposed to conventional applications.\n\nWhat's a framework? As an experiment, I'll define the term broadly to mean applications that make it possible to _create things_, as opposed to applications that make it possible to _accomplish tasks_. Frameworks are generative in a way that normal applications are not. Instead of controlling systems, crunching numbers, automating processes, boosting efficiency, or providing entertainment, frameworks are set apart by the fact that the allow the user to spawn off new things that are independent of the software itself.\n\nMicrosoft Word is used to create **documents**; Wordpress is used to create **blogs** and **blog posts**; Drupal is used to create **websites**; Ruby on Rails is used to build **web applications**; Illustrator is used to create **vector graphics**; Maya is used to create **3d models and animations**.\n\nOmeka and Neatline fit straightforwardly into this definition. Omeka is used to build online digital collections; Neatline, a framework-within-a-framework built on top of Omeka, is used to create interactive maps and timelines. In each case, the final unit of analysis is some sort of discrete, addressable thing that is generated with the assistance of the software. It can be viewed, visited, or printed. Frameworks empower users to create things that would be difficult or completely impossible to create without the assistance of the software.\n\nThe paradox, though, is that frameworks have to simultaneously constrict the user's agency in the act of expanding it. Barring some kind of mythological ur-framework that would allow for direct, unmediated, and unbounded realization of thought (Prospero's book of magic), all frameworks, whether implicitly or explicitly, have to define a range of final outputs that will be \"supported\" by the software. In practice, this means paring down the supported outputs to a vanishingly small subset of the original possibility space. Frameworks are defined as much by what they disallow as by what they allow.\n\nFor the developer, deciding on the \"range\" of the framework is a difficult and sometimes agonizing process because it involves a fundamental tradeoff between _power_ and _accessibility_ - and, by extension, the size of the potential audience. As a framework becomes more powerful and allows a wider range of possible outputs, it also becomes more complex and locks out users who aren't willing to invest the effort to become proficient with the tool. As a framework becomes more narrow and focused, a larger number of people will be able and willing to use it, but the diversity of the final outputs drops, and the tool becomes suitable for a much smaller range of use cases. It's a zero-sum game.\n\nOver the course of the last couple months, I've realized that this opposition between power and ease-of-use provides an interesting vocabulary for defining Neatline and situating it in the ecosystem of existing geospatial tools. Up until now, it seems to me that existing frameworks have clustered around the two ends of the power / ease-of-use spectrum. Consumer web applications like the Google mapmaker allow the user to drop pins and annotate them with short captions. This is delightfully easy, but all of the end-products look the same, and the tool doesn't really provide a critical mass of flexibility and the opportunity for real intellectual ownership that's required for serious scholarly use.\n\nMeanwhile, at the other end of the spectrum, desktop GIS applications like ArcMap provide an incredibly powerful and feature-rich platforms for analyzing geospatial data and creating visualizations. For projects that have access to custom software development, programming libraries like [OpenLayers](http://openlayers.org/), [Leaflet](http://leaflet.cloudmade.com/), [PolyMaps](http://polymaps.org/), and [Timeglider](http://timeglider.com/jquery/) provide flexible, highly-customizable toolkits for creating interactive maps and timeline - but only at the level of code.\n\nThere's been an underpopulated zone in the middle the spectrum, though - not many spatio-temporal tools have tried to more evenly balance power and accessibility. Neatline tries to land in a \"goldilocks\" zone between the two poles. It tries to be simple enough out-of-the-box that it can be used by a large majority of scholars and students who do not have programming experience or advanced GIS expertise, but still complex enough to allow for significant diversity in the structure and style of the final output.\n\nThis means, of course, that Neatline _could_ be more powerful and _could_ be easier to use. My argument, though, is that it couldn't both - at least, not without tripping over itself and breaking apart into incoherence.\n\nInstead of choosing one pole at the expense of the other, we decided to make a studious attempt to balance the two. This is difficult to do - perhaps more difficult than committing wholesale to one or the other, which can often have the effect of locking in a cascading series of almost automatic design decisions leading towards a more singular objective. Building \"middle-ground\" frameworks requires a constant (re-)calibration of the feature set over the course of the development process, a sort of gyroscopic vigilance to keep the software perched in the tricky zone between flexibility and accessibility.\n\nLike all real challenges, though, this one was also fantastically exciting to tackle. Now that Neatline is out in the wild, I can't wait to see what people create with it.\n"},{"id":"2012-07-20-parent-child-relationships-in-neatline","title":"Parent-child relationships in Neatline","author":"david-mcclure","date":"2012-07-20 01:19:18 -0400","categories":["Geospatial and Temporal"],"url":"parent-child-relationships-in-neatline","content":"One of the most powerful features in [Neatline](http://neatline.org), our newly-released [Omeka](http://omeka.org)-based tool for geo-temporal interpretation of humanities collections, is the ability to create parent-child relationships between records in an exhibit. Any record can be the parent of any other record, and there are no limits on the depth of the nesting - a parent record can itself have a parent record, and so on and so forth.\n\nThis relationship is established _from_ the child _to_ the parent. To set a parent record, click into the \"Relations\" fieldset and use the dropdown to select a record:\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/parent-record-field.jpg)](http://www.scholarslab.org/geospatial-and-temporal/parent-child-relationships-in-neatline/attachment/parent-record-field/)\n\nWhat's the point of this? When you set a parent record, the child _automatically inherits all of the styling and visibility settings of the parent_. In a nutshell, this makes it possible to create \"batches\" of records that share a common set of styles and phase in and out of visibility in unison.\n\nFor example, imagine that you need to split the records in your exhibit into two a \"blue\" category and a \"red\" category. Instead of combing through each record and typing in the exact same lineup of styles for all the records in each of the categories, you can just create two abstract \"template\" records that contain the style defaults for each group and associate each of the content records with one of the templates.\n\nWith six records, three blue and three red, that would look like this:\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/blue-red-item-browser.jpg)](http://www.scholarslab.org/geospatial-and-temporal/parent-child-relationships-in-neatline/attachment/blue-red-item-browser/)\n\nBlue 1, Blue 2, and Blue 3 are children of [Blue Parent], and Red 1, Red 2, and Red 3 are children of [Red Parent]:\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/blue-1.jpg)](http://www.scholarslab.org/geospatial-and-temporal/parent-child-relationships-in-neatline/attachment/blue-1/)\n\nAnd in the final exhibit, the colors are rendered correctly without ever having to set a single style on Blue 1, Blue 2, Blue 3, Red 1, Red 2, or Red 3:\n\n\nPowered by [Neatline](http://neatline.org/) | [View fullscreen](http://sandbox.scholarslab.org/webservice/david/parent-records)\n\nThis also works for the record visibility settings that control the date range on the timeline during which the a record is visible - the \"Start Visible Date\" and \"End Visible Date\" fields in the \"Temporal\" fieldset. In the example above, say that you want the blue points to be visible from 1900-1960, and the red points from 1940-2000. Set these visibility intervals on the parent records, and the child records will phase in and out of visibility in unison:\n\n\nPowered by [Neatline](http://neatline.org/) | [View fullscreen](http://sandbox.scholarslab.org/webservice/david/parent-records-date)\n\nWhat if you need to selectively override the defaults, though? What if you want a record to inherit most style and visibility settings from upstream in the inheritance chain, but you want to adjust one or two settings to differentiate the record?\n\nFor example, imagine you want one of the blue points to be yellow - but you still want it to phase in and out of view with the other blue points. Instead of having to break away the record and re-set all of the settings just in order to make the color change, you can just directly change the color setting on the record, and all of the other unchanged settings will continue to inherit from upwards in the chain:\n\n\nPowered by [Neatline](http://neatline.org/) | [View fullscreen](http://sandbox.scholarslab.org/webservice/david/parent-records-overrides)\n\nNeatline always tries to find record-specific value first, meaning that an inherited value can always be clobbered by a locally-set value (think of it as `!important` in CSS). If Neatline doesn't find a record-specific value, it starts to traverse up the inheritance chain to the parent record(s), and stops when it finds a record-specific value on one of the parents. If none of the parents have a value for the attribute in question, then Neatline falls back on the exhibit default values, which can be configured in the \"Map Settings\" dropdown tab.\n\n**Parent records in action**\n\nHow does this work with real content? The [Battle of Chancellorsville demo exhibit](http://hotchkiss.scholarslab.org/neatline-exhibits/show/battle-of-chancellorsville/fullscreen) makes heavy use of parent records. This is a complex exhibit with a lot of moving parts. There are three separate base maps, one for each of the three days of the battle - May 2, May 3, and May 4, 1863. Each of the maps has a large collection of spatial annotations and numbered waypoints that are relevant to _just one of the maps_ - as the map switches out in response to the position of the timeline, the corresponding set of annotations and waypoints needs to phase into view at the same time. Meanwhile, the spatial vectors can be broken down into categories that should share similar styles - the Union and Confederate lines should all share the same shades of blue and red.\n\nYou could just go through and directly set the correct colors and visibility dates on each individual record. This is labor-intensive, though, and it tends to lock you into design decisions that you make at the beginning of the process – if you change your mind down the road and want to adjust the Union blue, you’d have to work through 50-odd records and update them individually.\n\nParent records make it possible to formalize the conceptual relationships and manipulate the groupings in bulk – once the correct inheritance chain is set up, you can set the style and visibility settings a single time at the top of the stack and the settings will cascade downwards to all of the children. For this exhibit, the inheritance structure looks like this:\n\n`\n\n**May 2, 1863** (visible: May 2 - May 3)\n--- **[may 2 condeferate lines]** (color: #b52f2f)\n-------- May annotation 1 (visible: May 2 - May 3; color: #b52f2f)\n-------- May annotation 2 (\" \")\n-------- May annotation 3 (\" \")\n-------- (...)\n--- **[may 2 union lines]** (color: #093696)\n-------- May annotation 4 (visible\" May 2 - May 3; color: #093696)\n-------- May annotation 5 (\" \")\n-------- May annotation 6 (\" \")\n-------- (...)\n\n**May 3, 1863** (visible: May 3 - May 4)\n--- **[may 3 condeferate lines]** (color: #b52f2f)\n-------- May annotation 7 (visible: May 3 - May 4; color: #b52f2f)\n-------- May annotation 8 (\" \")\n-------- May annotation 9 (\" \")\n-------- (...)\n--- **[may 3 union lines]** (color: #093696)\n------ May annotation 10 (visible: May 3 - May 4; color: #093696)\n------ May annotation 11 (\" \")\n------ May annotation 12 (\" \")\n------ (...)\n\n**May 4, 1863** (visible: May 4 - May 5)\n--- **[may 4 condeferate lines]** (color: #b52f2f)\n-------- May annotation 13 (visible: May 4 - May 5; color: #b52f2f)\n-------- May annotation 14 (\" \")\n-------- May annotation 15 (\" \")\n-------- (...)\n--- **[may 4 union lines]** (color: #093696)\n-------- May annotation 16 (visible: May 4 - May 5; color: #093696)\n-------- May annotation 17 (\" \")\n-------- May annotation 18 (\" \")\n-------- (...)\n\n`\n\nThe top-level visibility dates are set on the three records that house the base maps. Under each of the three map records, abstract style records define the colors and opacities for the Union and Confederate lines. The actual content records then inherit from these records, receiving both the top-level visibility parameters on the map records and the styles on the abstract records.\n\nNow, there is some duplication of content here - the colors for Union blue and Confederate red have to be set separately in each of the three sets of abstract styling records. This is because all of the styles/visibilities on record can only be a part of a single inheritance chain, making it necessary to \"split\" each of the three chains under the top-level map records.\n\nOriginally, I toyed around with the idea of making it possible to create \"style-specific\" inheritance chains - so, for example, a record could inherit its fill color from one record, its line width from another, its visibility from another, etc. In the end, though, this would have required a large amount of added UI overhead, and the same results can be achieved with a minimal amount of extra work with the technique used here.\n"},{"id":"2012-07-27-diy-aerial-photography-and-edgar-allan-poe","title":"DIY Aerial Photography and Edgar Allan Poe","author":"kelly-johnston","date":"2012-07-27 05:21:16 -0400","categories":["Geospatial and Temporal"],"url":"diy-aerial-photography-and-edgar-allan-poe","content":"Earlier this year Professor [Megan Marlatt](http://www.virginia.edu/art/studio/faculty/marlatt.html) from the University of Virginia McIntire Department of Art began work with her students to create a jumbo outdoor mural titled [\"Hello Pluto, Good-bye Kitty\"](http://www.virginia.edu/uvatoday/newsRelease.php?id=17953) based on Edgar Allan Poe's short story \"[The Black Cat](http://etext.lib.virginia.edu/etcbin/toccer-new2?id=PoeBlac.sgm&images=images/modeng&data=/texts/english/modeng/parsed&tag=public&part=1&division=div1)\".   The mural design covered a large suburban parking lot.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/Hello-Pluto-Good-bye-Kitty-1024x860.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-and-edgar-allan-poe/attachment/hello-pluto-good-bye-kitty/)\n\nFrom street level, viewers see small cats painted on the parking lot.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/CatsCloseUp-1024x663.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-and-edgar-allan-poe/attachment/dcim100gopro-10/)\n\nBut when we gain the larger view by lofting a camera attached to a helium balloon several hundred feet into the air we get our first views of the entire work in progress.\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/IMG_3349-1024x768.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-and-edgar-allan-poe/attachment/img_3349/)\n\n\"'Hello Pluto, Good-bye Kitty\" was off to a fine start but much work was still to be done.[![Mural Under Construction](http://www.scholarslab.org/wp-content/uploads/2012/05/6976147874_138445fffa_b.jpg)](http://www.scholarslab.org/geospatial-and-temporal/update-diy-aerial-photography/attachment/6976147874_138445fffa_b/)\n\nWhen Professor Marlatt and her students completed their work a few weeks later we returned with our [aerial photography rig](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-in-a-crowd/) to document the finished product.[![](http://www.scholarslab.org/wp-content/uploads/2012/07/GOPR1610-1024x768.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-and-edgar-allan-poe/attachment/dcim100gopro-11/)\n\n[caption id=\"attachment_5308\" align=\"alignnone\" width=\"819\"][![](http://www.scholarslab.org/wp-content/uploads/2012/07/GOPR1623cropped-1024x730.jpg)](http://www.scholarslab.org/geospatial-and-temporal/diy-aerial-photography-and-edgar-allan-poe/attachment/dcim100gopro-12/) \"Hello Pluto, Good-bye Kitty\"[/caption]\n\nThis project illustrates some of the benefits of do it yourself aerial photography.  With a small investment of equipment and time we collected high resolution imagery to document change over time for a discrete study area. We see applications for these techniques across many academic disciplines.  So we're working now to fine-tune our approaches while we collaborate with faculty and students.\n\nContact us in the [Scholars' Lab](http://www.scholarslab.org/about/) to chat about how your work can benefit from do it yourself aerial photography.\n"},{"id":"2012-07-31-translating-neatline","title":"Translating Neatline","author":"jeremy-boggs","date":"2012-07-31 11:19:27 -0400","categories":["Research and Development"],"url":"translating-neatline","content":"If you’re fluent in English and another language, and would love to help with the [Neatline](http://neatline.org) project, please consider contributing a translation for our Neatline plugins!\n\nWe’re using a service called Transifex to manage translation work. To get started, just sign up for a free account on Transifex (or log in using your Twitter or Facebook account), then check out our [Neatline project page](https://www.transifex.com/projects/p/neatline/). If you already see a language listed, just click on it, then request to join that language team to begin contributing. If the language you'd like to contribute doesn't appear on the list, just click the link near the top of the languages list to “Request a new team” and we’ll add that team to the list, and add you to it. Once all that is set up, you can begin adding translations to any (or all!) of our plugins.\n\nOnce you’re added to a language team, you can click on that language, and see a list of the resources (i.e., Neatline plugins) we have available to translate. Clicking on any of the resources should bring up a modal window with various options for translating the resource, including a button to “Translate Online”. From there, you’ll be presented with a page that has the English word or phrase on one side, and a text area to contribute a translation on the other.\n\n[caption id=\"attachment_5429\" align=\"alignnone\" width=\"1008\"][![](http://www.scholarslab.org/wp-content/uploads/2012/07/Screen-shot-2012-07-31-at-3.18.19-PM.png)](http://www.scholarslab.org/slab-code/translating-neatline/attachment/screen-shot-2012-07-31-at-3-18-19-pm/) Translation interface on Transifex.net[/caption]\n\nTranslating static word strings—phrases like “Save your exhibit”, for example—are pretty straight-forward. Occasionally, though, you will run across a string that includes text like %s or %$1s. This text is a placeholder for dynamic content, generated by the Neatline plugin when used in Omeka. To translate strings with these placeholders, simply translate the other words in the string, and move the placeholder text wherever it would make sense for your translation. For example, the string “The timeline \"%s\" was successfully added!” is translated into Spanish as “Se agregó la secuencia \"%s\"!”\n\nThe Omeka team also manages their translations through Transifex, and they have some nice [documentation on contributing translations](http://omeka.org/codex/Translate_Omeka) using the platform; if you have questions, try checking their page. Additionally, Transifex also has some nice documentation for many features of their service, including [contributing translations](http://help.transifex.com/intro/translating.html#translating).\n\nAs translations are completed, we'll add them to each plugin's `languages` directory before each release. So if you'd like to see Neatline in a different language, please consider contributing a translation. We appreciate your help in giving Neatline a truly global reach!\n"},{"id":"2012-08-03-data-visualizations-learning-d3-js","title":"Data visualizations: Learning d3.js","author":"katina-rogers","date":"2012-08-03 08:35:30 -0400","categories":["Visualization and Data Mining"],"url":"data-visualizations-learning-d3-js","content":"_[cross-posted at [katinarogers.com](http://wp.me/p2CaGd-9R)]_\n\nThe [ SCI study on humanities graduate programs and career preparation](http://uvasci.org/current-work/graduate-education/) is humming along, and while survey responses come in, I've been working on determining how best to translate the data into meaningful graphics. After a lot of experimenting, I think the winner is [d3.js](http://d3js.org/). Short for for Data-Driven Documents, D3 is Michael Bostock's creation; a quick glance at his [gallery](https://github.com/mbostock/d3/wiki/Gallery) shows the kinds of beautiful and complex visualizations it's capable of. It's a low-level tool, though, which means that learning to use it even in a rudimentary way has already involved picking up some html, css, and javascript along the way. It's a lot to chew on, but I think I'm starting to turn a corner as a blurry whirl of concepts, terms, and commands are slowly resolving themselves into some clarity.\n\nWhile I don't have anything that cool that to show yet, I'm excited that I do have a little something. Here's the fruit of everything I've learned so far:\n[iframe height=\"200\" width=\"690\" src=\"http://katinalynn.github.com/dataviz/demo_randomCircles\"]\n\nIt might not look like much, but you guys, I drew those shapes with CODE! (And a lot of help. Mostly I drew them with a lot of help, actually.) In my [earlier post on text mining](http://katinarogers.com/2012/06/29/playing-with-visual-text-analysis-using-voyant/ ), I also included some images made from data -- but in that case, the graphics you see in the post are nothing more than links back to the tool itself, which does all the work. The image here is different. Here's what's so great about it:\n\n1.  The shapes are determined by data in the code. In this case, the size of the circles is determined by a dataset of randomly generated numbers, because that's what's specified in the code. Don't believe me? Reload the page -- the shapes change!\n\n2.  I actually understand the code. I can play with it, change things, and not break it. I made this particular image by starting with samples from [Scott Murray's great tutorials](http://alignedleft.com/tutorials/d3), then building on what I learned, combining elements from various lessons to create a new graphic.\n\n3.  I learned a ton trying to get the image to display in this post; turns out that it's not as simple as uploading an image or linking to a page, or even pasting the code into the post. I now have a much better sense of what does and doesn't work in WordPress; how iframes work; how to really use an FTP client; how to create and work with GitHub repositories; and how to publish GitHub Pages so that you can actually see the rendered images, not the source code.\n\nI have been learning tech skills in scattered bits and pieces, and this is the first time some of the threads have come together instead of constantly branching off in new directions. I feel like I'm starting to understand how to actually make something on the web. (There must be something in the air in the [ Scholars' Lab](http://scholarslab.org) that makes it impossible to resist the desire to make things.) It feels empowering -- I was incredibly giddy when I finally got the images to display in the draft post -- but also really humbling, like I'm trying to tie my shoes while wearing mittens. Right now I'm just excited to be doing something that I can point to and look at. It feels a little like magic.\n\nI'm also excited about D3 itself, which [@thisisaaronland](https://twitter.com/thisisaaronland) recommended when I told him about my project. In recent weeks I've been exploring all manner of data visualization tools, and while I knew that any of them would require that I learn new skills (or else settle for paltry Excel charts), I was becoming overwhelmed by the options. Everything I looked at seemed to need a different language -- python, R, etc. -- and I was already feeling like I had started too many new things without becoming proficient in any of them. D3 is quite powerful, and learning it should help me to start applying some of what I've learned while also pushing me into new terrain. It's also what the Praxis team used for the visualisations in [ Prism](http://prism.scholarslab.org/), so all signs point to it being a great visualization tool that's worth taking the time to learn.\n"},{"id":"2012-08-09-timr-optimizing-web-requests","title":"timr: Optimizing Web Requests","author":"eric-rochester","date":"2012-08-09 05:38:20 -0400","categories":["Research and Development"],"url":"timr-optimizing-web-requests","content":"[![Stopwatch](http://farm4.staticflickr.com/3443/3297205226_a12b175d49_n.jpg)](http://www.flickr.com/photos/wwarby/3297205226/)\n\n\n\n\n\nOne of the fundamental tensions in programming is balancing the program’s requirements for time (programmer time and running time) against its space requirements (disk space and memory space). Optimizing one of these costs—i.e., looking for ways to shift that balance, usually to have the program run faster—is a common task.\n\n\n\n\nRecently, I’ve needed to speed up requests on a couple of different websites I’ve worked on: [Neatline](http://neatline.scholarslab.org/) and a small, personal work-in-progress I call [What is DH?](http://whatisdh.herokuapp.com/).\n\n\n\n\nOf course, optimizing programs too early can turn your program into an unreadable mess and waste your time. ([The Wikipedia page on Program Optmization](http://en.wikipedia.org/wiki/Program_optimization) has a good overview of the issues and trade-offs.) The rule is: **don’t optimize**. But if you must do it, do it right. That’s where this post comes in.\n\n\n\n\n# Lather, Rinse, Repeat\n\n\n\n\nA typical work flow when optimizing a program goes something like this:\n\n\n\n\n\n\n  1. **Measure** how long it takes or now much memory it takes right now. Don’t skip this.\n\n\n  2. If it’s good enough, **stop**; otherwise, keep going.\n\n\n  3. Change something.\n\n\n  4. `GOTO 1`.\n\n\n\n\nThat seems simple enough, but it’s really quite complicated. For example, in a web app, many things slow down requests.\n\n\n\n\n\n\n  * One slow database query.\n\n\n  * Too many database queries.\n\n\n  * Pulling in too much unused data from the database.\n\n\n  * One intensive computation.\n\n\n  * A bunch of small computations.\n\n\n\n\nI’m leaving out maybe one or two things, but you get the idea.\n\n\n\n\nThe timings are also complicated by a number of factors:\n\n\n\n\n\n\n  * The interpreter needs to allocate a bunch of memory (instead of using pre-allocated memory), which is relatively slow.\n\n\n  * The interpreter executing your program could decide to [take out the garbage](http://en.wikipedia.org/wiki/Garbage_collection_(computer_science)) during the run, effectively tying up your program.\n\n\n  * Your computer/OS may suddenly decide that it has to do some intensive computation _right now_, ’cause, well, you know, computers are helpful like that.\n\n\n  * A bunch of small tasks may start up, creating a smaller, but still noticeable, performance hit.\n\n\n\n\nYou have no control over any of this, and they will all throw off the timings. Generally, I’ve learned to take a number of measurements (3–5, say), and take the _lowest_. Not the average. The lowest will be the time of the processing, with the least about of other things interfering.\n\n\n\n\n# You’re Wrong!\n\n\n\n\nThere’s one complication I haven’t mentioned yet. The biggest problem with optimizing code is this.\n\n\n\n\n<blockquote>\n\n> \n> Your intuitions about what is so slow are **wrong**.\n> \n> \n</blockquote>\n\n\n\n\nMaybe not always, but often enough that you shouldn’t trust them.\n\n\n\n\nOr to put it another way:\n\n\n\n\n<blockquote>\n\n> \n> Bottlenecks occur in surprising places, so don’t try to second guess and put in a speed hack until you have proven that’s where the bottleneck is. — Rob Pike\n> \n> \n</blockquote>\n\n\n\n\n(And to be fair, the tool I’m getting ready to describe, timr, doesn’t help you identify which part of your code is taking so much time, but it will tell you whether what you changed helped or not. Finding bottlenecks will be the subject of another blog post.)\n\n\n\n\n# My Kingdom for Some Data\n\n\n\n\nBecause you’re going to be wrong, optimization is largely a data-driven task. What data?\n\n\n\n\n\n\n  1. Multiple timings for each small change you make. You probably only want to look at a summary each group of timings, however.\n\n\n  2. The return value of each web request. Whatever you changes you make, you probably don’t want this to change.\n\n\n\n\n_Data_ is just another word for _lots of bookkeeping_, which is another way of saying _boring and error-prone_. Software developers hate _boring and error-prone_, and I’m no exception. As I was working on optimizing an AJAX call in Neatline, I created a small script to help me keep track of the data I was accumulating. I call this [timr](https://github.com/erochest/timr) (because leaving out vowels is always a good idea).\n\n\n\n\n# Installing\n\n\n\n\nTimr requires Python, and if you have [Python](http://python.org/) and [Pip](http://pypi.python.org/pypi/pip/), you can install it with:\n\n\n\n[sourcecode language=\"bash\"]\npip install timr\n[/sourcecode]\n\n\n\n# Using\n\n\n\n\nTimr is a command-line tool, and once it’s installed, using it is pretty straight-forward.\n\n\n\n\n## Configuration Files\n\n\n\n\nThe easiest way to use it is to gather all the command-line arguments for a project into an ad hoc configuration file.\n\n\n\n\nFor example, save this as `fetch.conf`. It will time a POST request with my name, and it will send the output to `fetch-output.csv`:\n\n\n\n[sourcecode autolinks=\"false\"]\n--method\nPOST\n--url\nhttp://whatever.com/resource/\n--header\nAccept: application/json\n--data\nfirst_name=Eric\n--data\nsurname=Rochester\n--output\nfetch-output.csv\n[/sourcecode]\n\n\n\n<blockquote>\n\n> \n> _NB: Remove the extra lines around the URL. For some reason, WordPress adds those in, but they shouldn’t be there and will cause an error if they’re included._\n> \n> \n</blockquote>\n\n\n\n\nThese values won’t change between runs, so this provides consistency and documentation.\n\n\n\n\n## Fetch\n\n\n\n\nNow, call `timr fetch` with the arguments from the configuration file, plus the message that you want attached to the timing group (in this case, initial timings).\n\n\n\n[sourcecode language=\"bash\"]\ntimr fetch @fetch.conf -m \"initial timings\"\n[/sourcecode]\n\n\n\nThis executes the POST request multiple times (4 times, by default) and write the resulting times out to a CSV file.\n\n\n\n\n## Report\n\n\n\n\nLooking at the raw output isn’t that helpful, however. Instead, you want to summarize and aggregate the timing sessions.\n\n\n\n\nMost of the time, I just dump the aggregate data out to the screen:\n\n\n\n[sourcecode language=\"bash\"]\ntimr report --input=fetch-output.csv\n[/sourcecode]\n\n\n\nBut sometimes I want a pretty chart or graph. Timr doesn’t do visualizations, but you can send the CSV to a file. This way you could pull it into Excel or something that does do visualizations.\n\n\n\n[sourcecode language=\"bash\"]\ntimr report --input=fetch-output.csv --output=report-output.csv\n[/sourcecode]\n\n\n\nThat’s really all there is to it.\n\n\n\n\n## E.G.\n\n\n\n\nFor example, let’s see how fast a Google search for _timr_ is over a couple of sessions.\n\n\n\n\nFirst, we’ll create a configuration file named `google.conf`:\n\n\n\n[sourcecode autolinks=\"false\"]\n--method\nGET\n--url\nhttp://www.google.com\n--data\nq=timr\n--output\ngoogle-timr.csv\n[/sourcecode]\n\n\n\nNow run it a couple of times:\n\n\n\n[sourcecode autolinks=\"false\"]\ntimr fetch @google.conf -m \"initial search\"\ntimr fetch @google.conf -m \"another session\"\n[/sourcecode]\n\n\n\n<blockquote>\n\n> \n> _This doesn’t actually pull up the search results. Instead, it goes to the page that looks like it should have results, but only has the search suggestion drop-down at the top of the page. I’m not going to worry about that right now. After all, trying to optimize Google search results isn’t very useful unless you work at Google._\n> \n> \n</blockquote>\n\n\n\n\nLet’s see what this outputs:\n\n\n\n[sourcecode]\n2012-08-07 10:13:08.871731,03a227c0-e09a-11e1-ad5b-c82a1417b0e9,initial search,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.141083955765\n2012-08-07 10:13:08.871731,03a227c0-e09a-11e1-ad5b-c82a1417b0e9,initial search,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.0433859825134\n2012-08-07 10:13:08.871731,03a227c0-e09a-11e1-ad5b-c82a1417b0e9,initial search,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.0436539649963\n2012-08-07 10:13:08.871731,03a227c0-e09a-11e1-ad5b-c82a1417b0e9,initial search,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.044303894043\n2012-08-07 10:14:03.237169,240e6f5c-e09a-11e1-962c-c82a1417b0e9,another session,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.389742851257\n2012-08-07 10:14:03.237169,240e6f5c-e09a-11e1-962c-c82a1417b0e9,another session,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.0447700023651\n2012-08-07 10:14:03.237169,240e6f5c-e09a-11e1-962c-c82a1417b0e9,another session,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.0436999797821\n2012-08-07 10:14:03.237169,240e6f5c-e09a-11e1-962c-c82a1417b0e9,another session,02a0f14a92b4e0070ee17275f1d78c3a7db1ba68,955,0.0441081523895\n[/sourcecode]\n\n\n\nThe fields here are\n\n\n\n\n\n\n  1. a timestamp for the run,\n\n\n  2. a unique identifier hash for the session,\n\n\n  3. the session message,\n\n\n  4. a SHA hash of the results,\n\n\n  5. the number of bytes returned, and\n\n\n  6. the elapsed seconds for the request.\n\n\n\n\nNow let’s generate the report, dumping it to a file:\n\n\n\n[sourcecode language=\"bash\"]\ntimr report --input=google-timr.csv --output=google-report.csv\n[/sourcecode]\n\n\n\nAnd this outputs:\n\n\n\n[sourcecode]\n03a227c0-e09a-11e1-ad5b-c82a1417b0e9,initial search,0.0433859825134,0.141083955765,0.0681069493294,0.0486528640897\n240e6f5c-e09a-11e1-962c-c82a1417b0e9,another session,0.0436999797821,0.389742851257,0.130580246448,0.172775632452\n[/sourcecode]\n\n\n\nThe fields here are\n\n\n\n\n\n\n  1. the session identifier,\n\n\n  2. the session message, and\n\n\n  3. some summary statistics on the timings:\n\n\n    1. minimum time,\n\n\n    2. maximum time,\n\n\n    3. mean time, and\n\n\n    4. standard deviation.\n\n\n\n\n\n\nFrom this we can see several things:\n\n\n\n\n\n\n  * The minimum times are very close (0.0433 and 0.434).\n\n\n  * There’s a lot more variance in the maximum times (0.141 and 0.390). This could be caused by network latency or other issues and doesn’t accurately reflect the time it took Google to process the query. But looking at the output from the `timr fetch` calls, the first request takes the longest, and that could be because the Python VM is warming up.\n\n\n  * The added time of the first request throws off the mean and standard deviation, so they’re not that useful either.\n\n\n\n\n## More Information and Feedback\n\n\n\n\nFor more information about timr, see [the readme](https://github.com/erochest/timr#readme).\n\n\n\n\nTimr is a very new tool, and there are lots of missing features or even bugs. If you have a feature request or encounter a problem, please [create a new Github issue](https://github.com/erochest/timr/issues/new).\n\n\n\n\nFor example, I could imagine that having the option to throw out the longest or first timing when generating the report would be helpful. What do you think?\n\n\n\n"},{"id":"2012-08-20-using-neatline-with-historical-maps-georeferencing","title":"Using Neatline with historical maps :: Part 1 - Georeferencing","author":"david-mcclure","date":"2012-08-20 05:24:20 -0400","categories":["Geospatial and Temporal"],"url":"using-neatline-with-historical-maps-georeferencing","content":"_[Cross-posted from [dclure.org](http://dclure.org/tutorials/neatline-maps-georeferencing/) and [neatline.org](http://neatline.org/2012/08/20/using-neatline-with-historical-maps-part-1-georeferencing/)]_\n\nOut of the box, [Neatline](http://neatline.org/) (our recently-released framework for building geotemporal exhibits) can be used to create geo-temporal exhibits based on \"modern-geography\" base-layers - OpenStreetMap, Google satellite and street maps, and a collection of [beautiful, stylized layers from Stamen Design](http://maps.stamen.com/#watercolor/12/37.7706/-122.3782). For historical and literary projects, though, one of Neatline's most powerful features is its deep integration with Geoserver, an open-source geospatial server that can pipe georeferenced historical maps directly into Neatline exhibits. For some examples of this, [check](http://hotchkiss.scholarslab.org/neatline-exhibits/show/my-dear-little-nelly/fullscreen) [out](http://hotchkiss.scholarslab.org/neatline-exhibits/show/battle-of-chancellorsville/fullscreen) [these](http://hotchkiss.neatline.org/neatline-exhibits/show/chancellorsville-may-2-1863-132/fullscreen) [four](http://hotchkiss.scholarslab.org/neatline-exhibits/show/chancellorsville-may-3-4-1863-138/fullscreen) demo exhibits built on Civil War battle maps by Jedediah Hotchkiss.\n\nGeoserver is a pretty complex piece of software, and the process of assigning geographic coordinates to static image files (called \"georeferencing\" or \"georectifying\") can be a bit tricky at first. This is the first post in a three-part series that will walk through the entire process of rectifying a historical map using ArcMap, post-processing the image, uploading it to Geoserver, and importing the final web map service into a Neatline exhibit.\n\n**Georectification**\n\nTo start, all you need is a static image file that can be positioned in some way or another on top of a real-geography base layer. Usually, this is a map of some sort, but it could also be aerial photography, or, in more experimental and interpretive use-cases, it could even be a totally non-geographic image that would gain some kind of meaning from being situated in a geospatial context (for example, see the georeferenced manuscript pages in the \"[My Dear Little Nelly](http://hotchkiss.scholarslab.org/neatline-exhibits/show/my-dear-little-nelly/fullscreen)\" exhibit).\n\nSince the final map will be presented in an interactive environment that lets the user zoom in and out at will, it's best to try to find a high-resolution version of the image you want to work with, which will make it possible to zoom further in before the image starts to noticeably pixelate. That said, the images don't need to be excessively large - as Kelly Johnston (one of the GIS specialists in the Scholars' Lab) pointed out, _extremely_ high-fidelity images (~10,000 pixels in height or width) often don't really provide that much more value than somewhat smaller images, and can have the effect of choking up Geoserver and slowing down the speed with which the map is rendered in the final Neatline exhibit. For historical and literary use cases, I've found that images with dimensions in the 3000-5000 pixel range provide a good balance of resolution and speed.\n\nIn this tutorial, I'll be working with map #124 in the [Hotchkiss Map Collection](http://memory.loc.gov/ammem/collections/maps/hotchkiss/index.html) at the Library of Congress (see the full list of maps [here](http://memory.loc.gov/ammem/collections/maps/hotchkiss/hotchkisslist.pdf)). To get the static image file, go to the [view page for the map](http://memory.loc.gov/cgi-bin/map_item.pl?data=/home/www/data/gmd/gmd388/g3884/g3884f/cwh00124.jp2&style=gmd&itemLink=r?ammem/gmd:@field(NUMBER+@band(g3884f+cwh00124))&title=[Map%20of%20Fredericksburg,%20Va.,%20and%20vicinity].) and right click on the \"Download JPEG2000 image\" link at the bottom of the screen and click \"Save Link As...\"\n\nWith the image in hand, let's fire up ArcMap and get the environment set up:\n\n\n\n\n\n\n  1. **Add a base map** by clicking on `File > Add Data > Add Basemap`. The base map is the real-geography foundation, the \"true\" map against which the image will be referenced. Select one of the nine options and click \"Add.\" This is largely just a matter of preference. For for maps with a lot of human geography (roads, railroads, cities), I like the \"Bing Maps Road\" layer, and for maps with natural geography (rivers, mountains, coastlines) I like the \"USA Topo Maps\" layer. After you've added a base map, a listing the layer will appear in the \"Table of Contents\" column on the left, which lists out all of the assets available in the environment. You can toggle layers on and off by clicking the checkbox next to the layer title.\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/base-layer-300x240.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/base-layer.jpg)\n\n\n\n\n\n  2. **Add the static image** that you want rectify by clicking on `File > Add Data > Add Data`. Navigate to the location of the image, select it, and click \"Add.\" (Note: If the folder containing the image is not already available in the dropdown menu to the right of \"Look in,\" you may have to \"connect\" to the folder by clicking on the folder icon with the black \"+\" symbol in the toolbar to the right. Select the folder, click \"OK,\" and the folder should become available in the main dropdown menu.) If you get a popup asking if you want to generate pyramids, click \"No,\" and if you get an alert labeled \"Unknown Spatial Reference,\" click \"OK\" (ArcMap is just reacting to the fact that the image doesn't have existing geo-coordinates).\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/add-data-300x207.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/add-data.jpg)\n\n\n\n\n\n  3. **Enable the Georeferencing toolbar** by clicking `Customize > Toolbars > Georeferencing`. The toolbar will appear at the top of the screen, and can be merged into the main top bar by dragging it upwards in the direction of the main navigation controls.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/georeferencing-toolbar-300x54.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/georeferencing-toolbar.jpg)\n\n\n\n\n\n  4. **Move to the rough location of the image that's being rectified** by using the navigation controls at the left of the top toolbar to zoom the base map to the approximate location and bounds of the historical map. In this example, since the image I'm working with shows the town of Fredericksburg and the course of the Rappahannock southeast of the town, I'll center the viewport a bit below and left of Fredericksburg, maybe zoomed back a bit to show the whole area that will be covered by the image.\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/initial-focus-300x203.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/initial-focus.jpg)\n\n\n\n\n\n  5. **Show the static image** by clicking on `Georeferencing > Fit To Display`. This just plasters the map directly on top of the base layer, using the bounds of the current viewport (set in the first step) to determine the position and scale of the image. Basically, this is just setting a crude, starting starting set of geo-coordinates that can be refined by laying down point associations.\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/fit-to-display-300x204.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/fit-to-display.jpg)\n\n\n\n\n\n\nNow, the actual rectification. All this entails is creating a series of associations (at least two, as many as ~15-20) between points on the static image and points on the real-geography base layer. As you add points, ArcMap will automatically pan, rotate, scale, and ultimately \"warp\" the image to match the underlying base layer.\n\n\n\n\n  1. **Lay a positioning point**: I like to start by picking the most obvious, central, easy-to-find point on the historical map. In this case, I'll use the position at which the Richmond Fredericksburg Railroad crosses over the west bank Rappahannock. To lay the first point, click on the \"Add Control Points\" button in the Georeferencing toolbar and click at the exact position on the historical map that you want to use as the starting point. Then, without clicking down on the map viewport again, move the cursor over to the \"Table of Contents\" pane and check off the historical map, leaving just the base layer visible. Then, click on the location on the base layer that corresponds to the original location on the historical map.\n\nOnce you've clicked for a second time, the dotted line between the two clicks will disappear. Display the historical map again by checking the box next to its title in the \"Table of Contents.\" The image will now be anchored onto the base layer around the location of the first point association.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/first-point-300x203.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/first-point.jpg)\n\n\n\n\n\n  2. **Lay a scaling and rotation point**: Next, pick another easily-mappable point on the historical map, this time ideally near the edges of the image, or at least some significant distance from the first point. Follow the same steps of clicking on the historical map, hiding the historical map, clicking on the corresponding location on the base layer, and then re-enabling the historical map to see the effect.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/second-point-300x204.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/second-point.jpg)\n\n\n\n\n\n\nAt this point, you already have a minimally rectified image - the second point will both scale the image down to roughly correct proportions and rotate the image to the correct orientation. From this point forward, adding more points will make the rectification increasingly accurate and granular by \"warping\" the image, like a sheet of rubber, to fit the lattice of points as accurately as possible.\n\nHow many points is enough? Really, it depends on the accuracy of the map and objectives of the Neatline exhibit. In this case, Hotchkiss' map is already quite accurate, and the just first two points do a pretty good job of orienting the map and showing how it fits into the larger geography of the region. For literary and historical projects that don't gain anything from extreme precision, a handful of points (2-5) is often sufficient.\n\nWhen a higher level of precision is required, though, or when the historial map is significantly inaccurate (as is the case for older maps), more points (10-20) can be necessary. It's not an exact science - just lay points until it looks right.\n\nAs you work (especially in cases where you're laying down a lot points) experiment with different \"transformation\" algorithms by clicking `Georeferencing > Transformations` and selecting one of the five options (1st Order Polynomial, 2nd Order Polynomial, etc). Behind the scenes, these algorithms represent different computational approaches to \"fitting\" the image based on the set of control points - some of the transformations will leave the image roughly polygonal, whereas others will dramatically \"warp\" the shape of the image to make it conform more accurately to the point associations. Depending on the type of image you're working with and its accuracy relative to the base layer, different transformations will produce more or less pleasing results. For now, I'll just leave it at 1st Order Polynomial.\n\nOnce you're done laying points, save off the image as a georeferenced .tiff file by clicking `Georeferencing > Rectify`. As desired, change the filename and target directory, and click \"Save.\"\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/rectify-300x223.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/rectify.jpg)\n\n**Links**\n\n[ArcGIS georeferencing documentation](http://help.arcgis.com/en/arcgisdesktop/10.0/help/index.html#//009t000000mn000000)\n[Quantum GIS georeferencing tutorial](http://qgis.spatialthoughts.com/2012/02/tutorial-georeferencing-topo-sheets.html) (open-source alternative to ArcMap)\n[Georeferencing - making historic maps spatial](http://spatial.scholarslab.org/stepbystep/making-historic-maps-spatial-georeferencing/)\n\n_[Cross-posted with [dclure.org](http://dclure.org/tutorials/neatline-maps-georeferencing/)]_\n"},{"id":"2012-08-23-using-neatline-with-historical-maps-part-2-transparency","title":"Using Neatline with historical maps :: Part 2 - Transparency","author":"david-mcclure","date":"2012-08-23 05:33:08 -0400","categories":["Geospatial and Temporal"],"url":"using-neatline-with-historical-maps-part-2-transparency","content":"**Update 8/27/12:**\n\nAfter posting this last week, [a comment by KaCeBe](http://www.scholarslab.org/geospatial-and-temporal/using-neatline-with-historical-maps-part-2-transparency/comment-page-1/#comment-23142) led me to go back and look for a way to get Geoserver to render transparent borders without having to manually add an alpha channel to the file. Although I still can't find way to make Geoserver do it automatically, I did find [this thread on the OSGeo.org forums](http://osgeo-org.1560.n6.nabble.com/Trying-to-get-nodata-in-GeoTIFF-to-display-as-transparent-td3853784.html) in which user [bovermyer](http://osgeo-org.1560.n6.nabble.com/template/NamlServlet.jtp?macro=user_nodes&user=198969) finds a solution that's much faster than the Photoshop workflow described in this post.\n\nWith `gdal` installed (see below), open up the terminal and run this command:\n\n`gdalwarp -srcnodata 0 -dstalpha file1.tif file2.tif`\n\n...where `file1.tif` is the name of the original file generated by ArcMap and `file2.tif` is the name of the new, transparency-added copy of the `file1.tif` generated by gdal. Then (re)build the geotiff with this command:\n\n`gdal_translate -of GTiff -a_srs EPSG:4326 file2.tif file2_rebuilt.tif`\n\n...which we've found is necessary to avoid errors during the Geoserver upload process. At this point, `file2_rebuilt.tif` is ready to be loaded into Geoserver and brought into a Neatline exhibit.\n\nMuch faster than pointing-and-clicking in Photoshop!\n\n\n\n* * *\n\n\n\n_[Cross-posted with [dclure.org](http://dclure.org/tutorials/neatline-maps-transparency/) and [neatline.org](http://neatline.org/2012/08/23/using-neatline-with-historical-maps-part-2-transparency/)]_\n\n_This is part 2 of a 3-post tutorial that walks through process of georeferencing a historical map and using it in Geoserver and Neatline. Check out [part 1](http://www.scholarslab.org/geospatial-and-temporal/using-neatline-with-historical-maps-georeferencing/), which covers rectification in ArcMap._\n\nIn the [first part](http://dclure.org/?p=948) of this series, we brought a static image into ArcMap and converted it onto a georeferenced .tif file. In this article, we'll post-process the image in Photoshop to get it ready to be loaded into Geoserver.\n\n**The problem: Black borders around the image**\n\nIf you open up the newly-generated .tif file in a regular image editing program, you'll see that ArcMap added in regions of black around the actual map to make it fill the rectangular aspect ratio of the file. This happens almost every time, since the process of rectification usually involves rotating the image away from its original orientation.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/borders-296x300.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/borders.jpg)\n\nIn the context of a Neatline exhibit, this is problematic because the black borders will completely occlude the real-geography base layer (or underlying historical maps) immediately surrounding the image. Fortunately, former Scholars' Lab GIS assistant Dave Richardson figured out how to strip out the borders in Photoshop by converting them into transparencies. This step is a bit of a nuisance, but we've found that it dramatically improves the final appearance of the map.\n\nHere's how to do it:\n\n\n\n\n\n\n  1. Go to the directory that the file was originally saved to. You'll notice that ArcMap actually generated four files - the .tif, along with a .tfw, tif.aux.xml, and .tif.ovr. Leave all the files in place, since we'll need them at the end of the process to rebuild the geospatial header information after we post-process the image. Open up the main .tif file in Photoshop.\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/files1.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/files1.jpg)\n\n\n\n\n\n  2. In Photoshop, right click on the starting background layer and click \"Layer from Background.\" This will delete the locked background and replace it with a regular layer with the same content.\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/layer-from-background-300x248.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/layer-from-background.jpg)\n\n\n\n\n\n  3. Use the \"Magic Wand Tool\"  [![](http://www.scholarslab.org/wp-content/uploads/2012/08/magic-wand.jpg)](http://www.scholarslab.org/geospatial-and-temporal/using-neatline-with-historical-maps-part-2-transparency/attachment/magic-wand/) to select each of the borders by holding down the shift key and clicking inside the black areas. A dotten line will snap to the edges of the borders. If the wand tool is selecting parts of the actual map image, drop down the \"Tolerance\" setting to 1, which will limit the selection to the exact color value of the clicked location on the image. Once the borders are selected, press the delete key to clear out the selection. At this point, the image should be surrounded by the default, checkered background graphic.\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/border-delete-300x280.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/border-delete.jpg)\n\n\n\n\n\n  4. Add an alpha channel to the image by clicking on the \"Channels\" tab on the top toolbar of the layers window (If the \"Channels\" tab isn't available by default, activate it by clicking `Window > Channels`). Click the dropdown icon at the right of the toolbar, and click \"New Channel.\" Check the \"Masked Areas\" radio button, and set the color to be pure black with 0% opacity. Click \"OK\" to create the channel.\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/alpha-300x183.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/alpha.jpg)\n\n\n\n\n\n  5. Now, activate the Magic Wand Tool again and select each of the checkered, transparent areas around the image (the regions that were originally filled with the black borders). Then, invert the selection by clicking on `Select > Inverse`. At this point, the selection should exactly frame the map itself (the portion of the image that should _not_ be transparent).\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/inverse-170x300.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/inverse.jpg)\n\n\n\n\n\n  6. Back over in the Channels tab, click on the listing for the Alpha channel that was created in step 4 and hide the RBG channels by clicking the visibility checkbox next to the top-level RGB listing. This will cause the image to go totally black, with the selection of the map region still active on top of the alpha channel.\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/black-selection-300x280.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/black-selection.jpg)\n\n\n\n\n\n  7. Activate the Paint Bucket Tool  [![](http://www.scholarslab.org/wp-content/uploads/2012/08/paint-bucket.jpg)](http://www.scholarslab.org/geospatial-and-temporal/using-neatline-with-historical-maps-part-2-transparency/attachment/paint-bucket/) and set the foreground color to pure white (If you don't see the icon for the paint bucket in the Tools column, click and hold the icon for the \"Gradient\" tool  [![](http://www.scholarslab.org/wp-content/uploads/2012/08/gradient-tool.jpg)](http://www.scholarslab.org/geospatial-and-temporal/using-neatline-with-historical-maps-part-2-transparency/attachment/gradient-tool/) and a drop-down select will appear with a listing for the Paint Bucket). Then apply the paint bucket on the selected area on the Alpha channel, creating a white area over the region occupied by the map.\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/white-selection-300x281.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/white-selection.jpg)\n\n\n\n\n\n  8. Make sure that both the Alpha channel and all of the RGB color channels are marked as visible in the Channels window. Then go to `File > Save As`. So as not to override the name of the original file, change the name to something like `[original filename]_processed`. Uncheck \"Layers,\" check \"As a Copy\" and \"Alpha Channels,\" and click \"Save.\"\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/save-as-300x198.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/save-as.jpg)\n\n\n\n\n\n  9. On the \"Tiff Options\" dialog box, leave \"Save Image Pyramid\" and \"Save Transparency\" unchecked and make sure \"Discard Layers and Save a Copy\" is checked.\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/tiff-options-222x300.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/tiff-options.jpg)\n\n\n\n\n\n\nNow, we have a second version of the .tiff file with an Alpha channel that converts the black borders into transparent regions. The problem, though, is that the process of re-saving the file strips out the critical geospatial information in the original .tiff - we'll have to insert this data back into the processed file before it can be used in Geoserver and Neatline.\n\n**Rebuilding the geotiff**\n\nWe'll take care of this using a utility called `gdal`, a powerful command line library that can do a wide variety of transformations on geospatial files. Head over to [gdal.org](http://www.gdal.org/) for full documentation on how to install the command line utilities. On Mac OSX, using the [homebrew](http://mxcl.github.com/homebrew/) package manager, it should be as easy as `brew install gdal`. If you're on Windows, a binary distribution of the tool can be found [here](http://trac.osgeo.org/osgeo4w/wiki).\n\nWith gdal installed, fire up the terminal and change into the directory with the original .tif, the processed .tif, and the three *.tfw files.\n\n\n\n\n\n\n  1. First, create a copy the original .tfw file with a name that matches the processed .tif file that was created in step 8 above. So, if the original .tif was called `hotchkiss.tif`, and the processed file was saved as `hotchkiss_processed.tif`, copy `hotchkiss.tfw` as `hotchkiss_processed.tfw` (this can be done with `cp hotchkiss.tfw hotchkiss_processed.tfw`). The file names have to match in order for gdal to know where to pull information about the coordinate projection when we rebuild the header.\n\n\n\n\n  2. Now, still assuming we're working with files named hotchkiss_processed.tif and hotchkiss_processed.tfw, rebuild the header with this command:\n\n`gdal_translate -of GTiff -a_srs EPSG:4326 hotchkiss_processed.tif hotchkiss_processed_rebuilt.tif`.\n\n\n(**Note**: It doesn't actually matter what you call the derivative files at the various steps of the process. All that matters is that the .tfw file matches the name of the processed .tif file.)\n\n\n\n\n\nThis will create a new file called `hotchkiss_processed_rebuilt.tif` that contains the transparency channel and the reconstructed geospatial information. At this point, the file is ready to be uploaded to Geoserver and brought into a Neatline exhibit.\n"},{"id":"2012-08-29-using-neatline-with-historical-maps-geoserver","title":"Using Neatline with historical maps :: Part 3 - GeoServer","author":"david-mcclure","date":"2012-08-29 05:55:07 -0400","categories":["Geospatial and Temporal"],"url":"using-neatline-with-historical-maps-geoserver","content":"**Note** This is specifically for Omeka/Neatline 1.x. If you are using Omeka/Neatline 2.x, you can upload your maps to Geoserver with Option 2 below. Follow [Editing Record Imagery](http://docs.neatline.org/style-tab-imagery.html) for working with the WMS layers.\n\n_[Cross-posted with [dclure.org](http://dclure.org/tutorials/neatline-maps-geoserver/) and [neatline.org](http://neatline.org/2012/08/29/using-neatline-with-historical-maps-part-3-geoserver/)]_\n\n_This is part 3 of a 3-post tutorial that walks through process of georeferencing a historical map and using it in GeoServer and Neatline._\n\nIn [part 1](http://www.scholarslab.org/geospatial-and-temporal/using-neatline-with-historical-maps-georeferencing/) of this series, we used ArcMap to convert a static image into a georeferenced .tiff file. In [part 2](http://www.scholarslab.org/geospatial-and-temporal/using-neatline-with-historical-maps-part-2-transparency/), we post-processed the file with `gdal` to remove the black borders around the image. In this article, we'll load the .tiff file into GeoServer and import the final web map service into a Neatline exhibit.\n\n**Generating the web map service on GeoServer**\n\nThere are two ways to upload the .tiff file to GeoServer - the entire process can be performed through the Omeka interface using the Neatline Maps plugin, or the file can be uploaded directly onto the machine running GeoServer and the service created by way of the GeoServer administrative interface.\n\nThe first option is easier, but there's a fundamental restriction that makes it unworkable in certain situations - since Neatline Maps has to upload the .tif file through Omeka before it can create the map service via the GeoServer API, it's impossible to upload files through Neatline Maps that are larger than the file upload limit set by the [`upload_max_filesize`](http://www.php.net/manual/en/ini.core.php#ini.upload-max-filesize) and [`post_max_size`](http://www.php.net/manual/en/ini.core.php#ini.post-max-size) settings in the php.ini file on your server.\n\nDepending on the hosting environment, these values can be set to anywhere from 2-20 megabytes by default. If you have access to the php.ini file, you can bump up the limit, but beyond a certain point it probably makes more sense just to upload the file directly to the server running GeoServer and create the web services manually using the GeoServer administrative interface. Since high-resolution .tiff files can weigh in a hundreds of megabytes or even gigabytes, this is often a more controlled and reliable approach, especially in cases where you're working with multiple files at once.\n\nRegardless of how the file is uploaded, the final process of importing the map service into Omeka and Neatline works the same way.\n\n**Option 1: Upload through Neatline Maps**\n\nIf your file is small enough to be uploaded through Omeka, the Neatline Maps plugin provides plug-and-play connectivity with GeoServer:\n\n\n\n\t\n  1. With Neatline Maps installed, click on the \"Neatline Maps\" tab in the top toolbar of the Omeka administrative interface and click on \"Create Server.\" Fill in the URL, Username, and Password for your GeoServer. In the Name section, enter a plaintext identifier for the server (used for content management in Omeka) and use the Workspace field to specify the workspace on the GeoServer installation that will house the new stores and layers. Click \"Save\" to create the server record.(**Note**: If you want to upload files to more than one installation of GeoServer, you can create as many server records as you want. At any given point, though, only one of the record can be marked as the \"Active\" server - this the server that the plugin will use to handle new .tif uploads).\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/create-server-300x245.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/create-server.jpg)\n\n\t\n  2. Create an item to associate the web map service with (or edit an existing item). In the Item add/edit form, click on the \"Files\" tab, click on \"Choose File,\" and select the .tiff file as you would for a regular file upload. When you save the item, Neatline Maps will automatically detect that you're trying to upload a georeferenced .tif file and create a corresponding web map service by way of the GeoServer API.[![](http://www.scholarslab.org/wp-content/uploads/2012/07/upload-file-300x124.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/upload-file.jpg)\n\nOnce you've saved the file, if you go back into the Item edit form and click on the \"Web Map Service\" tab, you'll notice that \"WMS Address\" and \"Layers\" fields have been automatically updated to point to the new web map service. On the show page for the item, the map will be displayed in a small, interactive widget below the default metadata fields.\n\n\n**Option 2: Upload directly to GeoServer**\n\n\n\n\t\n  1. First, upload the file to the server running GeoServer with `scp` or another file transfer protocol. It's usually a good idea to get the file out of the `/tmp` directory, but it doesn't matter beyond that - GeoServer can read the entire file system. We've gotten into the habit of putting the source .tiff files in `/var/geotiff`.\n\n\t\n  2. In the GeoServer administrative interface, click on \"Stores\" in the left column and then click \"Add new Store.\" On the next screen, click GeoTIFF under the \"Raster Data Sources\" heading.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/new-data-store-300x159.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/new-data-store.jpg)\n\n\t\n  3. Select a workspace for the store and enter a name. Under \"Connection Parameters,\" click the \"Browse..\" link, and use the pop-up window to navigate to the file. Click \"Save\" to create the store.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/connection-parameters-300x162.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/connection-parameters.jpg)\n\n\t\n  4. Next, we have to publish the store as a public-facing layer. On the next screen, click the \"Publish\" link.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/publish-300x158.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/publish.jpg)\n\n\t\n  5. Now, the tricky part. We have to manually tell GeoServer to deliver the layer using a coordinate projection system that Neatline can use to layer the map on top of the real-geography base layers in OpenLayers. Scroll down to the \"Coordinate Reference Systems\" heading and enter `EPSG:900913` into the \"Declared SRS\" field. Under \"SRS handling,\" select \"Force declared.\" Under the \"Bounding Boxes\" heading, click both the \"Compute from data\" and \"Compute from native bound\" links.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/coordinates-274x300.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/coordinates.jpg)\n\n\nNow, with the layer created, we can associate the new web map service with an item in your Omeka collection by manually filling in the two fields in the \"Web Map Services\" tab:\n\n\t\n  1. Go back the Omeka administrative interface and find the item that you want to associate the map with (or just create a new item). Open up the edit form for the item.\n\n\t\n  2. Click the \"Web Map Services\" tab. Fill in the the top-level WMS address for the GeoServer installation (this always ends with `/wms`, and might look something like `localhost:8080/GeoServer/wms`) and enter the list of comma-delimited layers that you want to be associated with the item. For example, if you have a workspace called \"hotchkiss\" with layers \"chancellorsville\" and \"fredericksburg,\" you could enter:\n\n`hotchkiss:chancellorsville,hotchkiss:fredericksburg`.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/07/wms-tab-300x181.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/07/wms-tab.jpg)\n\n\t\n  3. Save the item.\n\n\n**Use the map in a Neatline exhibit**\n\nThe two methods both have the end result of filling in the two fields in the \"Web Map Services\" tab. The only difference is in whether the .tif file is uploaded through Omeka or directly into GeoServer.\n\nOnce an item is linked to a web map service, Neatline automatically detects the map and loads it into an exhibit when the item is activated on the map. With the item queried into the editing environment for an exhibit, just check the middle of the three checkboxes next to the listing for the item in the content management panel:\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/08/map-activation-300x178.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/08/map-activation.jpg)\n\n...and the WMS layer will appear on the map:\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/08/map-in-exhibit-300x195.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/08/map-in-exhibit.jpg)\n"},{"id":"2012-09-06-bulk-editing-in-vim","title":"Bulk Editing in Vim","author":"eric-rochester","date":"2012-09-06 04:57:25 -0400","categories":["Research and Development"],"url":"bulk-editing-in-vim","content":"I regularly have to perform a short sequence of small, regular edits on a collection of files. If you work with computers long enough, that’s something everyone has to do.\n\n\n\n\nOften I reach for a scripting language. But other times the edit is so small that even `sed` seems like overkill. Or maybe the edits are just the wrong kind of complexity to capture easily with code. My fingers may be able to make the changes quickly and repetitively, but when I try to break down how a script would do it, I get a headache.\n\n\n\n\nOver the years, I’ve been confronted with problems like this often enough that I’ve developed a well-tested approach using [Vim](http://www.vim.org/). It’s become one of those tools that I don’t really think much about: I just use it from time to time, and it makes my life easier.\n\n\n\n\nBut not long ago, [Jeremy](http://clioweb.org/) mentioned that he had a small change to make to a series of files in [NeatlineMaps](https://github.com/scholarslab/NeatlineMaps). Usually, he switches to TextMate for tasks like this, but he agreed that to let me show him how I would handle this in Vim.\n\n\n\n\nHeh.\n\n\n\n\nWhenever I try to explain to someone how to do something in Vim, I invariably sound like, Then hit _escape_, _4h_, _0_, now type whatever. It’s kind of funny, but it’s not a lot of fun, either for me or for the person I’m shouting keystrokes at.\n\n\n\n\nHopefully, this will make a better blog post.\n\n\n\n\nHere’s what we did:\n\n\n\n\n# The Problem\n\n\n\n\nJeremy had tried to add some Vim [mode lines](http://vim.wikia.com/wiki/Modeline_magic) to some PHP files. These are comments at the top of a file for setting options in Vim. Currently, they look [like this](https://github.com/scholarslab/NeatlineMaps/blob/master/NeatlineMapsPlugin.php#L2):\n\n\n\n[sourcecode language=\"php\"]\n/* vim: set expandtab tabstop=4 shiftwidth=4 softtabstop=4: */\n[/sourcecode]\n\n\n\nBut they weren’t working. It turned out that what should have been a colon near the end of the line was actually a semicolon, and once that was fixed, the settings worked fine.\n\n\n\n\nThat was all right. But he needed to make that change on almost every file in NeatlineMaps.\n\n\n\n\n# The Solution\n\n\n\n\nThe process I showed him has four parts. Let’s break them down.\n\n\n\n\n## Part One: `:args`\n\n\n\n\nFirst, we have to load the files to process. When you open Vim from the command-line and pass in files there, the file names are stored in the [argument list](http://vimdoc.sourceforge.net/htmldoc/editing.html#:args). You can access the argument list inside Vim—either to see what files are in it or to set the files it contains—using the `:args` command:\n\n\n\n[sourcecode language=\"bash\"]\n:args **/*.php\n[/sourcecode]\n\n\n\nThis searches for all the files in the `NeatlineMaps` directory and subdirectories that have a `.php` extension. These files are loaded into the argument list.\n\n\n\n\nWhat’s nice about the argument list is how easily you can navigate over it using a few simple commands:\n\n\n\n\n\n\n  * `:rewind` Moves to the beginning of the list.\n\n\n  * `:next` Moves to the next file in the list.\n\n\n  * `:Next` Moves to the previous file in the list.\n\n\n  * `:previous` Also moves to the previous file in the list.\n\n\n  * `:last` Moves to the last file in the list.\n\n\n\n\nAll these can also be abbreviated. So for example, you can use `:n` and `:N` to move forward and backward. Have a `:n` mapped to control-n, so navigating forward is especially easy.\n\n\n\n\n## Part Two: `q`\n\n\n\n\nWith the first file loaded into the buffer, now we make the change that we want to make on all files and record the keystrokes into a buffer. For this we chose the _t_ buffer. There’s no reason for that particular letter: It was just the first one I thought of:\n\n\n\n[sourcecode language=\"bash\"]\nqt\n[/sourcecode]\n\n\n\nNow the bottom of the Vim screen should say `recording`. At this point, we go ahead and make the edit.\n\n\n\n\n## Part Three: `:s/../../e`\n\n\n\n\nWhat I had Jeremy do was slightly more complicated and precise, but basically, I had him do this:\n\n\n\n[sourcecode language=\"bash\"]\n:%s/softtabstop=4;/softtabstop=4:/e\n[/sourcecode]\n\n\n\nThis looks over the whole file (`%`) and performs a search-and-replace (`s`). It searches for the string _softtabstop=4;_ and replaces it with the same string, except it used a colon (_softtabstop=4:_). The `e` at the end just means that it should ignore errors and keep chugging. That way, if a file does not have a modline (and not all did), it would keep going.\n\n\n\n\nOnce we’ve made the change, let’s save it and move to the next file.\n\n\n\n[sourcecode language=\"bash\"]\n:wn\n[/sourcecode]\n\n\n\nThis combines the _w_rite command and the _n_ext command (from above).\n\n\n\n\nThat’s all we need to do for each file. Now hit _q_ to stop recording:\n\n\n\n[sourcecode language=\"bash\"]\nq\n[/sourcecode]\n\n\n\nYou can replay that now by pressing `@t`. Jeremy and I did that a few times to make sure it was doing what we wanted and wasn’t chewing up the files and spitting the pieces back in our faces.\n\n\n\n\n## Part Four: _n_`@`\n\n\n\n\nOnce you’re sure that everything’s safe, change the rest of the files. Most commands in Vim can take a numerical prefix, which tells Vim how many times to perform the command. For example, _j_ moves down one line, and _10j_ moves down 10 lines.\n\n\n\n\nIn this case, tell it to play the recorded keystrokes 100 times:\n\n\n\n[sourcecode language=\"bash\"]\n100@t\n[/sourcecode]\n\n\n\nAnd Vim goes to work. It will stop on the first error, which will happen when `:next` reached the last file in the argument list and isn’t able to move any further.\n\n\n\n\n# Solved\n\n\n\n\nWell, looking back, this particular problem would have been perfect for `sed`. But sometimes that requires looking at documentation.\n\n\n\n\nAnd that’s it. It seems more complicated than it actually is, and once you’ve been through it a few times, you can do it very quickly. Vim’s ability to record and replay keystrokes, combined with its commands to navigate in and across files, make an incredibly powerful combination.\n\n\n\n\nTo show how easy this process is, here is a screencast of me walking through the problem outlined above on NeatlineMaps code. You may want to [click through](https://vimeo.com/48900819) to a larger version, more readable version of the video.\n\n\n\n\n\n[Bulk Editing Vim](http://vimeo.com/48900819) from [Eric Rochester](http://vimeo.com/user2087066) on [Vimeo](http://vimeo.com).\n\n\n\n\n"},{"id":"2012-09-06-omeka-neatline-metadata-survey","title":"Omeka + Neatline Metadata Survey","author":"ronda-grizzle","date":"2012-09-06 11:19:50 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"omeka-neatline-metadata-survey","content":"As part of our collaboration on [Omeka](http://omeka.org/) + [Neatline](http://neatline.org/), the Scholars' Lab and Omeka development teams are seeking your assistance to help make our projects more useful across many scholarly disciplines--including beyond the humanities and cultural heritage fields in which they originated. We've developed a short survey asking questions about new data types that Omeka+Neatline could display and metadata formats that it might import and describe.\n\nWhile Omeka + Neatline can handle many metadata standards and formats familiar to humanities scholars, archivists, and museum professionals--such as Encoded Archival Description schema (EAD), Text Encoding Initiative schema (TEI), Dublin Core (DC) and Visual Resources Association Core (VRA Core)--it is also possible to import simple CSV files or any flat XML format, with potential to handle other formats more applicable to your field's standards.\n\nWe'd like to hear about other specific standards and formats that could make Omeka + Neatline more helpful to your research and scholarship.\n\nThank you for taking a few minutes to answer our survey!\n\n\n[Please click here to take the survey](http://www.surveymonkey.com/s/CCD5PDX)\n"},{"id":"2012-09-10-geocoding-for-neatline-part-i","title":"Geocoding for Neatline - Part I","author":"wayne-graham","date":"2012-09-10 05:18:03 -0400","categories":["Digital Humanities","Research and Development"],"url":"geocoding-for-neatline-part-i","content":"Recently I was asked if there was a way to [import place names in connection with lat/lon points](https://twitter.com/S_moores/status/222369341390331904). Twitter's character limitation does't provide an adequate format to respond, and this technique can be quite useful outside of Neatline too, so I thought I would dive in a bit and explain a method to can get prepare place names for use in Omeka and beyond. This will be a two part series where I cover the basics of geocoding locations in a CSV file, then move to using these points with the [NeatlineFeatures](http://omeka.org/add-ons/plugins/neatlinefeatures/) and [Neatline](http://omeka.org/add-ons/plugins/neatline/) plugins for [Omeka](http://omeka.org/).\n\n\n\n# Geocoding\n\n\nGeocoding is a method of deriving geographic descriptions (a latitude/longitude point or series of points) from  geographic data like an address (or portion of an address) or place name. The more granular the geographic data, the more accurate the location information. For example, Charlottesville, Virginia is far more accurate than Virginia, and the center point that a geocoder would return would use different points to represent this data. As you can see, the red marker is the center point for geocoding '**Virginia**,' where the blue marker codes the center of '**Charlottesville, Virginia**.'\n\n[iframe src=\"http://jsfiddle.net/wsgrah/rbHhj/embedded/result,js,html,css/\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\"]\n\nThere are some complexities, as data can be a bit ambiguous (e.g. W Jefferson St. vs Jefferson St.), but clever engineers have been working on issues like that and (generally) do a good job figuring out what is intended from partial address information. It is also worth noting that different services have different algorithms for calculating the result from a partial address (center of a county, city, state, country, etc.), so be sure to check your results to make sure they're what you are expecing!\n\n\n\n## Web Services\n\n\nWith the growth of different mapping services over the last several years, getting access to geographic information has become pretty straight forward thanks to a lot of really smart engineers working on the problem of providing location-based information. Most likely you've consumed these services (even if you didn't realize) and you've probably heard of some of the big companies that provide [APIs](http://en.wikipedia.org/wiki/Application_programming_interface) to their Geocoding services like [Google](https://developers.google.com/maps/documentation/geocoding/), [Yahoo!](http://developer.yahoo.com/maps/rest/V1/geocode.html), and [Microsoft](https://www.microsoft.com/maps/developers/web.aspx). These are great resources, and provide well documented access to their data, but do place some restrictions on the use of the data retrieved from their system. Generally these restrictions are on the commercial re-use of the information, but do read and understand the Terms of Service to make sure your intended use is in compliance with the Terms of Service for the geocoding service you use for  your project.\n\nWhile these companies provide access to their information, I did want to take a moment to  highlight some other services that are not backed by large corporate entities. [Geocoder.us](http://geocoder.us/) is a service that has good coverage if you're data is in the US ([Geocoder.ca](http://geocoder.ca/) provides a service for Canadian addresses) and allows up to 50,000 requests per day.  [USC's GIS Research Laboratory](https://webgis.usc.edu/Services/Geocode/Default.aspx) provides a service for up to 2,500 addresses per day, with very few restrictions on its use. The last one I'll mention (and there are many more of these services), [Geonames.org](http://www.geonames.org/) has world-wide coverage, with more than 8 million place names. This service is particularly useful for things you don't have a street address for (e.g. [Eiffel Tower](http://www.geonames.org/search.html?q=eiffel+tower&country=)).\n\n\n\n# First Steps\n\n\nI'll start out by saying it's not necessary for you to do any coding whatsoever to do geocoding. However, this is a good programming exercise, allowing you to work with a number of technologies. If you have no interest in writing custom code for this, you can check out [Google's Fusion Tables](http://support.google.com/fusiontables/bin/answer.py?hl=en&answer=1012281) to do something similar to what I will explain here.\n\nThe examples in this exercise will be written in Ruby, but the concept can be ported to just about any language ([node](http://search.npmjs.org/#/wheredat), [Python](http://code.google.com/p/geopy/), [PHP](http://geocoder-php.org/), [Haskell](https://github.com/mrd/geocode-google), [jQuery](https://github.com/tristandunn/jquery-auto-geocoder), ...).\n\nIf you don't have Ruby (use a 1.9 version) installed on your computer, you will need that ([Windows](http://railsinstaller.org/), [OS X](https://rvm.io/), [Linux](https://rvm.io/)). I'll also be using the [Geocoder](http://www.rubygeocoder.com/) gem as well as the built-in Ruby libraries for working with spreadsheets (specifically [CSV](http://en.wikipedia.org/wiki/Comma-separated_values), comma-separated values).\n\n\n\n\n## Project Setup\n\n\n\nThe basic approach for this project will be to create a project directory, install the necessary libraries, create a spreadsheet of the addresses we want to look up, write a short program that will retrieve the latitude/longitude points (and other information) we need, then write those back this information to a new spreadsheet we can use later on.\n\nAssuming you have Ruby (and [rubygems](http://rubygems.org/)) properly installed, all you will need to get going is set up a new project space. A few simple commands will get this project going in your **terminal**:\n\n[code lang=\"bash\"]\nmkdir -p ~/projects/geocode\ngem install geocoder\ntouch ~/projects/geocode/locations.csv\ntouch ~/projects/geocode/geocoder.rb\ncd ~/projects/geocode/\n[/code]\n\nThis set of commands commands created a new directory (`~/projects/geocode`) for the project if it does not exist, installed the **geocoder** library, and created a couple of empty files to store our locations and our program logic. Lastly, we change the current directory (`cd ~/projects/geocode` for the terminal to be the project directory containing the files.\n\n\n\n## The Datasource\n\n\nIn order to explore this technique, we need some data. A really easy format for working with text is the CSV format, or comma separated values. For the purposes of this exercise, paste the following in to a file named `locations.csv`.\n\n[code]\nAddress,City,State\n\"1600 Pennsylvania Ave\",Washington,DC\n\"931 Thomas Jefferson Parkway\",Charlottesville,VA\n\"Eiffel Tower\"\n[/code]\n\nThe CSV format is pretty straight forward; you literally separate the values (columns) with commas. The one gotcha is that if you have something with a comma in it, you will need to escape the comma with a backslash (e.g. \"Charlottesville, VA\" as a single field).\n\n\n\n## Reading the File\n\n\nNow with a little bit of data, we can turn our attention to reading the values out of the CSV file with an actual program. This is done with the [Ruby CSV class](http://ruby-doc.org/stdlib-1.9.3/libdoc/csv/rdoc/CSV.html), which provides methods to read and manipulate the contents of CSV data. Our program for this exercise will go in  the `geocoder.rb` file. With your favorite text editor, open the `geocoder.rb` file and add the following:\n\n[code lang=\"ruby\"]\nrequire 'csv'\n\nLOCATIONS = './locations.csv'\n\nCSV.foreach(LOCATIONS, :headers => true, :header_converters => :symbol) do |line|\n  p line[:address]\n  p line[:city]\n  p line[:state]\nend\n[/code]\n\nThis code reads the CSV file, converts each header value in to a symbol (for easy reference) then steps over each line in the file and prints the `address`, `city`, and `state` fields on a new line in the terminal. You can run this program in the terminal by executing the Ruby script with `ruby geocoder.rb`. When you do, you should see output along these lines:\n\n[code]\n○ → ruby geocoder.rb\n\"1600 Pensylvania Ave\"\n\"Washington\"\n\"DC\"\n\"931 Thomas Jefferson Parkway\"\n\"Charlottesville\"\n\"VA\"\n[/code]\n\n\n\n## Adding Geocoding\n\n\n\nNow that we can read the data, we can change (or refactor) the program to use the geocoder gem and write logic to look up location information. At this point, we need to _include the Geocoder library_, concatenate the address fields together, then retrieve the location information.\n\n[code lang=\"ruby\"]\nrequire 'csv'\nrequire 'geocoder'\nLOCATIONS = './locations.csv'\n\nCSV.foreach(LOCATIONS, :headers => true, :header_converters => :symbol) do |line|\n  address_string = \"#{line[:address]}, #{line[:city]}, #{line[:state]}\"\n  result = Geocoder.search(address_string)\n  p result\nend\n\n[/code]\n\nNow when you run the program (`ruby geocoder.rb`), you'll see that there is a lot more information returned from the geocoding web service. To make this a bit more useful for our purposes here, we can use the `latitude` and `longitude` convenience methods to display the latitude and longitude coordinates.\n\n[code lang=\"ruby\"]\n\nrequire 'csv'\nrequire 'geocoder'\n\nLOCATIONS = './locations.csv'\n\nCSV.foreach(LOCATIONS, :headers => true, :header_converters => :symbol) do |line|\n  address_string = \"#{line[:address]}, #{line[:city]}, #{line[:state]}\"\n  result = Geocoder.search(address_string).first\n  lat = result.latitude\n  lon = result.longitude\n\n  puts \"#{lat}, #{lon}\"\nend\n\n[/code]\n\nNow when you run the program, you should see that the program prints the latitude and longitude for the address line.\n\n[code lang=\"bash\"]\n○ → ruby geocoder.rb\n38.8976777, -77.03651700000002\n38.0054041, -78.4563433\n48.858278, 2.294254\n[/code]\n\nThis is great, but what we really want to do is write this back to a CSV file in a format that we can use in Omeka. The Neatline plugins currently use a format called [WKT format](http://en.wikipedia.org/wiki/Well-known_text) to describe geographic information, so we need to get our results in this format using the WKT ** \"Point\"** data definition. If you can remember any of your middle school algebra, a point is a set of coordinates (typically X and Y). The Earth's X axis is longitude, and the Y axis latitude, so we just use this format and wrap the coordinates with \"**POINT()**:\n\n[code lang=\"ruby\"]\nrequire 'csv'\nrequire 'geocoder'\n\nLOCATIONS = './locations.csv'\n\nputs \"address,city,state,point,lat,lon\"\n\nCSV.foreach(LOCATIONS, :headers =>\n true, :header_converters =>\n :symbol) do |line|\n    address_string = \"#{line[:address]}, #{line[:city]}, #{line[:state]}\"\n    result = Geocoder.search(address_string).first\n    lat = result.latitude\n    lon = result.longitude\n\n    point = \"POINT(#{lon} #{lat})\"\n\n    puts \"#{address_string}, #{point}, #{lat}, #{lon}\"\n\nend\n[/code]\n\nNotice I **removed the comma in the POINT value**; this is important as this is specified in the WKT format. Now with a shell trick ([not this kind](http://blog.affiliatetip.com/wp-content/uploads/2011/02/iStock_000000395946XSmall.jpg)), we can run the program and generate a file that contains the new data. The trick is actually the [I/O redirection command](http://tldp.org/LDP/abs/html/io-redirection.html) (`<`) which can takes the output of one command and redirects it to a file. I generally prefer (when possible) to generate new files when dealing with massaging data. I've just deleted too many files accidentally in code.\n\n[code lang=\"bash\"]\nruby geocoder.rb > geocoded.csv\n[/code]\n\nThis command will run the file, but redirect the content that was being shown on the screen in to a file named '`geocoded.csv`.'\n\n\n\n## Coordinate Systems\n\n\n\nDid you know that there were a lot of different ways to actually define latitude and longitude? Before taking this job I had only really seen coordinates expressed in either in the degrees, minutes, seconds format (e.g.  38° 1′ 48″ N, 78° 28′ 44″ W) or it's decimal equivalent (e.g. 38.03, -78.478889). Turns out there are a lot different ways to actually describe these coordinates because our maps are flat, and our planet is not. If the world was actually a real sphere, this wouldn't be a difficult problem, but the Earth's shape actually what is referred to as an [oblate spheroid](http://en.wikipedia.org/wiki/Oblate_spheroid), which makes getting precise locations on from the curved Earth to a flat map problematic. How you deal with the conversion of points on the spheroid (Earth) to a map is a projection. This conversion can introduce distortion, like the maps I remember in school growing up where Greenland is larger than Africa.  Projections are chosen according to the purpose of the map to preserve qualities like shape, area, distance, or direction. The European Petroleum Survey Group (EPSG) maintains a database of all the myriad projections and datums. After a while, you start to know the more regularly used projections, and the results we got back from the Geocoder gem are in a projection for the [WGS 84](http://en.wikipedia.org/wiki/World_Geodetic_System) coded [EPSG:4326](http://spatialreference.org/ref/epsg/4326/).  (**Note:** for a nice piece on projections, check out [Projection Lessons in Maps](http://www.scholarslab.org/geospatial-and-temporal/projection-lessons-in-maps/).)\n\n\nBut what does this have to do with the coordinates? In Neatline we are using a different coordinate system to make some of the conversions in using Google base maps a bit easier. Basically we need to take the decimal degrees and covert them to meters. For example, the coordinates of the White House, this:\n\n[code lang=\"text\"]\nPOINT(-77.03651700000002 38.8976777)\n[/code]\n\nWhich uses a spherical interpretation of the globe becomes the following when converted to meters.\n\n[code lang=\"text\"]\nPOINT(-8575665.843733624 4707025.360473459)\n[/code]\n\nThese are the same points, just described differently. So how can we handle this in the code? There are services that you can go out and use to re-project your data, but this one is pretty straight forward with a little trigonometry (didn't think you'd read that today, did you). With a little mathematical hand-waving, I wrote the following method to convert degrees to meters:\n\n[code lang=\"ruby\"]\ndef degrees_to_meters(lon, lat)\n    half_circumference = 20037508.34\n    x = lon * half_circumference / 180\n    y = Math.log(Math.tan((90 + lat) * Math::PI / 360)) / (Math::PI / 180)\n\n    y = y * half_circumference / 180\n\n    return [x, y]\nend\n[/code]\n\nThe `20037508.34` the the above code is half the circumference of the earth in meters (I looked it up), and there are some math tricks to account for a generalized flattening of the earth. Now we can call this function and calculate the projected coordinates to use in a Neatline exhibit.\n\n[code lang=\"ruby\"]\nrequire 'csv'\nrequire 'geocoder'\n\nLOCATIONS = './locations.csv'\n\ndef degrees_to_meters(lon, lat)\n    half_circumference = 20037508.34\n    x = lon * half_circumference / 180\n    y = Math.log(Math.tan((90 + lat) * Math::PI / 360)) / (Math::PI / 180)\n\n    y = y * half_circumference / 180\n\n    return [x, y]\nend\n\nputs 'address,city,state,point,lat,lon'\nCSV.foreach(LOCATIONS, :headers => true, :header_converters => :symbol) do |line|\n    address_string = \"#{line[:address]}, #{line[:city]}, #{line[:state]}\"\n    result = Geocoder.search(address_string).first\n\n    lat = result.latitude\n    lon = result.longitude\n\n    #point = \"POINT(#{lon} #{lat})\"\n    projected = degrees_to_meters(lon, lat)\n    point = \"POINT(#{projected[0]} #{projected[1]})\"\n\n    puts \"#{address_string}, #{point}, #{lat}, #{lon}\"\nend\n[/code]\n\nWhen you run the program as described above, you will get results similar to this:\n\n[code]\naddress,city,state,point,lat,lon\n1600 Pennsylvania Ave, Washington, DC, POINT(-8575665.843733624 4707025.360473459), 4707025.360473459, -8575665.843733624\n931 Thomas Jefferson Parkway, Charlottesville, VA, POINT(-8733720.184442518 4580189.258447956), 4580189.258447956, -8733720.184442518\nEiffel Tower, , , POINT(255395.18699487977 6250848.2584100235), 6250848.2584100235, 255395.18699487977\n[/code]\n\nNow you can regenerate your CSV file of the properly formatted location information from the terminal:\n\n\n[code lang=\"bash\"]\nruby geocoder.rb > geocoded.csv\n[/code]\n\nIf you've already created a `geocoded.csv` file, the above command will overwrite the file. If you're wanting to append similar content to the same file, you can use the `>>` operator which will add the output to the end of the file.\n\n\n\n# Summary\n\n\n\nIn this post I covered the basics of using geocoding services through a programming API, as well as reading CSV files, and redirecting output to a new file. I waved my hands with a little magic (some math, some programming, some Unix commands), but there are a lot of techniques here you can use in a variety of scenarious. In my next post, I will cover how to automate populating this information in an Omeka instance, automating the population of this information in new items, then them in a Neatline exhibit.\n"},{"id":"2012-09-11-introductions","title":"Introductions","author":"claire-maiers","date":"2012-09-11 07:04:00 -0400","categories":["Grad Student Research"],"url":"introductions","content":"Hello All,\n\nIn this introductory post I am going to tell you a little bit about myself, my research interests, and the sources of my enthusiasm for [Praxis](http://praxis.scholarslab.org/) and [Prism](http://prism.scholarslab.org/).\n\nSo, let’s start with the basics:  I am currently a third year graduate student at UVa’s Department of Sociology.  Though, as a sociologist, I am trained to think scientifically about the social landscape, I also have a strong connection to the humanities.  I have a B.A. in music (with a focus on vocal performance) and a M.A. in musicology (focus on research and scholarship).  Given my background, it might not be surprising that I specialize in cultural sociology.  This means a lot more than an interest in applying social theory to culture or the arts.  It means that I am interested in the way that culture shapes our thinking, constrains or enables action and agency, and structures our experience of the world.  More specifically, I am interested in the way that the available scripts (you might also say logics or frameworks) within a culture structure our way of making meaning and making truth claims.\n\nMy interest in the Digital Humanities really began when I attended the interest meeting about Praxis last spring.  Hearing the 2011-2012 team talk about Prism quickly got me excited about the research possibilities of the Digital Humanities.  Where most data sets available to sociologists provide demographic information and perhaps basic information about opinions or attitudes, a tool like Prism would allow us to get a better handle on how people think through and make meaning from texts--something that is very difficult to get at through traditional survey methods.  Although Prism clearly has other potential (I am also enthusiastic about the way it could be integrated into the classroom) , I am hoping that the coming year will prepare me to incorporate some of the innovations of the Digital Humanities into my own research.\n"},{"id":"2012-09-11-the-impossible-proposal","title":"The Impossible Proposal","author":"chris-peck","date":"2012-09-11 07:05:34 -0400","categories":["Grad Student Research"],"url":"the-impossible-proposal","content":"For my first post as a [Praxis](http://praxis.scholarslab.org/) Fellow I'd like to share an exercise that a [mentor](http://stephenjrushmusic.com/) of mine often used to kick-off [courses in interdisciplinary collaboration](http://www.music.umich.edu/current_students/perf_opps/dme/index.html): The Impossible Proposal. At the first meeting of a class of engineers, artists, musicians, dancers, writers, etc. tasked with creating a tangible project by the end of the semester, our professor would ask us to come up with a proposal for an impossible project—not just __difficult but totally and utterly _impossible_. Few of us succeeded on the first try.\n\nThe point here is not simply [inspirational](http://www.youtube.com/watch?v=RfHnzYEHAow). The practice of generating impossible ideas is like weight training for your brainstorming muscles. And on the path to the impossible are many possibilities that could otherwise be overlooked.\n\nIn those classes we managed to do a number of things that would have seemed impossible (or at least unwise). We floated a large light-responsive sound installation on the pond next to the music building. (My proudest distinction as a freshman music student was to achieve the rank of 'first-chair' radio-controlled boat operator.) I'm pretty sure that project started off as a failed response to the Impossible Proposal assignment—a 'non-impossible' idea.\n\nThe Impossible Proposal is now a reflex for me in the early stages of a project. I can't help it, even when I start to notice that more pragmatically-oriented collaborators are losing patience. At the first meeting of the Praxis Fellows last week I proposed that Prism should be able to record the eye movements of readers. But—just like the floating installation—it turns out this proposal is far from impossible. After the meeting Wayne informed me that UVa employs such technology in usability testing for websites, and he even seemed prepared to dive into the technical details of how this could be incorporated into our project...\n\nSo at today's meeting I'll do better. Maybe something involving [time travel](http://www.youtube.com/watch?v=G7kmHa9kYtg)?\n"},{"id":"2012-09-12-geocoding-for-neatline-part-ii","title":"Geocoding for Neatline - Part II","author":"wayne-graham","date":"2012-09-11 21:00:07 -0400","categories":["Digital Humanities","Research and Development"],"url":"geocoding-for-neatline-part-ii","content":"In my last post ([Geocoding for Neatline - Part I]( http://www.scholarslab.org/digital-humanities/geocoding-for-neatline-part-i)), I covered how to programmatically geocode a set of addresses and generate a CSV file for use in Neatline. In this post, I'll go over how to actually post this information in Omeka and make it available for use in your Neatline exhibit.\n\n\n\n# Requirements\n\n\n\nAs in the previous post, I'll be making use of Ruby here, but I'll be making use of a different gem ([Mechanize](https://rubygems.org/gems/mechanize)) to handle interacting with an Omeka server. This is what you'll need to get going:\n\n\n\n\n\n  * A running [Omeka](http://omeka.org/) instance with the [Neatline](http://omeka.org/add-ons/plugins/neatline/), and [NeatlineFeatures](http://omeka.org/add-ons/plugins/neatlinefeatures/) plugins installed.\n\n\n  * [Ruby](http://www.ruby-lang.org/en/)\n\n\n  * A text editor (e.g. [vim](http://www.vim.org/), [Sublime Text 2](http://www.sublimetext.com/2), [notepad++](http://notepad-plus-plus.org/))\n\n\n  * [CSV data](https://gist.github.com/3307210#file_geocoded.csv) from the last exercise\n\n\n  * The terminal\n\n\n\n\n\n# The Technique\n\n\n\nWith a [prepared geocoded CSV file](https://gist.github.com/3307210#file_geocoded.csv), we can start dealing with how to actually get this data in to Omeka. If you've done an Omeka project in the past, you may be familiar with the [CSVImport plugin](http://omeka.org/add-ons/plugins/csv-import/), and this may be a first impulse to use. Unfortunately, because of some technical reasons I won't get in to here, this won't work. However, as a developer, this simply becomes a constraint for a different system. This is where the [Mechanize](http://rubygems.org/gems/mechanize) gem comes to the rescue, allowing us to automate filling out the Omeka forms for our items. The first step here is to install the library with the `gem` command in the terminal:\n\n[code lang=\"bash\"]\ngem install mechanize\n[/code]\n\nThe basic idea in using [Mechanize](http://rubygems.org/gems/mechanize), which allows us to write a set of automated steps, is to take the file [we just generated in the previous post](https://gist.github.com/3307210#file_geocoded.csv), read all the information, then fill out the Omeka form and save the newly created item. In a new script (e.g. `populate.rb`), we require the libraries we'll be using:\n\n[code lang=\"ruby\"]\nrequire 'rubygems'\nrequire 'mechanize'\nrequire 'csv'\n\n# code to process CSV points\n[/code]\n\nIn Mechanize you can define a user agent (a web browser), and it's a good practice to define the user agent as a browser that you don't use on a daily basis to avoid any caching or username/password issues. For me, I set this to \"Mac Safari\" (you can use this on Windows too), but you can choose from any of the [user agent aliases](https://github.com/tenderlove/mechanize/blob/master/lib/mechanize.rb#L90) Mechanize provides.\n\n[code lang=\"ruby\"]\nagent = Mechanize.new {|a|\n  a.user_agent_alias = 'Mac Safari'\n}\n\n# code to fill out Omeka forms\n[/code]\n\nNow we just need to mechanize how to log on to Omeka. I'm doing everything locally, so you will need to fix the path to the Omeka admin area as needed:\n\n[code lang=\"ruby\"]\nagent.get('http://localhost/omeka/admin/') do |page|\n  omeka_page = page.form_with(:action => '/omeka/admin/users/login']) do |form|\n    form.username = 'your user name'\n    form.password = 'your password'\n  end.submit\n\n  # read CSV file\nend\n[/code]\n\nIf you ran this code right now, it wouldn't actually do anything visually, but this bit of code finds the form on the admin page that contains the login information, then sets the username and password on the form, and submits it, effectively authorizing you to do other things with Omeka in the context of the program.\n\nNext, we want to read the CSV file (`geocoded.csv`) that we [generated in the previous post](https://gist.github.com/3307210#file_geocoded.csv) to read the data. This is done in the same way before:\n\n[code lang=\"ruby\"]\nCSV.foreach('./geocoded.csv', :headers => true, :header_converters => :symbol) do |line|\n    # add logic to fill out form\nend\n[/code]\n\nThis should look familiar. The code just reads the CSV file, converts the headers to symbols, and steps through each line. For each line (row) in the CSV file, we want to tell Mechanize to click on the 'Items' link (the Items tab) then the 'Add Items Link' to get to the form to fill out, which will look like the following:\n\n[code lang=\"ruby\"]\n# click on items\nitem_page = agent.click(omeka_page.link_with(:text => %r/Items/))\n# click on add items\nadd_item_page = agent.click(item_page.link_with(:text => %r/Add an Item/))\n\n# Add the item to the form\n[/code]\n\nThis code tells Mechanize to click on the link with the text of Items with a [regular expression](https://en.wikipedia.org/wiki/Regular_expression). In this case, the regular expression isn't necessary, but is useful when you need to do a partial match on a link (or some other component) that is on a page, and something I do by default.\n\nNow the program is at the new item form, and it's time to set data on the form from the CSV file. This is slightly trickier because of the way in which Omeka names its form fields. There are two areas we want to populate, the **Title** field and the elements to actually create the point for NeatlineFeatures. If you look at the source code of the **Item Add** page, you'll notice that the Title field actually has a name of '`Elements[50][0][text][/text]`,' which looks scary, but the program knows what to do with it. I also know that the **Coverage** field in Omeka is '`Elements[38][0]`' with a field name after it (`[text][/text]`, `[wkt]`, `[zoom]`, `[mapon]`, ...). We can populate this information with data from our spreadsheet now:\n\n[gist id=3307210 file=add_item_snippet.rb]\n\nThis bit actually fills out the form, turning the map component on, setting a zoom level of `10` for the map, then setting a center point to focus the map. If you've been following along, your script should now look like this:\n\n[gist id=3307210 file=populate.rb]\n\nAfter running this script, when you log on to Omeka, you should see newly created items with their locations populated and a pretty map that you can use in Neatline. **Note:** these items were not set to be 'public', but you can easily add this to the script, and can be your homework.\n\nAssuming you've read the [documentation on creating Neatline exhibits](http://neatline.org/plugins/neatline/), you can now simply add these items and have them placed spatially by clicking on the **Map** icon in the Neatline editor.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/09/neatline_features-300x199.png)](http://www.scholarslab.org/wp-content/uploads/2012/09/neatline_features.png)\n\n\n\n## Summary\n\n\nThis set of posts shows you how to automate geocoding of your data and integrating it with a Neatline exhibit. These automated methods can save you a lot of time doing data entry, but you should keep in mind that a really great Neatline exhibit really requires you getting in to the exhibit and using your imagination to tell an interactive story. How your users interact with your exhibit depends not only on the quality of your data, but the time you spend working on the interactions.\n"},{"id":"2012-09-12-grad-fellows-forum-introducing-the-2012-2013-fellows","title":"Grad Fellows Forum: Introducing the 2012-2013 Fellows","author":"ronda-grizzle","date":"2012-09-12 12:36:28 -0400","categories":["Podcasts"],"url":"grad-fellows-forum-introducing-the-2012-2013-fellows","content":"**Graduate Fellows Forum**\n**Introducing the 2012 - 2013 Graduate Fellows & Praxis Fellows**\n\nOn September 5, 2012, the Scholars' Lab kicked off the Fall semester with a Graduate Fellows Forum welcoming our new Graduate Fellows and Praxis Fellows and getting an update on the work developing Prism done by our 2011-2012 Praxis cohort.\n\nPraxis Fellows:\n\n\n\n\t\n  * **Cecilia Márquez**, History\n\n\t\n  * **Chris Peck**, Music\n\n\t\n  * **Claire Maiers**, Sociology\n\n\t\n  * **Gwen Nally**, Philosophy\n\n\t\n  * **Shane Lin**, History\n\n\t\n  * **Brandon Walsh**, English\n\n\nGraduate Fellows:\n\n\t\n  * **David Flaherty**, History\n\n\t\n  * **Lydia Rodriguez**, Anthropology\n\n\t\n  * **Annie Swafford**, English\n\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.18488681804/enclosure.mp3\"]\n"},{"id":"2012-09-13-binsh","title":"#!/bin/sh","author":"shane-lin","date":"2012-09-13 13:40:45 -0400","categories":["Grad Student Research"],"url":"binsh","content":"(My apologies; Brandon beat me to the Hello World joke.)\n\nMy name is Shane Lin. I am one of the new [Praxis](http://praxis.scholarslab.org/) cohort.\n\nThis is my second year as a PhD student in the history department. My background is a bit different from that of most of my peers: I studied computer science and history at the University of Texas and subsequently worked for four years as a software engineer for a local Seattle bookstore. A foolhardy sense of romanticism caused me to abandon my career and turn instead to the Academy.  Now, I'm focused on the history of information, with a particular interest in cryptography and the Internet.\n\nI became interested in the Praxis Program because, although I have a fairly strong technical background, I didn't actually know much about digital humanities. I feel like the type of research that I'm drawn to suffers the opposite problem to many other historical fields: instead of suffering a dearth of documents, I'm inundated by a titanic flood. It's intimidating, to say the least, to try to use something like a Usenet newsgroup or a mailing list archive with tens of thousands of participants as a source. Getting a better grasp on digital humanities techniques and approaches will no doubt be an invaluable experience.\n\nI'm looking forward to dusting off the skills of my former (and now hopefully future) life and especially to foisting my own technical prejudices and ideological biases on my fellows.\n\n\"No, no - over here where you typed in 'Perl' - you really meant 'Python'\"\n"},{"id":"2012-09-13-hello-world","title":"Hello World","author":"brandon-walsh","date":"2012-09-13 06:17:28 -0400","categories":["Grad Student Research"],"url":"hello-world","content":"For some time now I have led a double life as a musician and book lover. As a third year PhD in the English department at UVA, I work primarily on twentieth-century fiction in relation to music and sound. These interests drew me to [Praxis](http://praxis.scholarslab.org/) in the first place: writing about sound is incredibly difficult in a print medium where the reader can’t hear what I describe. I am very excited to be a part of the Praxis team this year, where as a DH novitiate I hope to learn how technology can help make those two fields work together more easily. The Scholars’ Lab team seems happy to have us as well, welcoming the new team with open arms and inscrutable computer science jokes. Last year’s Praxis team reached out to us many months ago from [their charter](http://praxis.scholarslab.org/charter.html):\n\n\n<blockquote>Preparations for future cohorts\n\nIn order to allow the next Praxis Program team to start work right away on a project, we will make suggestions for that project before our tenure is over.</blockquote>\n\n\nHow nice to be thought of! But I think this excerpt shows how our situation is fundamentally different from theirs: last year’s team had to deal only with the future, but our team also has to deal with the past. It would be easy for us to feel anxiety as latecomers to the Praxis party, so I think it’s important that our own charter reflect the ways in which our work will talk back to last year’s team.\n\nAs I look through [last year’s blog posts](http://www.scholarslab.org/category/praxis-program/), I’m struck by the problem of knowledge we have in store for us. Implicit in the suggestions offered to us are all the ideas that last year’s team discarded, thought better of, and revised. Those are what I really want to see! The archive of blog posts can only give a skeletal sense of the past: there is no replacement for sitting in that chair all last year.\n\nSo beyond the question of how we deal with their great suggestions, I am struck by a more basic dilemma: what sort of dialogue will we have with last year’s group? I imagine that the cheerful team in the Scholars Lab will welcome repeat conversations as opportunities to rethink and retool, but I also believe that we can benefit from the experience of those who came before. I hope that we can continue the conversation with both those members of last year’s team that are still on grounds and those that have moved on to wonderful jobs across the country.\n\nEven so, it is also important for us to recognize that, try as we might, we can never know everything about the project’s history to date. Our team will work better in the long run if we welcome the unknown and greet it just as enthusiastically as it welcomes us.\n"},{"id":"2012-09-13-my-first-praxis-post-for-lack-of-a-better-title","title":"My First Praxis Post (for lack of a better title)","author":"cecilia-márquez","date":"2012-09-13 07:39:31 -0400","categories":["Grad Student Research"],"url":"my-first-praxis-post-for-lack-of-a-better-title","content":"Internet introductions are by far the weirdest introductions because you have no idea who is reading what you’re writing.  For all I know people I went to high-school with will find me here blogging about the Digital Humanities, or a future employer, more likely it will be my mother and grandmother.  So here goes internet...don’t let me down...and hi Mom!\n\nI’m Cecilia Márquez.  I’m have a BA from Swarthmore College in Black Studies and Gender and Sexuality Studies.  Now I’m a second year PhD student in the History Department here at UVA.  My work is mostly focused on African American History, Labor History and Latino History.\n\nMy interest in [Praxis](http://praxis.scholarslab.org/) came as a shock to most people who know me.  I have a serious aversion to most technology and an even bigger aversion to being confused.  So a program that focused on embracing and challenging my ignorance of technology felt a little like a leap into the belly of the beast.  I am happy to report that after three meetings I have not felt the desire to run for the door or throw my computer at the wall.\n\nAt our first Praxis Team meeting--aside from me laughing nervously at all of the programming jokes I didn’t understand--we talked about our charter.  The conversation made me think more about how my values influenced my interest in the Digital Humanities.\n\nI believe in the importance of working collectively.  Although I often stumble doing this effectively I believe that working in community will always be better than the solitary experience of the library basement.  On a related point I have to laugh all the time, especially when I’m working, and especially when what I’m working on is incredibly hard.  I always want to bring a sense of humor to the work I do and so far the Praxis team has not come up short on good humor.  I’m committed to the democratization of knowledge--both historical knowledge and technological knowledge.  This is obviously central to the work of the Digital Humanities.\n\nThe next time I blog I will have gotten through HTML/CSS training... hopefully me and my computer emerge unscathed.\n"},{"id":"2012-09-13-praxis-2-0","title":"Praxis 2.0","author":"gwen-nally","date":"2012-09-13 06:14:20 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"praxis-2-0","content":"The 2012-2013 [Praxis](http://praxis.scholarslab.org/) team has assembled! We’ve moved into our lounge (who knew Alderman Library could be so comfortable?) and started to think about chartering this year’s team project. More on the charter and the project in the weeks to come...\n\nAlthough we’ve just met, if I had to describe my fellows in a single word, I’d probably say that we are a “diverse” team. We hail from across the humanities--from sociology to history, music, english, and philosophy. (I am a sixth year graduate student in the philosophy department, interested primarily in topics in ancient philosophy, applied ethics, and aesthetics. My dissertation, _The Ascent to Beauty: the Epistemology of Goodness in the Middle Dialogues_, proposes a unified theory of knowledge in the ascent passages from this period in Plato’s work. My research tackles questions like “Why is goodness the ‘highest’ form?” and “Why does Plato identify goodness with beauty?”)\n\nThe fact that we Praxis fellows have such different backgrounds seems to be what really distinguishes us. Digital humanities is known for its interdisciplinarity; even so, most DH projects seem to be undertaken with specific individuals filling certain predetermined roles. A visualization project might, for example, consist of one or more researchers, usually from a single or neighboring disciplines, a project manager, digital technologists, and designers. Our team seems really unique in that, in addition to a team of skilled digital researchers, technologists, and designers, we hail from five different humanities disciplines. What’s more, unlike other DH teams we were not chosen to address any particular research question. Over the next several weeks, we’ll have the rare opportunity to design and undertake a project of our own devising. We might pick up where last year’s team left off or, who knows, we might enter into completely uncharted territory? Whatever we decide, this year’s project will, I hope, showcase the successes, failures, and inner workings of a truly interdisciplinary team.\n"},{"id":"2012-09-17-moonlighting-with-the-praxis-crew","title":"Moonlighting with the Praxis Crew","author":"katina-rogers","date":"2012-09-17 06:28:07 -0400","categories":["Announcements","Grad Student Research"],"url":"moonlighting-with-the-praxis-crew","content":"One great perk of my role with the Scholarly Communication Institute is that I have the opportunity not only to learn about how the Praxis Program functions, but also to learn alongside the team members, particularly in this second year. The semester has barely begun, and already the collective wheels are turning as the new group begins to develop its own charter, to brainstorm new directions for [Prism](http://prism.scholarslab.org/), and to learn the fundamental skills they'll need as they extend, modify, and rethink it.\n\nI'm excited to take part from the start of the academic year. When I joined SCI in April 2012, the first Praxis cohort was nearing the completion of their year-long project. I watched from my Brooklyn perch as they untangled the last knots in functionality, design, deadlines, and communications as they prepared to launch Prism. They had already learned and assimilated the skills that the project required, and I was impressed by how fluidly the group worked together to navigate the complex hurdles of polishing and releasing their work.\n\nAs the second cohort begins its year together, I have the opportunity to observe from day one. Sitting in on the group's weekly meetings (as a disembodied Skype presence), my goals are double: First, consistent with SCI's intention to work with a handful of unique but allied programs in the [development of a Praxis Network](http://uvasci.org/current-work/graduate-education/), I'll be watching to see how the program functions, which elements of it are particular to the Scholars' Lab ecosystem, and which might be useful to other programs wishing to develop similar initiatives.\n\nSecond, and more selfishly, I'll be joining as an eager student. In the past few months I've worked at learning some of the skills required to create the kinds of data visualizations I want to use to report on SCI's study of alternative academic careers, and also to gain more control over my website. Giddy over minor successes with Vim, GitHub, HTML/CSS, and D3.js, I've tasted the Kool-Aid, and am excited to learn more. Since I'm not physically in the Scholars' Lab space, I'm not only looking forward to the technical instruction of the SLab gurus, but also (even more) to the collective nature of the process as we all go a bit out of our comfort zones, embracing the challenges and failures that are a part of learning new skills. \n"},{"id":"2012-09-18-code-spelunking-with-ctags-and-vim","title":"Code Spelunking with Ctags and Vim","author":"eric-rochester","date":"2012-09-18 06:05:54 -0400","categories":["Research and Development"],"url":"code-spelunking-with-ctags-and-vim","content":"When I’m programming, I spend a lot of time code spelunking (_use the source, Luke!_). A lot of times, the best documentation for a system is the source code for it. Knowing how to get to important places in your code and in others’ code makes a huge difference in how productive you are and even in what you can figure out how to do.\n\n\n\n\nAnd the most important location for any method, function, class, variable, or whatsit is where it’s defined. Usually there’s documentation near. Sometimes there is information about parameters. It’s all very useful.\n\n\n\n\nFor finding those places, [`CTags`](http://ctags.sourceforge.net/) is indispensable.\n\n\n\n\n# What is `ctags`?\n\n\n\n\n[`CTags`](http://ctags.sourceforge.net/) is program that finds the lines of code where things are defined. It knows [41 languages](http://ctags.sourceforge.net/languages.html). (And if your favorite language isn’t on the list, you can probably find another program that generates compatible tags files for it.) You run it occasionally, and it indexes your code and stores it in a file named `tags`. Your editor reads this file and helps you jump through your code.\n\n\n\n\n# Installing `ctags`\n\n\n\n\nThe first step in using `ctags` is to install it.\n\n\n\n\n## Windows\n\n\n\n\nThe [`ctags` page](http://ctags.sourceforge.net/) has a link to a ZIP file with a binary of `ctags` compiled for Windows. Download this and unzip it somewhere on your `%PATH%`.\n\n\n\n\n## Linux\n\n\n\n\nIf you’re using Linux, all major distributions have a package for `ctags`. See your documentation for details.\n\n\n\n\n## Mac\n\n\n\n\nI’ve saved the Mac installation for last, because it’s the most complicated. You weren’t expecting that, were you?\n\n\n\n\nMac OS X comes with a program named `ctags`.\n\n\n\n\nUnfortunately, it is not, in fact, Exuberant CTags, and it’s far more limited.\n\n\n\n\nYou can get the correct version of `ctags` using [Homebrew](http://mxcl.github.com/homebrew/):\n\n\n\n[sourcecode language=\"bash\"]\n> brew install ctags\n[/sourcecode]\n\n\n\nNow you have to make sure that your shell finds the right version:\n\n\n\n[sourcecode language=\"bash\"]\n> which ctags\n/usr/bin/\n[/sourcecode]\n\n\n\nWell, that’s not right. Let’s rearrange our `$PATH`.\n\n\n\n[sourcecode language=\"bash\"]\n> PATH=/usr/local/bin:$PATH\n> which ctags\n/usr/local/bin\n[/sourcecode]\n\n\n\nThat’s better. You probably want to put that into your `~/.bash_profile` file to make sure you find the right `ctags` in the future also.\n\n\n\n\n# Using `ctags` in a Text Editor\n\n\n\n\nNow that `ctags` is installed, let’s put it to work. First, open up a command line or terminal or whatever you call it and change into the main directory of your project. Right now, I’m working on [NeatlineFeatures](https://github.com/scholarslab/NeatlineFeatures), so I’ll use that for this example:\n\n\n\n[sourcecode language=\"bash\"]\n> cd ~/omeka/plugins/NeatlineFeatures/\n[/sourcecode]\n\n\n\nNow, let’s run ctags over the code base. We want to to walk through the entire directory tree. But here’s the catch: we really want it to walk over the entire [Omeka](http://omeka.org/) directory, including the Features plugin:\n\n\n\n[sourcecode language=\"bash\"]\n> ctags -R ../..\n[/sourcecode]\n\n\n\nThis runs `ctags` over everything in the Omeka directory (`../..`) and all subdirectories (`-R`).\n\n\n\n\nThis will take a while, and you’ll get some warnings about JavaScript files. Don’t worry about them. CTags has a few problems parsing JavaScript, but that won’t stop it from indexing the other files.\n\n\n\n\nLet’s see what we have:\n\n\n\n[sourcecode language=\"bash\"]\n> ls -lh tags\n-rw-r--r--  1 err8n  staff    23M Sep 14 16:00 tags\n> wc -l tags\n   75917 tags\n[/sourcecode]\n\n\n\nWow! More than 76,000 lines and 23MB. That’s a lot of indexing. But then, Omeka’s a large codebase.\n\n\n\n\nWhat’s in the file? Let’s not worry about that right now. It’s plain text, and there is some metadata and lines detailing identifiers and files and line numbers. It’s actually a little scary, and we don’t have to worry about that anyway.\n\n\n\n\n(If you’re really curious about the file format, [look at the format page](http://ctags.sourceforge.net/FORMAT).)\n\n\n\n\nThe tags file can be used by [a bunch of different text editors](http://ctags.sourceforge.net/tools.html). In fact, there’s more than is on that list. Here are links to integrating tags into some popular editors:\n\n\n\n\n\n\n  * [OpenCTags](http://openctags.sourceforge.net/): An add-on for using tags with Crimson Editor, EditPlus, UltraEdit, and Notepad++.\n\n\n  * [Emacs](http://www.gnu.org/software/emacs/emacs.html): Comes with `etags`, so it supports tags out of the box.\n\n\n  * [CTags bundle](https://github.com/textmate/ctags.tmbundle): A bundle for using tags in [TextMate](http://macromates.com/).\n\n\n  * [Sublime Text 2 and CTags](https://github.com/SublimeText/CTags): An add-on for using tags with [Sublime Text 2](http://www.sublimetext.com/2).\n\n\n\n\nI use Vim, and it comes with support for tags files built in. The rest of this post shows about how to use tags from Vim.\n\n\n\n\n# Using with Vim\n\n\n\n\nFirst let’s start Vim from the directory containing the tags file. If you want to use gVim or MacVim, this may be different (hint: use the `:cd` command). I’ll just use the command line to start MacVim on one of the models.\n\n\n\n[sourcecode language=\"bash\"]\n> mvim models/NeatlineFeatureTable.php\n[/sourcecode]\n\n\n\n## The Tag Stack\n\n\n\n\nThe main point of all this, of course, are the powerful navigation commands. Let’s see what they are.\n\n\n\n\nVim maintains a stack of locations where we’ve been. This stack starts out empty.\n\n\n\n\n## Navigating Forward\n\n\n\n\nWhen you jump to a tag, your current location is added to the stack. As you jump ahead, more locations are added to the stack. Here’s how to jump forward and add locations to the stack.\n\n\n\n\nFirst say I want to look at the code for `Omeka_Db_Table`. I just move down to where it’s mentioned in the code and hit `Control-]`.\n\n\n\n\nAnd I’m there. I can open up the class and look in it.\n\n\n\n\nBut say this doesn’t tell me what I want. I really want to look at `Zend_Db_Table`. I don’t see it, so I can’t use `Control-]`. Instead, from command mode, I type out `:tag Zend_Db_Table`.\n\n\n\n\nAnd I’m there.\n\n\n\n\nBut it’s empty. I need to jump to `Zend_Db_Table_Abstract`. I just move my cursor down there and hit `Control-]`.\n\n\n\n\nSay I want to know how `fetchAll` is defined. I move down there and open it up. But how else is it defined? I put my cursor on `fetchAll` and hit `Control-]`. At the bottom of the Vim window, it says tag 1 of 11 or more.\n\n\n\n\nInteresting. How do I get to them?\n\n\n\n\nIn normal mode, I just use the command `:tselect`. Now Vim displays a list of everywhere that `fetchAll` is defined. I can select the number for which one I want, and Vim moves me there.\n\n\n\n\n## Examining the Tag Stack\n\n\n\n\nNow I’ve jumped several times, and I’m a little confused about where I am. How do I find myself again?\n\n\n\n\nIn normal mode, use the `:tags` command. Vim will print out a list of which tags you’ve jumped to and where it is.\n\n\n\n\n## Navigating Backward\n\n\n\n\nAt this point, I want to move back to where I was. To do that, I just hit `Control-t` multiple times. Each time I do, it pops one position off the stack and moves be back to the previous tag location.\n\n\n\n\n## Jumping into a new window\n\n\n\n\nOf course, it would be nice to be able to see what I’m working on _and_ the tag too. Yes, I can be demanding. Fortunately, Vim can oblige me.\n\n\n\n\nUnfortunately, it’s not as convenient as `Control-]`, but it’s not bad. (And creating a normal-mode map for this isn’t difficult.)\n\n\n\n\nIn normal mode, just give the command `:stjump [identifier]`. This splits the window, and in the new split, it jumps to the tag. If there’s more than one definition for the tag, it prompts you for which you want, just like `:tselect` does.\n\n\n\n\n## Tag Navigation Cheat Sheet\n\n\n\n\nSo here’s what we’ve learned today:\n\n\n<table >\n<tbody >\n<tr class=\"odd\" >\n\n<td style=\"text-align: left;\" >`:tag [identifier]`\n</td>\n\n<td style=\"text-align: left;\" >Jump to the identifier.\n</td>\n</tr>\n<tr class=\"even\" >\n\n<td style=\"text-align: left;\" >`:tags`\n</td>\n\n<td style=\"text-align: left;\" >List the tag stack.\n</td>\n</tr>\n<tr class=\"odd\" >\n\n<td style=\"text-align: left;\" >`Control-]`\n</td>\n\n<td style=\"text-align: left;\" >Jump to the tag under the cursor.\n</td>\n</tr>\n<tr class=\"even\" >\n\n<td style=\"text-align: left;\" >`:tselect`\n</td>\n\n<td style=\"text-align: left;\" >Select which tag location to go to for the current tag.\n</td>\n</tr>\n<tr class=\"odd\" >\n\n<td style=\"text-align: left;\" >`Control-t`\n</td>\n\n<td style=\"text-align: left;\" >Jump back from the current tag.\n</td>\n</tr>\n<tr class=\"even\" >\n\n<td style=\"text-align: left;\" >`:stjump [identifier]`\n</td>\n\n<td style=\"text-align: left;\" >Jump to the identifier in a new split window.\n</td>\n</tr>\n</tbody>\n</table>\n\n\n## What does it look like?\n\n\n\n\n\n[CTags in Vim](http://vimeo.com/49681267) from [Scholars’ Lab](http://vimeo.com/scholarslab) on [Vimeo](http://vimeo.com).\n\n\n\n\n\n\n<blockquote>\n\n> \n> _With bonus content!_\n> \n> \n</blockquote>\n\n\n\n\n# Next Steps\n\n\n\n\nI’ve just presented the basics. Here’s some more about using tags in Vim.\n\n\n\n\n## Learning More\n\n\n\n\nThe Vim documentation for [tags](http://vimdoc.sourceforge.net/htmldoc/tagsrch.html) lists all of the many commands Vim has for working with tags.\n\n\n\n\n## Tagbar\n\n\n\n\n[Tagbar](http://majutsushi.github.com/tagbar/) is a Vim plugin that shows the structure of your code for the file you’re in. It opens a side panel and displays the classes, methods, and other identifiers defined in the current file.\n\n\n\n\n## Running Automatically\n\n\n\n\nFinally, [Tim Pope](http://tpo.pe/) has an excellent blog post on how to integrate Git, CTags, and Vim. He also explains how to set up your git repositories to run ctags automatically whenever you commit. It also makes it easy to customize how you run ctags for each project.\n\n\n\n\nI _highly_ recommend this system. It makes `ctags` even more awesome than it already is.\n\n\n\n\n* * *\n\n\n\n\nSo let us know, what’s your favorite development productivity or code navigation tool?\n\n\n\n"},{"id":"2012-09-20-on-not-knowing","title":"On Not Knowing","author":"cecilia-márquez","date":"2012-09-20 11:26:34 -0400","categories":["Grad Student Research"],"url":"on-not-knowing","content":"I made it [through HTML/CSS](http://praxis.scholarslab.org/topics/html-and-css/) and miraculously I still have a computer and most my sanity. These weeks of learning HTML/CSS have happened to coincide with my first weeks of being a Teaching Assistant. Having these experiences together has been invaluable for a few reasons.\n\n1. It has forced me to be a student again. While I am still in graduate courses as a student, it has been many years since I was in a course where my knowledge of the subject was so limited and my teachers knowledge so extensive. I’ve already gained a lot of insight into how to calmly and politely point out to a student that what they are doing is completely off base. This is a gift the Praxis team has given to me.\n\n2. If has reacquainted me with the feeling of “not knowing.” In graduate school it is rare that I find myself completely out of my depth. I take courses in my field with professors who study the same things I study. In Praxis I have had to embrace confusion, work through frustration, and learn to ask (what feel like) stupid questions, over and over. This is a gift the Praxis team has given to my students. Reconnecting with feelings of “not knowing” have helped me act more compassionately with my students and embrace my role as an educator to help them work through their frustration and confusion.\n\nTo watch all these fun emotions manifest you can follow the development of my very first website here: [http://cmarque1.github.com/](http://cmarque1.github.com/)\n\nRight now it is mostly a mess but in time I’m hoping for masterpiece status.\n"},{"id":"2012-09-21-failure","title":"Failure","author":"brandon-walsh","date":"2012-09-21 12:07:47 -0400","categories":["Grad Student Research"],"url":"failure","content":"In middle school I built a website about the seven wonders of the ancient world. Nothing fancy – just images and some links – and I never published it. Building a personal website over the past few days as per Jeremy’s request feels a bit like coming full circle. My HTML skills remain prepubescent at best, and my barebones site keeps the spirit of GeoCities in the early 1990’s alive and well. Check it out. [http://bmw9t.github.com](http://bmw9t.github.com).\n\nI admit to some hesitation in posting this fledgling site for all to see: so much more could be done to bring it into the twenty-first century. But I keep coming back to Bethany’s encouragement last week to fail in public, advice with which I am in love. My graduate training thus far has emphasized polish and perfection, for clear and obvious reasons. But even at this early stage, my work with Praxis feels more electric knowing that mistakes are welcome and that failure is viewed as a space of experimentation and elaboration rather than embarrassment.\n\nI cannot imagine any sort of collaborative activity (teaching included) that would not benefit from a healthy injection of interpersonal risk. We can’t really work together until we know each other, and that depth of knowledge only comes from admitting that we don’t have all the answers and that we don’t always succeed. Opening yourself up to such failures and recognizing their importance as part of any process is a necessary step towards collaboration that is more honest and certainly more human.\n\nA promise for you out there in the ether: by the end of the year I will have turned failing into an art form. I’m sure in May I will look back on this early site with disdain and an eye to incorporating all sorts of tech wizardry. Maybe I’ll add a GIF. For now, I’m off to break the Internet.\n"},{"id":"2012-09-21-praxis-the-innovator","title":"Praxis: The Innovator","author":"claire-maiers","date":"2012-09-21 06:43:52 -0400","categories":["Grad Student Research"],"url":"praxis-the-innovator","content":"In addition to getting a crash course in [html and css](http://praxis.scholarslab.org/topics/html-and-css/), we’ve spent our time in the fellows’ lounge this week actually putting some prose together for our charter.  One of the great things about this process has been the way in which working on the charter has actually provided a platform for us to get to know each other and to begin to have a number of other important conversations.  Will we continue with Prism?  Who is Prism for: academics, researchers, teachers, the entire web-surfing public?  Is it possible to address the needs of all the disciplines we collectively represent in this project?\n\nThis process has also raised another concern for me that I only began to articulate yesterday.   One of our motivations for working on our own version of the charter was to convey our own ethos in the text.  As Gwen pointed out during one of our first meetings, we don’t want to create problems by foreseeing them.  If the charter can be seen as a self-fulfilling prophecy—an attempt to articulate the desired goals and experiences for the next year that will motivate us to meet those goals—we didn’t want to convey an expectation of problems and conflicts.  So far, I think we’ve been successful in this regard.  But it has left me wondering what else we are importing into Praxis without notice or intent.\n\nTo my mind, the Digital Humanities represent a possibly radical corner of the academic landscape.  Here—as the word “interdisciplinarity” instructs us—we are not supposed to adhere to the confines of our disciplines.  What other “rules” are we meant to challenge?\n\nIn pondering that question, my sociological mind turns to cultural theory.  One of the things that sociologists often talk about is the way that culture instructs us not only in _what_ we should do, but in _how_ we should do it.  We take an immense number of things for granted, assuming that other options do not exist.  The cultural instructions for how to do something in one area of life often spill over into another.  Something meant to be innovation becomes mundane.\n\nThese cultural instructions that we rely on are difficult to notice.  Thus, we might import the rules and processes of bureaucracy, for example, into our own project without realizing it.   This was one of my concerns this week when we discussed potentially including a clause for conflict resolution (requiring a neutral third party) into our charter.  Not only did this seem to contradict the ethos we wanted to convey, but it imposed a seemingly overly bureaucratic construct into what has the potential to be a new kind of scholarly work environment and project.  In another example, there have also been concerns (from myself included) about how our project will fit the requirements of respective disciplines and be accepted as legitimate scholarship.  While we certainly benefit from structure and clearly articulated goals, why must we rely on the taken-for-granted goals and rules of bureaucratic educational institutions?\n\nI want to encourage us to be on the lookout for these taken-for-granted assumptions.  I want Praxis to be something that breaks academic molds--something innovative.  Doing this requires both attentiveness and a willingness to have our own assumptions and habits challenged.\n"},{"id":"2012-09-24-good-practice","title":"Good Practice","author":"gwen-nally","date":"2012-09-24 13:02:54 -0400","categories":["Grad Student Research"],"url":"good-practice","content":"This week we've made major strides towards adopting a charter. It's interesting to note (as [Claire](http://www.scholarslab.org/author/cdm6zf/) does in [her post](http://www.scholarslab.org/praxis-program/praxis-the-innovator/)) that charters are often somewhat pessimistic, anticipating the problems of working in a group and setting out certain rules for managing these \"inevitable\" conflicts. We've decided to try for a slightly more positive document, one that focuses on our goals and group _ethos_.\n\nWhile it might sound a bit cliché to say that we've decided to \"stay positive\", there is something to the idea that writing conflict management procedures into the charter might somehow cause us to identify ourselves as a group destined for conflict. Oscar Wilde, in his essay 'On the Decay of the Art of Lying', tells a story about a young woman, who is so influenced by a fictional character (one that appears in a French serial) that, when the character starts to make destructive decisions, running away with an “inferior” man, the woman feels compelled to follow. Wilde points out that the young woman's identity is so tightly bound up in the fate of the character that she cannot extricate herself from making the same bad choices. At least where identity is concerned, we are all susceptible this kind of self-fulfilling influence. This is all just to say that, if we begin to label ourselves as a group in need of conflict management strategies, we might force ourselves to become a group that actually needs them.\n\nOur attitude, at least this far, has been to think instead about certain good practices (Eupraxia?) that capture the spirit of the program, the people that brought us together, and all the good stuff that DH has to offer. The list reads something like this: Be nice. Be professional (but not too professional). Be respectful. Listen. Reach for consensus. Enjoy the process. Fail in public. Reflect. Retool. Have potlucks.\n"},{"id":"2012-09-24-omeka-capistrano-recipes","title":"Omeka Capistrano Recipes","author":"wayne-graham","date":"2012-09-24 09:31:57 -0400","categories":["Research and Development"],"url":"omeka-capistrano-recipes","content":"The Scholars' Lab has been working a lot with Omeka over the last several years, and in that time I've accumulated a bunch of different installations of Omeka. On [neatline.org](http://www.neatline.org) alone, there are four different Omeka containers running. If I were a glutton for punishment, I would manage these by setting up a new space on the target server, downloading Omeka, going out and grabbing the individual plugins, and setting everything up by hand. There are a few downfalls in this, upgrades are a pain, you can forget what you did from one instance to the next, and it's horribly inefficient. And, if you've ever spent more than a few minutes with me, you'll know how much I despise inefficiency...\n\nWe use [capistrano](https://github.com/capistrano/capistrano/wiki) to automate deployments for our faculty projects, so that was the tool I was using. Basically I have a directory on my computer named `deployments` that contain sub-directories for all the projects I am responsible for deploying. In it are the capistrano scripts I use to manage software deployments on those servers. The actual code lives in various git repositories, and these directories literally just have capistrano scripts for deploying software in them. The basic layout looks like this:\n\n[code lang=\"bash\"]\n$ ls -l | awk '{print $9}'\n\nGemfile\nGemfile.lock\nfalmouth.lib.virginia.edu\nhenshaw.neatline.org\nhotchkiss.neatline.org\nlovecraft.neatline.org\nneatline.org\nsandbox.neatline.org\n[/code]\n\nWhen I need to deploy a new release, I just go in to that  project's directory and use `cap deploy` and not worry about much.\n\nMost of those directories had copy-n-pasted code from each other, with the main difference in them being the name of the application (e.g. henshaw, hotchkiss, lovecraft, etc.). With that level of copy-n-pasting, things are sure to go south eventually, so over the weekend I abstracted these tasks out in to a gem and pushed the [omeka-recipes](https://rubygems.org/gems/omeka-recipes) gem up to rubygems.\n\nThere are some useful tasks (like backing up your database and tailing your log files), as well as helping walk through the setup of a new instance. Basically you'll need `ssh` access to the remote server, know the path to deploy the software to, and a MySQL admin password. The `cap setup` task will generate the db.ini file you'll need on the remote server; no need to learn vim, nano, or emacs!\n\nThere is some [documentation](http://rubydoc.info/gems/omeka-recipes/0.2.0/frames), and there's a bit more work to do getting this ready for Omeka 2.0 (namely the shift in the naming of the 'archives' directory), but if you use Omeka at all, check it out. As always, you can leave an issue on the [issue tracker](https://github.com/waynegraham/omeka-recipes/issues), and I'd love pull requests.\n"},{"id":"2012-09-26-ignoring-your-first-child","title":"Ignoring your First Child","author":"jeremy-boggs","date":"2012-09-26 06:00:37 -0400","categories":["Research and Development"],"url":"ignoring-your-first-child","content":"And no, I'm not talking about human children. I'm talking about CSS selectors!\n\nI usually have instances in my web designs where I would like to apply some styles to all the elements of a particular type _except_ the first one. For example, when displaying a navigation list, I like to add a light border in between each list item, but don't want to add a border to the first one. There are a bunch of ways to do this, depending on which browsers you want to support.\n\nThe most straightforward way is to just select your element normally in one declaration, then select the first child of that element using the [`:first-child` pseudo-class](http://www.w3.org/TR/CSS2/selector.html#first-child), like so:\n\n[code lang=\"css\"]\n/* Select all list items. */\nli {\n    border-top: 1px dotted #ccc;\n}\n\n/* Select the list item that's the first child. */\nli:first-child {\n    border-top:none;\n}\n[/code]\n\nThis CSS will make sure each list item is separated by a border, and takes away the top border from the first list item, like so:\n\n\n\nOf course, you've got that extra selector to basically take away the border for the first child. That's fine, too, but there are ways of writing this declaration with one selector to apply styles to elements except the first one.\n\nWhen the [`:not` pseudo-class](http://www.w3.org/TR/2001/CR-css3-selectors-20011113/#negation) first came out, it was love at first sight for me. It's a handy little selector, allowing you to select anything _except_ elements that met some condition. It is, as the W3C documentation says, a _negation_ class.\n\nSo, using the `:not` pseudo-class, we could rewrite our CSS to look like this:\n\n[code lang=\"css\"]\n\n/* Select all list items except the first child. */\nli:not(:first-child) {\n    border-top: 1px dotted #ccc;\n}\n[/code]\n\nNice and simple, and only requires writing one CSS declaration instead of two. And you can see, the results are the same:\n\n\n\nI've used this approach in a lot of designs and felt pretty happy with for the most part. One problem, however, is that the `:not` pseudo-class is not supported by IE8 and earlier. Until recently, to get around that, I'd load a JavaScript polyfill called Selectivizr to basically add support for CSS3 pseudo-classes and a few other selectors to IE 8 and earlier. (More information about Selectivizr, including the features it adds and documentation on how to implement it, on their [website](http://selectivizr.com/).)\n\nOnly recently did I realize there's a better way, one supported by IE8 **and** IE7 that doesn't require the JavaScript polyfill. That way involves using the the [adjacent sibling selector](http://www.w3.org/TR/CSS2/selector.html#adjacent-selectors), which is just a `+` sign, to select only the list items that are adjacent siblings of another list item. Using the adjacent sibling selector, our CSS would look like this:\n\n[code lang=\"css\"]\n\n/* Select any list item that is an\n   adjacent sibling of another list item. */\nli + li {\n    border-top: 1px dotted #ccc;\n}\n\n[/code]\n\nThis selector doesn't style the first list item, because it doesn't have a sibling before it. And again, the results are the same:\n\n\n\nI like this approach now because it keeps the CSS minimal (no need to undo the style with a second selector, which I like to avoid) and it's supported all the way back to IE7.\n\nOf course, there are plenty of instances where CSS3 selectors like `:not` are the only way to accomplish what you'd like, and using those selectors and adding support for them with something like Selectivizr is a perfectly fine approach. (I do this for a lot of projects.) But it's better to make sure there are other ways you can write your selectors that are backwards compatible as much as possible.\n"},{"id":"2012-09-28-living-in-the-future","title":"Living in the Future","author":"chris-peck","date":"2012-09-28 05:28:19 -0400","categories":["Grad Student Research"],"url":"living-in-the-future","content":"Okay, so it can't [make tea](http://www.youtube.com/watch?v=eVUuaDXBhs4), but this [MakerBot](http://store.makerbot.com/replicator.html) gizmo is pretty dang cool. And there's a K-cup machine on the other side of the office, so I guess we can make do with a replicator that only does plastic.\n\nThat's right, The Scholars Lab has a MakerBot Replicator. Several of us new Praxis Fellows have been giggling with delight over the shear magic of the thing. Jeremy [helped me print](http://www.youtube.com/watch?v=cL64XYqHHSI) a 3-tone whistle [design we found on thingiverse](http://www.thingiverse.com/thing:21188), and it really works! Here's a [recording of me blowing into the it, slowed down by a factor of 8](http://soundcloud.com/chris-peck-6/3-tone-whistle-at-1-8th-speed) so you can really get the full effect of the three tones.\n\nWatching a 3-d model become actual plastic right before your eyes is of course pretty cool. But my glee about the Replicator is not mere idiot glee. I plan to put this thing to use. I've been composing music for small mass-produced instruments for a while now, including [a series of performances involving plastic soprano recorders](https://vimeo.com/8750482). (Yes, like the ones you played in grade school. Okay, so maybe _some_ idiot glee is still at play here.) These instruments have a number of advantages over those conventionally chosen by composers of concert music. For starters, they cost about $2.49. That means that on a modest budget I was able to purchase identical instruments for a group of 40 or so performers, many of whom were self-described \"non-musicians.\" Part of the concept of that project was to create an ensemble that included dancers, visual artists, and other sorts of people with diverse backgrounds in addition to a few \"trained\" players.\n\nI've continued to use small, cheap instruments such as harmonicas in a number of collaborations with choreographers. Just a few weeks ago I ordered a large box of plastic whistles online, in part to discover any subtle variation between them in terms of pitch, sound quality, or response. Such variation could be compositionally useful. When I learned in our first-day tour of the Scholars Lab that a similar plastic whistle could be \"printed\" from a digital model, I had a mild flash of inspiration:  why rely on mass-produced small plastic instruments? I could design my own.\n\nA quick search of thingiverse.com and I've already found an [ocarina](http://www.thingiverse.com/thing:13136) and even a [recorder](http://www.thingiverse.com/thing:12168). The next step is to learn something about 3-D modeling so I can start modifying theses designs. What will be the effect of subtle variations in sizes of various parts of the mouthpiece, placement of finger holes, etc? My first \"original\" design will probably be a simple tuned set of whistles made by printing off a number of them at different scales. Someone has already made a [double-sized variation](http://www.thingiverse.com/thing:2892) on the standard whistle used as a Replicator test print, which of course should sound an octave lower than the normal size. With [other factors](http://hyperphysics.phy-astr.gsu.edu/hbase/music/et2.html) I should be able to build an entire scale and end up with something like an irritainment handbell choir.\n\nI'm also starting to fantasize about designing a physics of music class where the final project would be an instrument design for the Replicator. Do you think we can make a bugle or a small-scale trombone? How about a shawm or a simple clarinet? This takes me back to 8th grade, when I built a flute out of PVC pipe for a science fair project. I guess I've been waiting for the MakerBot to appear in my life for quite some time.\n\nJeremy says that he and Wayne have been talking about how to put together a lab so that more UVa folks can access this technology, and I hope that comes to fruition. I've been showing off my new 3-tone whistle to friends in the Music Department, and many of them already have ideas for projects. [Max](http://maxwelltfirn.wordpress.com/) had independently been thinking of using desktop fabrication to build boxes for electronics projects. (If you've ever tried to cut a slot for a slide potentiometer in a Radio Shack project box with a Dremel then you understand how exciting this is!) And he just sent me [an article](http://www.wired.com/design/2012/09/formlabs-creates-a-low-cost-light-based-3-d-printer/?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+wired%2Findex+%28Wired%3A+Top+Stories%29&utm_content=Google+Feedfetcher) about a new low-cost 3-D printer that uses stereolithography instead of heated plastic like the MakerBot, meaning much finer resolution. With developments like this on the horizon its exciting time to start experimenting.\n\nAs I said, my fellow fellows have been excited about the Replicator too. Fellow [Gwen](http://www.scholarslab.org/author/egn9b/) has been [printing jewelry](http://paperballroom.com/3d-printing/). But my question for you, internet, is what (if anything) does this have to do with [Prism](http://prism.scholarslab.org/)? Collective interpretation of tchotchkes anyone?\n"},{"id":"2012-10-01-casing-your-text","title":"Casing your Text","author":"jeremy-boggs","date":"2012-10-01 11:06:47 -0400","categories":["Research and Development"],"url":"casing-your-text","content":"One of the first things I remember doing as a graduate assistant was editing a few dozen HTML files to change all of the headings, which someone had typed in upper case, to use title case. This was not too much fun. It was even more painful to realize later that you can easily change the [letter case](http://en.wikipedia.org/wiki/Letter_case) of any text by using a handy CSS property called [`text-transform`](http://www.w3.org/TR/CSS21/text.html#caps-prop).\n\nThe `text-transform` property has been around since CSS2.1, and has wide support in modern browsers. It allows you to change the letter case of any element's text that you select. It has five possible values: `capitalize`, `uppercase`, `lowercase`, `none`, and `inherit`.\n\nThe two values I most often use are `uppercase` and `lowercase`. `capitalize` will capitalize all the words in a string of text, which can be handy, but can also capitalize words that don't need to be capitalized (and that, of course, [changes depending on which style guide you want to use](http://grammar.quickanddirtytips.com/capitalizing-titles.aspx)). `uppercase` will set everything in a text string to upper case, while `lowercase`, will, you guessed it, set everything in a text to lower case. Let's have a look.\n\nTo use the upper case for an element, we'll use `text-transform: uppercase;` in our style sheet:\n\n[code lang=\"css\"]\n\nh2 {\n    text-transform: uppercase;\n}\n\n[/code]\n\nThis will upper-case all `h2` elements on your page, like so:\n\n\n\nTo use lower case for an element, we'll use `text-transform: lowercase;` in our style sheet:\n\n[code lang=\"css\"]\n\nh2 {\n    text-transform: lowercase;\n}\n\n[/code]\n\nThis will make all h2 elements on your page lower-cased, like so:\n\n\n\nOf course, it's totally fine to type your text directly in a specific case, whether upper or lower, but I would recommend doing so only if its semantically meaningful to the text itself, and not as a stylistic measure. If you are using a specific case for presentation, save yourself a lot of trouble and just write out the text using normal capitalization. You can then set its style using `text-transform` in your CSS. That way, if you have to change it to something else later, you'll only need to edit one line of CSS—not hundreds in different HTML pages.\n"},{"id":"2012-10-01-marking-and-explanation-in-prism-2","title":"Marking and Explanation in Prism","author":"brandon-walsh","date":"2012-10-01 11:08:17 -0400","categories":["Grad Student Research"],"url":"marking-and-explanation-in-prism-2","content":"My first experience with [Prism](http://prism.scholarslab.org/) last spring brought something of an existential crisis along with it, when I was asked to mark my beloved _A Portrait of the Artist as a Young Man_ in terms of realism/modernism:\n\n\n<blockquote>He was baby tuckoo. The moocow came down the road where Betty Byrne lived: she sold lemon platt.\n\nO, the wild rose blossoms\n\nOn the little green place.</blockquote>\n\n\nConsidering _Portrait _as a modernist novel was nothing new, but being asked to consider individual words in that context was altogether unsettling. Words like “tuckoo” and “moocow” are easy to mark as modernist, but “baby” is perfectly reasonable within a realist text; it only begins to feel experimental as a part of the larger phrase “baby tuckoo.” And how do we account for the two lines of verse, where the words themselves are not particularly experimental but the juxtaposition between prose and poetry creates just the sort of genre bending you would expect from modernism?\n\nI think this very brief analysis tells us something very important about how we make meaning: interpretive choices depend not just on the words in the text but on perceived relationships among different textual groupings within the text as a whole. Part of the perceived value of a crowdsourcing tool like Prism comes from its ability to generate conversations about these interpretive decisions. I wonder, though, if we can include these conservations in the tool itself.\n\nThe obvious response to this perceived need would be to prompt users to justify each interpretive decision they make, but this seems undesirable. I expect that the interface would become unwieldy very quickly if we asked users to explain _every_ marking that they make, and I imagine that the defensive posture this would engender in users could be inimical to the nature of the tool’s ethos.\n\nPerhaps we could consider an opt-in approach, where we offer users the option to select a particular marking and offer a short explanation after marking and just before saving the highlights. Then these terms could be footnoted in some way for visualization. Perhaps we could even run some sort of thesaurus program to link similar explanations across different marked passages.\n\nI don’t mean to imply that this is a flaw in the Prism’s design: I think one of the more mind expanding aspects of the tool is how it lays bare the process by which word transforms into meaning. I hope we can find a way to front-end this strength in the user interface.\n"},{"id":"2012-10-02-fun-with-prism","title":"Fun with Prism","author":"gwen-nally","date":"2012-10-02 06:44:24 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"fun-with-prism","content":"Prism could be a tool that we use for scholarly entertainment (like [Old Weather](http://www.oldweather.org/)). It could also be an extremely powerful tool for research--provided that we make the controls fluid enough.\n\nEarlier this week, [Claire](http://www.scholarslab.org/author/cdm6zf/) and I dreamed up a rather elaborate interface that would showcase Prism's playful qualities, in order to generate \"subjects\" interested in participating in the research side of things. While brainstorming, we started casually referring to all the possible projects housed on Prism as \"rooms\", each with its own \"door\" through which users would enter. Among other adjustments that we imagined, we thought a lot about customization, allowing a project leader to make a number of crucial decisions before launching the room. Who would be allowed to mark up the text? And how would a reader be presented with the request? Perhaps the most useful innovation we discussed was the option to remove the fixed \"categories\" from the front-end experience, so as to provide for freer responses. But then what? Would readers be able to provide their own tags and categories? Would it be desirable to run these tags through some kind of linguistic software in order to generate more general data? Might all of this be left up to the person launching the project?\n\nThis conversation led to a shift in my thinking about how different disciplines might approach Prism--I even started to imagine uses in my own field. In the past, I've been a bit skeptical of crowd-sourcing in that very few questions in philosophy have traditionally been answered by appealing to surveys or statistical analysis (with the exception of some [pretty cool stuff in experimental philosophy](http://pantheon.yale.edu/~jk762/ExperimentalPhilosophy.html)). But thinking about these rooms, with fluid controls and the possibility of limiting the number of readers, got me thinking: Prism could also be a powerful tool for experts. I even got a little giddy when I realized that this could be a tool for, say, presenting what the very best living readers of Ancient Greek might have to say about certain portions of Plato's _Republic_. (Big fun, right?!) This type of \"expert-sourcing\" could provide students with an excellent commentary on a given text, and might also serve to define new areas for research. An interpretive void--what the experts don't mark up--could be just as useful as what they do mark, drawing our attention to passages that have received less scholarly attention.\n\nIn keeping with this idea, I've also started to think about the possibility of showcasing finished or closed projects. People could visit certain rooms to see the ways that certain groups have marked important texts. I can imagine all sorts of fun rooms:  Classical musicians respond to a piece of Noise Music, Research participants in placebo-controlled trials hash out the informed consent documents, Followers of Kabbalah interpret the _Torah_, Fourth-graders comment on the US Constitution...\n"},{"id":"2012-10-02-images-in-prism","title":"Images in Prism","author":"cecilia-márquez","date":"2012-10-02 06:46:11 -0400","categories":["Grad Student Research"],"url":"images-in-prism","content":"One thing we have thought about in recent weeks is the role of Prism in our goals for the semester.  As part of this brainstorming I wanted to share some of my thoughts about a potential future for Prism.\n\nWhat I was most interested in is how we can use images with Prism.  Images call for a different type of literacy that is potentially more accessible and inviting.  Specifically I am interested in looking at historical photographs as texts to interpret through crowdsourcing.  For example, this photo is an image used in a [colleague’s work](http://www.virginia.edu/history/user/272).\n![](https://lh6.googleusercontent.com/cAfsAM2BJcb-2YP9MW_IwZdP6J13dNWqkLOUOI6-3N0CdLe9PpI8FgfBx3en8y81j_VQLfb2jvB6_scTOQYQS8_n4XO-D3BFCffYdC6OHwGyPGn42pOE)\nThis image, used by the Southern Rural Action Inc., was taken in Georgia in the 1970s.  However, little of this information is self-evident in the photo.  What I would be interested in finding out is what viewers, both those educated in Southern African American history, and those viewers who may only know Southern African American history through popular culture.  Offering both these groups a forum to analyze these photos will serve a few goals.  First, it can illuminate the role of Southern African Americans in the imaginary of those who examine the image through Prism.  Second, the call for audiences to bring an analytical eye to bear will hopefully expose the constructed nature of photographs.  Hopefully it will show how photographs are deployed for political ends and are not neutral snapshots of a moment in time.\n\nWhat I imagine is the viewer being introduced to an un-annotated photograph where they can respond to questions like:\n\n\n\n\t\n  * Who is in this photo?\n\n\t\n  * Who are the subjects in this photo?\n\n\t\n  * Where is this photo taken?\n\n\t\n  * What is happening in this photo?\n\n\t\n  * Who is the audience of this photo?\n\n\nAfter completing their own analysis the viewer can go on to see the bibliographic information of the photo (the who, what, when, where) and additionally the analysis of a specialist on the topic.  This layer of annotation could include a more discursive analysis of the image that can expose layers of meaning in the photo.\n\nThis is just one idea and I look forward to seeing what is possible!\n"},{"id":"2012-10-03-hot-off-the-presses-the-solrsearch-plugin","title":"Hot off the Presses: the SolrSearch Plugin","author":"eric-rochester","date":"2012-10-03 07:13:45 -0400","categories":["Announcements"],"url":"hot-off-the-presses-the-solrsearch-plugin","content":"The Scholars’ Lab is pleased to announce the first release of the [SolrSearch](https://github.com/scholarslab/SolrSearch) [Omeka](http://omeka.org/) plugin.\n\n\n\n\n\nSolrSearch allows you to replace Omeka’s default search with [Solr](http://lucene.apache.org/solr/). Solr is a standard, popular, open source, fast text search engine server. It handles hit highlighting, date math, numeric aggregation functions (mean, max, etc.), indexing for 33 languages, replication, and many, many more things. It's used by [whitehouse.gov](http://www.whitehouse.gov/), [Instagram](http://instagr.am/), AT&T;'s [yp.com](http://yp.com/), [Ticketmaster](http://www.ticketmaster.com/), and [Netflix](http://www.netflix.com/), to name a few (see the list of [Public websites using Solr](http://wiki.apache.org/solr/PublicServers)).\n\n\n\n\n\nIt does require running Solr as a separate server process (although possibly on the same machine), so it does require more resources--both personnel and technical--but it's often worth the investment.\n\n\n\n\n\n**Search Pages and Exhibits** SolrSearch now indexes Simple Pages and Exhibits.\n\n\n\n\n\n**Performance** Did I mention that Solr is fast? It’s been optimized for high-traffic sites, and it can easily handle much more data than MySQL full text search can.\n\n\n\n\n\n**Scalability** And because it’s been engineered for large, high-traffic sites, Solr can handle more data, faster than MySQL. This especially becomes an issue when you have collections with a large number of items or items with a lot of data attached to each.\n\n\n\n[![Configuration](http://www.scholarslab.org/wp-content/uploads/2012/10/Screen-shot-2012-10-02-at-3.12.56-PM-300x167.png)](http://www.scholarslab.org/announcements/hot-off-the-presses-the-solrsearch-plugin/attachment/screen-shot-2012-10-02-at-3-12-56-pm/)\n\n\n**Configuration** The SolrSearch plugin in highly configurable. You can decide which fields to search, which can be used for facets, and how to label them.\n\n\n\n\n\n**Facets** [Facets](http://en.wikipedia.org/wiki/Faceted_search) slice up your items and allow users to navigate through those slices. For example, [The Falmouth Project](http://falmouth.lib.virginia.edu/) used an early version of the SolrSearch plugin to give users not only free-text search, but also to allow users to browse the buildings it records by neighborhood, date, and use.\n\n\n\n\n\nYou can find the download on the [SolrSearch plugin page](http://omeka.org/add-ons/plugins/solrsearch/). The code is hosted on the [SolrSearch github page](https://github.com/scholarslab/SolrSearch). If you have any feedback about the plugin, find any bugs, or want to suggest features, head over to the [issues page](https://github.com/scholarslab/SolrSearch/issues). And if you have questions, feel free to post in the [Omeka forums](http://omeka.org/forums/).\n\n\n\n\n\nAs always, we look forward to seeing how you’ll use this.\n"},{"id":"2012-10-08-hot-off-the-presses-2-bagit-plugin","title":"Hot off the Presses 2: BagIt Plugin","author":"eric-rochester","date":"2012-10-08 07:41:46 -0400","categories":["Announcements"],"url":"hot-off-the-presses-2-bagit-plugin","content":"[![Canvas Bag - Tutorial24](http://farm5.staticflickr.com/4058/4258185459_8cdd4ac6bd_m.jpg)](http://www.flickr.com/photos/kittybabylove/4258185459/)\n\nPhoto by [kittybabylove](http://www.flickr.com/photos/kittybabylove/) [![Creative Commons License](http://i.creativecommons.org/l/by-nc-nd/2.0/80x15.png)](http://creativecommons.org/licenses/by-nc-nd/2.0/)\n\n\n\n\n\nContinuing our roll-out of Omeka plugins we’ve been working on here at the Scholars’ Lab, I’m pleased to announce the [BagIt plugin](http://omeka.org/add-ons/plugins/bagit/) for [Omeka](http://omeka.org/).\n\n\n\n\n[BagIt](https://wiki.ucop.edu/display/Curation/BagIt) is a specification by the [Library of Congress](http://www.loc.gov/index.html) for creating containers of files with metadata. However, the files don’t actually have to be in the container. There is a `fetch.txt` file, which lists URLs for content to add to the container when you take everything out of it.\n\n\n\n\nThe first part of this release is the [BagIt PHP library](https://github.com/scholarslab/BagItPHP). This is a generic PHP library for working with BagIt files. We announced an earlier version of this [here](http://www.scholarslab.org/announcements/announcement-bagitphp-library/), but we’ve updated it and fixed some bugs. If you’re using it, you may want to grab the latest copy of it.\n\n\n\n\nThe second part is the [BagIt Omeka plugin](http://omeka.org/add-ons/plugins/bagit/). This is built upon the BagIt library and provides an easy-to-use user interface for it. You can create a bag from a set of Omeka files. You can ingest bags into the Omeka [Dropbox plugin](http://omeka.org/codex/Plugins/Dropbox), and from there you can attach them to items.\n\n\n\n\nThis plugin does have a couple of requirements. Both the library and the plugin require the [Archive_Tar](http://pear.php.net/package/Archive_Tar) PHP library, and the plugin depends on the [Dropbox plugin](http://omeka.org/codex/Plugins/Dropbox).\n\n\n\n\nYou can find the download on the [BagIt plugin page](http://omeka.org/add-ons/plugins/bagit/). The code is hosted on the [BagItPHP github page](https://github.com/scholarslab/BagItPHP) and the [BagItPlugin github page](https://github.com/scholarslab/BagItPlugin). If you have any feedback about the library or the plugin, find any bugs, or want to suggest a feature, visit the [issues page](https://github.com/scholarslab/BagItPlugin/issues). And if you have questions, feel free to post in the [Omeka forums](http://omeka.org/forums/).\n"},{"id":"2012-10-10-hot-off-the-presses-3-fedoraconnector-plugin","title":"Hot off the Presses 3: FedoraConnector Plugin","author":"eric-rochester","date":"2012-10-10 07:58:19 -0400","categories":["Announcements"],"url":"hot-off-the-presses-3-fedoraconnector-plugin","content":"[![Vintage Fedora and Hat Box](http://farm3.staticflickr.com/2071/1700651383_455be0bb4c_n.jpg)](http://www.flickr.com/photos/swingcandy/1700651383/)\n\n\n\n\n\n\nPhoto by [Swing Candy](http://www.flickr.com/photos/swingcandy/) [![Creative Commons License](http://i.creativecommons.org/l/by-nc-sa/2.0/80x15.png)](http://creativecommons.org/licenses/by-nc-sa/2.0/)\n\n\n\n\n\n\nFor part three of our release parade, we’re showcasing the 1.0.0 release of [the FedoraConnector](http://omeka.org/add-ons/plugins/fedoraconnector/) plugin for [Omeka](http://omeka.org/).\n\n\n\n\n[Fedora Commons](http://www.fedora-commons.org/) is a digital repository management system. It’s used by libraries to manage and scale their online repository and digital assets and collections. As such, it’s often used by larger institutions; however, this isn’t aimed at those organizations. Instead, it’s meant to be used by people who wish to pull information from institutions that use Fedora Commons.\n\n\n\n\nThe FedoraConnector plugin doesn’t help you discover resources in a repository. But once you have the [PID](https://wiki.duraspace.org/display/FEDORA34/Fedora+Identifiers) for something that you’d like to include in an Omeka site, you can use that to pull in the metadata for that item, as well as any images or other content streams associated with it.\n\n\n\n\nThere is a little magic involved. You have to know the incantation to reach your Fedora server and you have to know how to get the PIDs for the items you’re interested in. But [David McClure](http://www.scholarslab.org/people/david-mcclure/) has worked hard to hammer out an easy, fluid workflow for getting data from Fedora into Omeka.\n\n\n\n\nAnd once you know these things, this plugin will allow you to pull content into your Omeka site that you otherwise wouldn’t have access to: things you’ve found in your library catalogue and wished you could include, but didn’t know how.\n\n\n\n\n* * *\n\n\n\n\nYou can download the plugin on the [FedoraConnector plugin page](http://omeka.org/add-ons/plugins/fedoraconnector/). The code is on the [github repository](https://github.com/scholarslab/FedoraConnector). If you have feedback, complaints, or feature requests, visit the [issues page](https://github.com/scholarslab/FedoraConnector/issues). And if you have any questions, feel free to post to the [Omeka forums](http://omeka.org/forums/).\n"},{"id":"2012-10-17-the-direction-of-prism","title":"The Direction of Prism","author":"brandon-walsh","date":"2012-10-17 07:27:23 -0400","categories":["Grad Student Research"],"url":"the-direction-of-prism","content":"This week, the team has been throwing around a number of ideas as to how we can further develop Prism. I keep falling back on spatial metaphors to categorize the changes currently in play. I threw the following model onto the SLab white board, wherein I propose two types of changes: vertical and horizontal.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/10/IMG00279-20121003-1220-300x225.jpg)](http://www.scholarslab.org/praxis-program/the-direction-of-prism/attachment/img00279-20121003-1220/)\n\nFirst, the vertical axis, those changes which work within the tool’s existing functionality. Some changes deepen a function already in place. The suggestions offered by last year’s team for other visualizations would fall into this category. A new visualization can offer a more dynamic interaction with a set of markings, but users are still engaging with the same dataset. The fundamental function of the tool would remain the same. These sorts of vertical changes offer a deeper, more robust mode of interaction with the operations that the tool can already carry out.\n\nI oppose these to horizontal changes, which more fundamentally alter the number and types of things that a tool can do. We have talked a lot about carrying Prism into other media: music, images, and video. We have also talked about freeing up the controls on Prism, making it possible for users to upload their own texts or to hide the categories prior to marking. These sorts of modifications change the face of Prism more drastically, resulting in different data sets and new possibilities for the users.\n\nThese horizontal changes seem to carry the highest risk. As we discussed at our meeting last week, we risk diluting Prism’s identity if we try to add in too many features: a tool that can do anything might just be a tool that can do nothing effectively. The SLab crew also pointed out how some of our broadening changes might carry us into territory that has already been covered by other developers and other tools. At the same time, though, some of suggestions for horizontal growth are incredibly exciting. Taking Prism into other media, freeing up the controls – these changes could help Prism reach a broader audience by teaching it new tricks. I think we should take them seriously.\n\nOf course, this horizontal/vertical distinction does not quite hold up under close scrutiny. I think it would be a mistake to think of depth-oriented changes as being somehow narrower than broader changes. A new visualization could fundamentally alter your relation to the data in such a way that it feels as though the functionality has altered entirely. No modification slots entirely into one or the other category, and the spatial metaphors might muddy the waters unnecessarily. All the same, I find the distinction helpful for mapping out the morphing shape of the Prism of the future.\n\nSo do we build depth, or do we broaden our base? Do we dig into the features already present, or do we add new ones? At what point does Prism cease to be Prism? What would Prism look like if we graphed it?\n"},{"id":"2012-10-17-the-whiteboard","title":"The Whiteboard","author":"gwen-nally","date":"2012-10-17 07:26:05 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"the-whiteboard","content":"This is what the whiteboard in the grad lounge looks like this week.\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/10/whiteboard2-300x225.png)](http://www.scholarslab.org/digital-humanities/the-whiteboard/attachment/whiteboard-3/)\n\nWe've been dreaming pretty big. Here are a few of the questions that we've been mulling over:\n\n**1. What will Prism look like?** What sort of interface would allow users to upload their own materials and browse other ongoing (or completed) projects? (This is where [Claire and I started thinking about different \"rooms\".](http://www.scholarslab.org/digital-humanities/fun-with-prism/))\n\n**2. What kinds of things might we leave to the control of users?** Would users be able to upload different kinds of \"texts\", like images and music? Could users hide or display different categories to guide interpretation? How might we make for freer responses? Can freer responses be turned into usable data? Would there be some value to closing projects? Or should all projects be kept open to the public?\n\n**3. How might we visualize whatever data we collect? **\n\n**4.  Do we want to add more features or make the features that already exist more robust? **\n\n**5. What should we do next?**\n\nIt seems like the next big hurdle for us will be to carefully weigh our ambitions against our limited time-frame. While brainstorming, Brandon suggested that we're going to need to be careful to take on manageable goals. One way to do this might be (#4) to focus our efforts on making the existing features more robust. This is an attractive idea. We could begin by trying to realize the sorts of visualizations that last year's cohort dreamed up.\n\nAlthough we're not really sure how to answer these questions, we may be a bit closer than we were. In yesterday's meeting, I finally realized something that had been lost on me: Prism serves a unique role in the DH community. It is the first tool of its kind in that it crowd-sources user interpretations. There have been many crowd-sourcing tools, but most (if not all?) have treated the user as a means of collecting or transcribing large amounts of data.  So whatever we decide to do, I hope that we are careful not to loose sight of this innovation--capturing user interpretation as usable data is, in a sense, what makes Prism tick.\n"},{"id":"2012-10-18-digital-humanities-growing-pains","title":"Digital Humanities Growing Pains","author":"cecilia-márquez","date":"2012-10-18 10:05:18 -0400","categories":["Grad Student Research"],"url":"digital-humanities-growing-pains","content":"This has been a tough week for me and the Digital Humanities. We are all grappling seriously with what we want to do this year with [Prism ](http://prism.scholarslab.org/)or with some other project entirely. This has led to some really amazing, but at times tense and frustrating, conversations amongst the group. Part of what is blocking me from being able to move forward is trying assess the use of Prism. Is it a pedagogical tool? is it a tool for researchers? is it a tool for entertainment? Who benefits from the production of these crowdsource interpretations?\n\nIn my mind there are some obvious benefits as a historian to having many sets of eyes on a particular primary source: meanings I may have missed become apparent or alternative readings emerge. But that is a fairly utilitarian goal for Prism, to benefit me as an academic. Additionally, the interdisciplinarity of our group forecloses that as a viable goal. What serves me as a historian doesn’t serve my colleagues in Music or Sociology.\n\nIf the goal is to use it as a pedagogical tool then we will certainly have to figure out how to set some controls on who can comment on a page to ensure only a class will comment. But even then, as a pedagogical tool, does this project really serve the “crowd.” I understand the goal is to harness the [“interpretive energy](http://www.scholarslab.org/digital-humanities/crowdsourcing-interpretation/)” of groups of people, my concern is once that energy is harnessed what do we do with that information?\n\nI get stuck in a cycle of being skeptical of “crowdsourcing” because it anonymizes and mechanizes human creativity but also at the same time finding great value in some of the projects for myself as an academic. However, this personal benefit feels self-serving and I worry it is not committed to a democratization of knowledge, which brings me back to the “crowd.”\n\nI would love answers to any or all of these questions. These are just some of the things I’m pondering.\n\nIn other news...we ratified our charter today and you can look forward to reading it in the next few days!\n"},{"id":"2012-10-19-a-project-for-prism","title":"A Project for Prism","author":"gwen-nally","date":"2012-10-19 02:48:34 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"a-project-for-prism","content":"In the last meeting we played the transparency game: Everyone highlights a transparency on top of a text according to a set of categories. At the end of the exercise, all the transparencies are stacked together.\n\nWhat does this show? In theory, a number of interesting results emerge.  The game could show where there is consensus. It could also show conflicting interpretations or unexplored parts of a text. (This is very exciting stuff. I can imagine using Prism in a classroom to generate debates or in a research setting to mark those regions of a text that are in need of more attention.)\n\nIn practice, though, our transparency game generated some pretty confusing results. When we stacked the transparencies together, nothing really jumped out. It was visual chaos. Did something go wrong? Had we not thought about the texts carefully enough? Was our sample size too small? Were our categories misleading? Should we have been more specific? Did we break it? Maybe the transparency game is just supposed to generate more questions?\n\nWe also noticed a number of somewhat troubling features of the game. The biggest issue was that some people tend to mark a text heavily. Others tend to mark only a few lines. The result: Those who mark less seem to have less of an impact in the aggregate visualization. Is this a problem? Should we be assigning some weight to the markings?\n\nIn all the confusion, a few of us noticed that we began to unstack the transparencies, spreading them out on the table. Although this didn't lead to any clearer results, we noticed that it was a natural impulse to unstack them. This points, I think, to an interesting tension that is built into the very fabric of Prism, a tension between individual interpretation--something very familiar to academics--and the \"aggregate interpretation\" that Prism generates.\n\nThe meeting ended with an assignment to design a project for Prism.\n\n...\n\nHere is my project:\n\nOverview: The text I have in mind is Plato's _Phaedrus. _The research question would be to use crowd-sourcing to determine the perceived truth-value of Socrates' statements. There is a long literature about whether and to what extent we ought to take Socrates' statements literally. Many of the dialogues suggest that there are certain types of argumentative practices--lying, deception, misleading, trickery, confusion tactics--that are inappropriate for the philosopher to adopt insofar as he is concerned with the pursuit of truth. But there is also a long history of viewing Socrates as a trickster, a dissembler, and even a liar. This project would ask readers to decide: Is there evidence to suggest that Socrates does, at times, mislead his interlocutors? And, more importantly, does this occur even in the midst of otherwise serious philosophical investigations?\n\nThe categories: A) Serious discourse B) Ironic discourse C) Humorous discourse and D) Misleading discourse. A big question for me is how to frame these categories. Should people be able to interpret the categories freely? Or should I say more? I'm inclined to say more, insofar as I'm interested in particular features of a very old text. Situating the research question in the scholarship might help to shape people's readings in a productive way and avoid anachronism (Shane's right!).\n\nThe audience: People who are comfortable with Plato's dialogues, or more generally, with ancient Greek culture. Explaining the literature might make this a project better suited to a mass audience.\n\nSome modifications: 1) A place to explain the research question. 2) A place to explain the categories. 3) A longer text. I would want people to engage with and mark the entire dialogue. Prism is, at the moment, set up very much like the transparency game. Each text is no more than a page long. But this precludes the ability to delve into themes, structures and other features of a work that require a more synoptic approach.\n"},{"id":"2012-10-19-neatline-omeka-theme-name-contest","title":"Neatline Omeka Theme Name Contest","author":"jeremy-boggs","date":"2012-10-19 05:56:51 -0400","categories":["Announcements"],"url":"neatline-omeka-theme-name-contest","content":"Yesterday I [tweeted](https://twitter.com/clioweb/status/258958763975909376) asking for name suggestions for an [Omeka](http://omeka.org) theme based on the design of [Neatline.org](http://neatline.org). We've already gotten a few great responses, but we've decided to kick it up a notch. We've got a few Neatline t-shirts. They're nice t-shirts, as demonstrated by our friendly Scholars' Lab Waynebot:\n\n[gallery size=\"large\" link=\"file\" columns=\"1\"]\n\nBetween now and next Thursday morning, say 9AM Charlottesville time (east coast US), leave us a name suggestion through Twitter (using the #neatlinetheme hash tag) or in the comments on this post. The Neatline team will look them over, pick one, and the winner will get a lovely Neatline t-shirt, some Scholars' Lab stickers, and a credit line in the theme's code. The person who mentions a name first will get credit for it. No multiple winners, we don't have that many tshirts! The name could be a play on \"Neatline\" or something having to do with functionality of our lovely suite of Neatline plugins. Or it could be something totally random. Common decency, of course, is most appreciated.\n\nWe'll announce a winner next Friday morning. So send us a theme name!\n"},{"id":"2012-10-19-to-crowdsource-or-not-to-crowdsource","title":"To Crowdsource or Not To Crowdsource?","author":"claire-maiers","date":"2012-10-19 02:49:39 -0400","categories":["Grad Student Research"],"url":"to-crowdsource-or-not-to-crowdsource","content":"Sneaking its way into many of our conversations of the last month and half has been a debate over the value of crowdsourcing.  Should we do it?  Is it useful?  My original intention with this post was to offer a defense of crowdsourcing as a valuable endeavor for academia.  While I still think that, ultimately, I am supportive of crowdsourcing or something similar, the fact that it has taken me two weeks of stops and starts to write this post speaks to my own struggles, doubts, and uncertainty about our project as a crowdsourcing project.  As I understand them, some of the concerns about crowdsourcing from our team are as follows:\n\n\n\n\t\n  1. Does crowdsourcing (used for interpretive purposes) create any kind of useful data, knowledge, or insights for academic purposes?\n\n\t\n  2. Does crowdsourcing treat people like cogs in a system, resulting in their dehumanization?\n\n\t\n  3. Assuming that we want our project to be relevant beyond academic walls, can a DH crowdsourcing project actually reach beyond those walls?  Are non-academics interested?  Will people from other walks of life even have the chance to be exposed to such a project?\n\n\nI think these are legitimate concerns which we need to address.  However, I am going to argue that we should, nevertheless, endeavor to incorporate some sort of crowdsourcing aspect into our project.  Without roaming too far away from the central issues, I will try to explain why:\n\nOne of my primary concerns with academic pursuits is a failure to consider the implication of our profession, research, and practices for the society beyond our own institutional borders.  This might be a surprising criticism of a world that often (though not always) hails the insights of feminism and postmodernism, which invite an acknowledgment of subjective meanings and encourage a self-reflective awareness during research and writing.  However, while this approach may be used within the confines a research project, I often the lament the degree to which we fail to ask important self-reflective questions about research and academia in general.  What is the role of a university?  Of a library?  How do our practices matter in the world beyond the ivory tower?  How might our scholarship influence policy, definitions of truth, or identity?   What are the lines of communication and influence between our universities and the rest of society?   Is our scholarship relevant to someone aside from other academics?   As privileged members of the some 30% of the U.S. population who graduate from college and the even smaller enclave who make their living in academic institutions, I feel we are obligated to ask these questions.\n\nPart of what attracted me to the Praxis program was the use of crowdsourcing, which I saw as an opportunity to engage in these big questions by bridging the space between the ivory tower and the world beyond.   Not only could a crowdsourcing tool have research potential, it could also help to make us aware of the world outside our own institutions and to (hopefully) keep an eye toward that world as we pursue our own scholarship.   I am still excited about this potential despite our concerns over crowdsourcing.  So here are my thoughts on the three questions listed above:\n\nWill a crowdsourcing project produce useful information for academic pursuits?  Perhaps not in direct manner.  But, it could clue us into important and relevant questions which we can then address in our work, ensuring a certain degree of relevancy between our work and the nonacademic world.\n\nDoes crowdsourcing turn people into dehumanized cogs?  This is such an important question as we endeavor to be responsible and reflective scholars!  The way in which crowdsourcing has been used in the past to harness the energy of the masses makes this a legitimate concern.  However, I think our team is in agreement that we are interested in a project that centers on interpretation rather than using “the crowd” to accomplish a particular project.  Perhaps this implies that we need a new term for this approach---maybe we are not really doing crowdsourcing so much as suggesting that through a collaborative interpretive project we could get a sense of the pulse of a community.   Does this still result in dehumanization of individuals?  I am uncertain—this is definitely a concern which deserves more conversation.\n\nCan a DH project really reach beyond the walls of the academy?  Could it provide a line of communication between professional scholars and others?   I think that the right kind of project with an inviting, playful interface and an approach that is presented in non-specialist language could, in fact, do just that.  However, I agree with some of my team members, that actually reaching beyond the academic community would be challenging.  Reaching beyond the 30% of the population which is college educated might be even more unlikely.  But I am still excited by the _potential_ to do these things, and don’t think that the difficulties they present should prevent us from undertaking them.\n\nI’m sure there are other concerns when it comes to building a tool that depends on crowd participation.   I feel as if I have barely tipped the iceberg on this issue, so comment away.  Should we crowdsource?   What are the benefits?  The pitfalls?  How does this decision relate to the larger mission of our work and the work of academic institutions in general?\n"},{"id":"2012-10-22-prism-project-proposal","title":"Prism Project Proposal","author":"cecilia-márquez","date":"2012-10-22 07:17:20 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"prism-project-proposal","content":"Full disclosure: this list of questions came from a prompt by [David McClure](http://www.scholarslab.org/people/david-mcclure/).\n\nThis also grows out of a [blog post](http://www.scholarslab.org/praxis-program/images-in-prism/) I wrote earlier about my interest in utilizing images in Prism.\n\n_What's the overarching intellectual goal?_\n\nAnalyze what visual cues in photographs trigger regional distinctions for their viewers.  Using photographs from the 1950s-1960s I am curious what images signal the “South” or the “North” in viewers of the photographs.  Specifically I am interested in the relationship between “blackness” and “Southernness” in these images.\n\n_What text/image/file? Which tags?_\n\nI have three images that I would be interested in processing through Prism.  They each portray some form of racial control or a response to that control.  As you can see, some fit easily into our memory of Jim Crow, a man at a segregated water fountain where separate is clearly not equal.  However, I am also interested in including photographs that frustrate our national memory of Jim Crow.  For example, in what region would a Jim Crow sign that refuses “Spanish” or “Mexicans” reside?  Finally, an image like the one below where there are no obvious signs of Jim Crow.  I’m curious what region people will choose in a photograph devoid of obvious regional signifiers.\n\nIn terms of tags I guess I would have a tag that says: Region and like on Facebook you could select a portion of the image and indicate what region you think that it is indicative of and why.  For the photos that include people, I would  have a tag like “subjects” where people can select the person and include a response about who they think this person is, what they are experiencing, why they are being photographed etc.  A last tag would not rest in the physical photograph itself but rather a space for readers to indicate why they thought the photograph was taken, what the political goals were etc.  I guess images don’t fit easily into the highlight model of Prism but I think there could be some fascinating visualizations that come out of this!\n![](https://lh4.googleusercontent.com/L--HV9AUxwcbuPQku-RvizkvEfzNZhVTASU4ErRmmJxTQMmTgwl8bBmUKG4F6BiHeDbpsF14mZvMsvH7OhZl3y9li6dYDjoAy4fxHTKR-OAFktCp-EU2)\n![](https://lh6.googleusercontent.com/4OJi36MMWvpegIPmvSkGV4YGzYAIbJmv2GS3Szh_O4smAJWWGYH3ESL5jsUfzZHtO5HNAKQyFe8Gf2jBioHil9dVljntFOcDY0T32srrAIIN1WhIoEve)\n\n![](https://lh4.googleusercontent.com/oerjGrgUanYZIY7rHRl2E33Z_T4i4YX4ehYWzQTFrm39f0dudfqocSMlq8gnQbvN4vxxHEunoMwmvgYf3Yx4KK-rUJcNziXa_eu51hTcPcRPlDq9scz_)\n\n_Who would participate? In what context? _\n\nI want participation to be as open as possible.  My preference would be to maintain some demographic data of who is participating (ex. age, race, gender, location).  I could imagine, for example, splitting the responses up by region, considering what Northerners consider the South and what Southerners consider the South.  I am not interested in a closed environment setting, rather I think the more people across different regions, education status, ages, genders etc. responded the more interesting the outcomes could be.\n\n_How does it converse (if at all) with existing disciplinary lines of inquiry?_\n\nThere are important historiographical implications for this project, specifically in thinking about the relationship between the cultural production that emerges out of segregation and the subsequent memory of Jim Crow.  The specific interest in the Jim Crow signage as text grows out [Elizabeth Abel’s](http://english.berkeley.edu/profiles/5) recent book [Signs of these Times: the Visual Politics of Jim Crow](http://www.ucpress.edu/book.php?isbn=9780520261839), in which she examines the layers of discursive meaning in varied Jim Crow segregation signs.  [Grace Hale’s](http://www.virginia.edu/history/user/27) work, [Making Whiteness](http://www.amazon.com/Making-Whiteness-Culture-Segregation-1890-1940/dp/product-description/0679776206),  also informs the roots of this project.  She suggests that the rise of modern consumer culture created spaces of conflict in which the “culture of segregation” was formed.  It is intriguing to consider how these spaces of consumption memorialized in photography and subsequently in the national imaginary?  There are obviously many other scholars whose work this intersects with, those are perhaps for a later post.\n"},{"id":"2012-10-23-prism-proposal-against-anonymity","title":"Prism Proposal: Against Anonymity","author":"brandon-walsh","date":"2012-10-23 09:14:53 -0400","categories":["Grad Student Research"],"url":"prism-proposal-against-anonymity","content":"Jeremy asked us to offer a Prism exercise that would take the tool in a new direction. Rather than expanding the tool’s parameters, I would like to think about how we can get a more nuanced understanding of the information it already collects. In what follows I offer a vaporware exercise that we will play in person, suggest some thoughts on the disciplinary and pedagogical questions it might answer, and I conclude with some suggestions for adapting the exercise for digital use.\n\nThe idea: rather than marking multiple categories, all participants mark a single category. Each person has his or her own unique marking color.\n\nThe text: The specific text is not important for this particular idea, but for the sake of the exercise I will use [“The Weary Blues”](http://www.wwnorton.com/college/english/nap/weary_blues_hughes.htm) by Langston Hughes\n\nThe marking: Other(s)\n\nThe research/pedagogical issues at stake: The exercise was designed with pedagogy in mind. In the final visualization, the color constellation tells us who marked what in a clearer way: red for Brandon, green for Cecilia, etc. This is absolutely vital if we are going to use the tool for pedagogical purposes. Rather than submerging the voices into a bundle of data with no way back, this approach allows us to see the overlap and disjunctures between different interpretations more clearly. In the vaporware version, we can’t really do this without pulling the transparencies apart again to see them individually, which destroys the collective image. By restoring clarity to the individual voice, we can more easily open up a conversation with students. What did a particular student mark? Why? How does this relate to the aggregate? How does a particular voice interact with the crowd? These discussions of individual vs crowd work particularly well for this textual example: the poem obsesses over the relation between speaker and musician, poetry and tradition, individual and community, etc. Just as scholars would discuss how the lines blur between individuals and groups of people in the poem, the marking activity can open a meta-discussion about the nature of our own marking activity as an intellectual endeavor – a collaborative one nonetheless comprised of individual gestures.\n\nThere is also a fairly selfish short-term application to this: it simplifies things. As [Gwen](http://www.scholarslab.org/digital-humanities/a-project-for-prism/) mentioned, throwing together eight transparencies all marked for three different things comes across as a big glob of data. Maybe the above approach can declutter things for our short-term training as Prism initiates. In this vaporware experiment, a mark means one thing only—the color tells you who did it. One category might not mean fewer markings, but it would at least be easier for us to process with fewer different systems of meaning on the overhead. I imagine that this (hopefully) simplified exercise is something that we need to just get a sense of what sorts of conversations, interpretations, and gains can come out of Prism at all. In a similar manner, the current Prism visualization only deals with one marking at a time. You sort by marker, and you don’t see all the markings at once.\n\nThe above approach is designed for the vaporware transparency game, so it obviously only works with small groups of people. This makes me wonder if it is even a viable option for working with a “crowd.” It seems very useful for pedagogy, but would such a thing even be feasible with a class of 20 students? I don’t think so. You’d have to get a really big packet of markers and a very nuanced sense of the color pallet. So this is really an idea that needs the technology in order to function. Here is a variation on the idea behind this exercise that would work for the online tool.\n\nUsers mark as usual and go to visualize. Below or next to the list of markers, we have a dropdown menu with a list of the users. Mousing over a particular username causes their markings to appear in the text, allowing you to compare their markings with the crowd’s aggregate. How these individual markers are visualized in relation to the mass is up for debate. One idea to implement this would be to underline each word that the user marks: a really big word not underlined means that the user did not mark it, but a large portion of the crowd did. A small word underlined means that the user marked it, but the crowd did not. This is not a perfect system, so I’m open to ideas. And this doesn’t even begin to broach the question of how the sorts of conversations I imagine coming out of Prism can possibly emerge when users are not all in a single room together. But I do think this larger question of retaining the individual user’s gesture and their markings is an important one – a step towards retaining their voice and even their humanity, in just the sense that [Claire](http://www.scholarslab.org/praxis-program/to-crowdsource-or-not-to-crowdsource/) references.\n"},{"id":"2012-10-24-a-practical-prism-pedagogy-proposal","title":"A Practical Prism Pedagogy Proposal  ","author":"shane-lin","date":"2012-10-24 11:00:07 -0400","categories":["Grad Student Research"],"url":"a-practical-prism-pedagogy-proposal","content":"Everyone's been writing up proposals of new ways to use [Prism](http://prism.scholarslab.org/) and new functionality that can be implemented. One of the really exciting ideas (for me at least) that we've been tossing around collectively has been the idea of linking arguments to supporting evidence. In speaking with my fellow history department TAs, we've noticed that many of our students are doing poorly at uncovering how history literature connects these two things and some are not even sufficiently aware of what constitute evidence. Prism, used for pedagogy, could force a direct and individual engagement with these concepts.\n\nInstead of marking up documents along predetermined themes or to arbitrarily choose new ones, students could be asked to mark particular passages as arguments.Then, they can mark up passages within the document that act as support for the argument. Other students can choose to contribute additional evidence markup for an existing argument or to contribute their own. Instructors can monitor similar theses and marge them dynamically in order to prevent duplication.\n\nIn the end, students can compare what they've highlighted as arguments and evidence against what others have chosen. The resulting aggregate set of arguments and evidence can also readily serve as review materials.\n\nThis scenario diverges from the current implementation of Prism in a few ways. Central to this excercise is the ability to more nimbly compare one's own selections versus that of the group aggregate, but also with other individual selections. Some mechanism to  To help gamify Prism, we can offer statistics on the distance from the mean and the closest and furthest members from a user's selections. This data could then be used to group people with discordant views together for in-class discussion.\n\nAnalysis of these relationships would also be helpful to tease out connections between arguments and between evidence. For example, highlights might help uncover which arguments are tightly connected or else have little to do with each other because of a lack of overlap in evidence. Different types of evidence might also be marked differently, perhaps based on predetermined categories - by its primary or secondary nature, its position in different historiography schools, its type, or the class or gender of its author. Analysis could then show which arguments are most strongly supported by evidence of each type.\n"},{"id":"2012-10-24-not-joking","title":"Not Joking","author":"chris-peck","date":"2012-10-24 11:24:54 -0400","categories":["Grad Student Research"],"url":"not-joking","content":"Conventional wisdom holds that nothing is quite as [un-funny](http://www.youtube.com/watch?v=RZ6KWfZz4BY) as a joke explained. I was reminded of this last week when I felt compelled to explain to a class of 18-22-year-old music theory students why [d-minor is the saddest of all keys](http://www.youtube.com/watch?v=NgViOqGJEvM).\n\nBut could it also be the case that the funniest joke results from the over-explanation of a joke which was never actually very funny to begin with? To test this hypothesis, I propose the following social-annotation-with-transparencies game: The text is the score to the fourth movement of Haydn's Op.33 No. 2 quartet—known as \"The Joke.\" We'll listen to a recording two or three times. Each participant has an identical loop of string that they use to enclose the area on the score that they consider most humorous. Shapes of string loops are traced on transparencies and overlaid.\n"},{"id":"2012-10-24-plastic-debacles","title":"Plastic Debacles","author":"jeremy-boggs","date":"2012-10-24 07:00:07 -0400","categories":["Research and Development"],"url":"plastic-debacles","content":"The Scholars' Lab has a Makerbot Replicator with dual extruders, and it's become a Praxis Program favorite. We've printed musical instruments, bracelets, animals, puzzles, and even a topographic map. If you get into 3D printing, chances are pretty high you're gonna have some failed prints. Things happen in the course of tinkering with models and print settings. Temperature fluctuations, drafts around the printer, the level of the build plate, the speed of the print, all kinds of stuff can affect the outcome. Debugging this stuff is a learning experience itself, and I find myself learning a little more each time a print gets messed up.\n\nSo here's a gallery of our plastic debacles. Lovely, lovely debacles:\n\n[gallery link=\"file\" ids=\"6572,6571,6569,6567,6566,6565,6564,6563,6562\"]\n"},{"id":"2012-10-24-reading-socially","title":"Reading Socially","author":"katina-rogers","date":"2012-10-24 12:52:20 -0400","categories":["Grad Student Research"],"url":"reading-socially","content":"As the Praxis team has been discussing the values and drawbacks of anonymity (or pseudonymity) in crowd-sourced interpretation, I've been thinking about what it means to read socially -- more specifically, what we rely on from other readers, and what we provide to other readers.\n\nLike most everyone, I spend a significant portion of my time reading, in many different formats -- from work-related research in the form of journal articles, news, blog posts, and popular media, to the professional/personal world of Twitter, to purely personal novels and side projects. The network of people around me has a strong effect on my reading patterns. I pick up book based on recommendations of friends with similar taste; I click countless links a day because someone I know or find interesting has mentioned them.\n\nUnfiltered, the volume would be simply overwhelming, so I rely on a variety of cues to decide where to direct my attention. Part of this filtering process has to do with what I'm thinking about or looking for. Another factor is the expertise or particular qualifications of the person recommending something (depending on the context, \"qualifications\" can mean anything from general brilliance, to specific topic knowledge, to an ability to spot something funny).\n\nSo, the background of the person influences my desire to read something, and also affects how I read it. These pointers are incredibly useful. At the same time, some social reading features -- I'm thinking particularly of the Kindle's \"Popular Highlights\" -- drive me completely crazy. Why do some aspects of social reading help me to read more effectively, while others distract me?\n\nA lot of what matters seems to be the identity and role of the person influencing my reading, and what that cues me to look for in the text. The anonymity of Kindle's Popular Highlights renders the annotation a meaningless distraction -- much like picking up an overly-highlighted used book. On the other hand, annotations from a respected colleague can add a great deal of depth to my own reading experience.\n\nPrism's current iteration isn't meant to guide the reader as she reads -- but rather, to aggregate the reading experiences of many users to bring new meaning to the text. What would it look like if Prism could not only capture readers' interpretations, but also enable readers to be guided toward different or deeper readings based on the interpretations of others? The team has been talking about how different ways of sorting the kinds of markings that people make might make participation more meaningful, and I'm looking forward to seeing where te discussion leads.\n"},{"id":"2012-10-25-crowdsourcing-for-profit-and-pleasure","title":"Crowdsourcing for Profit and Pleasure","author":"shane-lin","date":"2012-10-25 05:27:14 -0400","categories":["Grad Student Research"],"url":"crowdsourcing-for-profit-and-pleasure","content":"This post is in response to Claire's thoughtful writeup on [Crowdsourcing](https://www.scholarslab.org/praxis-program/to-crowdsource-or-not-to-crowdsource/), which I think raises and tries to answer some absolutely salient questions. Originally, I think the intent was to wrote a simple \"con\" piece, but since Claire - and Brandon in reply - have taken such nuanced and sophisticated positions, I suppose that I'll have to do the same lest I appear a rube (don't be mean, Cecilia).\n\nI'll switch up the order to keep things interesting.\n\n_**Does crowdsourcing turn people into dehumanized cogs? **_\n\nWe need to first break down what crowdsourcing means. Crowd, used here, is like one of those weasel-words that my students use when they don't want to get into specifics - like \"the people\", or (occasionally) \"the rabble\". Crowdsourcing is the delegation of tasks to an arbitrary collection of individuals. Typically, this is done to harness the quality which they, as a faceless wall of flesh, share in common: being humans instead of computers. The problems where being humans are useful are therefore generally those that are impossible (maybe in [one](http://en.wikipedia.org/wiki/Turing_machine) of the [two](http://en.wikipedia.org/wiki/Turing_test) Turing senses) or unfeasible for computers to do. Definitionally, if we care about who people are, it's not crowdsourcing. It's just, you know, sourcing.\n\nCrowdsourcing strips away the individual, as a feature. Does that mean that it dehumanizes? I guess that depends. In the Mario Salvio New Left sense, I guess it would. But clearly, in social science disciplines where the individual is not so important as the aggregate study of humanity, this is not the case at all. Which is why, I suppose, crowdsourcing platforms like Amazon Mechanical Turk have been [widely used](http://experimentalturk.wordpress.com/) for experiments in these fields.\n\n**_Will a crowdsourcing project produce useful information for academic pursuits?_**\n\nI don't know.\n\nBut I think that trying to approach this question agnostic to discipline is  a mistake. Interdisciplinary is a laudable goal, but in the end, it's hard to deny that there are substantial differences not just in approach but in purpose that divide the various humanities. Crowdsourcing is a tool; it makes as much sense to consider its worth to \"humanities\" as determining the value of a cyclotron to \"science\".\n\nIn my own field of history, as my fellows have heard endlessly, I feel that crowdsourcing is not very useful for research because of the inescapable fact that obtaining any kind of data from  \"the crowd\" happens in the present and not in the past. This is the kind of inescapable statement as  \"cyclotrons aren't useful for biology because its interactions are not on a macro scale.\" Clearly, these may not be concerns for disciplines that aren't history or biology. But that's the point, I suppose.\n\n**_Can a DH crowdsourcing project really reach beyond the walls of the academy?_**\n\nDo we mean that the crowd is outside of the ivory tower or that the users are?\n\nFor the former, I think that there are certainly many interesting crowdsourced transcription projects (Bentham, Old Weather), that have found success and wide appeal. One really interesting academic (though not humanities) use of crowdsourcing that's gotten alot of attention of late is the protein-folding game FoldIt ([http://fold.it/portal/](http://fold.it/portal/)). FoldIt, and I guess [Ender's Game](http://en.wikipedia.org/wiki/Ender's_Game), really illuminate the power of gamification in attracting an active and broad audience for such esoteric subjects as viral pathology and intersteller genocide.\n\nCecilia and I actually had a brief discussion returning personal statistics in Prism, like calculating a particular user's distance from the mean or the ability to show shortest-distance and farthest-distance users. That's one step toward, if not gamification, then individualized feedback. Of course, this discussion naturally led to talk of using this metric for online dating.\n\n\"Hey baby, I see that we both highlighted the same sentence in The Raven...\"\n"},{"id":"2012-10-25-prism-proposal-cultural-bundles","title":"Prism Proposal: Cultural Bundles","author":"claire-maiers","date":"2012-10-25 05:08:38 -0400","categories":["Grad Student Research"],"url":"prism-proposal-cultural-bundles","content":"Prelude to a Proposal: As both [Cecelia](http://www.scholarslab.org/praxis-program/digital-humanities-growing-pains/) and [Gwen](http://www.scholarslab.org/digital-humanities/a-project-for-prism/) have indicated, our conversations from the last week have been marked by a lot of confusion, tension, and doubt about our project and goals.  In an attempt to bring some clarity back, we are each posting a potential project for Prism this week.  As I sat down to write about one such project, I began by reading through project proposals of some of my fellow team members (or  _Praxisers_?).  Given the frustration of the past week, I was surprised to discover that there is a notable amount of agreement over our concerns about Prism and possible directions that our project could take.  As I walk you through my proposal, I will touch on what I see as some emerging themes.\n\nApprehensions about a Prism Proposal:  I am extremely interested in how people make meaning from a text.  However, I worry that deploying Prism in its present form (or a form that only allows for the highlighting of certain categories) might not allow for data collection that captures the process of meaning making.  Like [Gwen](http://www.scholarslab.org/digital-humanities/a-project-for-prism/), I share a concern about the interpretation of text without a consideration of the text as a whole.  In order to get at the [polysemic](http://en.wikipedia.org/wiki/Polysemy) nature of a text, we need to know more than how individuals interpret specific words or phrases.  We need to know what message the reader pulls out of the text as a whole.  In order to apply a more structural approach and to understand how the internal relationships within a text generate meaning, we would need to be able to mark more than categories.  We would also need to indicate how various parts of the text relate to each other.  Despite these concerns, I offer a proposal which could be conducted within the current Prism premise by including a few additions to the processes of analysis and visualization.  Though the particular text is not important, I have included a small excerpt from a wedding ceremony as a test case.\n\nA Question for Prism: In cultural studies, we often talk about the way cultural references come bundled together.  We can think of this in two ways.  In the first, the e[lective affinity](http://www.sociologyencyclopedia.com/public/tocnode?id=g9781405124331_yr2011_chunk_g978140512433111_ss1-27) between two theoretically distinct worldviews causes the two to become intertwined (for example, scholars often discuss the way in which [Protestantism and capitalism](http://en.wikipedia.org/wiki/The_Protestant_Ethic_and_the_Spirit_of_Capitalism) have a shared ethic of work, allowing for an easy expansion of capitalism in the protestant world).   However, bundles also exist within the minds of individuals.  People can hold associations in their heads between beliefs and concepts which are not necessarily connected in practice.  For example, when someone reads the word “capitalism,” it may also signal concepts such as “the West,” “democracy,” “freedom,” or “exploitation.”  In another example, the word \"love,\" may bring concepts such as \"marriage,\" \"commitment,\" \"fate,\" or \"betrayal\" to mind.  One use for Prism would be to assist scholars in identifying these bundles and figuring out how they frame the interpretive process.\n\nTwo-Stepping: In order to do this, I think there would need to be a two-step process.  Like other Praxisers, I am concerned about imposing my own categories upon the interpretive process.  So, I suggest that specific Prism projects would benefit from a more open stage.  However, as our transparency activity from last week demonstrated, completely free form interpretation failed to produce anything sensible in the aggregate.  To solve this problem, I would introduce some constraints, asking users to highlight phrases and label them with a single word of their choosing.  Either a researcher or a semantic linguistic program would then work to develop a limited number of categories relevant to the text.  In stage two, the same text would be posted for markup again with the user-generated categories.  This process mimics a well-established interpretive processed used in the social sciences (called [Grounded Theory Coding](http://en.wikipedia.org/wiki/Grounded_theory#Strauss_.26_Corbin.27s_approach): see [Charmaz](http://books.google.com/books?id=v1qP1KbXz1AC&printsec=frontcover&dq=charmaz&hl=en&sa=X&ei=HgmIUKnIBebv0gHD3YDYBQ&ved=0CC8Q6AEwAA) and Strauss and [Glazer](http://books.google.com/books?id=rtiNK68Xt08C&printsec=frontcover&dq=strauss+and+glaser&hl=en&sa=X&ei=OAmIUJO2Lof40gH80ICABQ&ved=0CCwQ6AEwAA#v=onepage&q=strauss%20and%20glaser&f=false)).   However, a tool like Prism allows for an innovative modification: the initial categories to be developed by users (those who actually make meaning), rather than the researcher.\n\nAnalysis and Visuals: As indicated by several other [Praxisers](http://www.scholarslab.org/praxis-program/prism-proposal-against-anonymity/), I think it would be crucial to keep each user’s interpretation autonomous.  Although I think some sort of aggregate interpretation is important, it would also be necessary to be able to sort interpretations according to users.  In order to see how cultural bundles might influence meaning, I would want to be able to answer two key questions.  What is the actual content of the cultural bundles that influence the interpretation of this text?  Can a user’s interpretation of a particular phrase indicate how that user will interpret subsequent phrases?  Answering these questions would require that I can sort the aggregate interpretation to show only certain users.  For example, I might want to see a visualization of all the users’ interpretations who marked a particular phrase with a particular category.   Did all those users who indicate that \"one true love\" is about fate mark the text in similar ways?  Do they tend to make the same interpretation of the phrase \"lawfully wedded?\"  Are there patterns that let indicate the specific content of cultural bundles related to love and marriage?\n\nWedding Vows:\n\nI, (Bride's Name), take you, (Groom's Name),\nto be my  lawfully wedded husband,\nsecure in the knowledge that you will be\nmy constant friend,\nmy faithful partner in life,\nand my one true love.\n"},{"id":"2012-10-26-neatline-omeka-theme-name-winners","title":"Neatline Omeka Theme Name Winners!","author":"jeremy-boggs","date":"2012-10-26 09:28:20 -0400","categories":["Announcements"],"url":"neatline-omeka-theme-name-winners","content":"After extensive deliberations at the Scholars' Lab, we're pleased to announce that we have two winners of the Neatline Omeka Theme Name Contest:Amanda Visconti and the theme name \"Astrolabe,\" and Franky Abbott with the theme name \"Neatscape.\" We'll be getting in touch with Amanda and Franky separately, to get those beautiful Neatline tshirts their way. This means we'll develop and release **two** Omeka themes. (I'll have my work cut out for me, but it's wonderful work.) We'll make those themes compatible with the upcoming 2.0 version of Omeka, so look for those themes shortly after Omeka 2.0 is released!\n"},{"id":"2012-10-30-kindle-prism-pdf-prism","title":"Kindle Prism? PDF Prism?","author":"chris-peck","date":"2012-10-30 07:26:31 -0400","categories":["Grad Student Research"],"url":"kindle-prism-pdf-prism","content":"It turns out that Amazon already does some (very basic) analysis of crowdsourced interpretation. They publish several [lists of most popular highlights](https://kindle.amazon.com/most_popular) from Kindle readers.\n\nApparently this group of readers really thinks the first sentence of Pride and Prejudice is significant for one reason or another. But far more find something highlight-worthy about this line from Catching Fire (The Second Book of The Hunger Games):\n\n\n<blockquote>Because sometimes things happen to people and they’re not equipped to deal with them.</blockquote>\n\n\nThis is a bit like Prism with tens of thousands if not millions of users (the quote above was highlighted by 17784 users at this time of this post) and no constraints on the _meaning_ of highlights—no \"categories.\" The parallel was [noted at least in passing by last year's team](http://www.scholarslab.org/praxis-program/what-ive-learned-from-my-kindle-part-ii-and-other-thoughts-on-prism-and-markers/). Kindle has come up a number of times with the current team so far, and most recently in [Katina's post this week](http://www.scholarslab.org/praxis-program/reading-socially/) about the social aspects of reading. Perhaps it's time to revisit the comparison?\n\nIt would be interesting to try out some of the Prism visualization ideas on such a large pool of annotations. We could also experiment with computational linguistics techniques to make sense out of free text comments attached to highlights by users (it's been proposed that Prism could use free text input too instead of fixed categories). But from what I can tell there's no API that would allow us to work with the data.  The closest I've found is a [tool developed to scrape a single user's highlights from the web](https://github.com/johnpaulhayes/Kindle-Highlights/blob/master/README). But one user wouldn't do us much good. Access to the entire pool of public highlights is what would make this really interesting.\n\nAnother way we could think about overlap with Prism is in the reader software itself. How would this project change if we considered developing Prism as a plugin for existing software already in use by [futuristic readers](http://www.youtube.com/watch?v=n5qXd4D-M0E) such as ourselves? Is there functionality in these apps/platforms that could benefit Prism?\n\nAdobe Reader, for instance, has a wider palette of annotation tools (not just highlighting but free text comments, ovals, translucent boxes, arrows, etc.) that could lead to different user experiences and also different visualizations. Proposal: next week let's try some annotation games with the Praxis team, but using PDFs instead of transparencies. E-mail them to me and I can separate the highlights and overlay them in Illustrator in a few minutes...  An automated tool to merge highlights on PDFs would (I think) be very doable, and could even incorporate some Prismy visualizations.\n"},{"id":"2012-10-30-social-interpretation-repertoire","title":"Social Interpretation Repertoire","author":"chris-peck","date":"2012-10-30 07:24:29 -0400","categories":["Grad Student Research"],"url":"social-interpretation-repertoire","content":"Can approaches from (experimental) music/sound to social interpretation of text shed any light on our thinking about Prism? Let's get the [Praxis Band](https://github.com/scholarslab/praxis/wiki/Praxis-Band) together and find out:\n\n**1. Significant Rhythm Hocket**\n\n**Text:** a [poem](http://www.youtube.com/watch?v=7bW-EvOLFCc), perhaps a poem that has some [repetition](http://ubumexico.centro.org.mx/sound/gysin_brion/Mektoub/Gysin-Brion_Mektoub_05-I-Am-That-I-Am.mp3), text as well as a recorded reading.\n\n**Additional Materials:** a sound playback device with headphones and a musical instrument for each participant. Instrument can be small percussion/found object/otherwise improvised or homemade.\n\n**Instructions:** Mark the one word (or syllable) you find most significant in the poem. Mark each occurrence of that word. Group listens to recorded reading of poem simultaneously (all press play on your iPods at the same time). Devise a sound of appropriate length on your instrument. Make this sound (always the same sound) each time you hear your word.\n\n**2. Inverse Rally**\n\n**Text: **a political speech.\n\n**Materials: **Sound recording equipment and sound playback device with speaker for each participant. (Or a multichannel playback system with one channel per participant.)\n\n**Instructions: **each participant rehearses the speech and records their own performance. Recordings are assembled and played back simultaneously on a loop through a multi-channel sound installation. Correlation between sound file data (or sonograms? normalized in some fashion?) is visualized.\n\n**3. Laugh-o-meter**\n\n**Text: **A recorded text that's meant to get laughs. Perhaps a comedy record?\n\n**Additional Materials: **Sound recording equipment. A situation in which group can be recorded together as well as separately. Playback with headphones for each participant.\n\n**Instructions:** Participants' vocal reactions are recorded while listening to comedy. Ideally they are recorded with a high quality microphone in a quiet space (like a recording studio) so that subtle subvocal reactions are recorded as well as laughing proper. Group is also recorded while listening to the text together for comparison. Sound recordings are correlated with the text and visualized. And, of course, we compile the separately recorded laugh tracks, mix them, and listen to them without the original comedy.\n"},{"id":"2012-11-01-neatline-release-omnibus-edition","title":"Neatline Release Omnibus Edition","author":"eric-rochester","date":"2012-11-01 07:32:18 -0400","categories":["Announcements"],"url":"neatline-release-omnibus-edition","content":"For the next and final round of the [Omeka](http://omeka.org/) plugin release parade, I’m pleased to announce minor or patch releases for all [Neatline](http://neatline.org/) plugins. Neatline is an [Omeka](http://omeka.org/) plugin that helps you tell stories in time and space from your Omeka collection. For more information, see our [original announcement](http://www.scholarslab.org/announcements/announcing-neatline/) or the [Neatline site](http://neatline.org/).\n\n\n\n\nFor now, these are maintenance releases that patch up bugs and improve performance. Stay tuned over the course of the next couple months, though, for news about some exciting new directions for Neatline. We're hard at work on a new round of development that's going to migrate Neatline over to Omeka 2.0, make it a lot easier to use Neatline in a multi-user classroom environment, and make it possible to connect paragraphs, sentences, and words in TEI texts to specific locations in Neatline exhibits.\n\n\n\n\n\nHere’s some information about the bug-fix releases.\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/11/Screen-shot-2012-11-01-at-11.07.36-AM.png)](http://www.scholarslab.org/announcements/neatline-release-omnibus-edition/attachment/screen-shot-2012-11-01-at-11-07-36-am/)\n\n\n\n## [Neatline](http://omeka.org/add-ons/plugins/neatline/)\n\n\n\n\n\nSummary\n\n    The main changes here are bug fixes. We also now store the GIS feature data as [KML](http://en.wikipedia.org/wiki/Kml) instead of [Well-Known Text](http://en.wikipedia.org/wiki/Well-known_text).\n\n\nVersion\n\n    1.1.1\n\n\n[Commit Details](https://github.com/scholarslab/Neatline/compare/1.0.0...1.1.1)\n\n    \n\n\n[Plug-in Page](http://omeka.org/add-ons/plugins/neatline/)\n\n    \n\n\n[On Github](https://github.com/scholarslab/Neatline)\n\n    \n\n\n[Issues](https://github.com/scholarslab/Neatline/issues)\n\n    \n\n\n\n\n\n\n## [NeatlineMaps](http://omeka.org/add-ons/plugins/neatlinemaps/)\n\n\n\n\n\nSummary\n\n    The main changes here are bug fixes and adding internationalization support.\n\n\nVersion\n\n    1.0.1\n\n\n[Commit Details](https://github.com/scholarslab/NeatlineMaps/compare/1.0.0...1.0.1)\n\n    \n\n\n[Plug-in Page](http://omeka.org/add-ons/plugins/neatlinemaps/)\n\n    \n\n\n[On Github](https://github.com/scholarslab/NeatlineMaps)\n\n    \n\n\n[Issues](https://github.com/scholarslab/NeatlineMaps/issues)\n\n    \n\n\n\n\n\n\n## [NeatlineTime](http://omeka.org/add-ons/plugins/neatlinetime/)\n\n\n\n\n\nSummary\n\n    The main changes here are extra configuration options, bug fixes, and internationalization support.\n\n\nVersion\n\n    1.1.0\n\n\n[Commit Details](https://github.com/scholarslab/NeatlineTime/compare/1.0.0...1.1.0)\n\n    \n\n\n[Plug-in Page](http://omeka.org/add-ons/plugins/neatlinetime/)\n\n    \n\n\n[On Github](https://github.com/scholarslab/NeatlineTime)\n\n    \n\n\n[Issues](https://github.com/scholarslab/NeatlineTime/issues)\n\n    \n\n\n\n\n\n\n## [NeatlineFeatures](http://omeka.org/add-ons/plugins/neatlinefeatures/)\n\n\n\n\n\nSummary\n\n    The main changes here are bug fixes, [KML](http://en.wikipedia.org/wiki/Kml) data, and internationalization support.\n\n\nVersion\n\n    1.1.0\n\n\n[Commit Details](https://github.com/scholarslab/NeatlineFeatures/compare/1.0.0...1.1.0)\n\n    \n\n\n[Plug-in Page](http://omeka.org/add-ons/plugins/neatlinefeatures/)\n\n    \n\n\n[On Github](https://github.com/scholarslab/NeatlineFeatures)\n\n    \n\n\n[Issues](https://github.com/scholarslab/NeatlineFeatures/issues)\n\n    \n\n\n\n\n\n\n* * *\n\n\n\n\nFor all of these, you can download the plugins from their plugin pages and provide feature suggestions or report problems on their issues pages. For general questions and help, feel free to ask on the [Omeka forums](http://omeka.org/forums/).\n"},{"id":"2012-11-05-outside-the-pipeline-from-anecdote-to-data","title":"Outside the Pipeline: From Anecdote to Data","author":"katina-rogers","date":"2012-11-05 05:24:20 -0500","categories":["Announcements"],"url":"outside-the-pipeline-from-anecdote-to-data","content":"_I gave the following presentation at [SCI's recent meeting on rethinking graduate education](http://uvasci.org/current-work/graduate-education/rethinking-grad-ed-oct-2012/). It was the first time I've publicly discussed results from the [study on career preparation in humanities graduate programs](http://mediacommons.futureofthebook.org/alt-ac/who-we-are) that I've [written about previously](http://katinarogers.com/2012/07/10/announcing-a-new-sci-study-on-alternative-academic-career-paths/). I was honored to discuss the topic with our extremely knowledgeable group of [participants](http://uvasci.org/wp-content/uploads/2012/09/Participants_22OCT12.pdf), and the thoughtful questions and comments that the talk generated will inform my thinking as I work toward a more formal report and analysis. I would welcome additional comments and questions.\n\nA [PDF of the presentation](http://uvasci.org/wp-content/uploads/2012/09/SurveyReport_22OCT12_web.pdf) is also available, and has been cross-posted to [SCI's website](http://uvasci.org/current-work/graduate-education/rethinking-grad-ed-oct-2012/) and [my personal site](http://wp.me/p2CaGd-cZ)._\n\n[![](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.001.jpg)](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.001.jpg)\n\nI’m thrilled to have the opportunity to present some of the early findings from the Scholarly Communication Institute’s recent study on perceptions of career preparation in humanities graduate programs.\n\nThe impetus for this study came from recommendations made at [SCI’s ninth summer meeting in 2011](http://uvasci.org/past-institutes/new-model-scholarly-communication/), where rethinking graduate education emerged as one of the critical priorities for the current humanities landscape. The study complements the series of meetings SCI is hosting this year and next, of which this meeting is the first. The primary goal of the study is to move from anecdote to data in the conversation about alternative academic careers and career preparation, in hopes of providing a body of data that can help support programs wishing to modify their graduate curricula.   We finished collecting data at the beginning of October, so the analysis is not complete, but already raises some provocative questions.\n\nWe were very pleased with the number of responses that we received. At the same time, the response rate also highlighted an important discrepancy.\n\n[![](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.002.jpg)](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.002.jpg)\n\nThe study included two surveys. The primary survey targeted people with advanced humanities degrees who self-identify as working in alternative academic careers. (A somewhat loose definition, to be sure -- I’ll discuss our methodology in a moment.) A second, shorter survey targeted employers that oversee one or more employees with advanced humanities degrees.\n\nWe set an initial goal of 200 responses on the main survey, and 100 on the employer survey. We were blown away by the responses to the main survey, which totaled nearly 800 when we closed it, for almost four times our goal. The employer survey, however, attracted far fewer responses, totaling around 80. This is a significant finding in itself, as it shows a pronounced disconnect between the motivations of job seekers compared to employers. Any recommendations SCI makes must keep this discrepancy in mind.\n\nI’d like to jump straight into some of our findings, many of which will not come as a surprise -- but again, the goal was to get numbers to back up the general sense that many of us have about these questions.\n\n[![](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.003.jpg)](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.003.jpg)\n\nFirst, a large majority of students enter graduate school expecting to pursue careers as professors -- a total of 74%. What is perhaps more interesting is their level of confidence: of that 74%, 80% report feeling fairly certain or completely certain that this was the career they would pursue. These expectations are not at all aligned with the current realities of the academic job market.\n\nWhat this signals to me is that we are failing at bringing informed students into the system. This raises a few questions:\n\n--First, whose responsibility is it to help incoming students understand their postgraduate options? This is a bit outside of the scope of this particular meeting, but it becomes our concern when students enter without the knowledge that they need.\n--Second, what is the role of faculty and advisors relative to uninformed students?\n--Third, and more speculative, should post-graduate planning play into admissions in any way? Is there an ethical responsibility here, especially considered in conjunction with increasing student debt? Put differently, should departments be admitting students -- particularly if funding is limited or unavailable -- if they do not understand their post-graduate options?\n\nDeepening the problem, students report receiving little or no preparation for careers outside the professoriate, even though we’re at a moment when the need for information about a variety of careers is most acute.\n\n[![](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.004.jpg)](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.004.jpg)\n\nOnly 18% reported feeling satisfied or very satisfied with the preparation they received for alternative academic careers. The responses are rooted in perception, so there may be resources available that students are not taking advantage of -- but whatever the reason, the bottom line is that students do not feel that they are being adequately prepared. Again, this probably comes as no surprise, but we have significant room for improvement.\n\nThis raises additional questions:\n\n--Are faculty adequately prepared to provide the kinds of advice that students need?\n--Should they be?\n--If it’s not the faculty’s role, then whose role is it? Perhaps alumni or third-party service providers could fill the gap; if so, what are the trade-offs of outsourcing these kinds of preparatory roles to organizations or individuals outside of departments?\n\nAlong with questions that asked people to choose from pre-selected options, we also included a number of open-ended questions. The survey tapped into what can be an intensely emotional topic, and the wide range of responses we received suggests that people felt comfortable being candid.\n\n[![](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.005.jpg)](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.005.jpg)\n\nThe variety of emotions expressed in open-ended responses varied from optimistic and happy, to bitter and resentful. Many people report feeling betrayed.\n\nBelow is another sampling of the kinds of responses we received in the open-ended questions. We’ll be doing more systematic analysis of these responses in the weeks ahead, but for the time being, this will give you an idea of the kinds of reactions and reflections that our respondents provided.\n\n[![](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.006.jpg)](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.006.jpg)\n\nWe received a wide range of practical suggestions, too, such as offering more one-off workshops; including short credit or non-credit courses; and connecting students with alumni working in varied positions. It’s worth noting that while many were skeptical about even the possibility of creating a meaningful cultural change, they emphasized that for sustainable change to occur at all, it is important that it comes from within existing structures if it is to be perceived as valid.\n\nOne thing seems clear: the persistent myth that there is nothing but a single academic job market available to graduates is damaging, and extricating graduate education from the expectation of tenure-track employment has the potential to benefit students, institutions, and the health of the humanities more broadly. However, as long as norms are reinforced within departments -- by faculty and students both -- it will be difficult for any change to be effective.\n\n[![](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.007.jpg)](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.007.jpg)\n\nLow tenure-track employment rates are not a new problem, but as the responses on the previous slides show, departments are not succeeding at providing accurate and realistic information to their students.\n\nPerceived reputational risk is a significant roadblock to increased transparency regarding post-graduate career paths. A common refrain among the respondents was a call to collect and publicize employment data. However, departments have little incentive to collect this information, and even less motivation to make it public, especially if they think it makes them appear unfavorable relative to peer departments.\n\nAre departments the only groups who can collect this kind of information? If they are, how can we change the incentives such that departments find it advantageous to publicize all types of employment among their graduates?\n\nTurning now to the employer survey: while we haven’t yet done rigorous analysis on the qualitative responses, I did want to give a taste of the [kinds of things employers indicate they would like to see](http://voyant-tools.org/tool/Cirrus/?corpus=1349209004700.8841&query=&stopList=1349209160421tm&docIndex=0&docId=d1349152549808.99d49c85-d13a-55c1-5c29-4962216214bf).\n\n[![](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.008.jpg)](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.008.jpg)\n\nFirst, some specific skills come up frequently, including project management, the ability to work with and manage people, collaboration, and written and verbal communication with a variety of audiences.\n\nIn addition, employers mention a number of broader aptitudes, like a commitment to public engagement, general work experience outside of academia, and an ability to adjust to the culture of different types of workplaces. Many employers placed high value on these employees’ understanding of academic structures and environments, but in order to serve as the valuable cultural translators that they could be, employees with graduate training also need to be sensitive to the ways in which academic environments differ from other workplace cultures. For those that graduate without limited (or no) outside work experience, the gap can be very challenging to bridge.\n\nWhile I’ve only scratched the surface of the study’s findings, I’d like to shift gears in order to highlight what we hope will be the first of many concrete actions to come of this study: the development of the Praxis Network.\n\n[![](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.009.jpg)](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.009.jpg)\n\nThis is a network of several existing programs that are focusing on innovative models of methodological training, along the lines of the [Praxis Program](http://praxis.scholarslab.org/) at the University of Virginia. We anticipate that many programs that want to make changes will want to look to existing models for guidance, and by highlighting a handful of differently-inflected programs, we can bring together some patterns and commonalities among them, while also underscoring the unique idiosyncrasies of each -- the strong individuals providing leadership; the particularities of an institution; the available infrastructure; the funding model; and so on. In addition to sharing information with the public, we hope that the network will enable increased possibilities for communication and collaboration among the participants of each program.\n\nIn addition to the Praxis Program, we are currently working with Ethan Watrall at the [Cultural Heritage Informatics Initiative](http://chi.anthropology.msu.edu/); Matt Gold with the [CUNY Digital Humanities Initiative](http://cunydhi.commons.gc.cuny.edu/); Claire Warwick and Melissa Terras at [UCL’s Centre for Digital Humanities](http://www.ucl.ac.uk/dh/courses/mamsc); and Bill Pannapacker with the [Hope College Mellon Scholars](http://www.hope.edu/academic/mellon/) program. The participating programs may fluctuate somewhat, but the intention is to keep the network small -- more as a showcase than a comprehensive directory.\n\nI’d like to circle back briefly to discuss our methods. First, the [study had two main phases](http://mediacommons.futureofthebook.org/alt-ac/who-we-are): so far, I’ve focused on the second, confidential phase. The first phase was public.\n\nIn order to scope out the terrain of individuals to include in the survey, the first phase involved creating a [public database](http://altacademy.wufoo.com/reports/who-we-are/) where people comfortable enough to publicly identify as “alt-ac” practitioners could add their names to form a loose community of peers.\n\n[![](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.010.jpg)](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.010.jpg)\n\nWe built the database within the framework of the [#alt-academy](http://mediacommons.futureofthebook.org/alt-ac/) project in order to leverage the energy of existing conversations. “Alt-ac” may not be a perfect moniker, but it has created a space to talk about careers that are not quite what most people envision as academic careers (within the professoriate), but that are not completely outside the academic sphere, either. Many people have found it to be an incredibly useful umbrella term, and have used it to talk about the kinds of intellectually stimulating careers that can be found outside the professoriate.\n\nWe were pleased with the initial turnout, and found that people were engaging more deeply than expected. It’s worth noting that even though the database has been open for a much longer period of time than the survey was (and it [remains open to new entries](http://altacademy.wufoo.com/forms/who-we-are/) now), far fewer people participated in the public space than in the confidential survey space. To me, this suggests that there is still a sense of discomfort -- and even shame -- about having pursued a job outside the traditional pipeline.\n\nOnce we launched the surveys themselves, we used this public group as an initial body of potential respondents. Because we were working with an unknown population, our subsequent distribution focused on “opt-in” strategies—social media, word of mouth, listervs, and traditional media coverage. While this method has definite weaknesses, we hoped to learn something not only from the content of the responses, but from the number and type of respondents.\n\nOne reason this study was important because even though the topic is deeply connected with other persistent issues in higher education, there were significant gaps in the data available from previous studies.\n\n[![](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.011.jpg)](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.011.jpg)\n\nOne of the most closely aligned studies was completed just recently by the Council on Graduate Schools and the Educational Testing Service. The study, _[Pathways Through Graduate Schools and Into Careers](http://pathwaysreport.org/)_, examines current and former graduate students’ career expectations, their awareness of career opportunities, and the actual career paths they pursued. However, the study focused on a broad range of disciplines, which means that it could not go into much depth on concerns that are particular to the humanities. Further, the data is unpublished, so at least at this point, it is not possible to disaggregate the humanities respondents from the STEM respondents.\n\nA 1996 study by Maresi Nerad and Joseph Cerny at the University of Washington, _[PhDs Ten Years Later](http://depts.washington.edu/cirgeweb/c/research/phd-career-path-surveys/phds-ten-years-later/)_, focused on similarly relevant questions. However, the only humanities discipline represented in their data was English. In addition, the study excluded people who left their programs before completing the degree, and because so many people exit their programs if they decide not to pursue an academic career, we wanted to include their perspectives. Of course, the data is now more than 15 years old, and the cohort graduated more than 25 years ago, making it overdue for an updated look.\n\nFinally, we will build off of information collected by broad surveys like the _[Survey of Earned Doctorates](http://www.nsf.gov/statistics/sed/)_, which provides useful baseline demographics but limited depth.\n\nThe data from the study does have some significant limitations:\n\n[![](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.012.jpg)](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.012.jpg)\n\nWe had limited data to use as a foundation or control, as I just mentioned; we were working with a population with fuzzy boundaries; and we relied on self-identification and self-reporting. For all of these reasons, the results should be considered more exploratory than definitive, and the respondents cannot be considered a representative sample. We see this survey as an important initial step, and we hope others will build on it.\n\n[![](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.013.jpg)](http://katinarogers.com/wp-content/uploads/2012/11/SurveyReport_22OCT12_web.013.jpg)\n\nWe still have a good deal of work ahead of us, and because we want this survey to be maximally useful to our partners in humanities centers and digital humanities centers, as well as the broader humanities ecosystem, I’d welcome input in a number of areas. In the weeks and months ahead, we’ll be engaging in deeper analysis of the data; we’ll be conducting follow-up interviews by phone and email with a number of participants who indicated their willingness to do so; and we’ll be continuing the conversation in more depth at subsequent SCI meetings. We’ll eventually be making recommendations based on the data analysis. Finally, we’ll be publishing a final report in partnership with the [Council on Library and Information Resources](http://www.clir.org/), and we’ll also publish the data so that others can work with it.\n\nAs the discussion on rethinking methodological training continues, I hope you’ll keep this study in mind and share your thoughts with me on how it can best serve its audiences. Thank you!\n\n_References_\nCouncil of Graduate Schools. Pathways Through Graduate School and Into Careers. 2012. <[http://pathwaysreport.org/](http://pathwaysreport.org/)>\n\n“Doctorate Recipients from U.S. Universities: 2010.” Based on data from the Survey of\nEarned Doctorates. National Science Foundation, June 2012. <[http://www.nsf.gov/statistics/sed/](http://www.nsf.gov/statistics/sed/)>\n\nNerad, Maresi, and Joseph Cerny. “PhDs: Ten Years Later.” Center for Innovation and Research in Graduate Education, University of Washington; 1996. <[http://depts.washington.edu/cirgeweb/c/research/phd-career-path-surveys/phds-ten-years-later/](http://depts.washington.edu/cirgeweb/c/research/phd-career-path-surveys/phds-ten-years-later/)>\n\nNerad, Maresi, and Joseph Cerny. “From Rumors to Facts: Career Outcomes of English PhDs.” ADE bulletin 32.7 (1999): 11. <[http://www.mla.org/bulletin_124043](http://www.mla.org/bulletin_124043)>\n\n_Photo Credits_\nSlide 1: “[Pipeline](http://www.flickr.com/photos/stigwaage/3218127924/)” by stigwaage\nSlide 7: “[Pencils](http://www.flickr.com/photos/ellebeere/4605043024/sizes/l/in/photostream/)” by Elle *\nSlide 11: “[He Didn’t ‘Mind the Gap’](http://www.flickr.com/photos/scottrsmith/4950869263/)” by Scott Smith\nSlide 12: “[Fenced In Part 2](http://www.flickr.com/photos/gomattolson/4821079720/)” by gomattolson\n"},{"id":"2012-11-06-teaching-git","title":"Teaching Git","author":"eric-rochester","date":"2012-11-06 08:38:22 -0500","categories":["Grad Student Research"],"url":"teaching-git","content":"In Praxis, we just finished covering [Git](http://git-scm.com/). Everyone seemed to catch on pretty well, so I thought I’d write a bit about my thought process as I was planning the sessions. There were a few principles I tried to keep in mind:\n\n\n\n[caption id=\"\" align=\"alignright\" width=\"240\"]![](http://farm1.staticflickr.com/10/11250506_58dee63095_m.jpg)Photo by [The Rocketeer](http://www.flickr.com/photos/kt/11250506/), [CC BY-NC-ND 2.0](http://creativecommons.org/licenses/by-nc-nd/2.0/)[/caption]\n\n\n\n**Repeat ourselves.** Rather than work on something new, we repeated Jeremy’s lessons on HTML and CSS, except that where he went into detail on HTML and skipped over Git, we went into detail on Git and skipped over HTML.\n\n\n\n\nThis meant that—although the project we were working on was a web page—everyone copied-and-pasted the exact same content for every page and commit. I [posted](https://raw.github.com/erochest/git-play/edd6619718f815203653cfd927ac11ffbac6f0ed/index.html) [links](https://raw.github.com/erochest/git-play/36ee68cb09a29d73f570fce0a6346d1dd67f60a1/index.html) to [raw](https://raw.github.com/erochest/git-play/f317f0b2cebf4f17381b7a8d493399eafb75183f/index.html) [HTML](https://raw.github.com/erochest/git-play/82d683e38908a6bc1ddcc66b068c76235c649965/index.html) [pages](https://raw.github.com/erochest/git-play/cbd3827be861f9fe6e6d1de48ef425fb09cc347b/index.html) that everyone could copy into their text editors. We then examined the changes and committed them in Git.\n\n\n\n\nThis meant that the task we were performing was a little familiar so no one had to think about it. Instead they could focus on Git and the new concepts we were encountering there.\n\n\n\n\n**Repeat ourselves.** I tend to get a bit abstract when I’m explaining things, but I was careful to keep things concrete. I stopped to explain constantly.\n\n\n\n\nHowever, I didn’t expect that the explanations would make sense until everyone had gone through them in practice several times. At one point, I even told everyone that I wasn’t going to ask if they had questions, because I knew that they did. And I wasn’t going to answer them.\n\n\n\n\nEnlightenment would come through use.\n\n\n\n\n(I should mention that this is why I’m not actually posting the tutorial itself: the guided practice _was_ the tutorial.)\n\n\n\n\n**Repeat ourselves.** In different media.\n\n\n\n\nI explained the task, and we did them. I explained the concepts. I also kept two diagrams: one of the working area, staging area, and committed repository and one of the commit log tree. As we talked and I explained what was happening, I kept updating those.\n\n\n\n\nThese diagrams also made good discussion points to make sure everyone was keeping up.\n\n\n\n\n**It’s an onion, all the way down.** Git sees the world as a series of concentric circles, and my explanation followed that.\n\n\n\n\nFirst we worked only in the working directory. Then we moved a file into staging. Then into the repository itself. For this we needed a limited number of git commands: `status`, `diff`, `add`, `commit`, and `log`.\n\n\n\n\nThen we introduced branching, so we used `checkout`, `branch`, and `merge`.\n\n\n\n\nFor the next session, I introduced the remote repositories and `push`.\n\n\n\n\n**Keep a Cheatsheet on Hand.** I printed out a [cheatsheet](http://rogerdudler.github.com/git-guide/files/git_cheat_sheet.pdf) for everyone and passed them out in the first session.\n\n\n\n\nMy central theory for all of this was that we learn technical things not by explanation, but by practice. However, we often need to have someone hold our hands and guide the way while we’re learning. This seemed to work pretty well in these sessions.\n\n\n\n\nI’d be interested to hear from those in those Praxis sessions. Let me know what worked and what didn’t.\n\n\n\n\nAnd I’d like to hear from everyone else. Are there Git tutorials that you like.\n"},{"id":"2012-11-07-forking-fetching-pushing-pulling","title":"Forking, Fetching, Pushing, Pulling","author":"jeremy-boggs","date":"2012-11-07 09:45:34 -0500","categories":["Grad Student Research","Research and Development"],"url":"forking-fetching-pushing-pulling","content":"Even though a lot of open source projects encourage folks to look at the code and modify it, they don't just let anyone add anything back to the original project. Projects usually have one or several people with direct commit access, who don't need permission to do commits. This doesn't mean you can't contribute to the project; you'll just need to get your own copy of the code, make changes there, and then send them back to the original project for review.\n\nContributing to an open source project can be a lot of fun, and [Github](http://github.com) makes that process pretty easy. Still, there are a few steps to keep in mind to make sure your workflow is sound and your contributions have better chances of getting accepted. To follow along, you'll need an account on Github, and you'll need to have git installed on your machine. This post won't go into detail about using git, but if you're not familiar with it, check out Eric's post on [Teaching Git](http://www.scholarslab.org/praxis-program/teaching-git/) and the [git resources we've collected for the Praxis Program](http://praxis.scholarslab.org/topics/intro-to-git/).\n\nFor this post, I'm going to use [Omeka](http://omeka.org) as our example, since I've been sending a few pull requests their way as I've been developing themes for their upcoming 2.0 release. But the process I describe can easily be applied to many other projects. (In fact, [Scholars' Lab](http://scholarslab.org) would love for anyone to fork [any of our repositories](http://github.com/scholarslab) and send stuff back.)\n\n\n## Forking\n\n\n[caption id=\"attachment_6734\" align=\"alignright\" width=\"237\"][![](http://www.scholarslab.org/wp-content/uploads/2012/11/Screen-shot-2012-11-07-at-2.25.08-PM.png)](http://www.scholarslab.org/dh-developer/forking-fetching-pushing-pulling/attachment/screen-shot-2012-11-07-at-2-25-08-pm/) Dashboard for my fork of Omeka, indicating that it has been forked from omeka/Omeka.[/caption]\n\nJust to be clear, forking is a _Github_ thing, not a git thing. When you fork a repo on Github, you're essentially making a copy of a repo at a particular point in time to your own account on Github. For this example, I'll refer to the fork of the [Omeka repo](http://github.com/omeka/Omeka) on my own account. These are two separate repositories. I don't have direct commit access on 'omeka/Omeka', but I do on my own, 'clioweb/Omeka':\n\n\n\n\t\n  * Omeka repo - http://github.com/omeka/Omeka\n\n\t\n  * My repo - http://github.com/clioweb/Omeka\n\n\nThe dashboard for you fork will indicate that it was forked from somewhere.\n\n[caption id=\"attachment_6727\" align=\"alignright\" width=\"133\"][![](http://www.scholarslab.org/wp-content/uploads/2012/11/Screen-shot-2012-11-07-at-2.10.41-PM.png)](http://www.scholarslab.org/dh-developer/forking-fetching-pushing-pulling/attachment/screen-shot-2012-11-07-at-2-10-41-pm/) The fork button[/caption]\n\nGithub's own help pages include a nice run-down on [how to fork a repo](https://help.github.com/articles/fork-a-repo), but to summarize: To fork any project on Github, you'll can just click that big \"Fork\" button near the top right of the project's Github page. Github will then ask you where you want to fork it, you choose which answer, and Github will do the rest. It shouldn't take too long to fork, but that depends on the size of the original repo.\n\nOnce you have your own fork, you can clone your own repo to your computer to start working on it. In your command line interface of choice, here's the command to do that on your own machine:\n\n[code lang=\"bash\"]\ngit clone git@github.com:clioweb/Omeka.git\n[/code]\n\nNow you can make changes and do commits however you like, without any fear of breaking the original repo! As I'll explain later, though, we won't just make commits however we like with our forked project. We'll want to follow some conventions to make our work more productive to send patches back to the original project.\n\n\n## Fetching\n\n\nWhen you fork a project, you get a copy of it at a specific moment in time. There aren't any built-in ways of automatically getting updates from the original repo after you forked it. The developers on the original repo are (hopefully!) going to keep developing on the project, and you'll want to get those updates regularly. You'll need to fetch those updates through git, and to do that you'll need to add another remote that points back to the original repo. To add this, you'll need to be in your clone's directory, then use the `git remote` command to `add` the original repo as a remote:\n\n[code lang=\"bash\"]\ngit remote add upstream https://github.com/omeka/Omeka.git\n[/code]\n\nThis creates a remote named `upstream` to our remotes list, and points to the origina Omeka repository. You could name this something other than \"upstream\" if you prefer.\n\nAfter adding this `upstream` remote, you should have not one, but _two_ remotes. When you clone your fork, that creates the first remote, `origin`, that points to your fork on Github and that you can push back to. The second one, `upstream`, points to the original repo you forked. This is a pretty good thing to keep in mind. When you clone a repo, you're making a copy locally, but it also make sure there's a way to send stuff back to the repo you cloned.\n\n_Now_ you can get any updates from the original project, merge them back into your own repository, then push those back to your fork. Making sure you're on your local `master` branch, you can do either of these:\n\n[code lang=\"bash\"]\ngit fetch upstream\ngit merge upstream/master\n[/code]\n\n**-or-**\n\n[code lang=\"bash\"]\ngit pull upstream master\n[/code]\n\nThe latter is just a shortcut for the former.\n\nNow that you have your local `master` branch updated with changes from the original repo, we can start working our own commits!\n\n\n## Pushing\n\n\nIn git, [\"branches are cheap and easy.\"](http://www.kernel.org/pub/software/scm/git/docs/gittutorial.html#_managing_branches) So the best thing to do when adding a new feature or fixing a bug in a git repo is to do that work on a separate \"topic\" branch. That is, a branch whose changes encompass, roughly, a single idea or topic: adds a single feature, or fixes a specific bug. This allows you to work on different features/bugs simultaneously and separately. You can keep updating a specific feature after you've submitted a pull request for it, which is handy if in the course of discussing the pull request other developers suggest changes to it.\n\nWhile you can add multiple features to a single topic branch, you might run into a situation where you don't like that feature anymore or, more commonly, the developers on the original repo only want one of the features and not both. Similarly for bug fixes. The granularity of this depends, of course, on the nature of the feature or bug you're addressing.\n\nTo make a branch, you'd use:\n\n[code lang=\"bash\"]\ngit checkout -b my-topic-branch\n[/code]\n\nHere you would replace 'my-topic-branch' with whatever you'd want the name of your topic branch to be. You'll want to name your topic branch something brief but descriptive, mainly to help you remember what that branch is about. For example, I recently [submitted a pull request](https://github.com/omeka/Omeka/pull/363) that added an options parameter to Omeka's JavaScript queueing functions. I named that topic branch `queue-js-options`.\n\nOnce you make your topic branch, you can now start editing code. Any commits you make will only be on this branch, too, so you can always check out the `master` branch while working on a feature to do some other work if you want.\n\nAs an aside, commit messages should provide enough description for someone to understand the nature of the changes. I've yet to run into a commit message that was too descriptive, but I've seen plenty of commit messages that weren't descriptive enough, so I try to err on the side of too much information. When I do a commit in git, I'll usually leave off the `message` flag in the command so I can write the message in a separate window. The first line of the commit message should be a short summary. The rest of the commit message can be as descriptive as necessary, and if you use Markdown syntax in your message, Github will format these message nicely. For example, here's my commit message for the `queue-js-options` feature:\n\n[caption id=\"attachment_6741\" align=\"alignnone\" width=\"932\"][![](http://www.scholarslab.org/wp-content/uploads/2012/11/Screen-shot-2012-11-07-at-2.41.23-PM.png)](https://github.com/clioweb/Omeka/commit/2db757ac00a54be26c72a84c008692db3e5a595e) Screenshot of my queue-js-options commit message in Github[/caption]\n\nOnce you have all your commits for a feature or bug fix ready, now you can push your topic branch up to your fork on Github to share it with others:\n\n[code lang=\"bash\"]\ngit push -u origin my-topic-branch\n[/code]\n\nThe -u flag will create a new branch to your fork on Github called 'my-topic-branch'. After you do this, you continue to make changes on that branch, you can push them up to your Github fork without the -u flag.\n\n\n## Pulling\n\n\nNow that you've got your topic branch on your Github fork, its time to [send a pull request](https://help.github.com/articles/using-pull-requests) back to the original project. Github can automatically tell if you've added a topic branch to your forked repo, and will display a \"Pull Request\" button on your fork's dashboard. Click that button, and Github will display a form confirming the pull request. It'll fill out the details using your commit messages (another good reason to be descriptive in your commit messages!), but allows you to modify that text. Modifying the pull request details won't change your commit messages. Once you submit, Github will create an issue on the original repository, with tabs to see the individual commits and the changes in the files:\n\n[caption id=\"attachment_6739\" align=\"alignnone\" width=\"938\"][![](http://www.scholarslab.org/wp-content/uploads/2012/11/Screen-shot-2012-11-07-at-2.38.43-PM.png)](https://github.com/omeka/Omeka/pull/363) Page for my pull request to Omeka[/caption]\n\nDevelopers on the original repository can comment on the request, see the code attached to it, and approve or reject the request. (Check out the [current pull requests on Omeka](https://github.com/omeka/Omeka/pulls) to see this in action.) If they do request changes, you can just update your topic branch locally, make your commits, and push them back up (this time, omitting the `-u` flag, since that topic branch now exists on Github.) In the meantime, you can go back to your local copy, create a new topic branch, and work on something else!\n\n\n## Summary\n\n\nHere's a quick rundown to get set up:\n\n\n\n\t\n  1. Fork the original repo\n\n\t\n  2. Clone your fork to your computer\n\n\t\n  3. Add the `upstream` remote pointing to the original repo\n\n\nHere's a quick rundown to send a feature or bugfix back to the original project:\n\n\t\n  1. Checkout the `master` branch.\n\n\t\n  2. Pull from the `upstream` remote to merge updates.\n\n\t\n  3. Create a topic branch for your feature or bug fix.\n\n\t\n  4. Make your commits, with good commit messages.\n\n\t\n  5. Push your topic branch back to your fork on Github\n\n\t\n  6. Send a pull request back to the original repo.\n\n\t\n  7. Converse with developers, make updates to your topic branch if necessary, and keep pushing back to your fork.\n\n\t\n  8. Repeat for a new feature or bug fix.\n\n\nHope this helps!\n"},{"id":"2012-11-14-poster-abstract-and-code-camp","title":"Poster Abstract and Code Camp","author":"brandon-walsh","date":"2012-11-14 10:09:01 -0500","categories":["Grad Student Research"],"url":"poster-abstract-and-code-camp","content":"It's been a tad quiet on the blog front over the past couple weeks. Here is what the Praxis squad has been up to.\n\nIt's been a great week for collaborative writing. We put together a poster abstract for [DH 2013](http://dh2013.unl.edu/) that crystallized a lot of our thoughts on _Prism_ and crowdsourcing thus far. Perhaps we will see some of our loyal readers there? The poster abstract that we put together discusses crowdsourcing as a continuum from more mechanical interaction to forms that maintain more of a sense of the autonomous individual. We concluded by conceiving of a few different types of interfaces for Prism and the ways in which they might affect the nature of the crowdsourcing process. Much of the abstract was put together with multiple people writing different parts of the document at the same time, which I thought was an interesting enactment of the sorts of questions and issues that we have been working through. [It was great to see each person's piece combined into the whole.](http://www.youtube.com/watch?v=ikqfgnZPPmE) Special shout-out to [Gwen](http://www.scholarslab.org/people/gwen-nally/) for taking the lead on the abstract coordination/writing and to [Jeremy](http://www.scholarslab.org/people/jeremy-boggs/) for his expert wisdom.\n\nPart of the reason for radio silence has been a distinct shift in our day to day work. After finishing the poster abstract, Wayne and company started hitting us hard and fast with code boot camp. So far we've dipped into command line, git, and ruby on rails. [Things are going pretty well](http://www.youtube.com/watch?v=sxEXBXG0ymg&feature=related) so far - it feels a little like taking a giant cyber-multivitamin as we prepare to put all of our plans for Prism into action in the next few months. When asked to explain git and version control, I like to do so by comparing it to time travel without the risk of creating a [universe collapsing paradox](http://www.youtube.com/watch?v=qKqd27h7KjM). For next week we are moving through the first ten exercises of [Learn Ruby the Hard Way](http://ruby.learncodethehardway.org/book/). The idea is that you type out letter for letter a series of code, trouble shoot if it doesn't run, and then reflect back on what you wrote. This seems a bit unusual - I would expect a more traditional format to teach you the concept first and then enact the code yourself. The LRTHW model works great for my personality type: I always have a hard time with training of any sort. Invariably in any new employee training, my attention immediately starts to wander as soon as my superior starts to explain something to me, and I always wind up learning by doing. So this reversed pedagogical model works well for me: I do first and then think back on the process. I can imagine it being very frustrating, though, if you are the kind of student that dutifully pays attention to the lesson on view.\n\nIt's an interesting model, and I wonder if it could be adapted for the humanities. What might that entail? Maybe I could have my students type out an essay paragraph word for word and then reflect on it after the fact. Would the model still work, or is there something about humanities thinking that requires that the thinking/learning precede the doing?\n"},{"id":"2012-11-16-ruby-cat-poem","title":"Ruby Cat Poem","author":"chris-peck","date":"2012-11-16 04:12:59 -0500","categories":["Grad Student Research"],"url":"ruby-cat-poem","content":"AA                  line\ntabbed\n\nmmmmmmmmmmmmm\nnononono                         mmmmmmmmmmmmm\n\nm\n\ndoof\ncat\nCat\n\na\n\nin\n\nCatnip  Grass\n\nI     llllllllllllllllllll\n\nsplit\n\na               tsil\n\ndo     II         Fishies                     I\n\nIIIIIIII\n\n\n\n\n—Learn Ruby The Hard Way, Ex. 10, modified to create cat poem ([source](https://github.com/chrispeck/learn_ruby_the_hard_way/blob/master/ex10-ec.rb))\n"},{"id":"2012-11-19-trial-by-fire","title":"Trial by Fire","author":"claire-maiers","date":"2012-11-19 09:48:12 -0500","categories":["Grad Student Research"],"url":"trial-by-fire","content":"After several weeks of dreaming big and working through some conceptual difficulties, we Praxis fellows have returned to the concrete task of learning to code and program.   Currently, we are wandering our way through the world of [Ruby on Rails](http://rubyonrails.org/).   It has been a while since I have had the opportunity to learn something completely new, and I’m finding the task both disorienting and rewarding.  Given that some of you out there might be experimenting with your own training programs, I thought I would take some time to discuss the approach to teaching and learning that we have been using.\n\nGenerally, I am not a fan of trial-by-fire method of teaching.  I was the kind of kid who learned to swim by practicing strokes in the grass, not by jumping head first in the water, figuring it would turn out all right.  I like to proceed methodically, slowly building up a familiarity with vocabulary, concepts, and skills.  Theory before application.\n\nGiven my preferred learning style, I was perfectly content when Wayne began the first lesson on Ruby, providing basic concepts, definitions, and reasons for choosing Ruby over other options.  But 20 minutes later, I was feeling pretty lost.   It wasn’t so much that the concepts or vocabulary were difficult—it was that I didn’t know what language we were speaking.  I had no grammar to help provide context, and I had no real world experience upon which I could map these new concepts.  So, even though, for example, definitions of the difference between a “language” and a “framework” were provided, those definitions were largely meaningless.\n\nBut luckily, the wise staff here at the scholars lab (read: Wayne) knew that we needed to pair our theory with practice in order to really make sense of Ruby.  We were sent to [Learn Ruby the Hard Way](http://ruby.learncodethehardway.org/) to complete a series of exercises.   This ebook is arranged in the trial-by-fire manner.   Rather than explaining Ruby and following it up with practice, the user is instructed to copy code and then run the program to see what happens.  No explanations of commands are provided, just questions for the user to answer.    To my surprise, this was an incredibly effective way to learn.  I had to examine the commands and the resulting printed screen to figure out how the commands worked for myself.  At the same time, I was building muscle memory for frequently used commands and processes.   My hope is that learning to program in this way will help me to learn at a deeper level---really retaining the skill rather that “cramming” the information only to forget it later.  However, this learning method has its downfalls: there were several places where the exercises themselves do not clarify extensively enough how a command works or how it is distinct from another command.  Luckily, we have the Scholars Lab staff to fill in these holes, and after a subsequent session with them, I am feeling pretty comfortable (for now).\n\nSo, what have I learned about learning:  I am finding a new appreciation for the trail-by-fire method.  You learn differently and have to think deeper when the answers are not simply given to you.  It is no longer about memorization or referencing a guide, but about experiential knowledge.   At the same time, I find there is value talking theoretically and having face-to-face conversation with experts and teachers, especially when paired hands-on experience.\n"},{"id":"2012-11-20-onward-and-upward","title":"Onward and Upward","author":"cecilia-márquez","date":"2012-11-20 10:58:21 -0500","categories":["Grad Student Research"],"url":"onward-and-upward","content":"This week we wrapped up git and got started on Ruby. I’m starting to build the muscle memory with git. Although they are mostly simple tasks I can make a change in an html document, stage those changes, and then commit them to a repository in [github](http://github.com/). This feels like major progress given how I was feeling about git the first weeks. I’m also building up some basic skills with Terminal. I can quickly figure out where in my computer I am, move through directories, build directories and make new documents.\n\nAlso, the first half of “[Ruby the Hard Way](http://ruby.learncodethehardway.org/book/)” went pretty smoothly. I can do basic arithmetic and am reconnecting with my former love of math. Overall I am feeling much more confident with my computer which is a nice change. For example, this past week my computer was headed toward a major meltdown. I spent a lot of time on a separate computer figuring out what could be the problem, accessed my computer infrastructure, identified the problem and then--in a shocking turn of events--FIXED IT! Certainly this is not something we learned in Praxis but one of my major goals coming into this program was feeling more comfortable using digital tools and I already feel much more confident and open to making mistakes and trying again!\n"},{"id":"2012-11-27-learning-ruby-again","title":"Learning Ruby (again)","author":"shane-lin","date":"2012-11-27 18:22:07 -0500","categories":["Grad Student Research"],"url":"learning-ruby-again","content":"Things are going a bit better than the last time I tried to pick up Ruby. Part of it is just the fact that I'm not learning it this time for work, but kind-of on the side. But I think a big part of it is just getting my feet wet with just the Ruby language rather than Rails. There's also the fact that two years ago, my main language was Java, but now I've switched back to  doing more Python hacking and so have been more comfortable right off the bat with things like dynamic typing and not worrying about things under the hood (to the extent that a Java programmer worries about things under the hood).\n\nOn the other hand, some things are already starting to annoy.  Things don't seem to be as bad as with Perl yet, but the ugly head of TMTOWTDI is already evident. It's an irreconcilable difference for a dyed-in-the-wool \"there's one way to do it\" guy like me.  I long for the velvet fascism of strict indentation. Not to mention that the sheer impudence of Ruby's syntactic sugar is more than a bit galling; I mean, my god, have some decency.\n\nOn the other other hand, Ruby does a better job of breaking some of my Java conventions. My Python designs tend to rely heavily on protocols and more hierarchical inheritance; with Ruby, I've been having some fun with mixins. I'm also looking forward to working with Ruby's reportedly more robust closure support.\n\nOn the balance, I feel like it's a language that I can actually get to enjoy. But this tiny sapling of optimism is overwhelmed by the staggering volcanic firestorm of apprehension at the rough beast of Rails, slouching inexorably toward us.\n"},{"id":"2012-11-27-your-digital-life-in-140-characters","title":"Your Digital Life in 140 Characters","author":"brandon-walsh","date":"2012-11-27 18:24:30 -0500","categories":["Grad Student Research"],"url":"your-digital-life-in-140-characters","content":"I just recently hopped on Twitter for the first time as part of a conference through UVA's [Institute for the Humanities and Global Cultures](http://www.virginia.edu/humanities/). It was a bit rough for the first five or six tweets as I worked out the kinks. Here are some thoughts that came out of the experience.\n\nI am hooked on tweeting conference presentations. I was a bit worried that tweeting would be too distracting during a talk, but I actually found that it made me a better listener. I was forced to listen hard to find discernible takeaways, which helped me to digest the information being thrown at me. Also, it gave me a way to occupy my wandering attention in a productive way. Tweeting also seems like a good way to extend the life of the conference after the proceedings are complete.\n\nOn that same note, the SLab crew has had a few conversations about how Twitter interacts with intellectual property. I felt obligated to assign many tweets I made to a particular person, assuming that I was giving credit where it was due. If our labor is intellectual, I wanted to do my best to make sure that I didn't obscure the architect of a particular idea. I wonder, though, if conference attendees would rather not be held permanently accountable for comments they make during a question and answer session. Is Twitter in this form a form of publishing? Do you need someone's permission to post their thoughts in the Twitterverse?\n\nI also find Twitter interesting for the amount of access it gives you to people and groups that you might not otherwise be able reach. Tweeting can put you in touch with so many people that would be otherwise inaccessible. It's really quite incredible - I've heard several stories now of people who got jobs or made industry contacts that were completely unexpected simply by shooting some messages out into the ether. I don't have an personal experience like this just yet, but I will keep you posted.\n\nA question: what is tweeting etiquette - more generally but also in the context of tweeting conference presentations? How many tweets are appropriate? I was averaging five or six tweets per talk, but I noticed that other people tweeting the conference were doing far fewer. I could not help but feel that my loyal followers (all four of you) were being bombarded by my messages about global cultures.\n\nIt remains to be seen how much I will tweet in my everyday life, but I think I will use it for conferences from now on. Follow me - [@walshbr](https://twitter.com/walshbr)\n"},{"id":"2012-11-28-mountain-lion-and-rvm","title":"Mountain Lion and RVM","author":"wayne-graham","date":"2012-11-28 06:32:30 -0500","categories":["Research and Development"],"url":"mountain-lion-and-rvm","content":"I recently upgraded my computer to use the latest version of OS X (Mountain Lion) and I ran in to a problem with the [rvm](https://rvm.io/) package manager. Basically I would get to the point of actually compiling the version of Ruby, and get this nasty error:\n\n\n    \n    \n    Error running 'env CFLAGS=-I/Users/wsg4w/.rvm/usr/include LDFLAGS=-L/Users/wsg4w/.rvm/usr/lib ./configure --enable-shared --disable-install-doc --prefix=/Users/wsg4w/.rvm/rubies/ruby-1.9.3-p327', please read /Users/wsg4w/.rvm/log/ruby-1.9.3-p327/configure.log\n    \n\n\n\nAfter reading through the log file, I noticed that the `make` utility wasn't installed on the system. What? I installed the full version of XCode, as well as the `apple-gcc42` packages installed. Turns out Apple doesn't include these essential command-line utilities in the default installation of XCode anymore and you have to install these from within in XCode. Basically you need to get in to the XCode **Preferences** and click on the **Downloads** tab and install the **Command Line Tools**. After this, I had to reinstall all rubies I had installed:\n\n[code lang=\"bash\"]\nrvm reinstall all --force\n[/code]\n\nAfter waiting a while for all the ruby versions I had on my computer to recompile, everything started working again and I could successfully compile code again!\n\n"},{"id":"2012-11-28-populating-mysql-tables-with-node-js","title":"Populating MySQL tables with Node.js","author":"david-mcclure","date":"2012-11-28 09:31:51 -0500","categories":["Research and Development"],"url":"populating-mysql-tables-with-node-js","content":"_[Cross-posted from [dclure.org](http://dclure.org/logs/populating-mysql-tables-with-node-js/)]_\n\nOver the course of the last week or so, I've been working on implementing \"as-needed\" spatial geometry loading for Neatline - the map queries for new data in real-time as the user pans and zooms on the map, just loading the geometries that fall inside the bounding box of the current viewport. Using the spatial analysis functions baked into MySQL, this makes it possible to build out exhibits with many hundreds of thousands of spatial records, provided that the content is organized (in terms of spatial distribution and min/max zoom thresholds) so that no more than a couple hundred records of visible at any given point. I needed a way to build out a _really_ big exhibit to run the new code through its paces.\n\nOriginally, mostly just because I was lazy to write the SQL, I had been generating testing data using a temporary development controller that called out to a helper functions that actually created the exhibits / records. These actions were invoked by Rake tasks that just spawned off GET requests to the controller actions. This works fine for relatively small data sets, but once I started trying to insert more than about 10,000 rows the loop ran for so long that the request timed out and the process died (at least, I think this was the problem).\n\nAnd, either way, this is just generally slow (all in PHP) and clunky (litters up the codebase). Instead, I decided to write a couple of little standalone scripts that would programmatically build out a big SQL insert and run it directly on the database. In the past, I might have done this with Python, but I remembered how difficult it was to get the Python <-> MySQL bindings working in the past and decided to try in with Node.\n\nThis turns out to be easy and performant. The basic gist, using the standard [node-mysql](https://github.com/felixge/node-mysql) package:\n\n\n\nIt's inefficient to run a separate `INSERT` query for each row; better to clump them together into a single, massive query, which can be accomplished by stacking up a bunch of parentheticals after the `VALUES`:\n\n\n\nThis can build out a 500,000-record exhibit in about 10 seconds:\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/11/neatline-big-exhibit.jpg)](http://www.scholarslab.org/wp-content/uploads/2012/11/neatline-big-exhibit.jpg)\n\nFull code [here](https://github.com/scholarslab/Neatline/blob/f0b9bc8626177ac9a21d6f9e72ed4d8af033a3fe/.dev/insert.js).\n"},{"id":"2012-11-29-fizz-buzz","title":"Fizz Buzz","author":"gwen-nally","date":"2012-11-29 16:53:47 -0500","categories":["Grad Student Research"],"url":"fizz-buzz","content":"Wayne asked us newbies to solve the Fizz Buzz problem for homework. [Here](https://github.com/egnally/Ruby/blob/master/homework.rb) is my solution. I got stuck at the start, where I couldn't remember how to make anything print--let alone all the numbers between 1 and 100. I also got stuck on how and where to introduce the iterative step. I find the way computers \"count\" to be quite confusing. Then, I wrote a program that ran indefinitely because I had the iterative step in the wrong spot. It hadn't occurred to me that computers read programs in order.\n\nThis exercise took me back to learning how to solve word problems in grade school.  The most difficult part is to figure out what is being asked and which tools are appropriate.\n"},{"id":"2012-11-29-holy-crap","title":"Holy crap","author":"shane-lin","date":"2012-11-29 16:52:17 -0500","categories":["Grad Student Research"],"url":"holy-crap","content":"You can dynamically add methods to built-in classes in Ruby without re-instantiating them?\n"},{"id":"2012-11-30-literals","title":"Literals","author":"shane-lin","date":"2012-11-30 06:22:27 -0500","categories":["Grad Student Research"],"url":"literals","content":"It turns out that Fixnums are special and are represented as \"immediate values\", which from what I understand is just Ruby for \"literals\". This kind of lets the air out of the whole \"everything in Ruby is an Object\" when they are really no such thing.\n"},{"id":"2012-12-04-make-it-work","title":"Make it Work!","author":"cecilia-márquez","date":"2012-12-04 06:46:40 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"make-it-work","content":"Tomorrow, in an ongoing effort to teach us how to use Ruby, we are embarking on the adventure that is “Pair Programming.”  We are going to create a “Jotto” game, courtesy of Eric by breaking it into discrete classes and having each pair work on a different class.  The goal is to have one person as the “driver” (typing all the code) and one person as the “navigator” (reading from the ruby documentation).  We can now move from the theoretical training that we’ve mostly been doing up until now and get some “on the job” training.\n\nI’m excited to work collaboratively with some of my Praxis cohort.  Given how solitary and isolating graduate school work can be, I’m looking forward to a project that is based on working collectively.  The natural parallel to draw for tomorrow (thank you David McClure) is Project Runway.  In fact once David said it it seemed impossible to not see the parallels.  ![](https://lh6.googleusercontent.com/v58_t_S-oT5VAF_s3kWbmljyuQnAdVZAcWpMRWmYefGmxJB0anBtGZ-xjMayUUVdRCCtSxeHS6JCy906FnBKZ4Pn3Z4WlE-W_iAyEmI2lSdkbN8Ide6K)\nA group of good looking young people struggling to break into a career that is almost impossible to get into, time deadlines will likely drive us to get into some comedic fights and then reconciliations, and we have a group of calm white guys walking around the office telling us to “make it work!”  Fortunately the competitive edge that often drives Project Runway contestants to tears is not looming over our heads.  This also means we don’t have to work behind each others backs and form alliances wherein we only share code with certain members of the group.  No, I think instead this will be the best of Project Runway, the collaborative spirit that generates new ideas and capitalizes on the productivity of working across lines of difference (either disciplinary or in terms of expertise).  If all of that falls through, maybe we’ll at least get a good work out in the process...\n\n![](https://lh5.googleusercontent.com/3OFNbHtHGAdSt8EHd3vGHlB26nTgaxk_Wfss6H3-qe3Tmg79_vJOBRvE6moLQ3MuzlVG_ZfdYw8R4ZGCr_WkVd4bvOeXbdpqQegywP3RQcf4RHOkKCde)\n\n**\n**\n"},{"id":"2012-12-04-speaker-series-dr-guoping-huang","title":"Speaker Series: Dr. Guoping Huang","author":"ronda-grizzle","date":"2012-12-04 05:04:52 -0500","categories":["Podcasts"],"url":"speaker-series-dr-guoping-huang","content":"**Speaker Series: Dr. Guoping Huang**\n**Geographic Information System (GIS) & the Humanities**\n\nOn September 25, 2012, Dr. Guoping Huang, Assistant Professor, Department of Urban and Environmental Planning at UVa spoke in the Scholars' Lab on GIS and the Humanities, discussing several digital humanities projects, including the [Digital Atlas of Roman and Medieval Civilizations](http://darmc.harvard.edu/icb/icb.do) (DARMC) project, the [WorldMap](http://worldmap.harvard.edu/) project, and the [Chinese Historical GIS project](http://www.fas.harvard.edu/~chgis/), to showcase how GIS can help humanists explore new grounds for interdisciplinary research.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.19050142938/enclosure.mp3\"]\n"},{"id":"2012-12-06-music-theory-in-ruby","title":"Music Theory in Ruby","author":"chris-peck","date":"2012-12-06 06:42:37 -0500","categories":["Grad Student Research"],"url":"music-theory-in-ruby","content":"Learning (and playing) with Ruby these past few weeks I've been looking for ways to solve modest, day-to-day Humanities problems. Digital Humanities, after all, doesn't just have to be about big questions like crowdsourcing, right?\n\nHere's something that's been making me very happy this week: automated generation of randomized music theory drills. I'm currently teaching an introductory theory course, and especially at the end of the semester the students are hungering for extra drills to prepare for their final exam. Wouldn't it be great if I had a magic machine that would just pump out an endless number of these worksheets?\n\n[![](http://www.scholarslab.org/wp-content/uploads/2012/11/interval-drill.png)](http://www.scholarslab.org/wp-content/uploads/2012/11/interval-drill-key.png)\n(click for the answer key)\n\nWell...now I do! This worksheet was generated with a Ruby script (here's the [source](https://github.com/chrispeck/musictheory/blob/master/interval_practice.rb)) writing music notation in [LilyPond](http://lilypond.org/). I also noticed that working out an algorithm for interval identification to generate the [key](http://www.scholarslab.org/wp-content/uploads/2012/11/interval-drill-key.png) helped me figure out how to better explain this task to the students.\n\nNext up? An entire generated exam! (I'll be a hero to future generations of instructors to this course.) I have some ideas about applications to more serious scholarly and artistic endeavors as well, but more on that later...\n"},{"id":"2012-12-10-digital-humanities-speaker-series-dr-w-gardner-campbell","title":"Digital Humanities Speaker Series: Dr. W. Gardner Campbell","author":"ronda-grizzle","date":"2012-12-10 08:05:22 -0500","categories":["Podcasts"],"url":"digital-humanities-speaker-series-dr-w-gardner-campbell","content":"**Digital Humanities Speaker Series: Dr. W. Gardner Campbell**\n**HD.EDU: Learning in a Digital Age**\n\nOn October 23, 2012, Dr. W. Gardner Campbell, Associate Professor of English and  Director of the Professional Development and Innovative Initiatives at Virginia Tech spoke on the future of learning and the changes wrought by technology in academica.\n\nThe Digital Humanities Speaker Series is co-sponsored by SHANTI, IATH, and the Scholars' Lab.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.20125708191/enclosure.mp3\"]\n"},{"id":"2012-12-10-fizzing-buzzing","title":"Fizzing, Buzzing","author":"brandon-walsh","date":"2012-12-10 05:56:33 -0500","categories":["Grad Student Research"],"url":"fizzing-buzzing","content":"Since [Gwen](http://www.scholarslab.org/people/gwen-nally/) just posted her solution to the [Fizz Buzz](http://www.scholarslab.org/praxis-program/fizz-buzz/) homework assignment,  I thought that I would throw mine up here as well. Here is my [solution](https://github.com/bmw9t/LRTHW/blob/master/praxis_exercises/fizz_buzz.rb).\n\nIt's pretty similar to Gwen's take on the problem. I just switched the order of a couple things and used a couple shortcuts. I also apparently have a penchant for extra parentheses to help organize things.\n\nI do feel like I perhaps missed the point of the exercise, given that it was tacked onto Wayne's discussion of object-oriented programming and classes. Perhaps we were meant to solve this problem using that approach? In any case, there is no one way to make breakfast; I'll take my approach for now, until I have a better grasp of object-oriented programming. I'll use what I know.\n"},{"id":"2012-12-10-grading-in-ruby","title":"Grading in Ruby","author":"brandon-walsh","date":"2012-12-10 05:57:34 -0500","categories":["Grad Student Research"],"url":"grading-in-ruby","content":"[Chris](http://www.scholarslab.org/people/chris-peck/) recently posted his [very exciting experiment](http://www.scholarslab.org/praxis-program/music-theory-in-ruby/) that uses Ruby to create music theory worksheets for his students. Inspired by this, I have been playing around on Ruby with much more modest aims: I wanted to use Ruby to do my grading for me. I always do my grading with an Excel spreadsheet and a series of formulas. I am not particularly skilled at that interface, though, and it usually takes me a while to get it to work right. I thought I might as well try to do the same thing in Ruby. Admittedly, this idea also came from Chris: he has been claiming that such a thing was a very real possibility for weeks now.\n\n[Here is the source code](https://github.com/bmw9t/LRTHW/blob/master/other%20stuff/enwr_grading.rb). It's a fairly rudimentary interface at this point, but it does seem to work. It allows you to compute a student's final grade in a course where assignment types are weighted differently.\n\nI hope to keep tinkering with this over time. Some things that I would want to include in later versions:\n\n1) **Clean up the code.** I don't feel very comfortable with classes at the moment, so I bend over backwards to accommodate that lack. I think the first step to making this more dynamic and workable will be to class it up. Right now the code only runs straight down, start to finish. Organizing the code in a more dynamic way will allow the program to be more workable to a variety of circumstances, which leads into my second point.\n\n2) **Make the interface more flexible for a variety of types of course arrangements**. Right now, it's really only set up to work for a certain type of class with a few types of assignments. I have it set up so that you input the number of different types of assignments you have, ex: paper one, paper two, midterm, and final. But there isn't really a way to account for multiple smaller assignments that make up a larger chunk of grading, like individual quizzes as part of a larger whole. Right now you would have to average the quiz grades ahead of time and then enter the composite grade in as one lump \"quizzes\" category. That also means that, if you want to drop the lowest quiz grade, you have to do that on your own. Also, the program completes its run after completing the grade for one student.\n\nMy priority right now was making it work for my purposes - a class with less than twenty students and a limited number of assignments. It seems to work for that. Take a shot at breaking it.\n"},{"id":"2012-12-12-scholarly-communication-brown-bag-series-speaker-brian-nosek","title":"Scholarly Communication Brown Bag Series Speaker: Brian Nosek","author":"ronda-grizzle","date":"2012-12-12 05:08:34 -0500","categories":["Podcasts"],"url":"scholarly-communication-brown-bag-series-speaker-brian-nosek","content":"**Scholarly Communication Brown Bag Series Speaker: Dr. Brian Nosek**\n**Scientific Utopia: A Radical View**\n\nOn November 29, 2012, [Dr. Brian Nosek](http://projectimplicit.net/nosek/), Associate Professor in the Department of Psychology at the University of Virginia, spoke at the Library's scholarly communication brown bag lunch on the changing landscape and future of scholarly communication in the sciences.\n\nSummary:\nHow can existing scientific communication practices be improved to increase efficiency in the accumulation of knowledge, and improve the alignment between daily practices and the values of the academic community? Brian will outline some present and possible futures of scientific communication-- from relatively mundane to borderline nutball—and describe his vision for a new utopia for scholarly communication.\n\nArticles by the Speaker:\nNosek, B. A., & Bar-Anan, Y. (2012). Scientific Utopia: I. Opening Scientific Communication. _Psychological Inquiry_, 23(3), 217–243.\n\nNosek, B. A., Spies, J. R., & Motyl, M. (2012). Scientific Utopia II. Restructuring Incentives and Practices to Promote Truth Over Publishability. _Perspectives on Psychological Science_, 7(6), 615–631.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.20590199748/enclosure.mp3\"]\n"},{"id":"2012-12-12-scholars-lab-speaker-series-mills-kelly","title":"Scholars' Lab Speaker Series: Mills Kelly","author":"ronda-grizzle","date":"2012-12-12 05:09:16 -0500","categories":["Podcasts"],"url":"scholars-lab-speaker-series-mills-kelly","content":"**Scholars' Lab Speaker Series: Dr. Mills Kelly**\n**Pedagogy of Disruption: What Happens When You Teach Students to Lie?**\n\nOn October 25, 2012, [Dr. Mills Kelly](http://historyarthistory.gmu.edu/people/tkelly7), Director of the Global Affairs Program in the Department of History and Art History at George Mason University, spoke on how student learning might be transformed if traditional modes of instruction were turned on their head. In his talk, Kelly, the professor of GMU’s “Lying About the Past” course, explores the up and downsides of disruptive approaches to teaching and learning.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.20577302454/enclosure.mp3\"]\n"},{"id":"2012-12-13-ruby-grading-2-0","title":"Ruby Grading 2.0","author":"brandon-walsh","date":"2012-12-13 06:33:40 -0500","categories":["Grad Student Research"],"url":"ruby-grading-2-0","content":"I just recently posted my [experiment with making a Ruby program that you can use for doing your own grading](http://www.scholarslab.org/praxis-program/grading-in-ruby/). I have since made several improvements upon the first draft of the code, so I present to you [Ruby Grading 2.0.](https://github.com/bmw9t/LRTHW/blob/master/other%20stuff/grading2.0.rb)\n\nThe changes:\n\n1) I converted the code to incorporate classes, which was a huge learning process for me that involved _lots_ of troubleshooting. I think I am starting to get the hang of classes and understand their importance. The conversion process was nightmarish, but they allow you to be a lot more flexible when trying to include new tasks. Plus, I learned a lot from the ordeal.\n\n2) I made it so that you can now input your entire roster into the grading program and calculate all your final grades there. You only input the syllabus parameters once, at the beginning. The program saves these and then applies them to all the students on your list. At the end it will spit out a nice little list of students' names and their final percentage grades.\n\n3) I also tweaked several small things as I went to account for differing input situations. Before the program couldn't account for certain types of input. For example, if you typed in \"three\" and not \"3\" when prompted for the number of assignments, the program would break. I still am not quite sure how best to deal with those sorts of situations; I couldn't really find a satisfactory test for whether or not input was convertible to an integer or not, so I think I invented one that seems to work (see lines 136-141 of the code). My test:\n\n[gist id=4252685]\n\nI cast the input string to an integer. If the integer is greater than one the code works. If not, something has gone wrong (who assigns fewer than one assignments), and it will ask for the input as an integer. It works, but it feels like a particularly dirty way of doing things. Next up, three things if I should continue: 1) Make it so that quizzes can be dynamically inputted into the program interface, with the possibility to drop the lowest one if necessary. 2) Consider making a version of the program that reads out of other files so that you don't have to do all your grading in one session. After all, we keep our gradebooks as we go, so this information is all typed somewhere else. It seems like the biggest time save would be to have it open a file, examine its contents, and spit out a calculated final grade for you. 3) Maybe include a method that will convert the percentages to letter grades.\n"},{"id":"2012-12-14-7067","title":"Now at ProfHacker: “Turning Up the Volume on Graduate Education Reform”","author":"katina-rogers","date":"2012-12-14 03:24:33 -0500","categories":["Announcements","Grad Student Research"],"url":"7067","content":"The last couple of weeks have seen a great deal of news and conversation about graduate education reform. I have a lot to say about it (unsurprisingly!); you can [find my take on it over at ProfHacker](http://chronicle.com/blogs/profhacker/graduate-education-reform/45043). The piece includes some discussion of SCI's latest work, the Praxis Program, and the budding Praxis Network, so I hope you’ll take a look!\n\nI’m also happy to note that I’ll be talking more about all of this at the upcoming MLA Convention in Boston—if you’re interested the topic, consider attending [this roundtable on Rebooting Graduate Training](http://www.mla.org/program_details?prog_id=749&year=2013). There will be ample time for discussion at the session, so come ready with questions and ideas.\n"},{"id":"2012-12-20-scholars-lab-speaker-series-jeremy-dibbell","title":"Scholars' Lab Speaker Series: Jeremy Dibbell","author":"ronda-grizzle","date":"2012-12-20 09:13:20 -0500","categories":["Podcasts"],"url":"scholars-lab-speaker-series-jeremy-dibbell","content":"**Scholars' Lab Speaker Series: Jeremy Dibbell**\n**The Libraries of Early America Project: Bringing Historical Libraries to Life with LibraryThing**\n\nOn December 4, 2012, Jeremy Dibbell, Rare Books and Social Media Librarian for LibraryThing, discussed the _Libraries of Early America Project_.\n\nSummary:\nThe Libraries of Early America project is an effort to digitize and make widely available the library collections of American readers from the early colonial period through 1825. Using the online book-cataloging site LibraryThing.com, scholars and volunteers from institutions around the country have begun the process of creating an extensive online database of early American libraries. Current subjects include Thomas Jefferson, John Adams, Benjamin Franklin, Lady Jean Skipwith, James and Mary Murray, and other early American readers (some well-known, others obscure).\n\nThe Libraries of Early America collections through LibraryThing allow users to quickly and easily make comparisons between libraries (what books did John Adams and Benjamin Franklin have in common, for example, or what books were most commonly shared among all the Signers of the Declaration of Independence?), and to search collections which may not exist today in physical form or which are spread across multiple institutions and private collections. Further, LibraryThing’s capabilities allow significant data about each book to be added to the record where known: transcriptions of marginalia, information about acquisition of the title, the binding, correspondence about a given book, or even a ink to a digital scan of the volumes (as with the John Adams collection at the Boston Public Library).\n\nSo far, data on more than 1,400 early American libraries has been added, with more information constantly being collected and included. I'll discuss the origins of the project, sources and methods, and future plans and enhancements, focusing on some of the new things we'll be doing in the future, including a fascinating look at libraries confiscated from Massachusetts loyalists during the American Revolution.\n\nSpeaker Bio:\nJeremy Dibbell is the Librarian for Rare Books and Social Media at LibraryThing. He received his B.A. from Union College and M.A./M.L.S. degrees in History and Library Science from Simmons College. In the summers, he can generally be found at the University of Virginia's Rare Book School, assisting with the school's weeklong courses. Along with the Libraries of Early America project, Jeremy's at work on a history of books and printing in Bermuda, writes regular columns for \"Fine Books & Collections\" magazine, and blogs about books and reading at [PhiloBiblos](http://philobiblos.blogspot.com/). He can be found on Twitter at [@JBD1](https://twitter.com/JBD1).\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.20622506184/enclosure.mp3\"]\n"},{"id":"2013-01-06-rebooting-graduate-training-an-mla-roundtable","title":"Rebooting Graduate Training: An MLA Roundtable","author":"katina-rogers","date":"2013-01-06 12:31:56 -0500","categories":["Announcements","Grad Student Research"],"url":"rebooting-graduate-training-an-mla-roundtable","content":"_Cross-posted at my [personal website.](http://katinarogers.com/2013/01/06/rebooting-graduate-training-mla)_\n\n_I gave the following talk at the [2013 MLA Convention](http://www.mla.org/convention) in Boston as part of an excellent roundtable organized by [Paul Fyfe](https://twitter.com/pfyfe), who has also collected a number of resources in a [Zotero library](https://www.zotero.org/groups/mla_computer_studies_in_language_and_literature_g011/items). The wide-ranging presentations sparked many thoughtful questions that I hope will lead to continued discussion about the ways that graduate training could be modified for the good of students, the discipline, and the public. Some of the slides are taken from [my earlier presentation](http://www.scholarslab.org/scholarly-communication-institute/outside-the-pipeline-from-anecdote-to-data/) on SCI's survey on career paths for humanities PhDs (a full report of which will be available later this year)._\n\n[![Rebooting Grad Ed_COPY.001](http://katinarogers.com/wp-content/uploads/2013/01/Rebooting-Grad-Ed_COPY.001.jpg)](http://katinarogers.com/?attachment_id=904)\n\nI’d like to take a step back from the question of _how_ to reboot graduate training, and think instead about _why_ we need significant change. Lately, some long-standing issues in higher ed, such as employment rates for PhD holders, seem to be receiving renewed attention that will hopefully set the stage for broad-based action. Some of these recent developments and articles include:\n\n\n\n\t\n  * The [report on the 2011 Survey of Earned Doctorates](http://www.nsf.gov/statistics/sed/digest/2011/index.cfm), which presented the grim fact that 43% of doctoral recipients have no job or postdoctoral plan upon receiving their degree; \n\n\n\t\n  * A proposal at Stanford, designed by past MLA President Russell Berman and other faculty members, to dramatically reduce time to degree and to reform many aspects of humanities education; and\n\n\n\t\n  * Current MLA President Michael Bérubé’s talk at the [Council of Graduate Schools’ annual meeting](http://www.cgsnet.org/2012-cgs-52nd-annual-meeting-presentations), in which he discussed the critical importance of reforming graduate training.\n\n\n_Note: I've written about the items above in more detail in [a recent ProfHacker post](http://chronicle.com/blogs/profhacker/graduate-education-reform/45043). The post includes a broader range of links to other write-ups on these topics._\n\nThere are numerous other examples of ongoing work of this nature, such as the MLA’s [Task Force on Doctoral Study in Modern Language and Literature](http://www.mla.org/tf_doctoral), which hosted an [excellent discussion on the topic](http://www.mla.org/program_details?prog_id=394&year=2013) at the convention.\n\nThese conversations help to amplify current work being done by a number of individuals and organizations, at the [Scholarly Communication Institute](http://uvasci.org) and elsewhere. My work with SCI has been deeply informed by the essays at [#Alt-Academy](http://mediacommons.futureofthebook.org/alt-ac/), for instance, an open-access publication of [Media Commons](http://mediacommons.futureofthebook.org/) that explores the many issues related to the sometimes tense relationship between scholarship and professional directions. Of course, I'm also indebted to the extraordinary work and people of the [Scholars’ Lab](http://scholarslab.org), which has housed SCI for the past few years.\n\n[![Rebooting Grad Ed_COPY.002](http://katinarogers.com/wp-content/uploads/2013/01/Rebooting-Grad-Ed_COPY.002.jpg)](http://katinarogers.com/?attachment_id=905)\n\nOver the past several months, SCI has embarked on a [study of career preparation](http://mediacommons.futureofthebook.org/alt-ac/who-we-are) among humanities scholars working in alternative academic careers, and we are also developing a network of innovative humanities programs to highlight possibilities for reform (more on that in a moment).\n\nThe goal of SCI’s study was to move from anecdote to data in the conversation about alternative academic careers. All of us know stories of the victories and challenges of pursuing an intellectually stimulating career beyond the tenure track, but there was little data to back up the narratives.\n\nWith that in mind, the study had two main phases. The first phase, which was public and exploratory,  involved creating a public database where people comfortable enough to publicly identify as “alt-ac” practitioners could add themselves to a loose community of peers. The second phase of the study consisted of two confidential surveys.\n\n[![Rebooting Grad Ed_COPY.003](http://katinarogers.com/wp-content/uploads/2013/01/Rebooting-Grad-Ed_COPY.003.jpg)](http://katinarogers.com/?attachment_id=906)\n\nWe built the study within the framework of the #Alt-Academy project in part to leverage the energy of existing conversations. While “alt-ac” isn’t a crisply defined term, many people have found it to be an incredibly useful umbrella under which they could gather to talk about the kinds of satisfying careers that can be found outside the professoriate.\n\nTrends among the nearly 800 responses we received to the main survey reveal a strong misalignment between the expectations of graduate students and the realities that they face upon completing their program.\n\n[![Rebooting Grad Ed_COPY.004](http://katinarogers.com/wp-content/uploads/2013/01/Rebooting-Grad-Ed_COPY.004.jpg)](http://katinarogers.com/?attachment_id=907)\n\nAs may be expected, a large majority of students enter graduate school expecting to pursue careers as professors—a total of 74%. What is perhaps more interesting is their level of confidence: of that 74%, 80% report feeling fairly certain or completely certain that this was the career they would pursue. Keep in mind that the survey respondents are **all** working outside the tenure track.\n\nThese expectations are not at all aligned with the current realities of the academic job market, as we know, and the urgency of finding a remedy to the lack of transparency is compounded by the rising amounts of student debt that burden so many graduates.\n\nDeepening the problem, students report receiving little or no preparation for careers outside the professoriate, even though we’re at a moment when the need for information about a variety of careers is most acute.\n\n[![Rebooting Grad Ed_COPY.005](http://katinarogers.com/wp-content/uploads/2013/01/Rebooting-Grad-Ed_COPY.005.jpg)](http://katinarogers.com/?attachment_id=908)\n\nOnly 18% reported feeling satisfied or very satisfied with the preparation they received for alternative academic careers. The responses are rooted in perception, so there may be resources available that students are not taking advantage of—but whatever the reason, they do not feel that they are being adequately prepared. Again, this probably comes as no surprise, but it does reveal that we have significant room for improvement.\n\nTo give you a small taste of perspectives on the other side of the employment equation, here are some of the kinds of things employers indicate they would like to see:\n\n[![Rebooting Grad Ed_COPY.006](http://katinarogers.com/wp-content/uploads/2013/01/Rebooting-Grad-Ed_COPY.006.jpg)](http://katinarogers.com/?attachment_id=909)\n\nSome specific skills come up frequently, including project management, the ability to work with and manage people, and written and verbal communication with a variety of audiences. Beyond that, employers mention a number of broader aptitudes, like a commitment to public engagement and an ability to adjust to the culture of different types of workplaces.\n\nThe good news is that all of the elements that employers seek would also be hugely beneficial for those grads that do go on to become professors. By rethinking their curricula in such a way that students gain experience in things like collaborative project development and public engagement, departments would be strengthening their students regardless of the path they choose to take.\n\nUVa’s [Praxis Program](http://praxis.scholarslab.org) is designed as one of several new initiatives that help to assess needs and opportunities, develop and articulate new models, and foster the growth of collaborative networks among relevant institutions and individuals.\n\nTo help showcase strong models of reform, SCI is now developing the **Praxis Network**: a network of several existing programs that have developed innovative models of methodological training along the lines of the Praxis Program at UVa.\n\n[![Rebooting Grad Ed_COPY.007](http://katinarogers.com/wp-content/uploads/2013/01/Rebooting-Grad-Ed_COPY.007.jpg)](http://katinarogers.com/?attachment_id=910)\n\nWe anticipate that many programs that want to make changes will want to look to existing models for guidance, and by highlighting a handful of differently-inflected programs, we can bring together some patterns among them, while also underscoring the unique idiosyncrasies of each. In addition to sharing information with the public, we hope that the network will enable increased possibilities for communication and collaboration among the participants of each program.\n\nOne thing seems clear: the persistent myth that there’s nothing but a single academic job market available to graduates is damaging, and extricating graduate education from the expectation of tenure-track employment has the potential to benefit students, institutions, and the health of the humanities more broadly.\n\n[![Rebooting Grad Ed_008](http://katinarogers.com/wp-content/uploads/2013/01/Rebooting-Grad-Ed_008.jpg)](http://katinarogers.com/?attachment_id=916)\n\nHowever, as long as norms are reinforced within departments—by faculty and students both—it will be difficult for any change to be effective.\n\nLow tenure-track employment rates are not a new problem, but as the survey responses show, departments by and large are not succeeding at providing accurate and realistic information to their students.\n\nFor change to be possible, it’s essential that institutional norms and measures of prestige shift in favor of highlighting successful outcomes across a broader spectrum of possibilities. SCI hopes that our current work will help begin to rise the tide of transparency and innovation.\n\n[![Rebooting Grad Ed_009](http://katinarogers.com/wp-content/uploads/2013/01/Rebooting-Grad-Ed_0091.jpg)](http://katinarogers.com/?attachment_id=918)\n"},{"id":"2013-01-15-speaker-series-meg-stewart","title":"Meg Stewart to talk about the Fulbright Scholar program, Thursday, Jan 17","author":"ronda-grizzle","date":"2013-01-15 06:02:18 -0500","categories":["Announcements","Digital Humanities","Geospatial and Temporal"],"url":"speaker-series-meg-stewart","content":"**[![](http://www.scholarslab.org/wp-content/uploads/2013/01/stewart201301171.jpg)](http://www.scholarslab.org/announcements/speaker-series-meg-stewart/attachment/stewart20130117-2/)A Fulbright Scholar Talks About Participatory GIS, the Caribbean, Google Earth and How a Fulbright Could Be in Your Future**\n\nThursday, January 17\n2:00 - 3:00pm\nAlderman Library, Room 421\n\n**Meg Stewart**\n** Academic Technology Consultant and Fulbright Ambassador**\n\n**A Fulbright Scholar in 2009-10 to the University of the West Indies in Barbados, Meg Stewart will talk about her experiences as a geospatial technologist in the Caribbean. Meg will also talk about the [Fulbright Scholar program](http://www.cies.org/) and encourage you to apply for a grant.**\n\n_Speaker Bio:\n_Meg Stewart is an academic technology professional working with professors and students to successfully integrate technology into teaching and learning. With a master’s degree in geology from the University of Nevada, Las Vegas, she worked in environmental consulting for a five years, and then went into higher education. Starting out as a GIS (geographic information systems) consultant in the earth science and geography department at Vassar College, she helped faculty members teach with GIS software. Stewart has written several papers on the topic of teaching with technology in higher education. Stewart received a Fulbright Scholar award in 2009-10 to the Centre for Resource Management and Environmental Studies at the University of the West Indies in Barbados. While at the UWI, she assisted with teaching a GIS class, gave lectures on teaching with Google Docs, tablet PCs, and other technologies in education, and went with students and two faculty members on a field course to Belize. A website detailing Ms. Stewart’s Fulbright experience can be seen at: [http://www.cies.org/ambassadors/mstewart](http://www.cies.org/ambassadors/mstewart).\n\n_Fulbright Ambassador Program:_\nThe Fulbright Ambassador Program trains and utilizes a select group of Fulbright Scholar alumni to serve as representatives for the Fulbright Scholar Program at campus workshops and academic conferences across the United States. Ambassadors have been selected from the full spectrum of U.S. academic disciplines, higher education institutions, and geographic regions and serve as official representatives of the Fulbright Scholar program at the events for which they are selected.\n"},{"id":"2013-01-16-but-i-dont-like-programming-gender-and-our-division-of-labor","title":"...but I don't like programming: gender and our division of labor","author":"claire-maiers","date":"2013-01-16 07:06:34 -0500","categories":["Grad Student Research"],"url":"but-i-dont-like-programming-gender-and-our-division-of-labor","content":"As last semester wound down, [Cecilia](http://www.scholarslab.org/people/cecilia-marquez/) and I  committed ourselves to honing our skills with Ruby.  During our first study session, we found ourselves talking about gender issues and the emerging role of each member within the Praxis team.  It is looking increasingly like the men will be more involved with programming, while the women of our group will focus on user interfaces, linking with social media, and management of the project.   During the last few meetings of our Ruby Boot Camp with Wayne, I’ve been aware of this emerging division, noticing that the women (perhaps mostly Cecilia and myself) have a tendency to ask more questions and to lay bare our lack of programming skills by joking about it or blatantly declaring our confusion.  In contrast, the men in our group are able to engage in two-way dialogue with Wayne in a vocabulary that is still largely foreign to me.\n\nDespite my sense of unease and disappointment in this stereotypically gendered division, I had been comforting myself by insisting that such a division of labor within Praxis was a result of coincidence.  In general, the men in our group came to Praxis with some previous experience in programming, while the women did not.  Both Cecilia and I are still in coursework and teaching, which meant that we had different scheduling difficulties from some of the others.  As Ruby boot camp amped up at the end of the semester, so too did the demands from coursework.  As a result, both Cecilia and I struggled to find time for our Ruby homework.  And finally, I don’t find myself drawn to programming.  While it is incredibly satisfying to solve a puzzle and get the program to complete some task, it is not the kind of work I would like to do all day long.\n\nSome of these circumstances actually are merely coincidences, but most are themselves structured results.   There are structured reasons why men often have more programming experience than women.    There are even structured reasons whyI have no great desire to be a programmer.  Gendered structures and practices work on us both externally _and _internally, shaping our desires, personalities, and goals.  (For more on these issues, check out this article on [Forbes](http://www.forbes.com/sites/work-in-progress/2012/06/20/stem-fields-and-the-gender-gap-where-are-the-women/) and one from the [Chronicle of Higher Ed](http://chronicle.com/article/Why-STEM-Fields-Still-Dont/135302/).)\n\nSo, the question is, what do we do now?  From a practical sense, it seems reasonable to let those who already know how to program to do their job.  The rest of us will find other ways to contribute.  This is certainly the most practical and efficient way to complete the set of ambitious goals that we are developing for the coming semester.   But----as some might recall, I wrote a[ blog post earlier this semester](http://www.scholarslab.org/praxis-program/praxis-the-innovator/) suggesting that Praxis had the potential to challenge norms within academia.  Shouldn’t we also try to undo gender stereotypes and stop the perpetuation of gendered structures?   Is there a way to make use of the skills we have brought to this team (gendered and otherwise) without perpetuating such norms?  My hope is to start a conversation about this issue.  I'm _looking_ forward to your thoughts.\n"},{"id":"2013-01-25-spring-2013-gis-workshops","title":"Spring 2013 GIS Workshops","author":"chris-gist","date":"2013-01-25 10:53:15 -0500","categories":["Geospatial and Temporal"],"url":"spring-2013-gis-workshops","content":"Every semester Kelly Johnston and I teach a workshop series around specific topics in GIS.   Typically, we stick to the basics for fall but branch out and mix it up a little by teaching new topics in spring.  Our sessions are one hour long and generally designed to be hands-on and don't require prior knowledge to participate.  Preregistration is not required. All sessions are free and open to the UVa and larger Charlottesville community.\n\nPlease find a PDF of the below schedule [here](http://www.scholarslab.org/wp-content/uploads/2013/01/spring13GISworkshops.pdf).\n\n**Acquiring and Using US Census Data in GIS**\n\nWednesday, February 6\n4:00 – 5:00pm\nCampbell Hall, Room 105\n\nSession repeats on\nThursday, February 7\n3:00 – 4:00pm\nAlderman Library, Room 421 (Electronic Classroom)\n\nThe United States Census has made big changes in their surveys and in the online tools to find and use US\nCensus datasets. Join us for a hands-on session introducing the newly redesigned American Factfinder online\ntool for discovery and access to free data from the US Census. No experience working with US Census data\nor geographic information systems is required.\n\n**Defining Watersheds with Digital Elevation Data**\n\nWednesday, February 13\n4:00 – 5:00pm\nCampbell Hall, Room 105\n\nSession repeats on\nThursday, February 14\n3:00 – 4:00pm\nAlderman Library, Room 421 (Electronic Classroom)\n\nWant to know the extent of any watershed? This session will teach you the process of delineating any\nwatershed in ArcGIS using elevation data.\n\n**Using Neatline**\n\nWednesday, February 20\n4:00 – 5:00pm\nCampbell Hall, Room 105\n\nSession repeats on\nThursday, February 21\n3:00 – 4:00pm\n\nAlderman Library, Room 421 (Electronic Classroom)\nNeatline is a set of plugins for Omeka developed by the Scholars’ Lab. With this tool, anyone can create\nbeautiful, complex maps and narrative sequences from collections of archives and artifacts, and to connect\nmaps and narratives with timelines that are more-than-usually sensitive to ambiguity and nuance. See\nhttp://neatline.org/ for more information.\n\n**Making Cartograms**\n\nWednesday, February 27\n4:00 – 5:00pm\nCampbell Hall, Room 105\n\nSession repeats on\nThursday, February 28\n3:00 – 4:00pm\nAlderman Library, Room 421 (Electronic Classroom)\n\nA cartogram is a thematic map that uses area to represent something other than area. Imagine a map where\ncountry area represents population, or cancer rates. You will learn how to send a powerful message with this\nthematic technique.\n\n**Introduction to GDAL**\n\nWednesday, March 6\n4:00 – 5:00pm\nCampbell Hall, Room 105\n\nSession repeats on\nThursday, March 7\n3:00 – 4:00pm\nAlderman Library, Room 421 (Electronic Classroom)\n\nThe Geospatial Data Abstraction Library is an open source utility library for raster geospatial data formats. As\na library, it presents a large number of utilities to the calling application for all supported formats. It also\ncomes with a variety of useful command line utilities for data translation and processing. We will focus on the\ncommand line utilities.\n\n**Do It Yourself Aerials**\n\nWednesday, March 20\n4:00 – 5:00pm\nCampbell Hall, Room 105\n\nSession repeats on\nThursday, March 21\n3:00 – 4:00pm\nAlderman Library, Room 421 (Electronic Classroom)\n\nWe have three aerial platforms, balloon, kite and hexcopter. Come get an update from us and find out how to\ndo your own aerial photography.\n\n**Introduction to Quantum GIS**\n\nWednesday, March 27\n4:00 – 5:00pm\nCampbell Hall, Room 105\n\nSession repeats on\nThursday, March 28\n3:00 – 4:00pm\nAlderman Library, Room 421 (Electronic Classroom)\n\nQuantum GIS (QGIS) is an open source, multi-platform GIS. While not nearly as powerful as ArcGIS, the\n80/20 rule applies. Probably 80% of the things most users want to do with GIS can be done with QGIS. The\nsession will introduce the interface and participants will make some nice maps. Learn more about QGIS at\nhttp://www.qgis.org.\n\n**Advanced Techniques with Quantum GIS**\n\nWednesday, April 3\n4:00 – 5:00pm\nCampbell Hall, Room 105\n\nSession repeats on\nThursday, April 4\n3:00 – 4:00pm\nAlderman Library, Room 421 (Electronic Classroom)\n\nOne of QGIS’s strengths is its ability to pull in various streaming open standard data services. We will pull\nsome data in from a remote location and do some spatial analysis.\n"},{"id":"2013-01-29-gendering-praxis","title":"Gendering Praxis","author":"cecilia-márquez","date":"2013-01-29 06:31:52 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"gendering-praxis","content":"As we embark on the next semester of the Praxis Program we have begun to think about our “roles” in the Prism project.  This has led to a surprising rush of gender feelings.  Parts of these feelings feel too second-wave feministy to be appropriate in this day and age, but regardless I find myself frustrated with myself for fitting into certain gendered tropes.  We have six people in our Praxis cohort, three are men and three are women and two of these men have previous experience in the coding/programming world.  Thus as we assign jobs I find myself gravitating towards design, project management, outreach and community engagement.  While the rest of the positions have yet to be assigned, its hard not to feel a division of labor emerging that is highly gendered.\n\nWhat makes this all the more frustrating is that my hesitation to take on “coding” responsibilities grows out of the realization last semester that I am not that interested in learning programming languages.  I’m happy that I have learned HTML/CSS and have a basic understanding of Ruby on Rails, my computer proficiency has expanded a thousand fold, and I will be certainly implementing digital tools in my dissertation.  However, I may not necessarily want to do the backend work necessary to make those tools manifest.  It is hard to let go of my deeply feminist goal of mastering coding languages and democratizing this knowledge for underrepresented groups in the digital world (women, people of color, queer people etc.).  This is not to say I have somehow given up on learning Ruby or other aspects of programming, but it is clear to me that I do not have the passion or drive to ever truly master these tools.\n\nAn additional challenge that my gender has presented me with is my frustration at not knowing and needing to constantly seek help.  Even though the Scholars Lab staff could not be more amazing, kind, and understanding as they explain what is likely obvious to them, I feel that my constant confusion and steep learning curve plays into gendered assumptions about women and the digital world.\n\nUnfortunately right now I have more questions than I do answers about gender in the digital humanities.  How do we make digital humanities spaces that are truly feminist and anti-racist that recognize the historical and structural inequalities each person brings to the space?  How can we as people from underrepresented groups work through feelings of insecurity or “out of placeness” in the digital world?  How do we implement progressive pedagogical approaches to the digital humanities that resist teacher/student dynamics that disempower students? I know that there are already groups doing this work and I am excited to spend more time engaging with their pedagogical approach (groups like [BlackGirlsCode](http://www.blackgirlscode.com/) and [TransformDH](http://transformdh.org/)).  In the meantime I’m curious what other people think...\n"},{"id":"2013-02-01-looking-forward","title":"Looking Forward","author":"brandon-walsh","date":"2013-02-01 06:52:33 -0500","categories":["Grad Student Research"],"url":"looking-forward","content":"As we move into the second half of our tenure on the [Praxis](http://praxis.scholarslab.org/) squad, we are feeling the pressure to start actually making something. To that end, the [SLab](http://www.scholarslab.org/) crew asked us to make a list of our priorities for [Prism](http://prism.scholarslab.org/). Hopefully this will help us make a to do list in the coming weeks. The following was the list that seemed to come out of yesterday's meeting, though it's possible that other people could have gotten different impressions from the conversation.\n\nThese seem to be our priorities, in no particular order except roughly that in which they were brought up:\n\n**Types of texts** - we're interested in opening up the Prism framework to deal with pictures, music, etc.\n\n**Getting a crowd** - we want to get a large group of people working with Prism, however that might be arranged.\n\n**Visualizations** - we're interested in expanding Prism's visualizations to allow you to see the individual set of markings in relation to the crowd and to let you see one marking category's visualization beside another.\n\n**User uploads** - we want to make it possible for users/instructors to upload a text for marking themselves, which we see as essential for classroom use.\n\n**Complicating the sense of how we highlight** - there has been discussion of complicating how the marking system works, allowing for a limit to the number of markings or a type of marking economy that is distributed across your selections to encourage different types of engagement with the data.\n\n**Design makeover** - we want to remake the design of the site with an eye to usability.\n\nI think my own priorities lie in making it possible for users to upload their own texts for marking and in possibly adding more visualizations. If forced to choose between the two, I would spend the time on making the tool capable of handling user uploads. As quickly became clear, any one of these goals spins off into dozens of other questions. What do we mean by \"upload a text\"? Will those texts be public? Private? How will we navigate copyright and fair use? In the coming days we're aiming to get a sense of our collective ordering of the list.\n"},{"id":"2013-02-06-neatline-one-million-records","title":"Neatline Feature Preview - 1,000,000 records in a single exhibit","author":"david-mcclure","date":"2013-02-06 04:37:30 -0500","categories":["Research and Development"],"url":"neatline-one-million-records","content":"_[Cross-posted with [dclure.org](http://dclure.org/logs/neatline-one-million-records/)]_\n\n**;tldr** - The upcoming version of Neatline makes it possible to build **huge** interactive maps with as many as 1,000,000 records in a single exhibit. It also introduces a new set of tools to search, filter, and organize geospatial data at that scale. Watch the screencast:\n\n\n\nOne of the biggest limitations of the first version of Neatline was the relatively small amount of data that could loaded into any individual exhibit. Since the entire collection of records was loaded in a single batch on page-load, exhibits were effectively constrained by the capabilities of the browser Javascript environment. Beyond a certain point (a couple hundred records), the front-end application would get loaded down with too much data, and performance would start to suffer.\n\nIn a certain sense, this constraint reflected the theoretical priorities of the first version of the project - small data over large data, hand-crafted exhibit-building over algorithmic visualization. But it also locks out a pretty large set of projects that need to be built on top of medium-to-large spatial data sets. In the upcoming version 1.2 release of the software (which also migrates the codebase to work with the [newly-released Omeka 2.0](http://omeka.org/blog/2013/01/24/omeka-2-0-drops-today/)) we've reworked the server-side codebase to make it possible to work with really large collections of data - as many as 1,000,000 records in a single Neatline exhibit. Three basic changes were needed to make this possible:\n\n\n\n\n\n\n  1. **Spatial data needed to loaded \"on-demand\" in the browser.** When the viewport is focused on San Francisco, the map doesn't need to load data for New York. Huge performance gains can be had by loading data \"as-needed\" - the new version of Neatline uses the [MySQL spatial extensions](http://dev.mysql.com/doc/refman/5.5/en/spatial-extensions.html) to dynamically query the collection when the user moves or zooms the map, and just loads the specific subset of records that fall inside the current viewport.\n\nAs long as the exhibit creator sensibly manages the content to ensure that no more than a couple hundred records are visible at any given point (which isn't actually much of a limitation - anything more tends to become bad information design), this means that the size of Neatline exhibits is effectively bounded only by the capabilities of the underlying MySQL database.\n\n\n\n  2. **The editor needed more advanced content management tools to work with large collections of records.** In the first version of Neatline, all the records in an exhibit were stacked up vertically in the editing panel. If the map can display 1,000,000 records, though, the editor needs more advanced tooling to effectively manage content at that scale. Neatline 1.2 adds full-text search, URL-addressable pagination, and a \"spatial\" search feature that makes use of the map as a mechanism to query and filter the collection of records.\n\n\n\n\n  3. **There needed to be an easy way to make batch updates on large sets of records in an exhibit.** Imagine you're mapping election returns from the the 2012 presidential election and have 20,000 points on neighborhoods that voted democratic. If you decide you want to change the shade of blue you're using for the dots, there has to be an easy way of updating all 20,000 records at once, instead of manually updating each of the records individually.\n\nIn version 1.2, we've made it possible to assign arbitrary tags to Neatline records, and then use a CSS-like styling language - inspired by projects like [Cascadenik](https://github.com/mapnik/Cascadenik) - to define portable stylesheets that make it easy to apply bulk updates to records with a given tag or set of tags.\n\n\n\n\nThese are big changes, and we're really excited about the new possibilities that open up with this level of scalability. At the same time, all development carries an opportunity cost - working on features A and B means you're not working on features C and D. Generally, Neatline is on a trajectory towards becoming a much more focused piece of software that hones in on a lean, extensible toolset for building interactive maps. We're taking a hard look at features that don't support that core competency.\n\nIn the coming weeks, we'll release an alpha version of the new codebase and solicit feedback from users to figure out what works and what doesn't. What's essential? What's expendable? What assumptions are we making that nobody else is making?\n"},{"id":"2013-02-11-art-in-the-scholars-lab","title":"Art in the Scholars' Lab","author":"becca-peters","date":"2013-02-11 06:29:34 -0500","categories":["Announcements","Research and Development"],"url":"art-in-the-scholars-lab","content":"The first time I walked into the Scholars’ Lab, I was amazed by the space.  With the high ceilings, black-and-white tile floors, and the bold red wall, it looked like something out of a magazine, not a computer lab at UVA!\n\nI’ve been at UVA for a long, long time and I’ve worked in some nice, if a little traditional, spaces.  But the Scholars’ Lab is the first place I’ve worked that has been so modern and energetic.  The décor is a direct result of the purpose of the space: energy, synergy, and creativity.  So it was no surprise to me that Bethany aspired to have the art in the Scholars’ Lab as dynamic as the rest of the environment.  In the spring of 2008, when I was new to the Scholars’ Lab and Bethany had been the director for less than a year, we set out to change our walls from hosting a set of functional, framed posters to a rotating art show.\n\n[caption id=\"attachment_7215\" align=\"alignright\" width=\"147\"][![](http://www.scholarslab.org/wp-content/uploads/2013/01/Erin-Chilton-Art1-247x300.jpg)](http://www.scholarslab.org/slab-events/art-in-the-scholars-lab/attachment/erin-chilton-art-2/) Art from Erin Chilton's show.[/caption]\n\nIn the late spring of 2008, Erin Chilton, a 4th year student with an exhibit of beautiful, realistic paintings, shared her art in our space.  The following fall, our own Digital Humanities Graduate Fellow, Jean Bauer, contributed several lovely color [photographs](https://www.scholarslab.org/wp-admin/).  They were beautiful in their simplicity.  We kept them up throughout the school year.\n\nWe hosted our first Aunspaugh Fellow in the fall of 2009.  (An [Aunspaugh Fifth Year Fellowship ](http://www.virginia.edu/art/studio/aunspaugh.html)enables a U.Va. student who has an undergraduate degree to spend a year of intensive effort in a studio area within the McIntire Department of Art.)  Jeff Trueblood’s dark paintings filled every wall with their bold, haunting beauty.  Jeff’s talent for creating an atmosphere with light and dark still excites my curiosity. Taking a flashlight to one of his pieces will illuminate what is beneath the trees or behind the lamppost.\n\nWe tried something different in the spring of 2010, opting for architectural renderings of Alderman Library.  Students in the School of Architecture's graduate Architecture Studio 6020 were assigned a project to re-vision Alderman’s West Wing as a catalyst for new learning environments.  Their large pieces and a few 3-D models were hung throughout the Scholars’ Lab for the spring semester and gave all of us interesting points to consider for the future of our beloved building.\n\nIn the fall of 2010, Aunspaugh Fellow Emily Corazon Nelson shared a series called “Nourish(ment)”.  Emily’s use of collage, staining, painting, and lamination with her photography created a dynamic show in the Scholars’ Lab.  Her work was inspired by an earlier project in which she and a few friends ran a garden and kitchen out of a biofuel bus and traveled around America, feeding people and developing relationships, while asking “what does nourishment mean to you?”   More of Emily’s art is [here](http://emilycorazon.tumblr.com/).\n\nDaniel Ballard’s work in paper making, printing, watercolor, and collage became, in the spring of 2011, a subtle exhibit of colors and forms to represent cityscapes and nature.  At the core of the show was Dan’s exploration into the implications of proposals for the future of cities.\n\nIn Fall 2011, we showcased a small exhibit by Bena Dam, another Aunspaugh Fellow.  Bena’s [photographs ](http://benadam.weebly.com/art.html)were beautiful, challenging and, in some ways, frightening.  Her goal was “to inject both a sense of whimsy and discomfort in [her] art.”  Her piece called “Hair Sew” (in which she sewed her fingers together with a strand of her own hair) and “Hung Up” (an optical illusion of Bena, headless, hanging by a wire hanger in her empty closet) were more startling than some might have liked, but personally I loved them because they did exactly what Bena set out to do: create feelings of discomfort and whimsy.\n\nLast spring, Takahiro Suzuki shared his black-and-white photographs in an exhibit he called “Beauty Through the Stillness”.  Taka’s goal was to highlight the barrenness of winter landscapes, while challenging the viewer to see the beauty in the quiet, desolate [images](http://thelibraryniche.blogspot.com/2011/11/takahiro-suzuki-nov-28-jan-2.html).  They were at times dark, but not heavy, and I especially enjoyed the surprise of noticing a cemetery in one image after having looked at it repeatedly without noticing it.\n\nOur current exhibit is by Aunspaugh Fellow Elisabeth Hogeman.  Ellie named her exhibit “[Meander Lines](http://elisabethhogeman.com/section/296691_Meander_Lines.html),” an homage to map-making and ecology.  Ellie’s primary idea in her photographs is to, in her words, “play with the idea of the female body as a natural form, making contained topographical landscapes out of the body, set alongside areas of uncontained dense vegetation located near sources of water.”  Her art is beautiful, soft, and unflinching.  As with our previous art exhibits, “Meander Lines” fits well into our Scholars’ Lab space because it is as dynamic, creative, and bold as the Scholars’ Lab itself.\n\nEvery semester we are challenged to think again about our surroundings, ourselves, our fears, and our hopes – kind of like every semester for our students!  I can't wait to unveil our new exhibit (yet to be determined) soon.  Stay tuned!\n"},{"id":"2013-02-12-dancing-with-ruby","title":"Dancing with Ruby","author":"chris-peck","date":"2013-02-12 10:57:13 -0500","categories":["Grad Student Research"],"url":"dancing-with-ruby","content":"As the Praxis Team grapples with the necessity of making decisions (difficult decisions!) about our priorities when it comes to creating at tangible product between now and May, I continue to find new uses for Ruby in my own non-Prism-related work.\n\nLast week [ a choreographer friend](http://goodmove.be) asked for help with a problem that came up in her rehearsal last week for our new project. She had devised a system of textual instructions to guide a dance improvisation. This meant that in rehearsal one of the dancers would have to sit out and play the role of reading the instructions. Her system was a chance procedure, so it had already occurred to her that it might be possible to record the instructions one by one and place the soundfiles in a shuffled iTunes playlist. But she wanted another level of organization as well: an instruction drawn at random from one of three categories in sequence. A random instruction from the first category, another random selection from the second category, one from the third, then back to the first category, and so on. She had seen me whip up [Max Patches](http://cycling74.com/whatismax/) to shuffle soundfiles in various ways and wondered if I could do something like that for this case.\n\nUsing Ruby's ability to execute shell commands, and the CLI for Mac's speech synthesizer, I was able to come up with a solution that didn't require recording all of the instructions.\n\n`\nrequire \"csv\"\nadjectives_file = \"adjectives.csv\"\npause = 5`\n\nadj = CSV.read(\nadjectives_file,\n{\n:headers => true,\n:header_converters => :symbol\n}\n)\n\nadj_by_cat = {}\n\nadj.each do |a|\nif adj_by_cat[a[:category]].nil?\nadj_by_cat[a[:category]] = []\nend\nadj_by_cat[a[:category]].push a[:adjective]\nend\n\n20.times do\nadj_by_cat.each do |cat, adjs|\ncmd = \"say \"#{adjs.shuffle.pop}\"; sleep #{pause};\"\n#puts cmd\n%x( #{cmd} )\nend\nend\n\nThis little script reads in the instructions (organized into categories in a CSV file) and speaks them in a synthesized voice like [this](https://soundcloud.com/chris-peck-6/adjective-shuffle-test). Fun, right?\n"},{"id":"2013-02-13-neatline-drawing-svg-on-maps","title":"Neatline Feature Preview - Importing SVG documents from Adobe Illustrator","author":"david-mcclure","date":"2013-02-13 05:27:13 -0500","categories":["Research and Development"],"url":"neatline-drawing-svg-on-maps","content":"_[Cross-posted with [dclure.org](k: http://dclure.org/logs/neatline-drawing-svg-on-maps)]_\n\n**;tldr** - The new version of Neatline makes it possible to take SVG documents created in vector editing software like Adobe Illustrator and [Inkscape](http://inkscape.org/) and \"drag\" them directly onto the map, just like a regular polygon. This makes it possible to create really sophisticated illustrations that go far beyond the blocky, \"sharp-edge\" style that we usually associate with digital maps. Check out the screencast (and scroll down for screenshots):\n\n\n\nThe first version of Neatline implemented a pretty standard set of GIS controls for sketching vector geometry onto maps - points, lines, and polygons. It was easy to sketch out simple shapes, but more difficult to create really intricate, complex illustrations.\n\nReally, this is a sort of ubiquitous problem with digital maps, which tend to be good at representing _points_, but bad at representing _curves_. Under the hood, shapes on digital maps are represented by a series of X/Y coordinate pairs, wrapped up into different geometry types that store information about how the points should be displayed. For example, in [Well-Known Text](http://en.wikipedia.org/wiki/Well-known_text) - the serialization format used by databases like PostGIS and MySQL - a line is represented by `LINESTRING(1 2,3 4,5 6)`, a polygon by `POLYGON((1 2,3 4,5 6,1 2))`, and so on and so forth. At the end of the day, everything is just a series of hard-coded points, strung together to form shapes.\n\nThis low-level organization in the data tends to bubble up to the level of user interfaces in the form of map sketching tools that make it easy to draw **jagged** shapes but hard to draw **smooth** shapes. For example, in the first version of Neatline, drawing this is easy:\n\n[![polygon](http://dclure.org/wp-content/uploads/2013/02/polygon-300x300.jpg)](http://dclure.org/wp-content/uploads/2013/02/polygon.jpg)\n\nBut this is much harder:\n\n[![arrow](http://dclure.org/wp-content/uploads/2013/02/arrow-300x224.jpg)](http://dclure.org/wp-content/uploads/2013/02/arrow.jpg)\n\nIt's still possible, but it's time-consuming and brittle - if you change your mind later and want to adjust the curvature of the arrow, you have to manually reposition dozens of points. This especially frustrating since, in other domains, this is a well-understood problem with lots of high-quality solutions: Vector graphics editors like Adobe Illustrator, [Inkscape](http://inkscape.org/), and even in-browser tools like [svg-edit](http://svg-edit.googlecode.com/svn/branches/2.6/editor/svg-editor.html) make it easy to create smooth, complex vector-based geometries that can be serialized to a portable XML format called [SVG](http://en.wikipedia.org/wiki/SVG) (Scalable Vector Graphics).\n\nIn the upcoming release of Neatline, we've made it possible to take SVG markup created in any vector editing tool and place it directly onto the map. Just save off any vector graphic as a SVG document, open up the file in a text editor, and paste the raw markup into the Neatline editor. Then just drag out the shape to any position, dimension, and orientation on the map. Once the new geometry is in place, it behaves just like regular points and polygons added with the default controls - it can be styled and edited just like anything else on the map.\n\nThis also opens up a whole new front of high-fidelity text-based annotation on digital maps. Since vector editors can convert strings of text into SVG paths, this makes it possible to sketch out labels, snippets, or even little paragraphs of content directly onto the map itself.\n\n[![neatline](http://dclure.org/wp-content/uploads/2013/02/neatline-1024x600.jpg)](http://dclure.org/wp-content/uploads/2013/02/neatline.jpg)\n\n[![neatline-closeup](http://dclure.org/wp-content/uploads/2013/02/neatline-closeup-1024x598.jpg)](http://dclure.org/wp-content/uploads/2013/02/neatline-closeup.jpg)\n\n[![slab](http://dclure.org/wp-content/uploads/2013/02/slab-1024x599.jpg)](http://dclure.org/wp-content/uploads/2013/02/slab.jpg)\n\n[![at](http://dclure.org/wp-content/uploads/2013/02/at-1024x600.jpg)](http://dclure.org/wp-content/uploads/2013/02/at.jpg)\n\n[![at-closeup](http://dclure.org/wp-content/uploads/2013/02/at-closeup-1024x597.jpg)](http://dclure.org/wp-content/uploads/2013/02/at-closeup.jpg)\n"},{"id":"2013-02-14-rails-is-kind-of-hard-to-get-up-and-running","title":"Rails is kind of hard to get up and running","author":"shane-lin","date":"2013-02-14 06:09:26 -0500","categories":["Grad Student Research"],"url":"rails-is-kind-of-hard-to-get-up-and-running","content":"To pile onto an overrused trope: for a web framework famed for its use of use, Rails (and maybe Ruby itself) are really surprisingly difficult to get up and running. In hindsight, the very existence of tools like RVM and Bundler to handle the management of Ruby environments and libraries is a bit of a hint. I've set up Prism twice now, on my old Thinkpad running Ubuntu Precise and now on my new Thinkpad running Ubuntu Quantal and I think I've had a bit of a different experience each time.\n\nThis time, I first ran into the usual problems with Ruby Bundler. I'm sure that once things are set up and running, Bundler is a lifesaver and that the speedbumps I ran into are more the fault of individual gems. But it's very clearly not the automated process that it's made out to be out of the box. Nokogiri and SQLite both caused the process to fail; figuring out why required me to manually gem install them: it was, of course, because my Linux distro doesn't come with their dependencies. It's certainly no big deal to Google what these dependencies are and apt-get them and it's reasonable that a Ruby gem manager shouldn't be expected to manage non-gem software, but a more streamlined process would be nice.\n\nThen, db:migrate threw back the \"you don't have a javascript runtime\" error, which directs me to the execjs gem. Again, it seems reasonable that it isn't a dependency because in theory any JS runtime will work, but it also seems reasonable that there should be a better mechanism somewhere upstream to handle this.\n\nThen, db-migrate returned the more inscrutable \"no such file to load -- ripper\" error. The first problem was that  \"ripper\" is not a very unique name, even within the ruby namespace (there's apparently a ruby-ripper gem for audio encoding). The ripper I was looking for is a 1.8 port of implementation built into Ruby 1.9. I was confused that this wasn't just handled by Bundler (or at least uncovered by it). When it didn't work, I eventually realized that Prism actually simply required Ruby 1.9. Maybe I should have known this from the start (I seem to recall figuring it out faster the first time around), but I also feel that this fact would have been less ambiguously messaged and discovered earlier with other languages with split versions (Python 2 vs 3, for example).\n\nSo, getting RVM installed on Ubuntu also required a bit of configuration, revealed through the web documentation rather than the actual error reporting. But even after getting it and Ruby 1.9 installed, Prism wouldn't run without Readline support. That came with a helpful error message about what packages to install, which actually turned out to be a Bad Thing, because installing it actually broke recompiling Ruby with Readline support on Ubuntu 12.04+...\n\nLong story short: it's a series of issues that aren't fundamentally unreasonable in themselves, but in aggregate leech away all of a person's resolve and aspiration, leaving only a desiccated and hollow husk.\n"},{"id":"2013-02-18-a-first-look","title":"A First Look","author":"gwen-nally","date":"2013-02-18 10:20:34 -0500","categories":["Grad Student Research"],"url":"a-first-look","content":"On Friday, the design team made a few mock-ups of what we'd like Prism to look like. We started with some of the basic pages that the next version of prism will probably have, a homepage, a login pop-up, etc.\n\nI feel that I have a much clearer idea of what needs to be done now that I've seen where we hope to take the site refresh. I've also noticed that the design team is making a number of important decisions as we go. For example, while designing the homepage mock-up we found ourselves asking questions about when and where people will be asked to login. Because we're making these decisions on the fly and we're not all working on the same things, I hope that others will have a chance to respond and revise. I also hope that these mock-ups will help the team figure out where to focus our energy in the weeks to come.\n\nHere's how we imagine the site might work. The Homepage will have two large buttons, something like DEMO and PLAY.\n\n[![FRONT PAGE 2](http://www.scholarslab.org/wp-content/uploads/2013/02/FRONT-PAGE-2-1024x682.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/02/FRONT-PAGE-2.jpg)\n\nThe DEMO button will hopefully lead to a tutorial page.\n\n[![DEMO](http://www.scholarslab.org/wp-content/uploads/2013/02/DEMO-1024x682.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/02/DEMO.jpg)\n\nThe PLAY button will lead to a pop-up window where users can login. Brandon is working on open authentication as we speak.\n\n[![LOGIN](http://www.scholarslab.org/wp-content/uploads/2013/02/LOGIN-1024x682.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/02/LOGIN.jpg)\n\nOnce a user has logged in, she'll have the choice to UPLOAD or BROWSE texts.\n\n[![RETURNING SIDE CLICK](http://www.scholarslab.org/wp-content/uploads/2013/02/RETURNING-SIDE-CLICK1-1024x682.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/02/RETURNING-SIDE-CLICK1.jpg)\n\nThe UPLOAD button might lead to some simple interface like the one below.\n\n[![UPLOAD](http://www.scholarslab.org/wp-content/uploads/2013/02/UPLOAD-1024x682.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/02/UPLOAD.jpg)\n\nAny thoughts? Revisions? Likes? Dislikes? I imagine Cecelia and I will start with the homepage over the next week, so it might be good to start our conversation there. The rest of this will certainly change as others on the team start to take on particular tasks.\n"},{"id":"2013-02-18-svg-to-wkt","title":"SVG-to-WKT: Converting vector graphics into spatial coordinates","author":"david-mcclure","date":"2013-02-18 06:03:26 -0500","categories":["Research and Development"],"url":"svg-to-wkt","content":"_[Cross-posted with [dclure.org](http://dclure.org/logs/svg-to-wkt/)]_\n\nLast week, I [wrote about](http://www.scholarslab.org/dh-developer/neatline-drawing-svg-on-maps/) the some of the new functionality in Neatline that makes it possible to take SVG documents created in vector-editing programs like Adobe Illustrator and drag them out as spatial geometry on the map. Under the hood, this involves converting the raw SVG markup - which encodes geometry relative to a \"document\" space (think of pixels in a Photoshop file) - into latitude/longitude coordinates that can be rendered dynamically on the map. Specifically, I needed to generate [Well-Known Text](http://en.wikipedia.org/wiki/Well-known_text) (WKT), the serialization format used by spatially-enabled relational databases like PostGIS and MySQL.\n\nIt turned out that there wasn't any pre-existing utility for this, so I wrote a little library called **[SVG-to-WKT](https://github.com/davidmcclure/svg-to-wkt)** that does the conversion.\n\nThe top-level **`convert`** method takes a raw SVG document and spits back the equivalent WKT `GEOMETRYCOLLECTION`:\n\n[code lang=\"javascript\"]\nSVGtoWKT.convert('<svg><polygon points=\"1,2 3,4 5,6\" /><line x1=\"7\" y1=\"8\" x2=\"9\" y2=\"10\" /></svg>');\n>>> \"GEOMETRYCOLLECTION(POLYGON((1 -2,3 -4,5 -6,1 -2)),LINESTRING(7 -8,9 -10))\"\n[/code]\n\nThe library supports all SVG elements that directly encode geometry information, and exposes the individual helper methods that handle each of the elements:\n\n**`line`**\n[code lang=\"javascript\"]\nSVGtoWKT.line(1, 2, 3, 4);\n>>> \"LINESTRING(1 -2,3 -4)\"\n[/code]\n\n**`polyline`**\n[code lang=\"javascript\"]\nSVGtoWKT.polyline('1,2 3,4');\n>>> \"LINESTRING(1 -2,3 -4)\"\n[/code]\n\n**`polygon`**\n[code lang=\"javascript\"]\nSVGtoWKT.polygon('1,2 3,4');\n>>> \"POLYGON((1 -2,3 -4,1 -2))\"\n[/code]\n\n**`rect`**\n[code lang=\"javascript\"]\nSVGtoWKT.rect(1, 2, 3, 4);\n>>> \"POLYGON((1 -2,4 -2,4 -6,1 -6,1 -2))\"\n[/code]\n\n**`circle`**\n[code lang=\"javascript\"]\nSVGtoWKT.circle(0, 0, 10);\n>>> \"POLYGON((10 0,9.95 -0.996,9.802 -1.981,9.556 -2.948,9.215 -3.884,8.782 -4.783,8.262 -5.633,7.66 -6.428,6.982 -7.159,6.235 -7.818,5.425 -8.4,4.562 -8.899,3.653 -9.309,2.708 -9.626,1.736 -9.848,0.747 -9.972,-0.249 -9.997,-1.243 -9.922,-2.225 -9.749,-3.185 -9.479,-4.113 -9.115,-5 -8.66,-5.837 -8.119,-6.617 -7.498,-7.331 -6.802,-7.971 -6.038,-8.533 -5.214,-9.01 -4.339,-9.397 -3.42,-9.691 -2.468,-9.888 -1.49,-9.988 -0.498,-9.988 0.498,-9.888 1.49,-9.691 2.468,-9.397 3.42,-9.01 4.339,-8.533 5.214,-7.971 6.038,-7.331 6.802,-6.617 7.498,-5.837 8.119,-5 8.66,-4.113 9.115,-3.185 9.479,-2.225 9.749,-1.243 9.922,-0.249 9.997,0.747 9.972,1.736 9.848,2.708 9.626,3.653 9.309,4.562 8.899,5.425 8.4,6.235 7.818,6.982 7.159,7.66 6.428,8.262 5.633,8.782 4.783,9.215 3.884,9.556 2.948,9.802 1.981,9.95 0.996,10 0))\"\n[/code]\n\n**`ellipse`**\n[code lang=\"javascript\"]\nSVGtoWKT.ellipse(0, 0, 10, 20);\n>>> \"POLYGON((10 0,9.98 -1.268,9.92 -2.532,9.819 -3.785,9.679 -5.023,9.501 -6.241,9.284 -7.433,9.029 -8.596,8.738 -9.724,8.413 -10.813,8.053 -11.858,7.66 -12.856,7.237 -13.802,6.785 -14.692,6.306 -15.523,5.801 -16.292,5.272 -16.995,4.723 -17.629,4.154 -18.193,3.569 -18.683,2.969 -19.098,2.358 -19.436,1.736 -19.696,1.108 -19.877,0.476 -19.977,-0.159 -19.997,-0.792 -19.937,-1.423 -19.796,-2.048 -19.576,-2.665 -19.277,-3.271 -18.9,-3.863 -18.447,-4.441 -17.92,-5 -17.321,-5.539 -16.651,-6.056 -15.915,-6.549 -15.115,-7.015 -14.254,-7.453 -13.335,-7.861 -12.363,-8.237 -11.341,-8.58 -10.274,-8.888 -9.165,-9.161 -8.019,-9.397 -6.84,-9.595 -5.635,-9.754 -4.406,-9.874 -3.16,-9.955 -1.901,-9.995 -0.635,-9.995 0.635,-9.955 1.901,-9.874 3.16,-9.754 4.406,-9.595 5.635,-9.397 6.84,-9.161 8.019,-8.888 9.165,-8.58 10.274,-8.237 11.341,-7.861 12.363,-7.453 13.335,-7.015 14.254,-6.549 15.115,-6.056 15.915,-5.539 16.651,-5 17.321,-4.441 17.92,-3.863 18.447,-3.271 18.9,-2.665 19.277,-2.048 19.576,-1.423 19.796,-0.792 19.937,-0.159 19.997,0.476 19.977,1.108 19.877,1.736 19.696,2.358 19.436,2.969 19.098,3.569 18.683,4.154 18.193,4.723 17.629,5.272 16.995,5.801 16.292,6.306 15.523,6.785 14.692,7.237 13.802,7.66 12.856,8.053 11.858,8.413 10.813,8.738 9.724,9.029 8.596,9.284 7.433,9.501 6.241,9.679 5.023,9.819 3.785,9.92 2.532,9.98 1.268,10 0))\"\n[/code]\n\n**`path`**\n[code lang=\"javascript\"]\nSVGtoWKT.path('M10 10 C 20 20, 40 20, 50 10Z');\n>>> \"POLYGON((10 -10,10.722 -10.689,11.474 -11.344,12.255 -11.964,13.062 -12.551,13.894 -13.102,14.747 -13.62,15.62 -14.103,16.51 -14.552,17.417 -14.968,18.339 -15.35,19.273 -15.7,20.219 -16.018,21.175 -16.304,22.139 -16.558,23.112 -16.782,24.09 -16.974,25.075 -17.137,26.064 -17.269,27.056 -17.371,28.051 -17.443,29.048 -17.486,30.045 -17.5,31.043 -17.484,32.04 -17.438,33.035 -17.363,34.027 -17.258,35.015 -17.123,35.999 -16.958,36.977 -16.763,37.949 -16.536,38.913 -16.279,39.868 -15.99,40.813 -15.67,41.746 -15.317,42.666 -14.931,43.571 -14.512,44.461 -14.06,45.332 -13.574,46.183 -13.053,47.012 -12.498,47.817 -11.909,48.595 -11.285,49.345 -10.627,49.909 -10,48.911 -10,47.914 -10,46.916 -10,45.918 -10,44.92 -10,43.923 -10,42.925 -10,41.927 -10,40.929 -10,39.932 -10,38.934 -10,37.936 -10,36.939 -10,35.941 -10,34.943 -10,33.945 -10,32.948 -10,31.95 -10,30.952 -10,29.954 -10,28.957 -10,27.959 -10,26.961 -10,25.964 -10,24.966 -10,23.968 -10,22.97 -10,21.973 -10,20.975 -10,19.977 -10,18.98 -10,17.982 -10,16.984 -10,15.986 -10,14.989 -10,13.991 -10,12.993 -10,11.995 -10,10.998 -10,10 -10))\"\n[/code]\n\nIf you look at the output strings, you'll notice that the Y-axis coordinates in the WKT are inverted relative to the input: `SVGtoWKT.polyline('1,2 3,4')` returns `LINESTRING(1 -2,3 -4)`, not `LINESTRING(1 2,3 4)`. This is because the Y-axis \"grows\" in the opposite direction on maps as it does in document space. In Illustrator, the coordinate grid starts at the top left corner, and the Y-axis increases as you move down on the page; on maps, the Y-axis increases as you move \"up,\" to the north. SVG-to-WKT just flips the Y-axis coordinates to make the orientation correct on the map.\n\n**TODO**\n\n\n\n\n\n  * Make it work in Node.js. This is actually a bit trickier that I thought it would be, because Node doesn't implement the browser-native methods that jQuery's `parseXML` uses. It may make sense to move to a generic XML parser that works in Node, which would be lighter-weight than jQuery anyway.\n\n\n\n  * Instead of just being purely functional (SVG in, WKT out), it might be useful to return some sort of `SVGDocument` object that could then be used to generate specific WKT strings at different density levels, orientations, etc. This would have come in handy while writing the custom OpenLayers handler that Neatline uses to actually position the generated WKT on the map (more on this later).\n\n\n\n  * Get rid of the Underscore.js dependency.\n\n\n\n"},{"id":"2013-02-19-design-team-progress","title":"Design Team Progress","author":"cecilia-márquez","date":"2013-02-19 06:47:07 -0500","categories":["Grad Student Research","Research and Development"],"url":"design-team-progress","content":"This week the design team, myself, [Brandon](http://www.scholarslab.org/author/bmw9t/) and [Gwen](http://www.scholarslab.org/people/gwen-nally/),  met for the first time to start to map out our vision for the exciting redesign.    This week I am spending figure out the CSS behind the Prism site and me and Gwen are working on wireframes (mostly I yell out random ideas and Gwen makes them look amazing).  We are trying to figure out how we want the site to progress etc.  This week I am also trying to figure out how to make the header work the way that we want.\n\nBrandon is working on getting OAuth up and running so that people can sign in using facebook, twitter or google.  We have some other exciting plans in the works if we can finish all of these tasks!  Our next major goal is try and get user uploads working so that people can upload their own text for Prism analysis.  That's all for now...next week we can start posting our design mockups so we can hear your feedback!\n"},{"id":"2013-02-19-works-in-progress","title":"Works in progress: Survey results, Praxis Network","author":"katina-rogers","date":"2013-02-19 05:10:42 -0500","categories":["Announcements"],"url":"works-in-progress","content":"_[Cross-posted on [my personal website](http://wp.me/p2CaGd-g0)]_\n\nThis spring marks a new phase for my work with SCI. Data collection for the [survey on career paths](http://katinarogers.com/2012/11/05/outside-the-pipeline-from-anecdote-to-data/) is complete, and analysis is underway, meaning that the next step will be much more focused on sharing outcomes. In some ways, this is a less comfortable step in the process for me (nerves! public speaking!), but also an exciting and satisfying one.\n\nI’m honored to be giving several invited talks over the next few months:\n\n\n\n\t\n  * March 8, 12:30–2:30 p.m., NYU (hosted by the [Humanities Initiative](http://www.humanitiesinitiative.org/)) \n\n\t\n  * April 10, 5–6:30 p.m., University of Delaware (hosted by the [Interdisciplinary Humanities Research Center](http://www.udel.edu/ihrc/projects/2013/2013-digital-humanities.html); here’s a [flyer](http://bit.ly/WuoXDi))\n\n\t\n  * April 17, 12–1 p.m., Stanford University (hosted by the [Humanities Education Focal Group](http://www.stanford.edu/dept/DLCL/cgi-bin/web/groups/humanities-education-0), which Russell Berman chairs)\n\n\nAll talks are open to the public, so please come if you’re in the area! I’d love to see friendly faces, and I’m very much hoping for dynamic discussion at each event.\n\nAlso in the spirit of sharing information and outcomes, I’ve been working with Jeremy Boggs on a website that will showcase a small handful of innovative programs for humanities graduate and undergraduate students. I can’t wait to unveil it; the programs are exciting, the website is beautiful, and overall I think it will be very useful for a range of audiences. In particular, I hope that it can be used to support the development of other new programs with similar goals of equipping humanities scholars to excel in the paths that they choose. The site is designed to be something of a response to the survey results—where the survey underscores opportunities for improvement in graduate curricula, the site (called the Praxis Network) points to specific efforts to rethink methodological training with an eye toward collaboration, project-driven scholarship, and public engagement.\n\nFinally, I’m incredibly pleased to be presenting a long paper at [DH2013](http://dh2013.unl.edu/) in July that will include elements of the survey as well as the Praxis Network, and I’m working on a final report to be published around the same time. We’ll also publish the data so that others can build on the research we've done in the past year.\n\nIt seems crazy, but by the time the DH conference rolls around, my time at SCI will be nearly finished. I’m thrilled that I’ll have so many opportunities to share our work between now and then.\n"},{"id":"2013-02-20-highlighting","title":"Highlighting (some design proposals for Prism)","author":"chris-peck","date":"2013-02-20 04:24:17 -0500","categories":["Grad Student Research"],"url":"highlighting","content":"[![](http://www.scholarslab.org/wp-content/uploads/2013/02/hilight_tool_sketches.gif)](http://www.scholarslab.org/wp-content/uploads/2013/02/hilight_tool_sketches.gif)\n"},{"id":"2013-02-21-on-learning-code","title":"On Learning Code","author":"brandon-walsh","date":"2013-02-21 05:44:03 -0500","categories":["Grad Student Research"],"url":"on-learning-code","content":"As we gear up for [Prism](http://prism.scholarslab.org) development proper, I have been trying to get up to speed with some coding basics so that I can hit the ground running. Here are the learning aids that I have found most useful over the past couple weeks. Nearly all of these materials can be found in the [Praxis Program's scratchpad](http://praxis.scholarslab.org).\n\n**Ruby**\n\n[Learn Ruby the Hard Way](http://ruby.learncodethehardway.org/book/) - I enjoyed the learning approach here: the site displays a chunk of code that you type out, fix, and then examine more closely. It might seem like working backwards, but it works well for people who learn by doing.\n\n[Ruby Warrior](https://github.com/ryanb/ruby-warrior) - Learn Ruby by programming your own video game! There is nothing quite like tricking your brain to work. Each new level presents a problem that you must solve by changing a Ruby file. This one is great for learning the syntax, but it will only go so far in teaching the methods available to you in Ruby. You'll be doing a lot of methods unique to the game, and I doubt that warrior.walk!(:backward) will come up in normal situations. Syntax is all, though.\n\n**Rails**\n\n[Michael Hartl's Ruby on Rails Tutorial](http://ruby.railstutorial.org/ruby-on-rails-tutorial-book) - Hartl walks you through building an application step by step via a Rails scaffolding. They are quite thorough and easy to follow.\n\n[RailsCasts](http://railscasts.com) - These very short video tutorials walk you through Ruby on Rails concepts with screencasts and commentary.\n\n**JavaScript**\n\n[Code Academy](http://www.codecademy.com/) - I've been using Code Academy for JavaScript, but they also have HTML, Ruby, Python, and several other coding tracks. There are a wealth of resources here arranged in interactive exercises, and the site also has lots of extra opportunities for practice once you finish the main courses. It also gives out points and badges as you code for added incentive!\n"},{"id":"2013-02-26-announcing-my-blog-songs-of-the-victorians-and-augmented-notes","title":"Announcing My Blog, Songs of the Victorians, and Augmented Notes","author":"annie-swafford","date":"2013-02-26 08:37:34 -0500","categories":["Digital Humanities","Grad Student Research","Research and Development"],"url":"announcing-my-blog-songs-of-the-victorians-and-augmented-notes","content":"I've been having an excellent and productive time as a Scholars' Lab Fellow: thanks to the amazing advice and support I've received from the Scholars' Lab staff, I've been making good progress on my two digital projects, [Songs of the Victorians](http://www.songsofthevictorians.com/) and Augmented Notes. For those of you who don't know about them, I'll give you a brief summery:\n\n[Songs of the Victorians](http://www.songsofthevictorians.com/index.html), which I've written about before [here](http://www.scholarslab.org/praxis-program/introducing-our-digital-work-%E2%80%9Csongs-of-the-victorians%E2%80%9D/) and [here](http://www.nines.org/news/?p=1291), is an archive and analysis of parlor and art song settings of Victorian poems.  I specialize in Victorian poetry and its intersections with music, and this project I'm building helps me in my interdisciplinary work.  For the four songs I include, users can examine an archive page with high resolution scans of the 1st edition printing of the score and an audio file of the work; when users click the audio file, each measure is highlighted in time with the music so scholars who can't read music can still follow along.  Each song also has an analysis component, in which I include an essay on the music's interpretation of the text, and users can click on parenthetical notes that occur after descriptions of the effect of a particular musical passage, and the relevant measures of the score will become highlighted in time with the music.  In case an image will make it more tantalizing, here's the \"Coming Soon\" page that's live on the web:\n\n[![Songs of the Victorians \"Coming Soon\" page](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-25-at-12.54.03-PM-1024x575.png)](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-25-at-12.54.03-PM.png)\n\nAugmented Notes, which is not yet live, is a tool that will help users generate their own sites like Songs of the Victorians.  Users will upload a scan, audio file, and MEI file and then follow simple instructions to output css, html, and javascript template files that they can then alter to make their own complete interdisciplinary site.\n\nIf you're interested in learning more about either of these projects or in how I'm building them, I have a blog, [Anglophile in Academia](http://anglophileinacademia.blogspot.com/), where I post updates.  I write a new post every Monday, and I'd love to hear comments and feedback from all of you!\n\nIf you don't know where to jump in, here are some highlights so far:\n\n**Progress report**:  [Here's](http://anglophileinacademia.blogspot.com/2013/02/sneak-peak-release-date-for-songs-of.html) where I announce the date of the sneak-peak release of Songs of the Victorians.\n\n**Content**:  If you're interested in the content of the analysis pages for Songs of the Victorians, read [this pos](http://anglophileinacademia.blogspot.com/2013/02/a-note-on-content-caroline-nortons.html)t on Caroline Norton's song \"Juanita.\"\n\n**Design**:  [Here's ](http://anglophileinacademia.blogspot.com/2013/02/more-design-creating-homepage-and-song.html)my most recent post on designing the homepage and song display page for Songs of the Victorians, [here's a post](http://anglophileinacademia.blogspot.com/2013/02/songs-of-victorians-redesigned.html) on the redesign of Songs of the Victorians and the rationale behind it, and [here's my post ](http://anglophileinacademia.blogspot.com/2013/01/hello-readers-as-some-of-you-know-its.html)about logo designs for both sites.\n\nI'll be making my weekly updates for the rest of the semester (and beyond), so make sure to subscribe or check it out every week!\n"},{"id":"2013-02-26-neatline-and-omeka-2","title":"Neatline and Omeka 2.0","author":"david-mcclure","date":"2013-02-26 08:53:00 -0500","categories":["Research and Development"],"url":"neatline-and-omeka-2","content":"![null](http://neatline.org/wp-content/themes/neatline-wp-theme/images/neatline-logo-rgb.png.pagespeed.ce.KWm9TWbLus.png)\n\n_[Cross-posted with [dclure.org](http://dclure.org/logs/neatline-and-omeka-2)]_\n\nWe've been getting a lot of questions about when Neatline plugins will be ready for the [newly-released Omeka 2.0](http://omeka.org/blog/2013/01/24/omeka-2-0-drops-today/). The answer is - very soon! In addition to migrating all of the plugins ([Neatline](http://neatline.org/plugins/neatline/), [Neatline Time](http://neatline.org/plugins/neatline-time/), [Neatline Maps](http://neatline.org/plugins/neatline-maps/), [Neatline Features](http://neatline.org/plugins/neatline-features/)) over to the new version of Omeka, we're also using this transition to roll out a major evolution of the Neatline feature-set that incorporates lots of feedback from the first version.\n\nSome of the new, Omeka-2.0-powered things on tap:\n\n\n\n\n\n  * [Real-time spatial querying on the map](http://dclure.org/logs/neatline-one-million-records/), which makes it possible to work with really large collections of data (as many as 1,000,000 records in a single exhibit);\n\n\n\n  * The ability to [import SVG documents from vector-editing programs like Adobe Illustrator](http://dclure.org/logs/neatline-drawing-svg-on-maps/), making it possible to render complex illustrations on the map;\n\n\n\n  * A portable stylesheet system that allows exhibit-builders to use a CSS-like syntax to apply bulk updates to large collections of records;\n\n\n\n  * An improved workflow for displaying Omeka items in Neatline exhibits - mix and match individual Dublin Core fields, entire metadata records, images, and other item attributes;\n\n\n\n  * A flexible workflow for adding custom base layers in exhibits, which makes it possible to use Neatline to annotate non-spatial materials: paintings, drawings, abstract maps, and anything else that can be captured as an image.\n\n\n\n  * A new set of hooks and filters - both on the server and in the browser - that make it easy to for developers to write modular add-ons and customizations for Neatline exhibits - legends, sliders, record display formats, integrations with long-format texts, etc.\n\n\n\n\nThe new version is just about feature-complete, and we're now in the process of tying up loose ends and writing the migration code to upgrade projects built on the 1.1.x releases. We're on schedule for a public beta by the end of March, and a full release by the end of the semester.\n\nGoing forward, we'll continue supporting the Omeka 1.5.x-compatible releases of Neatline from a maintenance standpoint, but we're moving all new development efforts into the new versions of the plugins, which only work with Omeka 2.0.\n\nAs the final pieces fall into place over the course of the next couple weeks, we'll start posting a series of alpha releases for developers and other folks who want to test-drive the new feature set. Between now and then, check out some of the feature-preview articles we've posted in the last couple weeks:\n\n**[Neatline Feature Preview – 1,000,000 records in an exhibit](http://dclure.org/logs/neatline-one-million-records/)**\n**[Neatline Feature Preview – Importing SVG documents from Adobe Illustrator](http://dclure.org/logs/neatline-drawing-svg-on-maps/)**\n\n\n\n\nAnd watch this space for ongoing weekly updates!\n"},{"id":"2013-02-28-prism-site-map","title":"Prism Site Map","author":"cecilia-márquez","date":"2013-02-28 11:20:40 -0500","categories":["Grad Student Research","Research and Development"],"url":"prism-site-map","content":"The design team got together again this week and we have mocked up a map of how the site will proceed.  Those who are interested can see it here: [prismmap](http://www.scholarslab.org/wp-content/uploads/2013/02/prismmap.pdf)\n\nThere are quite a few new pages that we are adding so we hope this map will make it easier to see how the progression works.  As we conference with Shane and other folks we may have to reorganize some of the order but I think this is more or less what it will look like.  We also have a second round of page mockups coming soon!\n"},{"id":"2013-03-01-gender-and-computing-ctd","title":"Gender and Computing (ctd)","author":"shane-lin","date":"2013-03-01 05:34:54 -0500","categories":["Grad Student Research"],"url":"gender-and-computing-ctd","content":"To add to the conversation about gender in computing (from [Claire](http://www.scholarslab.org/praxis-program/but-i-dont-like-programming-gender-and-our-division-of-labor/) and [Cecilia](http://www.scholarslab.org/digital-humanities/gendering-praxis/)), I just wanted to very briefly point out that while the gendered culture and gender gap in computing are not recent phenomena (on the former, Jennifer Light's [brief article](https://muse.jhu.edu/journals/technology_and_culture/v040/40.3light.html) on the submerging of women's roles on ENIAC is a good read), it's actually been getting much worse in the last few decades rather than better. Women's participation in both academic computer science and the information technology industry has been on the decline since the 1980s, the exact opposite trend as the great progress women have made over this period in education and business at large. As this gender division has widened and gendered stereotypes have become crystallized,  even the iconic figures - the Ada Lovelaces and Grace Hoppers - have been shunted off into \"token woman\" status that robs them of the titanic contributions they made as computing pioneers.\n\nThere's been alot of commentary on the reasons for this decline and I feel rather unqualified to judge their relative merits (I am personally convinced though that tech's \"startup\"  culture, originating in the 1970s but reaching its apex in the 1990s and 2000s, is a major contributor). Whatever the cause, this trend demands active measures to correct it. From my short exposure, DH has seemed a rare and pleasant refuge surrounded by the larger technological sea of gendered assumptions and toxic sexism. I don't have any better ideas of how, but Claire and Cecilia's suggestions that DH or Praxis be wielded to challenge norms within its larger communities (be it technology or academia) seems especially important now.\n"},{"id":"2013-03-01-restarting-marionette-applications","title":"Restarting Marionette applications","author":"david-mcclure","date":"2013-03-01 04:45:25 -0500","categories":["Research and Development"],"url":"restarting-marionette-applications","content":"_[Cross-posted from [dclure.org](http://dclure.org/logs/restarting-marionette-applications/)]_\n\nOver the course of the last couple months, I've been using Derick Bailey's superb [Marionette](https://github.com/marionettejs/backbone.marionette) framework for Backbone.js to build the new version of Neatline. Marionette sits somewhere in the hazy zone between a library and a framework - it's really a collection of architectural components for large front-end applications that can be composed in lots of different ways. I use Marionette mainly for the core set of message-passing utilities, which make it easy to define interactions among different parts of big applications - pub-sub event channels, command execution, request-response patterns, etc. I've come to completely rely on these structures, and can't really imagine writing non-trivial applications without them anymore.\n\nThe only big kink I've encountered is in the Jasmine suite. Since almost all of the integration-level test cases mutate the state of the application (trigger routes, open/close views, etc.), I needed to completely burn down the app and re-start it from scratch at the beginning of each test to ensure a clean slate. The top-level Marionette `Application` has a `start` method that walks down the tree of modules and runs the initializers. As it exists now, though, `start` can only be called once during the lifecycle of the application, and does nothing if it's called again later on.\n\nI was getting around this by defining independently-callable `init` methods for all of my modules and wiring them up to the regular Marionette start-up system:\n\n\n\nBut then manually calling all of the init methods in my Jasmine start-up routine to force-restart the application:\n\n\n\nThis is icky - I have to exactly recreate a specific start-up order that's automatically enforced in the application itself by `before:` and `after:` initialization events. And it introduces lots of opportunities for false-negatives - if you add a module, and forget to explicitly start it in the test suite, everything falls apart.\n\nReally, I wanted to just re-call `Neatline.start()` before every test. I realized tonight, though, that the application object can be tricked into restarting itself by (a) stopping all of the modules and (b) resetting the top-level `Callbacks` on the application:\n\n\n\nMuch cleaner. Assuming all state-bearing components are instantiated in the initializers, this has the desired effect of completely rebooting the application.\n\nI'd imagine this is a pretty common issue - is there any philosophical reason for the prohibition against re-calling `Application.start()` more than once?\n"},{"id":"2013-03-05-gradient-highlights","title":"Gradient Highlights","author":"chris-peck","date":"2013-03-05 06:47:22 -0500","categories":["Grad Student Research"],"url":"gradient-highlights","content":"While playing around with the CSS gradients that put the highlighting for each \"facet\" into lanes over the text, I think I've come up with an interesting new proposal for what the highlighting might look like:\n\n[![Screen Shot 2013-03-03 at 10.55.23 PM](http://www.scholarslab.org/wp-content/uploads/2013/03/Screen-Shot-2013-03-03-at-10.55.23-PM.png)](http://www.scholarslab.org/wp-content/uploads/2013/03/Screen-Shot-2013-03-03-at-10.55.23-PM.png)\n\nI like this because it allows each facet's highlight to be full height—thus feeling more like a highlighter pen and clearly bonding visually with its text—while still allowing the colors to be distinct in the areas of overlap. I think this approach might work well with up to four or five colors, and maybe that's what I'll try next.\n\nWhat was that about the value of failure and mistakes (a theme from some of our first meetings with the Praxis cohort in the fall)? I got this idea from mistyping a value when making a change to the CSS and seeing how easy it was to make this sort of gradient. I doubt I would have thought to try it otherwise. And in some ways it's far better than any of the mock-ups I came up with in Illustrator last week.\n"},{"id":"2013-03-07-css-victory","title":"CSS Victory!","author":"cecilia-márquez","date":"2013-03-07 07:13:27 -0500","categories":["Grad Student Research","Research and Development"],"url":"css-victory","content":"[![PrismDraft](http://www.scholarslab.org/wp-content/uploads/2013/03/PrismDraft-300x157.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/03/PrismDraft.jpg)\n\n\n\n\nSo the other day I spent most of my day working through the CSS on the Prism site.  It was incredibly annoying at first but at some point I really hit my stride and started to get it.  In order to start just figuring out where everything was located I made a feature branch (appropriately titled Honey Boo Boo) and started playing around until I could get it close to what I wanted it to look like.  I succeeded (as you can see above) in getting the homepage to look like what I imagined.  Currently I am using three of my femme superheroes as placeholders for the buttons (for those of you who don't know they are [from left to right] Honey Boo Boo, Nicki Minaj, and RuPaul).  So this is a rough sketch of what the homepage might look like, although we will probably tone down the fabulosity that is the current buttons.  Thoughts?  Feedback?\n"},{"id":"2013-03-07-the-place-of-beauty-in-scholarly-writing","title":"The place of beauty in scholarly writing","author":"katina-rogers","date":"2013-03-07 03:35:27 -0500","categories":["Announcements"],"url":"the-place-of-beauty-in-scholarly-writing","content":"_[Cross-posted on [my personal website](http://wp.me/p2CaGd-gr)]_\n\nI’ve just returned from two thought-provoking days of conversations about assessment and authority in new modes of scholarly production, the second in a series of three SCI meetings on the topic. We’ll synthesize the key outcomes and insights into a report very soon. For the moment, though, I want to think a little more about a question that occurred to me after the meeting: What is the place of beauty in academic writing? While this wasn’t something the group discussed directly, it did seem to be an undertone of certain threads of conversation.\n\nI got home from CHNM on Friday evening feeling pretty brain-dead from the hybrid (and quintessentially #altac) work of wrangling meeting logistics and absorbing  stimulating and thoughtful discussion. Ready to relax, I sat down to watch [Pina](http://www.imdb.com/title/tt1440266/) and was entranced within minutes; the film is stunning. The clips of Pina Bausch’s dance company, [Tanztheater Wuppertal](http://www.pina-bausch.de/en/dancetheatre/), are mesmerizing; they are made even more compelling by Wim Wenders’ directorial work. Something about the visual beauty of the film and the dance it portrayed helped me to think about the preceding conversations about scholarly work in a new light.\n\nOne topic of discussion at SCI was the significance of the editorial process to the perceived quality and authority of scholarly work. Thinking about this while watching the film, I was struck first of all by the interviews with individual dancers that fill a substantial portion of screentime. Each dancer speaks admiringly of Pina (always referring to her by her first name), many of them noting her ability to draw out astonishing performances through her perceptiveness and laconic guidance. The task of ferreting out talent in academic spheres can happen at many different junctures, and is the touchstone of good mentors (and editors) everywhere. But I’m not sure that we give enough credit to the role, as stories of scholarly enterprise often favor a notion of individual struggle and success. Pina’s influence, by contrast, is clearly credited as a guiding force and catalyst, both for individuals and for the company as a whole.\n\nThe second thing that I thought about while watching the compelling visual display was the necessity of expertise and practice in the dance productions, no matter how unlike traditional repertoire they may have been. Pina’s company was known for innovative and risky works that departed significantly from traditional dance productions, but that doesn’t mean that the dances are sloppy or unrehearsed. On the contrary, it is clear that the dancers have a deep foundation in traditional training, that the unusual choreography is equally demanding of precision, and that the productions are meticulously rehearsed. The result is both beautiful and powerful.\n\nAs we talk about new modes of scholarly production that depart from the traditional mechanisms of academic authority, it’s worth considering what careful research and new lines of inquiry look like when separated from the formats that have long been customary. As the velocity of publication increases (and is done on an ever-thinner shoestring, even at traditional presses), the editorial process is condensed. Writers may not polish their prose to the same degree, and the work may not benefit from thorough content refinement, copyediting, or layout decisions that publishers have historically taken on.\n\nGenerally speaking, I think that making scholarly work public more quickly is a significant enough benefit that it can bear the risk of a few rough edges. At the same time, perhaps especially for literary scholars whose work revolves around the ways that words are put together into sentences and stories to create both meaning and beauty, I’m acutely aware of the power of a beautifully-written text. The care and precision with which we construct our arguments is, I think, directly related to the ideas that we express. It’s useful to think about written style in terms of code, too, in which syntax and precision are strictly necessary to create a functioning program. Someone might prefer the flexibility of Perl or the comparative strictness of Python or C, but once she has chosen a language for the program, the corresponding rules must be followed. Precision isn’t an aesthetic choice in this case, but a requirement for functionality.\n\nAll of this brings me back to my initial question: What is the place of beauty in scholarly writing? In a [Twitter conversation](http://storify.com/katinalynn/beauty-and-scholarly-writing) with [Kari Kraus](http://www.karikraus.com/), I floated three possibilities: It may be a core value to our scholarly enterprise; it may be a pleasant ancillary; or it may be a risky distraction.\n\nI haven’t yet mentioned the risk factor, but it’s part of what initiated this line of thinking in the first place. Scholarly writing is, at its core, about the creation and dissemination of new knowledge; if that is the goal, then perhaps the packaging shouldn’t matter. [Jason Priem](http://jasonpriem.org/), co-founder of [ImpactStory](http://impactstory.org/) and a participant at the SCI meeting, worried that too much emphasis on polished grammar or design could serve as a choke point, preventing innovative ideas and arguments from reaching an audience. Scrutinizing the surface of the work, Jason argued, means that only those who have learned the codes afforded by elite education will see their work accepted as valuable, which potentially reinforces problematic classist limitations on the creation of new knowledge and lines of inquiry.\n\nThe risk that Kari and I mentioned in our conversation considers a somewhat different angle. Rather than focusing on the rejection of good ideas that lack polish, we mused about the potential acceptance of weak arguments couched in beautiful prose. While I don’t think that this is an especially common problem in academic writing—I would love it if our problem was an excess of gorgeous prose!—it is plausible enough that it makes me pause when I think about whether beautiful writing could be considered a core value of scholarly work in the humanities.\n\nUltimately, I think that beautiful writing is akin to precise, well-rehearsed movements in dance. The movements themselves are not sufficient to establish an interesting, cohesive work, but they are both elements of the piece’s beauty, and signposts indicating the care and work that are its foundation. The same is true with stylistic precision or fine visual design: they not only affect the audience’s encounter with the work, but also suggest the hard work and craftsmanship that have gone into it. Admittedly, that would mean that beauty is one part substance and one part signal, and I think there’s a fear that signals are mere dissimulation. But we’re affected by signals all the time, whether they are intended or not, and so we might as well be aware of the ways those signals are created and received.\n\nBut what about the realities of contemporary scholarly production, in which editorial oversight and refinement are increasingly unavailable to scholars wishing to share their work as widely as possible? This is where a dose of cautious optimism comes in. As I’ve watched the innovative models of SCI’s partner projects—[PressForward](http://pressforward.org/), [MLACommons](http://commons.mla.org/), and [Scalar](http://scalar.usc.edu/)—I am hopeful that scholars will have more and more ways to participate in ongoing conversations about their work that lead to increased refinement. Post-publication review mechanisms, whether in the form of [CommentPress](http://www.futureofthebook.org/commentpress/) or the multi-layered curation and editing of [Digital Humanities Now](http://digitalhumanitiesnow.org/) and the [Journal of Digital Humanities](http://journalofdigitalhumanities.org/), provide (arguably) richer opportunities for a scholar to work through ideas with input from a community of peers. The resulting work has the potential to be of higher quality than an article seen by only a few sets of eyes before its publication, and it is also likely to reach a wider and more diverse audience.\n\nIn the end, to recycle my own tweet, I just want to read (and, ideally, produce) more beautifully-written work. I hope that we’re creating systems that make that possible, and cultivating values that reward it.\n"},{"id":"2013-03-08-the-blind-leading-the-blind-a-noob-and-program-management","title":"The Blind Leading the Blind:  A Noob and Program Management","author":"claire-maiers","date":"2013-03-08 08:29:39 -0500","categories":["Grad Student Research"],"url":"the-blind-leading-the-blind-a-noob-and-program-management","content":"A better title for this post would be “The Blind leading the Slightly-less-blind.”  I thought it a little too wordy for a title, but it really does accurately express my experience of project management during these first few weeks of actually working on Prism.\n\nFiguring out how to manage a project when I know little of how that project will actually be completed has been daunting.  While most of our team has found a little niche for themselves---some area where they are more adept than the rest—I have found myself staring at the existing Prism code with that terrified-deer-in-the-headlights look.  This has left me uncertain of whether or not I can actually make decisions about the work plan for the rest of the semester.\n\nHowever, after several pep talks from both Bethany and Wayne and an incredibly beneficial tour of the Ruby code for Prism from Eric, I am starting to find my way.  I have a small sense of how our work will proceed this semester and how the larger pieces of the puzzle fit together.  I am hoping that as we move forward, this aspect of program management will get a bit easier.\n\nThe other challenge for me lies in understanding my relationship to the rest of the group.  This comes both from my unfamiliarity with project management in general and from the special circumstances of our group.   Given that I am not producing any tangible code or changes, I have been feeling that I am not really contributing to the project.  This has made me feel somewhat uncomfortable with suggesting deadlines for others or checking in to see how they are making progress.\n\nIn addition, I am sensitive to the fact that early in our time together as a team we discussed resisting traditional organizational structures.  This has occasionally left me even more uncertain of my role: am I a leader?  a liaison between smaller projects and tasks?  Should I make final decisions over things on which we do not reach consensus?  How do I hold my team members accountable without being authoritarian?   I am also happy to report that I have developed friendships with my teammates---but this can then lead to additional confusion when it comes to management.\n\nHopefully by the time of my next post I will be feeling a little less blind.  Until then, your suggestions and comments are welcome!\n"},{"id":"2013-03-11-size-matters","title":"Size Matters","author":"kelly-johnston","date":"2013-03-11 09:28:47 -0400","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"size-matters","content":"In geography, size matters.  On maps, large always wins over small.  We're human.  We're wired to quickly spot patterns and make visual comparisons.  See [Tufte, Edward](http://www.edwardtufte.com/tufte/books_vdqi).\n\nPicture a map of your own state.  How does it compare in size to the states next door, the largest states, the smallest, or Texas?\n\n[![](http://www.cardcow.com/images/set57/card00212_fr.jpg)](http://www.cardcow.com/images/set57/card00212_fr.jpg)\n\nI recently joined with map-minded folks to build [GeoTron 5000](http://www.geotron5000.com) to put the power of comparative geography and spatial literacy in hand.  Choose two places and the [GeoTron 5000](http://www.geotron5000.com) robot spins up two maps to show exactly how those places compare.\n\n[![Texas vs. Alaska](http://www.scholarslab.org/wp-content/uploads/2013/03/SizeMattersAKTX.png)](http://www.geotron5000.com)\n\nSo what's going on behind the scenes in GeoTron 5000 to enable these mappy comparisons?\n\n[![Natural Earth & Quantum GIS](http://www.scholarslab.org/wp-content/uploads/2013/02/SizeMattersSoftwareGroup.png)](http://www.naturalearthdata.com/)\n\n[GeoTron 5000](http://www.geotron5000.com) houses an international map library based primarily on [Natural Earth](http://www.naturalearthdata.com), a fantastic public domain vector dataset.  The Natural Earth maps were pre-processed using [Quantum GIS](http://www.qgis.org) geographic information systems software to present consistent comparisons of land area from California to Kyrgyzstan.  International country lists and official land areas were harvested from the [United Nations Statistics Division](http://unstats.un.org/unsd/methods/m49/m49regin.htm) via their [World Statistics Pocketbook](http://unstats.un.org/unsd/pocketbook/Pocketbook%202011.pdf) and [Demographic Yearbook](http://unstats.un.org/unsd/demographic/products/dyb/dyb2011.htm).  Domestic datasets are from the [US Census](http://census.gov).\n\n[GeoTron 5000](http://www.geotron5000.com) is free at the [Apple App Store](https://itunes.apple.com/us/app/geotron-5000/id539954589?ls=1&mt=8) and includes all 50 US States and the District of Columbia.  Additional geographies outside the USA are available for comparison via in-app purchase.  The app requires no cell service, no internet connection, and no international data plan when traveling.\n\nTravel is one of the best tests of our spatial literacy.  When away from familiar territory we can use the size of places we know well to better understand places we've never visited.  Travel guide books assume a high degree of spatial literacy when offering comparisons like \"Germany is about half the size of Texas\".  But spatial thinking is best served when we choose familiar frames of reference.  For example, to understand the relative size of China's Great Wall, [HowBigReally.com](http://howbigreally.com/) displays the massive wall scaled and centered over any location, here Charlottesville, Virginia:\n\n[![SizeMattersHowBigReally](http://www.scholarslab.org/wp-content/uploads/2013/02/SizeMattersHowBigReally.png)](http://www.scholarslab.org/wp-content/uploads/2013/02/SizeMattersHowBigReally.png)\n\nVisualizing comparative size and shape requires skill in [spatial thinking](http://www.ncbi.nlm.nih.gov/pmc/articles/pmc2863328/). Packing a suitcase, parking a car, finding a restaurant, finding your car when leaving the restaurant...all involve visualizing spatial relationships based on size, distance, shape, and changing points of reference.  Artsy [infographics](http://static02.mediaite.com/geekosystem/uploads/2010/10/true-size-of-africa.jpg) overlay the world on Africa and [popular television](http://www.youtube.com/watch?v=n8zBC2dvERM) explores Mercator's map distortions. We're all [thinking spatially](http://convergence.ucsb.edu/article/spatial-thinking) every day.\n\n\n<blockquote>_\"Spatial literacy is the competent and confident use of maps, mapping, and spatial thinking to address ideas, situations, and problems within daily life, society, and the world around us.\" - [Diana Stuart Sinton](http://dianamaps.com/2012/07/25/how-would-you-define-spatial-literacy/), Geographer and Spatial Thinker\n_</blockquote>\n\n\nUniversities host spatial studies [centers](http://www.spatial.ucsb.edu/) , organize spatial studies [conferences](http://digitalcommons.bucknell.edu/bugisconf/), and offer graduate level [training in spatial literacy](http://www.redlands.edu/academics/school-of-education/9762.aspx).  And spatial literacy is a topic of growing academic focus [beyond the higher ed classroom](http://www.isat.jmu.edu/geospatialsemester/).  Kids love maps.  Using maps to illustrate comparative size promotes [spatial thinking at an early age](http://www.temple.edu/psychology/newcombe/documents/early_ed_for_spatial.pdf).\n\n\n<blockquote>_\"Spatial thinking can be learned, and it can and should be taught at all levels in the education system.\" - [National Research Council](http://www.nap.edu/catalog.php?record_id=11019)_</blockquote>\n\n\nMuch of [Edward Tufte's](http://www.edwardtufte.com/tufte/) brilliant work on visual literacy is centered around maps.  In [Envisioning Information](http://www.edwardtufte.com/tufte/books_ei) he writes of maps:\n\n\n_\"No other method for the display of statistical information is so powerful.\"_\n\n\nScaled maps for geographic comparison using [How Big Really](http://howbigreally.com/) or [GeoTron 5000](http://www.geotron5000.com) inform spatial reasoning by answering the key question: compared to what?\n\nSize matters.\n\n_[Cross-posted with [johnston9494.blogspot.com](http://johnston9494.blogspot.com/2013/03/539-size-matters.html)]_\n"},{"id":"2013-03-18-prism-on-spring-break","title":"Prism on Spring Break","author":"chris-peck","date":"2013-03-18 07:38:31 -0400","categories":["Grad Student Research","Research and Development"],"url":"prism-on-spring-break","content":"Last week I was in Little Rock, Arkansas for the Society for American Music conference, but Prism seemed to be following me:\n\n[caption id=\"attachment_7682\" align=\"alignnone\" width=\"1024\"][![peerless](http://www.scholarslab.org/wp-content/uploads/2013/03/peerless_little_rock-1024x764.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/03/peerless_little_rock.jpg) peerless rainbow logo[/caption]\n\nThis logo looks uncannily like one of our sketches for Prism highlighting. So far I can't find anything on the internet about this logo or what kind of company it might be for. It seems like it must be a paint or lighting company? In any case, I bet our use of this visual treatment of text is probably cooler. There's something very compelling about refraction as a metaphor for collective interpretation of text. The crowd is a prism that reveals facets of the text, and the text is a prism that reveals facets of the crowd. The rainbow splayed across the text is an apt image for Prism's aspirations for bringing text to life through accumulated interpretations.\n"},{"id":"2013-03-19-speaker-series-dr-shawn-graham-on-practical-necromancy","title":"Speaker Series: Dr. Shawn Graham on \"Practical Necromancy,\" March 21","author":"eric-johnson","date":"2013-03-19 12:45:27 -0400","categories":["Announcements"],"url":"speaker-series-dr-shawn-graham-on-practical-necromancy","content":"On Thursday, the Scholars' Lab will have the pleasure of hosting Dr. Shawn Graham of Carleton University to talk about simulation and agent-based modeling in the humanities--and a bit about his life as a digital humanist.  He has provided a [preview of part one of his talk](http://electricarchaeology.ca/2013/03/19/practical-necromancy-talk-scholarslab-part-i/).\n\nDetails:\n\n\n**[![Practical Necromancy](http://www.scholarslab.org/wp-content/uploads/2013/03/shawngraham21.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/03/shawngraham21.jpg)Thursday, March 21**\n_2:00pm - 3:00pm_\n_Scholars' Lab, Alderman Library 4th floor_\n\n\n\n\n**Practical Necromancy: Simulation and Agent Based Modeling in the Humanities**\n\n\n\n\nDr. Shawn Graham\nAssistant Professor of Digital Humanities, Department of History\nCarleton University\n\n\n\n\nRaising the dead presents certain difficulties, but computation suggests a way forward. In Practical Necromancy, Dr. Graham discusses the use of agent based simulations to understand aspects of Greco-Roman antiquity, its perils and potentials, and how all of this fits into a worldview informed by the digital humanities.\n\n\n\n\nShawn Graham is a Roman archaeologist by training, and a digital humanist by accident. After many years in the academic wilderness, where he did everything from teaching high school, to starting up some heritage-based businesses, to hunkering down in the trenches of for-profit online education, he discovered that his interests in games for learning and teaching, and simulation more generally, had turned him into a Digital Humanist. He escaped to Carleton University in Ottawa, Canada, shortly thereafter, which also happens to be in his home town.\n\n\n\n\nHis work surveys the ways new media are used to construct cultural heritage knowledge, from the perspectives of practicing archaeologists, historians, and the wider public. It's an exploration of how 'historical consciousness' informs, and is formed by, digital media. He teaches primarily historical methods and digital history at all levels, including a graduate seminar in public history/digital history. In that class, the final project involves using various augmented reality platforms for public storytelling. Currently he is teaching a third year seminar on video games as historical artefacts which form a digital historical consciousness; the final project is the 'perfect video game for expressing history, giving a voice to the voicelss'. His students blog at [http://3812.graeworks.net](http://3812.graeworks.net) and he would love to have your comments on their work.\n"},{"id":"2013-03-20-announcing-the-praxis-network","title":"Announcing the Praxis Network","author":"katina-rogers","date":"2013-03-20 05:12:08 -0400","categories":["Announcements","Grad Student Research"],"url":"announcing-the-praxis-network","content":"How can humanities graduate programs better equip students for a wider range of careers, without sacrificing the core values or approaches of the discipline? **We are delighted to announce the launch of the [Praxis Network](http://praxis-network.org/)**, a new partnership of innovative graduate and undergraduate programs that are making effective interventions in the traditional models of humanities pedagogy and research.\n\nThe Praxis Network features graduate programs at the [University of Virginia](http://praxis.scholarslab.org/), [Michigan State University](http://chi.anthropology.msu.edu/), [CUNY Graduate Center](http://digitalfellows.commons.gc.cuny.edu/), [University College London](http://www.ucl.ac.uk/dh/courses/mamsc), and [Duke University](http://www.fhi.duke.edu/labs/phd), as well as undergraduate programs at [Hope College](http://www.hope.edu/academic/mellon/) and [Brock University](http://www.brocku.ca/humanities/departments-and-centres/interactive-arts-and-science). The partnership is **one of three complementary projects in the Scholarly Communication Institute’s current work on [rethinking graduate education](http://uvasci.org/current-work/graduate-education/)**.\n\nFirst, we are convening a series of experts’ meetings in conjunction with the [Consortium for Humanities Centers and Institutes](http://chcinetwork.org/) (CHCI) and [centerNet](http://digitalhumanities.org/centernet/), its digital counterpart, to discuss ways that traditional and digital humanities centers can effect change both within and across institutions.\n\nSecond, we have conducted **a [study](http://mediacommons.futureofthebook.org/alt-ac/who-we-are) on the level of career preparation provided by graduate programs** in order to assess the most important points of leverage. It is clear from the results, which will be published along with the data later this year, that most graduates and their employers find that they do not gain many of the skills that are important in their professional environments—such as collaboration, project management, and communication with varied audiences—through their graduate study.\n\nFinally, the Praxis Network provides a closer look at select programs that have taken unusual and effective approaches to addressing some of the issues that the survey uncovered. **The goals of each unique program are student-focused, digitally-inflected, interdisciplinary, and frequently oriented around collaborative projects.** The website, which is the first product of the partnership, takes the important step of sharing information about the commonalities and unique properties of these programs in a way that makes it easy to compare them.\n\nHumanities programs have the opportunity to better serve their students as well as the public by examining our core values and rethinking the methods we use to teach them. The Praxis Network programs show just a few possible ways to move toward **collaborative projects, public engagement, and embracing an ethos of openness and exploration. **\n"},{"id":"2013-03-20-head-graduate-programs","title":"Are you our new Head of Graduate Programs?","author":"katina-rogers","date":"2013-03-20 06:25:57 -0400","categories":["Announcements","Grad Student Research"],"url":"head-graduate-programs","content":"We are delighted to announce an exciting job opportunity here at the [Scholars’ Lab](http://www.scholarslab.org/) as the Head of Graduate Programs, which includes both the [Praxis Program](http://praxis.scholarslab.org/) and the [Graduate Fellows in Digital Humanities](http://www.scholarslab.org/graduate-fellowships/) program. Read on for more details!\n\n**Head of Graduate Programs**\nThe University of Virginia Library seeks an experienced, versatile digital scholar and administrator to lead programs for graduate students in our internationally recognized Scholars' Lab; home of the Praxis Program and a vibrant community of Graduate Fellows in Digital Humanities. The ideal candidate will have: deep familiarity with humanities scholarship and digital methods at the graduate level; an interest in experimental approaches to analysis, authoring, and publication; experience in teaching and administrative roles in higher education; and a commitment to the training of emerging scholars and alternative academic humanities professionals. Reporting to the [Director of Digital Research and Scholarship](http://nowviskie.org) for UVa Library, the Head of Scholars' Lab Graduate Programs joins an accomplished and forward-looking digital scholarship team, and is eligible for the self-directed research time that all of our staff members are granted for professional engagement and to pursue their own, often collaborative, R&D projects.\n\n**Primary Responsibilities**\nMentoring, managing day-to-day operations, and coordinating staff support for both team-based and individual graduate fellowship programs at U.Va. Library. Developing intellectual programming in the digital humanities for the Scholars' Lab and building community among emerging scholars at U.Va. Fostering collaboration on humanities training and research support with internal and external partners, including the [Praxis Network](http://praxis-network.org/).\n\n**Knowledge, Skills, and Abilities**\nWorking knowledge of digital humanities technologies and directions. Strong interest in mentoring junior scholars from project conceptualization to published outcomes. Excellent communications skills, including the ability to present complex technical information to a generalist audience and a clear understanding of humanities perspectives and needs. Previous experience in higher education administration and experience in scholarly research, writing, and digital project development preferred.\n\n**Education**\nGraduate study (PhD preferred) in a field related to humanities scholarship or humanistic aspects of information science.\n\n**Experience**\n4 to 7 years, with demonstrated ability as an instructor, mentor, writer, and researcher. Familiarity with development and delivery techniques for digital humanities content and software. Project management or supervisory experience highly desirable.\n\n**Salary and Benefits**\nSalary is commensurate with experience, and expected to range between approximately $65K and $75K per annum. Excellent benefits, including paid leave, TIAA/CREF and other retirement plans, along with generous funding for travel and professional development.\n\nFor full details, and to apply for the position, please see the [official posting](http://jobs.virginia.edu/applicants/Central?quickFind=69950). (If you need to search the [Jobs@UVa](http://jobs.virginia.edu) portal, the posting number is 0611761). The University of Virginia is an Equal Opportunity/Affirmative Action employer strongly committed to achieving excellence through cultural diversity. The University actively encourages applications and nominations from members of underrepresented groups.\n\nDon’t miss a chance to work with our wonderful students and incredible Scholars' Lab team!\n"},{"id":"2013-03-21-slab-out-about","title":"SLab Out & About","author":"ronda-grizzle","date":"2013-03-21 06:38:27 -0400","categories":["Announcements"],"url":"slab-out-about","content":"SLab folks get out and about! Here’s where we’ve been over the last few months:\n\nBethany Nowviskie was invited to give the [Japanese Association for Digital Humanities](http://www.jadh.org/jadh2012) keynote address in Tokyo in September 2012 (\"[Too Small to Fail](http://nowviskie.org/2012/too-small-to-fail/)\") and to participate in one of Michael Bérubé’s MLA Presidential Forum  events on _Avenues of Access_ at the [2013 Modern Language Association meeting](http://www.mla.org/convention). Her invited talk was called \"[Resistance in the Materials](http://nowviskie.org/2013/resistance-in-the-materials/).\" In Boston, Bethany also participated in  roundtable discussion with fellow members of MLA's Task Force on Doctoral Study. Last semester, she and David McClure were invited to give a lecture at the University of Maryland as part of MITH's _Digital Dialogues_ series. That talk and demo, entitled [_Space, Time, and the Problem of Scale: Digital Storytelling with Neatline_](http://mith.umd.edu/podcasts/bethany-nowviskie-and-david-mcclure-space-time-and-the-problem-of-scale-digital-storytelling-with-neatline/), took place in November 2012 and was repeated at UVa. This month, she will give a workshop, seminar, and public lecture as a Lansdowne visiting scholar at the University of Victoria.\n\nDavid McClure has been traveling here and there teaching and talking about [Neatline](http://neatline.org). In addition to the talk with Bethany (which they repeated at the Scholars' Lab), he and Jeremy Boggs led a web seminar for the National Institute for Technology in Liberal Education (NITLE) entitled [_Geotemporal Storytelling with Neatline_](http://www.nitle.org/live/events/147-geotemporal-storytelling-with-neatline). David also gave a two-day workshop at Beloit College and a lecture introducing Neatline at the [Alabama Digital Humanities Center](http://www.lib.ua.edu/digitalhumanities) in October 2012 entitled _Maps, Timelines, and Archives: Using Neatline to Plot Digital Collections in Space and Time_.\n\nKatina Rogers guest taught a session at Matt Gold’s graduate seminar at CUNY entitled _Debates in the Digital Humanities: Towards a Networked Academy_ and ran a session at [THATCamp CHNM](http://chnm2012.thatcamp.org/) on graduate education reform. Katina has also [guest blogged for the ProfHacker blog](http://chronicle.com/blogs/profhacker/graduate-education-reform/45043), taken part in [a roundtable at MLA 2013 on rebooting graduate education](http://katinarogers.com/2013/01/06/rebooting-graduate-training-mla/) (along with SLab Fellow Annie Swafford), and had her work featured in the Chronicle of Higher Education ([#alt-ac survey results](http://chronicle.com/article/In-Search-of-Hard-Data-on/134030/) and [graduate education reform](http://chronicle.com/blogs/conversation/2013/01/07/rebooting-graduate-education-in-the-humanities/)).\n\nIn January 2013, Eric Rochester spoke about digital tools in the [Association for Computers and the Humanities](http://ach.org) panel on interoperability entitled _Open Sesame_ at MLA 2013.\n\nNancy Kechner taught the Pharm D group at the UVa Medical Center to incorporate SPSS into their clinical research, allowing them to present their cutting edge results as Rounds and at conferences. Nancy's also helped the Infectious Disease group to use tools provided by the SLab to further their research.\n\nWhat are we looking forward to next? [Digital Humanities 2013](http://dh2013.unl.edu) at the University of Nebraska-Lincoln on July 16-19!\n"},{"id":"2013-03-21-spring-newsletter","title":"Spring Newsletter","author":"ronda-grizzle","date":"2013-03-21 07:58:14 -0400","categories":["Announcements"],"url":"spring-newsletter","content":"Our Spring 2013 newsletter (pdf) is [available for download](http://www.scholarslab.org/wp-content/uploads/2013/03/2013spring-1-final.pdf).\n\nIn this issue, you'll find information about Chris Gist and Kelly Johnston's GIS Workshop series, an update on the Scholarly Communication Institute's activities, and a round up of what the SLab staff have been up to as we get [out and about](http://www.scholarslab.org/announcements/slab-out-about/), spreading the digital humanities joy.\n\nWe hope you enjoy it!\n"},{"id":"2013-03-28-installing-omeka-through-amazon-web-services","title":"Installing Omeka through Amazon Web Services","author":"cory-duclos","date":"2013-03-28 11:56:09 -0400","categories":["Research and Development"],"url":"installing-omeka-through-amazon-web-services","content":"This set of instructions was developed, in part, during the [Digital Humanities Winter Institute](http://mith.umd.edu/dhwi/) at the University of Maryland. Under the direction of Wayne Graham, a small group set out to install [Omeka](http://omeka.org/) through [Amazon Web Services](https://aws.amazon.com/). The directions were put together collaboratively by David Kim and Cory Duclos.\n\nUnlike other Omeka installation guides, this one does not pretend to be \"easy.\" There are some technical abilities you will need, including being comfortable using a terminal. This guide was written using OS X, but if you are a Windows user, the Git Bash prompt that ships with [Git](http://git-scm.com/downloads) should work. Ideally, you should be doing this install on your own, private computer so that you can add security permissions for future server access.\n\n\n**Step 1**: Create an amazon web server account at [aws.amazon.com](http://aws.amazon.com). This will require entering a credit card and going through a phone verification process.\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-1-300x149.png)](http://www.scholarslab.org/research-and-development/installing-omeka-through-amazon-web-services/attachment/step-1/)\n\n\n\n**Step 2**: After setup, click on \"My Account\", then \"AWS Management Console\", then find the link to EC2 (Virtual Serves in the Cloud) and click on it\n\n\n![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-2.2-300x149.png)\n\n\nThen click \"Launch Instance\":\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-2.3-300x148.png)](http://www.scholarslab.org/?attachment_id=7118)\n\n\nSelect the \"Classic Wizard\" and click continue:\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-2.4-300x153.png)](http://www.scholarslab.org/research-and-development/installing-omeka-through-amazon-web-services/attachment/step-2-4/)\n\n\n\n\n**Step 3:** In Classic Wizard: Choose an AMI, select _Ubuntu, 12.04.01 LTS  64Bit_:\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-3.1-300x26.png)](http://www.scholarslab.org/?attachment_id=7120)\n\n\n\n\n**Step 4**: In Classic Wizard: Instance Details, Continue with default settings through the next three settings (1 instances; T1 Micro, etc.)\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-4.1-300x203.png)](http://www.scholarslab.org/?attachment_id=7121)\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-4.2-300x204.png)](http://www.scholarslab.org/research-and-development/installing-omeka-through-amazon-web-services/attachment/step-4-2/)\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-4.3-300x205.png)](http://www.scholarslab.org/research-and-development/installing-omeka-through-amazon-web-services/attachment/step-4-3/)\n\n\n\n\n**Step 5**: In Classic Wizard: Instance Details - Key/Value tables: Under Value add a name for your project:\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-5-300x204.png)](http://www.scholarslab.org/?attachment_id=7124)\n\n\n\n\n**Step 6**: In Classic Wizard: KeyPair\n\nCreate key pair, adding in a unique username (which will be used again in step 14).\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-6-300x202.png)](http://www.scholarslab.org/research-and-development/installing-omeka-through-amazon-web-services/attachment/step-6/)\n\n\nThis will cause your system to download a [.pem file](http://stackoverflow.com/questions/10733641/what-is-a-pem-file-and-how-to-use-it). Leave it where it is for now.\n\n\n\n**Step 7**: Classic Wizard: Configure Firewall\n\nFrom the drop down menu on the left, select select \"HTTP\" and click \"add rule\". It will appear on the right.\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-7-300x203.png)](http://www.scholarslab.org/?attachment_id=7126)\n\n\n\n\n**Step 8**: Classic Wizard: Review\n\nClick **Launch**\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-8-300x204.png)](http://www.scholarslab.org/?attachment_id=7127)\n\n\nthen **Close** in the next window.\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-8.2-300x156.png)](http://www.scholarslab.org/research-and-development/installing-omeka-through-amazon-web-services/attachment/step-8-2/)\n\n\n\n\n**Step 9**: The easy part is over.\n\n\n\n**Step 10**:\n\nBack in the EC2 dashboard, click on \"0 Running Instances\"\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-10-300x149.png)](http://www.scholarslab.org/research-and-development/installing-omeka-through-amazon-web-services/attachment/step-10/)\n\n\n\n\n**Step 11**:\n\nIn the subsequent panel, _RIGHT_ click on the code under \"**instance**\" and select '**connect**'.\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-11.1-300x153.png)](http://www.scholarslab.org/?attachment_id=7130)\n\n\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-11.2-300x153.png)](http://www.scholarslab.org/research-and-development/installing-omeka-through-amazon-web-services/attachment/step-11-2/)\n\n\n\n\n**Step 12**:\n\nSelect \"**Connect with standalone SSH Client**\" do not close window yet.\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-12-300x253.png)](http://www.scholarslab.org/?attachment_id=7132)\n\n\n\n\n**Step 13**:\n\nOpen the terminal (Applications/utilities/terminal for OS X users); note that your color scheme may vary from these screenshots.\n\nCreate a new directory called ec2 by running command\n\n[code lange=\"bash\"]\nmkdir -p ~/.ec2\n[/code]\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-13-300x170.png)](http://www.scholarslab.org/?attachment_id=7133)\n\n\n\n\n**Step 14**: run\n\n[code lang=\"bash\"]\nmv ~/Downloads/[username].pem ~/.ec2\n[/code]\n\nThe username is what you generated in Amazon EC2 in step 6.\n\nThis moves the pem file to the new directory named .ec2\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-14-300x171.png)](http://www.scholarslab.org/research-and-development/installing-omeka-through-amazon-web-services/attachment/step-14/)\n\n\n\n\n**Step 15**: Enter and run the command:\n\n[code lang=\"bash\"]chmod 400 ~/.ec2/*.pem\n[/code]\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-15-300x234.png)](http://www.scholarslab.org/?attachment_id=7135)\n\n\n\n\n**Step 16:** Back in your web browser, copy the line in the box labled \"**Enter the following command line:**\" (begins with '_ssh_') and paste it in terminal. If it tries to run by itself, you copied the return. _Hit the up arrow_ to call back the command so that it can be edited. You won't run the code exactly as you copied it.\n\nAfter -i add `~/.ec2/[username].pem`\n\nChange _amazon_ to _ubuntu_ so that it looks something like the last line here:\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-16-300x178.png)](http://www.scholarslab.org/?attachment_id=7137)\n\n\n\n\n**Step 17:** run\n\n[code lang=\"bash\"]\nsudo apt-get update\n[/code]\n\nto update the server libraries. This command will generate a wall of text.\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-17-300x173.png)](http://www.scholarslab.org/?attachment_id=7138)[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-17.1-300x171.png)](http://www.scholarslab.org/research-and-development/installing-omeka-through-amazon-web-services/attachment/step-17-1/)\n\n\n\n\n**Step 18:** run\n\n[code lang=\"bash\"]\nsudo apt-get upgrade\n[/code]\n\n.\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-18-300x168.png)](http://www.scholarslab.org/?attachment_id=7140)\n\n\nType \"_Yes_\" when prompted.\n\n\n[![](http://www.scholarslab.org/wp-content/uploads/2013/01/Step-18.1-300x172.png)](http://www.scholarslab.org/research-and-development/installing-omeka-through-amazon-web-services/attachment/step-18-1/)\n\n\nYour server is up and running. Now we can shift our attention to installing all the libraries Ubuntu needs to run Omeka.\n\n\n**Step 19:** Installing Server Packages\n\nAfter the server is up and running, we need to get the components that are needed to run a web server installed. I'll use a short-hand here to install a bunch of packages (and their dependencies). Then tell the Apache daemon to enable the mod_rewrite module that Omeka uses to make \"pretty\" URLs.\n\nNOTE: When logging onto the AWS server, you may be put into the ubuntu directory. You need to get to the main directory. Change the directory and verify by listing the files in that directory.\n\n[code lang=\"bash\"]\ncd /\nls\n[/code]\n\n[![Screen Shot 2013-02-21 at 7.13.47 PM](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-21-at-7.13.47-PM-300x80.png)](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-21-at-7.13.47-PM.png)\n\n[code lang=\"bash\"]\nsudo apt-get -y install apache2 php5 php5-xsl php5-mysql php5-curl mysql-server zip imagemagick sendmail\nsudo a2enmod rewrite\n[/code]\n\nAs part of the installation process, you'll be asked to create a 'root' account for the MySQL server. Just remember whatever you use for this account as you'll need it later to create the database and user for Omeka.\n\n[![Screen Shot 2013-02-14 at 1.56.58 PM](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-14-at-1.56.58-PM-300x239.png)](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-14-at-1.56.58-PM.png)\n\nAfter the installation and configuration has finished, you can test that the web server is running by pointing your browser at the server name you have (it'll be something like http://ec2_123.345.56.78/, whatever the server connection in the AWS panel is). If everything is correctly to this point, you should see a page in the browser that says \"It Works!\"\n\n\n\n**Step 20:** Download Omeka\n\nThe default location for the web applications for Apache2 is `/var/www/`. For the purposes of this tutorial, we'll download the Omeka application and mv the files here.\n\nAssuming you're still logged on to your server, you will need to issue the following commands to download Omeka:\n\n[code lang=\"bash\"]\ncd /tmp\ncurl -O http://omeka.org/files/omeka-1.5.3.zip\nunzip omeka-1.5.3\nsudo mv omeka-1.5.3 /var/www/omeka\nsudo chmod -R 777 /var/www/omeka/archive\n[/code]\n\n\n\n**Configure MySQL:**\n\nAssuming you're still on the server you're wanting to run Omeka on (and you're not wanting to mess with the Amazon RDS), you will need to configure the MySQL database to create a user, a database, and allow the user to work with the database locally. You'll be writing this password to the filesystem, so whatever password you choose, you don't really need to remember what the password is, just where it's at. For this reason, I recommend using a password generator (I use [Strong Password Generator](http://strongpasswordgenerator.com/) for these purposes). Whatever the password is, you will need to replace where I type '[your password]' in the following examples (and don't type the '$'or 'mysql'; these just differentiate the difference between the terminal and mysql prompts):\n\n[code lang=\"bash\"]\n$ mysql -u root -p\nEnter password:\nmysql> create database omeka;\nQuery OK, 1 row affected (0.00 sec)\nmysql> grant usage on *.* to omeka_user@localhost identified by '[your password]';\nQuery OK, 0 rows affected (0.00 sec)\nmysql> grant all privileges on omeka.* to omeka_user@localhost;\nQuery OK, 0 rows affected (0.00 sec)\n[/code]\n\nNow that the database is set up, we need to let Omeka know where to go to connect to the database.\n\n\n\n**Step 22:** Editing the Omeka `db.ini` file\n\nIf you went to the Omeka path on your system right now (e.g. http://yourEC2.instance/omeka), you'll notice that there's a big error on the page. We need to tell Omeka where to look for the database connection, and the tools for doing this can be kind of scary on the terminal.\n\nFirst get into you Omeka folder and find the db.ini file\n\n[code lang=\"bash\"]\ncd /var/www/omeka/\nls\n[/code]\n\nThen you can get in to edit this file using the sudo vim command.\n\n[code lang=\"bash\"]\nsudo vim db.ini\n[/code]\n\nYou'll see the contents of the db.ini file, and instructions to replace the X's with your own information. This can be done by pressing the i, which will allow you to insert your own text as follows:\n\n[code lan=\"html\"]\nhost = \"localhost\"\nusername = \"omeka_user\"\npassword = \"(password generated in step 20)\"\ndbname = \"omeka\"\n[/code]\n\n[![Screen Shot 2013-02-21 at 7.24.47 PM](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-21-at-7.24.47-PM-300x226.png)](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-21-at-7.24.47-PM.png)\nLeave everything else the same. Hit control + c to exit the edit mode and :wq to write and quit out of the program.\n\nNow restart apache\n\n[code lang=\"bash\"]\nsudo service apache2 restart\n[/code]\n\nIn your browser navigate to  http://yourEC2.instance/omeka and you should be ready to install.\n\n\n\n**Step 23:** Possible Error\nIt may be the case that you see an error about the mod_rewrite not being activated. To fix this, do the following:\n\n[code lan=\"bash\"]\ncd /etc/apache2/sites-available/\nsudo vim default\n[/code]\n\nNow you'll see the default file. You need to change the allow from to all under <Directory /var/www/> using the vim commands you used to change the db.ini file (see image)\n\n[![Screen Shot 2013-02-21 at 7.38.21 PM](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-21-at-7.38.21-PM-275x300.png)](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-21-at-7.38.21-PM.png)\n\nNow you need to change the .htaccess file in the Install directory\n\n[code lang=\"bash\"]\ncd /var/www/omeka/install\nsudo vim .htaccess\n[/code]\n\nFind the instruction in this document which tells you to uncomment a line and add your own directory. Make the changes using the vim commands from above.\n\n[ ](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-22-at-9.26.51-AM.png)\n\n[\n](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-22-at-9.27.57-AM.png)[![Screen Shot 2013-02-22 at 9.26.51 AM](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-22-at-9.26.51-AM-300x224.png)](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-22-at-9.26.51-AM.png)\n\n\n[![Screen Shot 2013-02-22 at 9.27.57 AM](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-22-at-9.27.57-AM-300x222.png)](http://www.scholarslab.org/wp-content/uploads/2013/02/Screen-Shot-2013-02-22-at-9.27.57-AM.png)\n\n\nThen restart apache.\n\n[code lang=\"bash\"]\nsudo service apache2 restart\n[/code]\n\nThen navigate in your browser back to your site and you should be able to run the install.\n\n\n\n**Step 22:** Adding Plugins\n\nTo add a plugin, navigate to the plugins folder in terminal, copy the link for the plugin download, and run the following commands. This will download a .zip file, unzip the file, and delete the original .zip file. The plugin should then be available in your omeka. The code below shows how to install the Neatline plugin, but this could work for any other plugin.\n\n[code lang=\"bash\"][/code]\n\n\ncd /var/www/omeka/plugins\nsudo curl -O http://omeka.org/wordpress/wp-content/uploads/Neatline-1.1.2.zip\nsudo unzip Neatline-1.1.2.zip\nsudo rm Neatline-1.1.2.zip\n\n[code][/code]\n"},{"id":"2013-03-29-2013-14-dh-fellowship","title":"Call for Applicants: UVa Graduate Fellowship in DH","author":"eric-johnson","date":"2013-03-29 10:17:58 -0400","categories":["Announcements","Grad Student Research"],"url":"2013-14-dh-fellowship","content":"[![fellows](http://www.scholarslab.org/wp-content/uploads/2009/10/fellows1.png)](http://www.scholarslab.org/wp-content/uploads/2009/10/fellows1.png)The Scholars’ Lab is proud to host prestigious [fellowship program](http://www.scholarslab.org/graduate-fellowships/) for ABD graduate students doing significant and innovative work in the digital humanities at the University of Virginia.\n\nSupported by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew & Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities, the Graduate Fellowship in DH is designed to advance the humanities and provide emerging digital scholars with an opportunity for growth.\n\nIn collaboration with other Library departments, the Scholars' Lab offers Grad Fellows consulting services and assistance with the creation and analysis of digital content, as well as advice on intellectual property issues and best practices in digital scholarship and DH software development. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars' Lab, make use of a dedicated grad lounge, and participate in one formal colloquium at the Library per semester.\n\n**Applications for the 2013-14 fellowship year are now being accepted.**  Please see the [DH Fellowship page](http://www.scholarslab.org/graduate-fellowship-in-digital-humanities/) for more information about the program, eligibility requirements, and application information. **Deadline: April 15th!**\n\nPlease contact [Eric Johnson](mailto:ej9k@virginia.edu), Head of Outreach & Public Services at the Scholars' Lab, with any questions.\n"},{"id":"2013-03-29-seeking-praxis-fellows","title":"Call for Applicants: Praxis Program Fellows","author":"eric-johnson","date":"2013-03-29 10:18:23 -0400","categories":["Announcements","Grad Student Research"],"url":"seeking-praxis-fellows","content":"![](http://static.scholarslab.org/images/praxis-program-logo.png)\n\nUVa grad students! **Apply by April 15th** for a unique, funded opportunity to work with an interdisciplinary group of your peers, learning digital humanities skills from the experts, and collaboratively designing and executing an innovative digital project.\n\nEach year, the Scholars' Lab [Praxis Program](http://praxis.scholarslab.org/) provides six UVa graduate students with hands-on experience in digital humanities collaboration and project creation. Team members work to take a shared project from conception to design and development, and finally to publication, community engagement, and analysis. Praxis is a unique and well-known training program in the international digital humanities community. Our fellows [blog about their experiences](http://www.scholarslab.org/category/praxis-program/) and develop increased facility with project management, collaboration, and the public humanities, even as they tackle (most for the first time, and with the mentorship of our wonderful [faculty and staff](http://www.scholarslab.org/people/)) new programming languages, tools, and digital methods.\n\nThe [2012-13 Praxis cohort](http://praxis.scholarslab.org/people.html) is in full swing, thanks to a generous grant from the [Andrew W. Mellon Foundation](http://mellon.org/), through UVa Library’s [Scholarly Communication Institute](http://uvasci.org/current-work/) (SCI).  The 2013-14 cohort will be supported by the [University of Virginia Library](http://library.virginia.edu/).  Recently, the Scholars' Lab joined with like-minded institutions to create a recently-launched [Praxis Network](http://praxis-network.org/), made up of allied but differently-inflected humanities education initiatives from around the world, mainly focused on graduate training, and all engaged in rethinking pedagogy and campus partnerships in relation to the digital humanities. A new Head of Graduate Programs will join us in 2013, working closely with Scholar's Lab [Grad and Praxis fellows](http://www.scholarslab.org/graduate-fellowships/).\n\n**We will welcome six new, competitively-selected Praxis students in late August 2013**. Each will be awarded $8000 in fellowship funds, and will be expected to devote approximately 10 hours per week in the fall and spring semesters, to learning together and building a collaborative digital humanities project in the Scholars’ Lab. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, and can make use of a dedicated grad lounge.\n\nAll _University of Virginia graduate students_ working within or committed to humanities disciplines are eligible to apply to join the 2013-14 cohort. We particularly encourage applications from women, LGBT students, and people of color, and will be working to put together an interdisciplinary and intellectually diverse team.\n\n**Information session, Monday, April 8th**\nPlease join Scholars’ Lab staff and current Praxis Fellows on Monday, April 8th, for a Q&A session on the program, to be held in the Scholars’ Lab at 4:00 p.m. RSVP with Eric Johnson, Head of Outreach & Public Services at [ej9k@virginia.edu](mailto:ej9k@virginia.edu). If you’re unable to join us for the session but have questions about the program, don’t hesitate to be in touch.\n\n**Application deadline: Friday, April 15th**\nThe application process is simple: direct an email to Eric Johnson at the address above. Please indicate why you’re interested in the Praxis Program, what you think you will gain from it, and what you feel you would bring to a collaborative digital humanities project. A small committee, consisting of Scholars’ Lab faculty and staff and past Praxis Program participants, will evaluate expressions of interest and schedule group interviews with finalists.\n"},{"id":"2013-04-01-a-million-likes","title":"Scholars' Lab Campaigns for a Million \"Likes\"","author":"bethany-nowviskie","date":"2013-04-01 05:08:43 -0400","categories":["Announcements"],"url":"a-million-likes","content":"*** * * TRANSCRIPT * * ***\n\nA [new social media campaign](https://www.facebook.com/scholarslab) is taking Facebook by storm! Staff of the [Scholars' Lab](http://scholarslab.org/), a prominent University of Virginia-based academic technology center, have gone viral with an adorable picture and charming plea. This is not the first time the SLab has captured the collective imagination of the international digital humanities community -- but it may be the last!\n\n**[PHOTO: Smiling staff gather around a whiteboard that reads: \"If the SLab page gets a million \"likes,\" Bethany will buy us an EMP doomsday device! *Zap!* Lights OUT! (Sci-Fi 4-EVA) *Bye-bye, DH!* (She doesn't think we can do it!) Please LIKE!\"]**\n\n\n[![A Million Likes](http://www.scholarslab.org/wp-content/uploads/2013/04/photo-1024x768.jpg)](https://www.facebook.com/scholarslab)\n\n\n**SEAL:** It all began in the early hours of March 28th, when Wayne Graham, head of Scholar's Lab R&D, received a call from systems administrators informing him he was to spend the next 16 hours tracking down the source of unusual activity on one of the lab's servers.\n\n**GRAHAM:** _I just couldn't _BEEEP_ing believe the _BEEP BEEEP BEEP-BEEP_ers _BEEP BEEEP_ed the _BEEP BEEP BEEEP BEEP_-box again. _BEEEEEEP.\n\nMeanwhile, Ronda Grizzle, who handles communications for the SLab, was waking to a message of her own.\n\n**[CLOSEUP: Inbox. Email title reads: \"Re: Re: RE: Newsletter Draft 47b-rev6 - just 1 more little tweak, plz!\"]**\n\nIn a characteristic display of Scholars' Lab camaraderie, [Neatline](http://neatline.org/) developer David McClure (who quite possibly sleeps at his desk) was waiting to comfort Grizzle on her arrival at the office. But things went rapidly downhill.\n\nInvestigating \"ongoing, illicit humanities crowdsourcing / sweatshop activities,\" student reporters from the [_Cavalier Daily_](http://www.cavalierdaily.com) delivered a FOIA request to stunned graduate fellows in the SLab Grad Lounge, site of the Library's innovative [Praxis Program](http://praxis-network.org/praxis-program.html). At the same time, a malfunctioning Ron Swanson chat-bot attached to the SLab's [IRC channel](irc://irc.freenode.net/#slab) began to exhibit what Brooklyn-based staffer Dr. Katina Rogers called \"disturbing signs of sentience.\"\n\n**ROGERS:** [via chat] _or maybe I've just been hanging out in here too long <j/k> waynebot++ #slab IRC FTW!_\n\nBy the end of what some observers called a \"not atypical\" Thursday, emergency personnel had been called to Alderman Library twice (first treating administrative assistant Becca Peters for severe paper-cuts incurred while processing fifty thousand [Scholarly Communication Institute](http://uvasci.org/) travel reimbursements, and later hurriedly wheeling a large, struggling man, prostrate and bound, to a waiting, unmarked van). Colleagues identified the man as Design Architect Jeremy Boggs. UVa Today was on the scene for this incident, and approached public services chief Eric Johnson for comment.\n\n**JOHNSON:** [sitting morosely beside a 3d-printer adorned with Scholars' Lab stickers] _He's been trying to level the build-plate. I -- I just can't talk about it!_ [sobs]\n\nThe same afternoon, GIS specialists Chris Gist and Kelly Johnston saw their popular program in \"[DIY aerial photography](http://www.scholarslab.org/?s=diy+aerial)\" crash to earth, as Charlottesville city council members authorized [aggressive counter-measures](http://usnews.nbcnews.com/_news/2013/02/05/16857529-virginia-city-becomes-first-to-pass-anti-drone-resolution?lite) to halt the Scholars' Lab's use of a helium balloon and small, mechanized quad-copter to photograph archaeological sites and public art installations at the request of UVa faculty.\n\n**GIST:** _A sling-shot, Councilman? Seriously?_\n**JOHNSTON:** [angrily] _That was **NOT** a drone!_\n\nBut it was in his support for the lab's core activity -- enabling UVa faculty and students to perform research and scholarship using new technology -- that Senior Developer Dr. Eric Rochester found a way forward.\n\n**ROCHESTER:** _I was painstakingly refining our web-based approach to delivering scanned historical maps to scholars for analysis and display, when my fingers slipped. Delete! It was... life-changing._\n\nHis colleagues agreed, and Scholars' Lab director Dr. Bethany Nowviskie issued a challenge. One packet of dry-erase markers and [a Facebook post](https://www.facebook.com/scholarslab) later, the campaign was underway. We caught up with Nowviskie, to hear her response to staff ambitions to destroy the digital humanities once and for all, using a devastating global electromagnetic pulse from a \"doomsday device.\"\n\n**NOWVISKIE:** _Well, it's important to say that I'm still waiting to hear from Procurement about whether I can put equipment capable of forever ending worldwide electronic communications on my UVa VISA card. But, to speak to the viral aspects of the project: perhaps I should have seen this coming. After all, a few years ago I had the experience of popularizing [a Twitter hashtag](https://twitter.com/search?q=altac&src=typd) among DHers and so-called alt-academics which, by some accounts, sparked an important movement in higher education._\n\n**SEAL:** _You refer, of course, to [#fortranDH](http://en.wikipedia.org/wiki/File:FortranCardPROJ039.agr.jpg)._\n\n**NOWVISKIE:** _The wave of the future._\n\nBut not according to her staff and their many supporters. At last count, the winsome smiles and can-do attitude the team exhibited on Facebook had garnered [their SLab page](https://www.facebook.com/scholarslab) a whopping 323 \"likes\" -- well on the way, say Scholars' Lab-trained digital humanities librarians Dr. Alex Gil (Columbia U) and Jean Bauer (Brown University), to the winning million.\n\n**BAUER:** _I clicked! And so have all my friends and colleagues, who work unceasingly to re-mediate, describe, and contextualize humanities texts and artifacts; to collect, visualize, and preserve born-digital information; to develop and test new analytic methods and modes of scholarly communication; and to use technology to question received notions of canonicity and expert knowledge. We see this as a game-changer -- a once-in-a-lifetime opportunity for the entire field of DH._\n\n**GIL:** [from atop the barricades] _Faculty, staff, and students! The time has come! I really, really, really need a nap._\n\nUVa Library grants officer Raylon Johnson could not be reached for comment. Friends in Financial Services indicated he may be headed \"somewhere tropical,\" adding, \"Don't worry about Ray.\"\n\nLong-term humanities computing practitioners identify the [past sixty years](http://en.wikipedia.org/wiki/Digital_humanities#History) of interdisciplinary, technology-assisted scholarship as a pragmatic and intellectual struggle toward new interpretive paradigms for historical understanding and the ongoing study of human arts and cultures. Others see the digital humanities as a fresh, recently-emerging, sweeping trend -- the academy's \"next big thing.\"\n\nBut thanks to the [Scholars' Lab](http://scholarslab.org) at the [University of Virginia Library](http://library.virginia.edu), a different consensus is building in social media: when it comes to DH, we're almost done.\n\n*** * * END TRANSCRIPT * * ***\n"},{"id":"2013-04-01-scholars-lab-talk-archive-page-and-future-plans-songs-of-the-victorians-and-augmented-notes","title":"Scholars' Lab Talk, Archive Page, and Future Plans: Songs of the Victorians and Augmented Notes","author":"annie-swafford","date":"2013-04-01 11:29:28 -0400","categories":["Grad Student Research"],"url":"scholars-lab-talk-archive-page-and-future-plans-songs-of-the-victorians-and-augmented-notes","content":"[Cross-posted from [Anglophile in Academia](http://anglophileinacademia.blogspot.com/2013/04/archive-page-available-for-balfes-come.html)]\n\n\n\n\nThis week, I've been preparing for my [Scholars' Lab](http://www.scholarslab.org/) [talk](https://www.google.com/calendar/render?eid=ZGFhMmh0NjZ2NjNuOWk4NnBxOW0xYTMwcGcgOGlrOHBpcGQ2cjY1Z24zaHVyZWM2YWxpMWNAZw&ctz&pli=1&sf=true&output=xml) on Wednesday, April 3rd at noon.  I'll be speaking about [Songs of the Victorians](http://www.songsofthevictorians.com/)  and [Augmented Notes](http://augmentednotes.com/) and demonstrating both of them.  Here's the poster [Ronda Grizzle](http://www.scholarslab.org/people/ronda-grizzle/) designed for it:\n\n\n\n\n[![](http://1.bp.blogspot.com/-N8UWFYwBeo8/UVmNBbCWH5I/AAAAAAAAAbo/wPQJxnWtPHM/s640/Screen+Shot+2013-04-01+at+9.34.14+AM.png)](http://1.bp.blogspot.com/-N8UWFYwBeo8/UVmNBbCWH5I/AAAAAAAAAbo/wPQJxnWtPHM/s1600/Screen+Shot+2013-04-01+at+9.34.14+AM.png)\n\n\n\n\n I hope I'll see you there if you live in the area!  There will be a podcast of the talk, and I'll also put my slides up on my blog.\n\n\n\n\n\n\n\n\n To help with the upcoming talk, I added the archive page for [Michael William Balfe's \"Come into the Garden, Maud\"](http://www.songsofthevictorians.com/balfe/archive.html).  I'll be adding the analysis page in the next few weeks.\n\n\n\n\n\n\n\n\nIn terms of Augmented Notes development, I added a new feature that lets users upload multiple pages of a score.  Users can click on the \"+ Add another page\" link, and a new upload button appears:\n\n\n\n\n[![](http://3.bp.blogspot.com/-asL6Qtf4qm8/UVmNeOd78dI/AAAAAAAAAb4/OsgMkR6jDFY/s640/Screen+Shot+2013-04-01+at+9.36.34+AM.png)](http://3.bp.blogspot.com/-asL6Qtf4qm8/UVmNeOd78dI/AAAAAAAAAb4/OsgMkR6jDFY/s1600/Screen+Shot+2013-04-01+at+9.36.34+AM.png)\n\nOver this coming week, I will try to add two new features: 1. When users click the submit button after setting the measure times, the measure time information will be added to a JSON file;  and 2. Once the previous feature is built, the site will output a .zip file with the html, css, and javascript files necessary for users to have their own very basic archive page like those in Songs of the Victorians.\n\n\n\n\n Stay tuned for a blog post later this week with my slides from my talk!\n\n\n"},{"id":"2013-04-02-omniauth","title":"Omniauth","author":"brandon-walsh","date":"2013-04-02 07:46:22 -0400","categories":["Grad Student Research"],"url":"omniauth","content":"After several weeks of grinding away with the generous help of [Eric](http://www.scholarslab.org/people/eric-rochester/) and [Shane](http://www.scholarslab.org/people/shane-lin/), Omniauth works with our current setup. Now a user can arrive at our site and log into the system using their Facebook or Google accounts as an alternative to creating their own Prism-specific login.\n\nOne significant difficulty we encountered was how to handle multiple authentications. In the default framework, the system creates a new user when you authenticate through a service. So, if a user clicks on Facebook authentication, that action creates a new user in the database. If that same user comes back and clicks Twitter instead, that user will then have a separate account unlinked to the first in the database. Rather than implement an authentication management system, we decided to add a disclaimer to the myPrisms page directing users to login with the correct accounts if they aren't seeing the information that they expect.\n\nAnother difficulty: initially we also wanted to include Twitter authentication, but Twitter returns only the user's nickname instead of an email. This is a problem for Devise, the gem that we use to handle the logins more generally. Devise has emails pretty firmly integrated into their framework, and things got quite whacky when we tried to disable email as the verifying function for new users. We successfully did so, but each new success raised new problems related to the nickname/email interactions. At length, we decided just to go with Facebook and Google for now. Any future authentications that we implement will be chosen based on whether or not they return user emails.\n\nNext, I'll be helping [Shane](http://www.scholarslab.org/people/shane-lin/) move forward with allowing User Uploads.\n"},{"id":"2013-04-17-matrix-time","title":"matrix time","author":"chris-peck","date":"2013-04-17 07:14:14 -0400","categories":["Grad Student Research"],"url":"matrix-time","content":"_From a recent e-mail exchange between Praxis team members._\n\nPraxer 1: I'm befuddled.\n\nPraxer 2: Blah blah blah line 173 blah blah comment out the blah blah reroute the encryptions blah blah facet table blah ruby this ruby that blah rake db:somethingorother blah blah...\n\nPraxer 1: Wow...you're totally living in The Matrix. You must be, like, seeing code _everywhere_.\n\nPraxer 2: Yes, and Praxer 3 is my [Mr. Smith](http://www.youtube.com/watch?v=atwY-VH8hGM).\n\n---\n\nI feel like what I've been doing for the last couple of weeks with regard to Prism is too embarrassing to blog about. Who really wants to hear about me staring at fifteen lines of JavaScript for several hours before I realize that it doesn't just look weird because I don't know JavaScript very well, or because it's actually not JavaScript but CoffeeScript, but because the code is using [d3](http://d3js.org/), even though there's nothing that looks d3-ish on the page (I suspect because d3 would be the perfect thing to realize some of last year's proposed visualizations: bar graphs and such from the word highlight frequencies)? All this so I can eventually get a JavaScript pop-up when I click on a word that displays the percentage of users who have highlighted that word for a particular facet.\n\nMy life in Praxis this week has also been about context switching. Going back and forth between squinting at the JavaScript console and feeling _incompetent_, and writing essays for my qualifying exams where I'm mustering my best performance of _mastery_ over specific subfields of contemporary music. I've noticed that programming requires a very different kind of time—or seems to anyway—than other sorts of work: reading, writing, or even composing. I can sit down to write or compose for twenty minutes and accomplish meaningful work. But with code it seems like several hours of uninterrupted time are often necessary. Even one hour is barely enough to get the development environment up and running and realize it's broken. This morning it took two hours—and help from two of the slab's excellent staff developers—just to get it up and running and realize that it was broken.\n\nI'm not trying to be a downer here, and I should be clear that I love programming and could do it all day if I didn't have other things to do (like these scary exams). But time management for DH projects might be a particular challenge because of the different kinds of time required for the D and the H.\n"},{"id":"2013-04-17-out-on-a-small-limb","title":"Out on a (Small) Limb","author":"brandon-walsh","date":"2013-04-17 06:24:45 -0400","categories":["Grad Student Research"],"url":"out-on-a-small-limb","content":"In writing the following post, I was struck by how close it felt to one [Alex Gil](http://www.scholarslab.org/people/alex-gil/) wrote last year about a similar Prism disaster that he called the [herokulypse](http://www.scholarslab.org/grad-student-research/and-then-the-light-bulb-blew/). Alex wrote his post on April 23 of last year, so we're a few days ahead of schedule for apocalyptic project events.\n\n--\n\nOver the past few weeks the Praxers have been diligently working on their own pieces of Prism. [Gwen](http://www.scholarslab.org/people/gwen-nally/) and [Cecilia](http://www.scholarslab.org/people/cecilia-marquez/) have been making strides with design, [Chris](http://www.scholarslab.org/people/chris-peck/) has worked on highlighting and now visualizing, [Shane](http://www.scholarslab.org/people/shane-lin/) has been doggedly attacking user uploads, and I worked on pieces of Omniauth, design, and uploads. On Monday we made our first attempt to really put things back together, and the [result was pretty horrific](http://www.youtube.com/watch?v=KOi9hHjmYq4&t=1m38s).\n\nMy sense was that we had all broken Prism a little bit in our own special way. Some of these broken pieces weren't really noticeable until we combined everything back together, when suddenly Prism became a big goopy mess. CSS was screwed up, the databases didn't work right, migrations were failing, and the basic functionality of the highlight tool erupted as whole pages somehow morphed into copies of other pages. The crisis was mitigated thanks to the swift responses of [Wayne](http://www.scholarslab.org/people/wayne-graham/) and [Jeremy](http://www.scholarslab.org/people/jeremy-boggs/), and things seem to be working again.\n\nA lot of this could have been mitigated, I think, if we had better coordinated our feature branches. Instead, a lot of us were working on a big feature branch: \"user uploads\" or \"omniauth.\" It would have been better to break those out into smaller pieces so that any damage from merging would be manageable and less the nightmarish event that ensued. An \"omniauth\" branch could become multiple branches, one each for Facebook, Twitter, Google, etc.\n\nAdmittedly, some of the work that we have been doing has been of the sort that I don't think you can really divide up easily. Shane and I have been working on refactoring the databases, and large scale changes like that seem like they should happen all at once. Even so, I think we could have found ways to organize our work better. This would have, of course, necessitated better planning on our part ahead of time.\n\nSurprise: the SLab gurus have been telling us all along to make more and smaller feature branches.\n\nListen to your digital elders. They know all the things.\n"},{"id":"2013-04-22-prism-in-my-unconscious","title":"Prism in my Unconscious","author":"cecilia-márquez","date":"2013-04-22 08:46:46 -0400","categories":["Grad Student Research"],"url":"prism-in-my-unconscious","content":"As we rapidly approach the re-launch of [Prism](http://prism.scholarslab.org/) I am spending more and more of my waking hours concerned about CSS and the site redesign.  We've made a lot of exciting changes including making the site aesthetic that of a \"scandanavian child's room\" (thank you Eric Rochester).  However, this week Prism has finally seeped into by unconscious.\n\nAs I was reviewing code this week there were occasional notes from Wayne about places to fix a table or ideas on how to redo a page.  Aside from hearing a military drill sergeant in the code I thought nothing was out of the ordinary.  The next night I had a dream that the code was yelling at me.  I'm sure there are some deeper psychoanalytic implications of this but for now I'm going to take it as a good sign that I am fully integrated with the code.  We are speaking the same language, even if its only in my dreams.\n\nIts either that or Wayne Graham is haunting me.\n"},{"id":"2013-04-23-humanities-unbound-careers-scholarship-beyond-the-tenure-track","title":"Humanities Unbound: Careers & Scholarship Beyond the Tenure Track","author":"katina-rogers","date":"2013-04-23 05:59:09 -0400","categories":["Research and Development"],"url":"humanities-unbound-careers-scholarship-beyond-the-tenure-track","content":"_[Cross-posted from [my personal site](http://wp.me/p2CaGd-hm).]\n\nI’ve had the privilege of talking about graduate education reform and career preparation for humanities scholars at several universities this spring, including Stanford, NYU, and the University of Delaware. I’ve adapted the following from those presentations. The full dataset from the study that I discuss will be available later this summer, along with a more formal report.\n\nAlready familiar with the background of this project? Jump straight to the survey results._\n\n![HumanitiesUnbound_APR13.001](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.001.jpg) [_Image source_](http://www.flickr.com/photos/9619972@N08/2232446869/)\n\nGraduate students in the humanities thinking about their future careers face a fundamental incongruity: though humanities scholars thrive in a wide range of positions, many graduate programs operate as though every PhD student will become a tenured professor. While the disconnect between the number of tenure-track jobs available and the single-minded focus with which graduate programs prepare students for that specific career is not at all new, the problem is becoming ever more urgent due to the increasing casualization of academic labor, as well as the high levels of debt that many students bear once they complete their degrees.\n\n<!-- more -->\n\nBefore I say anything more, though, I'd like to dispel a couple of associations that the title of my talk might call to mind.\n\n![HumanitiesUnbound_APR13.002](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.002.jpg) _Source: [image one](http://www.flickr.com/photos/quinet/7201343000/); [image two](http://www.flickr.com/photos/31878512@N06/3889347802/)_\n\nFirst, I don’t intend to compare the humanities as a discipline to Prometheus, though at times the angst of the job market may make it feel like an apt image.\n\nI also don’t mean to imply that the humanities are unraveling, though there is a useful connection to be made with that language. You may recall a recent article in the Chronicle of Higher Education by Michael Bérubé, past president of the Modern Language Association, which was called “[Humanities Unraveled](http://chronicle.com/article/Humanities-Unraveled/137291/).” In the article, Bérubé refers to the humanities as “a seamless garment of crisis”—noting that every element, from the dissertation, to scholarly publishing, to time-to-degree, to labor issues, are so deeply interrelated and in such advanced states of disrepair that it feels impossible to pull on one thread without the entire system coming apart. Bérubé’s article does an excellent job of showing the ways in which these elements fit together, and why it’s imperative that we not lose sight of the system as a whole and the many points of intersection as we work to find solutions.\n\nAs important as that is, it isn’t quite what I want the focus of this talk to be.\n\nInstead, the connection I want to make is something a bit more positive. I think that the discipline of the humanities should be disentangled—or, unbound—from the rigid academic pathway leading to the single goal of the tenure track job.\n\n![HumanitiesUnbound_APR13.003](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.003.jpg) [_Image source_](http://www.flickr.com/photos/stigwaage/3218127924/)\n\nInstead of imagining graduate school as a pipeline, keeping everyone contained and moving in one direction to a pre-determined endpoint, what if we thought more about a sprinkler, with water exploding out in all directions?\n\n![HumanitiesUnbound_APR13.004](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.004.jpg) [_Image source_](http://www.flickr.com/photos/22104733@N06/6009194370/)\n\nIt’s a rather simplistic metaphor, but I do think it’s a helpful image, especially since its outward spray is reminiscent of something that is not only practical, but that also brings joy.\n\nRather than focusing academic work inwardly, exclusively within academic institutions, humanities programs should be preparing students for much more flexibility in terms of audience and engagement. Even traditional scholarship has a growing potential to reach a wider and less specialized audience as scholarly publishing increasingly moves to open-access venues. But that’s only the tip of the iceberg. Scholars are publicly working through ideas on blogs and on Twitter, reaching a range of people from personal contacts, scholars, and grad students to interested strangers. Digital humanities projects are often public-facing and open to specialists and amateurs alike. And of course, people working in environments like museums, libraries, and presses have tangible public impact. Some humanities disciplines, such as history, do have a strong tradition in public engagement; [public history](http://ncph.org/) is a well-developed sub-discipline, and many scholars work in museums, government, archives, and so on. But even there, the public branch has often been considered distinct from—and, unfortunately, less prestigious than—the research-focused side of the discipline.\n\nThe humanities must get more serious about its role relative to the public for a number of reasons. First, if we believe that our work can be a social good, with broad relevance to our cultures and societies, then we should be attentive to the public value of that work for reasons intrinsic to the work itself. Second, as public funding for the humanities becomes increasingly scarce, we must make a case for continued and strengthened support. If our work is not even visible, let alone relevant, to more than an academic audience, then we will have a weak case indeed. Third, it has long been the case that a large proportion of graduates engage in careers outside the professoriate, and yet those roles have typically been undervalued. We owe it to current and future graduate students to equip them for a broader range of roles, and to attribute to those roles the merit that they deserve. Embracing a broad spectrum of career options—in and around universities, as well as cultural heritage organizations, non-profits, government, and the private sector—won’t fix everything, but it would represent a significant step in the right direction.\n\nGraduate programs _can_ help prepare their students for a much broader professional world than simply the professoriate, but lacking data or strong models, it can be extremely difficult for individual programs to know what kinds of changes would be effective, and also to make a case for those changes. That’s why [the Scholarly Communication Institute](http://uvasci.org) embarked on its [recent study](http://mediacommons.futureofthebook.org/alt-ac/who-we-are) of perceptions of career preparation among humanities scholars: to determine a baseline from which to make specific recommendations for curricular changes.\n\nIn the absence of data that we thought was a crucial foundation for reform, SCI decided to survey what has come to be known as the “alt-ac” community. But first we had to think about how to answer a common question: what does alt-ac even mean?\n\nThe term was coined—more or less accidentally—in a 2009 [Twitter conversation](http://storify.com/nowviskie/altac-origin-stories) between [Bethany Nowviskie](http://nowviskie.org) (of UVa and SCI) and [Jason Rhody](http://misc.wordherders.net/?page_id=2) (at the National Endowment for the Humanities). Short for “alternative academic,” the term became useful shorthand to refer to jobs in and around the academy, but outside the professoriate. At the time, jobs tended to be classified as either “academic” or “non-academic,” with everything outside the professoriate being relegated to the “non” category. But many scholars—including Nowviskie and Rhody—were (and are) doing innovative, productive, thought-provoking work in all kinds of positions. “Alt-ac” was a way to capture the kind of intellectually-stimulating roles that at that time were being brushed aside as viable scholarly career paths. Nowviskie went on to create [#Alt-Academy](http://mediacommons.futureofthebook.org/alt-ac/), an online volume of essays treating a range of topics related to the pursuit and development of these various careers.\n\nBecause the term is _not at all formal_, it means different things to different people. While the concept is something that people are eager to talk about, and while the term itself has both expanded in meaning and proven useful beyond what was initially imagined, there’s also a certain degree of discomfort with the phrase. Some find that it perpetuates an unfortunate (and false) binary of career options or reinforces the primacy of tenure-track employment—which, of course, is exactly what it was meant to alleviate. Others simply don’t find that the term resonates for them, considering it too narrow, or redundant with existing categories. Still others have expressed concern that #alt-ac is being held up as an unrealistic panacea for the perpetually abysmal academic job market, yet without necessarily creating new jobs.\n\nAll of these are valid critiques, and really, I would love it if the term became unncessary and we could simply speak of career choices without the looming dominance of the tenure track. Until that day comes, I tend to take a broad view of what “alt-ac” can signify, and I’ll continue to use it here as shorthand for a wide variety of career paths. I’m actually quite comfortable with the unsettled nature of the term, as I think that it helps us to continue having useful and important conversations. I highlight the disagreement just to say that because of the differing understandings of the term, before we could even begin to study the various constellations of alternative academic careers, we needed a better idea of who felt that the terminology resonated with the kind of work they were doing.\n\nTo get a sense of what others mean by “alt-ac”, we took the exploratory step of creating a [public database](http://altacademy.wufoo.com/reports/who-we-are/) where people could voluntarily add their names to a loose, public community of other alt-ac practitioners. We built the database within the framework of the #Alt-Academy project in order to leverage the energy of existing conversations, then stepped back to see what the results would be. We quickly had over two hundred and fifty entries from individuals in many different positions. Browsing the database is a great starting point for grad students or others who want to see the kinds of work that humanists are doing in the world. It’s still open to new entries, so feel free to check it out and [add your own name](http://altacademy.wufoo.com/forms/who-we-are/) if you think of your work along these lines.\n\nOnce we created the database, we moved on to the second, more formal phase of the study, which included two confidential surveys. The primary survey targeted people with advanced humanities degrees who self-identify as working in alternative academic careers—precisely the people who identified themselves in the database, and we so we used the database as an initial source of potential respondents. The secondary survey targeted employers that oversee people with advanced humanities degrees. By questioning both, we were able to gain a more well-rounded perspective that balances some of the challenges inherent in self-reported answers.\n\nA couple of things about the response rate highlighted important discrepancies. We set an initial goal of 200 responses on the main survey, and 100 on the employer survey. Responses to the main survey totaled nearly 800 when we closed it, for almost four times our goal.\n\n![HumanitiesUnbound_APR13.009](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.009.jpg)\n\nInterestingly, this is a much higher number than the public database saw, despite the fact that, first, the survey required a more significant investment of time, and second, it was open for responses during a much shorter window than the database, which is still open now. I suspect that this is in part due to a lingering discomfort with publicly stating that one has pursued a path outside the professoriate; in fact, when the database launched, a number of people contacted me to say that while they supported the project, they didn’t want advisors or others from their institution to know the kind of work they were doing. I find this incredibly distressing.\n\nThe employer survey, on the other hand, attracted far fewer responses, totaling around 75. This is a significant finding in itself, as it shows a pronounced disconnect between the motivations of job seekers compared to employers in thinking through the issues at hand. The questions of career paths and graduate education reform are understandably much more urgent for those that have gone through the process and made any number of difficult decisions.\n\n\nNow, to turn to the data.\n\nMuch of the data from the surveys will be unsurprising to people who have already been thinking about these questions. As we might anticipate, people report entering graduate school expecting to become professors; they receive very little advice or training for any other career; and yet many different circumstances lead them into other paths. But even if the broad contours of the results are unsurprising, the specific data is useful to ground the general anecdotal impressions that many of us have long shared. Indeed, one of the primary goals of the survey is to help move the conversation from anecdote to data.\n\nFor instance, asked to identify the career or careers they expected to pursue when they started graduate school, 74% indicated that they expected to obtain positions as tenure-track professors. As you can see, that response far outpaces any others, even though respondents could select multiple options. (As an aside, that’s also why the **results here and on many subsequent slides add up to more than 100%**. Because so many alt-ac positions are hybrid, we didn’t want to limit people to one option in their responses.)\n\n![HumanitiesUnbound_APR13.010](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.010.jpg)\n\nWhat is perhaps more interesting is their level of confidence: of that 74%, 80% report feeling fairly certain or completely certain that professorship was the career they would pursue.\n\n![HumanitiesUnbound_APR13.011](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.011.jpg)\n\nKeep in mind that because we targeted alt-ac practitioners, _none_ of the survey respondents are tenured or tenure-track professors; they are _all_ working in other domains. So even among the body of people who are working in other roles, the clear expectation at the outset of graduate school was for a future career as a professor.\n\nThese expectations are not at all aligned with the realities of the academic job market, and they haven’t been for some time. The labor equation for university teaching has continued to shift dramatically in recent years, with non-tenure-track and part-time labor constituting a [majority of instructional roles](http://www.aaup.org/NR/rdonlyres/7C3039DD-EF79-4E75-A20D-6F75BA01BE84/0/Trends.pdf). The [2011 report on the Survey of Earned Doctorates](http://www.nsf.gov/statistics/sed/digest/2011/) reported another alarming statistic: 43% of humanities PhD recipients have _no_ job or postdoctoral commitment on graduation—and that’s for any commitment, including temporary or contingent positions. Many graduate students begin their studies without a clear picture of their future employment prospects. What this signals to me is that we are failing at bringing informed students into the graduate education system.\n\nBut whose responsibility is it to provide information about career prospects? It seems clear to me that it’s something that must be addressed _before_ graduate education begins, and then reiterated in the admissions process and throughout the program, ideally by a trusted advisor. To be honest, I see this as an ethical issue: it is deeply problematic to admit students to a program if their expectations for the program’s outcome are not accurate. This is even more true if students are admitted to unfunded positions and must incur debt as they earn their degrees.\n\nDeepening the problem, students report receiving little or no preparation for careers outside the professoriate, even though we’re at a moment when the need for information about a variety of careers is most acute. Only 18% reported feeling satisfied or very satisfied with the preparation they received for alternative academic careers.\n\n![HumanitiesUnbound_APR13.012](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.012.jpg)\n\nThe responses are rooted in perception, so there may be resources available that students are not taking advantage of—but whatever the reason, they do not feel that they are being adequately prepared. Again, this probably comes as no surprise, but it does reveal that we have significant room for improvement.\n\nMany programs have access to two rich resources that they leave untapped: their own staff, and their own graduates.\n\n![HumanitiesUnbound_APR13.013](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.013.jpg)[_Image source_](http://www.flickr.com/photos/tumblingrun/6271299222/sizes/l/)\n\nAn easy first step for universities to take would be to list all of their non-professorial staff members that hold PhDs. Stanford has done something like this, compiling information about people in the Stanford system (including the Press and Libraries) that are willing to serve as [mentors for humanities graduate students](http://shc.stanford.edu/phd-mentors/). Stanford has also developed a [speaker series](http://vpge.stanford.edu/students/altacseries.html) that highlights scholars working in hybrid and non-faculty roles on campus.\n\nTracking graduates is a little more complicated, but no less important. There may be confidentiality issues involved, but there are surely ways that positions could be reported in aggregate and supplemented with a few individual examples. The more significant hurdle is that because prestige continues to be linked to tenure-track placement rate, many programs either do not track the careers of anyone outside this category, or if they do, they do not publish the information they have for fear (I suspect) of tarnishing their reputation relative to other schools.\n\nBut the vibrant alt-ac movement suggests that the time is ripe to measure prestige in other ways. I would think that programs would be eager to publicize the often high-level alt-ac careers of their graduates, and that such information could be a strong draw for prospective students. I’ve long wondered why graduate programs find it so difficult to track their former students, when development offices seem to be the very first to know the whereabouts of alumni. New research by a third-party consultancy, the [Lilli Research Group](http://lilligroup.com/), has shown that it’s possible to determine the [professional outcomes of graduates](http://chronicle.com/article/What-Doors-Does-a-PhD-in/135448/) with a surprising degree of accuracy, even using only public records, but I think that this work should be taken on internally. Programs simply must do a better job of knowing what kinds of work their graduates are doing.\n\nSo, where _are_ people working after graduate school? As you can see, instead of the professoriate, the respondents reported working in a number of different types of workplaces. A large majority of respondents categorized their positions as being within universities, libraries, and other cultural heritage organizations.\n\n![HumanitiesUnbound_APR13.014](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.014.jpg)\n\nParenthetically, this is one area where the definition of alt-ac gets fuzzy; some people prefer to think of alternative academic careers as only those that are within the university. I’ve mentioned that I take a broader view. Personally, I think it’s useful to consider not so much a specific job or career, but rather an approach: a way of seeing one’s work through the lens of academic training, and of incorporating scholarly methods into the way that work is done. It means engaging in work with the same intellectual curiosity that fueled the desire to go to graduate school in the first place, and applying the same kinds of skills—be they close reading, historical inquiry, written argumentation, or whatever else—to the tasks at hand. Thinking this way encourages us to seek out the unexpected places where people are finding their intellectual curiosity piqued and their research skills tested and sharpened.\n\nThe next slide might actually be a bit of a pleasant surprise. Despite a concern that encouraging graduate students to pursue more varied lines of employment pushes them into short-term positions with unstable funding, in fact relatively few respondents report this as their situation. Only 18% are in positions currently funded wholly or partially by grants, and, while this chart doesn’t show it, just 5% are in positions with a specified end date.\n\n![HumanitiesUnbound_APR13.015](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.015.jpg)\n\nThat is not to say that alternative academic positions are a quick fix; after all, we’re not talking about creating new jobs where none existed, but rather about recognizing existing opportunities. I especially want to reiterate that issues surrounding the academic labor market are pervasive and serious, particularly as universities rely on contingent labor to an ever-greater degree. But there are good, solid opportunities available that should figure into the ways that we train and advise graduate students. And the public will benefit from the perspective and expertise of humanities scholars whose voice extends beyond the classroom or academic journal.\n\nThe reasons that people pursue careers beyond the tenure track are varied and complex. Location tops the list, which makes sense as a contrast to the near total lack of geographic choice afforded by academic job searches.\n\n![HumanitiesUnbound_APR13.016](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.016.jpg)\n\nBeyond that, people report pursuing alt-ac jobs for reasons ranging from the practical and immediate—salary, benefits, family considerations—to more future- and goal-oriented reasons, such as the desire to gain new skills, contribute to society, and advance in one’s career. Many people filled in their own responses as well, with the most common trends in the open text field being a desire for greater freedom, dissatisfaction with what a faculty career would look like, and much more simply, the need to find a job. A note of urgency and, sometimes, desperation came through in a number of these responses.\n\nKeeping in mind that the employer sample was quite small compared to the main sample (and therefore less reliable), I’d like to look at some of the competencies that both groups considered important to the alt-ac positions that they hold or supervise. Some of the skills are core elements of graduate work, such as writing, research skills, and analytical skills. Both groups value many of the skills at similar levels; however, there are a couple of discrepancies.\n\n![HumanitiesUnbound_APR13.017](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.017.jpg)\n\nI find it particularly interesting that alt-ac employees _undervalued their research skills_ relative to employers. I suspect that there are two reasons for this: first, there may be some activities that employees do not recognize as research because it leads to a different end result (such as a decision being made, rather than a journal article being published). Second, it may be a skill that has become so natural that former grad students fail to recognize it as something that sets them apart in their jobs.\n\nOn the other hand, alt-ac employees tended to _overvalue_ the importance of project management among the competencies that their jobs required. That said, project management actually tops the list of areas where alt-ac employees needed training, according to employers:\n\n![HumanitiesUnbound_APR13.018](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.018.jpg)\n\nTo me, this suggests that employees overvalued the skill because they found it to be a challenging skill that they needed to learn on the job, and so it took on an outsized importance in their minds.\n\nEmployers also cited technical and managerial skills as areas that needed training. While the importance of those two skills would certainly depend on the type of position, others, such as collaboration, are useful in almost any work environment. Even simple things, like adapting to office culture, can also prove to be surprisingly challenging if graduates have not had much work experience outside of universities.\n\nThe good news is that all of the elements that make stronger employees would also be hugely beneficial for those grads that do go on to become professors. While students are generally well prepared for research and teaching, they aren’t necessarily ready for the service aspect of a professorship, which incorporates many of the same skills that other employers seek. Many of the skills can also contribute to more creative teaching and research. By rethinking their curricula in such a way that students gain experience in things like collaborative project development and public engagement, departments would be strengthening their students’ future prospects regardless of the paths they choose to take.\n\nIt’s not surprising that employers find that alt-ac employees need training in skills like project management and collaboration. Employees themselves also recognize that these are by and large not skills that they acquire in graduate school. Even among those who felt that their skills in these areas were strong, they noted that they gained them outside of their graduate program—for instance, through jobs or internships.\n\n![HumanitiesUnbound_10APR13.019](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_10APR13.019.jpg)\n\nOf course, the core skills of graduate training—especially research, writing, and analytical skills—are highly valued by employers. These skills are the reasons that employers will often hire PhDs even if the degree is not strictly required by the position. It’s important that students don’t undervalue (or insufficiently articulate) the ways that graduate study _already_ equips them for broader roles, particularly in the methods and generalized skills that are critical to the process.\n\nOne thing seems clear: the persistent myth that there’s nothing but a single academic job market available to graduates is damaging, and extricating graduate education from the expectation of tenure-track employment has the potential to benefit students, institutions, and the health of the humanities more broadly. However, as long as norms are reinforced within departments—by faculty and students both—it will be difficult for any change to be effective.\n\nAgain, low tenure-track employment rates are not a new problem, but as the survey responses show, departments by and large are not succeeding at providing accurate and realistic information to their students, and many graduates still feel stigmatized when they pursue different types of careers.\n\nFor change to be possible, it’s essential that institutional norms and measures of prestige shift in favor of highlighting successful outcomes across a broader spectrum of possibilities. And it’s important that graduate programs begin exploring ways that they can better prepare their students while maintaining the integrity of humanistic scholarship.\n\n![HumanitiesUnbound_APR13.019](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.019.jpg)[_Image source_](http://www.flickr.com/photos/december_snowdrift/1150313448/)\n\nAdmittedly, that can seem an incredibly difficult barrier to cross without successful examples to emulate. Having good models can not only ease the technical challenges of establishing a new program—like building support, establishing a program’s structure, and so on—but can also alleviate the potential social challenges of striking out on a new path.\n\nTo that end, SCI has just launched another project that we hope will be a useful complement to the survey data: the [Praxis Network](http://praxis-network.org).\n\n[![HumanitiesUnbound_APR13.020](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.020.jpg)](http://praxis-network.org)\n\nThe Praxis Network is a new showcase of a small handful of truly excellent programs that are already up and running. Each of these programs can be thought of as one possible response to the question of how to equip emerging scholars for a range of career outcomes without sacrificing the core values or methodologies of the humanities, and without increasing time-to-degree.\n\nInstitutions that wish to explore making changes can really benefit from seeing existing models, but currently, finding information about, and comparing, some of these new programs can be quite challenging. There are a lot of reasons for this: for instance, they may be housed in different parts of their institutions—such as departments, traditional or digital humanities centers, or libraries. They are often, but not always, small and nimble. Most of all, they are all quite different, though there are trends and commonalities among them. We wanted to create a space that made it easy for people to see what makes these great programs tick, and we wanted to present the information in a way that makes sense for anyone, whether they are administrators, faculty, students, or the general public.\n\nThe site doesn’t aim to be a comprehensive database of all praxis-type programs; instead, it’s meant to be a small cross-section of programs that are different enough from one another that together they give an impression of the landscape.\n\nThe anchor of the network is UVa’s [Praxis Program](http://praxis.scholarslab.org/), which brings together interdisciplinary cohorts of six doctoral students to collectively build a [single tool](http://prism.scholarslab.org/) that will be useful for humanities research or pedagogy. In the course of the year-long fellowship, they learn technical skills and project management under the mentorship of the [Scholars’ Lab](http://scholarslab.org) research and development team. But they also learn innumerable “soft” skills as they navigate the creation of a group charter, determine their priorities, think through their disciplinary values and assumptions, blog about the process, and publicly launch the tool.\n\nAlong with UVa’s [Praxis Program](http://praxis-network.org/praxis-program.html), which Bethany Nowviskie directs, we selected a few differently-inflected programs to highlight. Most are graduate programs. These include:\n\n\n\n\n  * \nthe [Cultural Heritage Informatics Initiative](http://praxis-network.org/chi-initiative.html), led by Ethan Watrall at Michigan State University; \n\n\n  * CUNY Graduate Center’s [Digital Fellows](http://praxis-network.org/cuny-gcdi.html), under Matt Gold;\n\n\n  * the joint [MA/MSc program in Digital Humanities](http://praxis-network.org/ucldh.html) at University College London, led by Simon Mahony;\n\n\n  * and the new [PhD Lab in Digital Knowledge](http://praxis-network.org/duke-phd-lab.html) at Duke University, run by Cathy Davidson and David Bell.\n\n\nWe’re also working with two undergraduate programs:\n\n\n  * the [Mellon Scholars](http://praxis-network.org/hope-mellon-scholars.html) program at Hope College, which William Pannapacker leads;\n\n\n  * and Brock University’s [Interactive Arts and Science Program](http://praxis-network.org/brock-iasc.html), under Kevin Kee.\n\n\n\nFor the moment, the website is the main product, and sharing information is the main goal. We hope that the ideas found on the site prove to be both useful and inspiring. There’s a wealth of exceptional work represented here, and we hope that it’s presented in a way that makes it easy for people to understand and compare various aspects of each program.\n\nThe [mission](http://praxis-network.org/index.html#mission) of each program might be the category that has the greatest degree of similarity across institutions. The goals of each are student-focused, digitally-inflected, interdisciplinary, and frequently oriented around collaborative projects.\n\nOne common tenet is learning in public and exposing the process, warts and all. Many of the students grapple with the notion that our learning happens in unexpected ways—including through failure—and come to a greater degree of comfort with exposing their own uncertainties. Because humanities students are socialized to show their work only once it’s polished, this can be a very uncomfortable experience, but it’s one that leads to particularly fruitful results.\n\nAll of the programs encourage some kind of public engagement, whether through blogs, public-facing projects, or more traditional venues like conference presentations. Some of the programs include partnerships with local cultural heritage sites, tech companies, or other potential employers, which allows students to immediately see potential applications for the work they’re doing.\n\nWe then look at [structure](http://praxis-network.org/index.html#structure), which shows a lot more variety. Some of the programs are fellowships that offer a stipend, while others require tuition; they may be as short as five weeks, or as long as three years. They may be credit-bearing or extracurricular, with requirements that are formal or loose.\n\nOne thing worth noting here is that most of the programs are fairly small and competitive. This has some real advantages: students benefit from strong mentorship and close collaborations with one another in small cohorts. When other institutions move toward making praxis-like changes, they often take the form of these kinds of small, competitive programs. This is a great start, but I’d also love to see more movement toward broad-based change that touches entire departments. To make that happen, we would have to see strong advocacy for curricular change and the acceptance of new modes of scholarship for credit and degrees. This kind of change is happening, but it’s happening very slowly.\n\nFor digital and other non-traditional work to be recognized as scholarship, universities need examples of existing work to point to. Importantly, the site highlights the [research products](http://praxis-network.org/index.html#research) that the students are generating through the course of the programs. They often look quite different from the typical seminar paper, and demonstrate not only rigorous scholarly work, but also a creativity and vibrancy not often found in standard papers.\n\nHighlighting this kind of work is aligned with the efforts of organizations like the [Modern Language Association](http://mla.org), which establishes [guidelines](http://www.mla.org/guidelines_evaluation_digital) and offers [workshops](http://wiki.mla.org/index.php/WORKSHOP_2012) on the evaluation of digital work for tenure and promotion.\n\nThe site then focuses on the [people](http://praxis-network.org/index.html#people) that are the core of each program. Every single program is characterized by strong, dedicated leaders and curious, intelligent students, and is invariably colored by the interactions among them. This can be both an asset and a challenge, as programs that rest on the shoulders of just a few individuals need to think about questions of continuity. But creating something new requires risk-taking, which is often easier for an individual or small group than for an entire department.\n\n[Resources](http://praxis-network.org/index.html#support) may feel like one of the most significant barriers to implementing programs of this nature, so we’ve detailed the support structure of each. Money is a crucial aspect of this: some programs are grant-funded, of course, while others have hard budget lines within their institutions. Physical space and personnel are also key resources, and play an important role in the scope of the program.\n\nA program’s [future goals](http://praxis-network.org/index.html#directions) depend on a combination of many of the factors above. In some cases, the programs hope to grow, either within their institution or by forming cross-institutional partnerships. In other cases, however, the program’s success depends on its smallness, so some do not wish to expand, but rather intend to focus on how to continue innovating while staying light-weight.\n\nAnd finally, the [nuts and bolts](http://praxis-network.org/index.html#nuts-and-bolts) of the programs. If you’re thinking of developing something like what you see on the site, the information in this section will help you to know some key questions, potential risks, and other things to keep in mind as you consider how to proceed.\n\nI think of each program in the Praxis Network as an instantiation of the kinds of innovative solutions that can alleviate the issues that the survey uncovered. Humanities programs have the opportunity to better serve their students as well as the public by really examining what our core values are, and rethinking the methods we use to teach them. The Praxis Network programs show just a few possible ways to move toward collaborative projects, public engagement, and embracing an ethos of openness and exploration.\n\nPraxis Network members will be meeting during the coming months to discuss what kinds of cross-institutional collaborations might be most effective. At the same time, SCI will [convene meetings](http://uvasci.org/current-work/graduate-education/) in conjunction with CHCI (the Consortium of Humanities Centers and Institutes) and centerNet, its digital counterpart, to further explore [points of intervention](https://docs.google.com/document/d/1C52n_4adKKt5-C9Tm6RJbe_I5F0-5sqDG4n5-MWo5_o/edit) and to develop potential pilot programs that could leverage the particular strengths of traditional and digital humanities centers.\n\nSo, those programs are great examples to follow, but you might be wondering what you can do right now, especially if you’re a graduate student.\n\n![HumanitiesUnbound_APR13.021](http://katinarogers.com/wp-content/uploads/2013/04/HumanitiesUnbound_APR13.021.jpg)[_Image source_](http://www.flickr.com/photos/rossmerrittphotography/5488996034/sizes/l/in/photostream/)\n\nFirst, if you’re not getting the information you need from your program, seek it out. If nothing else, start with the [resources](http://katinarogers.com/resources) that are available on my website.\n\nSecond, connect with people. Browse [#Alt-Academy](http://mediacommons.futureofthebook.org/alt-ac/) and the [database](http://altacademy.wufoo.com/reports/who-we-are/) that I mentioned earlier. Find people that are doing interesting work and see if they’d be willing to talk with you about how they got there—many of us are, especially since our own paths were so often not what we expected them to be! Encourage your department to help set up mentorships with alumni from the department, or with scholar-practitioners working in the university. Engage in online networks that are relevant to your interests. Find a mentor that can support you.\n\nThird, think about the skills and experience that you already have, how you might frame it for different kinds of jobs, and what else you might need to be a competitive applicant for positions that interest you. Depending on your circumstances, you might carefully consider whether to take an external job for a year of your graduate program rather than work as a TA if you have limited job experience outside of teaching.\n\nFinally, there are lightweight training opportunities that you can pursue even if your home department is quite traditional. The [Digital Humanities Summer](http://dhsi.org/) and [Winter Institutes](http://mith.umd.edu/community/dh-events/event/dhwi-2013/) are great examples of short programs that enable students (or faculty, staff, or anyone else) to pick up new skills while also building a strong network.\n\nBeyond what I’ve talked about here, so much more is possible within and across existing programs. SCI hopes that our current work will help begin to rise the tide of transparency and innovation more broadly. \n"},{"id":"2013-04-24-plot-your-course-in-space-and-time-a-look-at-scholars-labs-neatline","title":"Plot Your Course in Space and Time: A Look at Scholars’ Lab’s Neatline","author":"ronda-grizzle","date":"2013-04-24 10:29:12 -0400","categories":["Geospatial and Temporal","Podcasts"],"url":"plot-your-course-in-space-and-time-a-look-at-scholars-labs-neatline","content":"**Scholars' Lab Speaker Series: Bethany Nowviskie & David McClure**\n**Plot Your Course in Space and Time: A Look at Scholars’ Lab’s Neatline**\n\nIn January, Bethany Nowviskie, Director of Digital Research & Scholarship at UVa Library, and David McClure, Web Applications Specialist in the Scholars' Lab spoke about [Neatline](http://neatline.scholarslab.org/).\n\nSummary:\nWhat do you get when you cross texts and archival collections with rich, interactive maps and timelines? Neatline. Join us for a demo of current Neatline projects and upcoming features and to learn why Neatline emphasizes hand-crafted visualization and “small data” in a big-data world.\n\nSpeaker Bio:\nBethany Nowviskie: Computing humanist/humane computationalist since 1996. Director of Digital Research & Scholarship at the [University of Virginia Library](http://lib.virginia.edu/scholarslab) and Associate Director of the [Scholarly Communication Institute](http://uvasci.org/). President, [Association for Computers and the Humanities](http://ach.org/) and current chair of [MLA](http://mla.org/)‘s Committee on Information Technology.\n\nDavid McClure: Web Applications Developer on the Scholars’ Lab R&D team, David graduated from Yale University with a degree in the [Humanities](http://www.yale.edu/humanities/) in 2009 and worked as an independent web developer in San Francisco, New York, and Madison, Wisconsin before joining the lab in 2011. David is the lead developer on [Neatline](http://neatline.scholarslab.org/) and works on research projects that use software as a tool to advance traditional lines of inquiry in literary theory and aesthetics.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23169448143/enclosure.mp3\"]\n"},{"id":"2013-04-25-scholars-lab-speaker-series-gretchen-gueguen","title":"Scholars' Lab Speaker Series: Gretchen Gueguen","author":"ronda-grizzle","date":"2013-04-25 04:39:37 -0400","categories":["Podcasts"],"url":"scholars-lab-speaker-series-gretchen-gueguen","content":"**Scholars' Lab Speaker Series: Gretchen Gueguen**\n**Do Digital Archivists Dream of Electronic Records? Born Digital Collections in the Small Special Collections Library**\n\nIn February, Gretchen Gueguen, Digital Archivist in UVa Library's Digital Curation Services unit spoke about the challenges to fundamental principles of the archival practice by electronic communication._.\n\nSummary:\nThe information age has ushered in the biggest changes in human communication since the rise of printed text. The dynamic and ephemeral nature of electronic communication presents stark challenges to the fundamental principles of the archival practice. Join us for a look at how the tradition of collecting and creating archives is facing this paradigm shift and how the historical record will be shaped for the future.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23237371565/enclosure.mp3\"]\n"},{"id":"2013-04-26-digital-humanities-speaker-series-walter-sheidel","title":"Digital Humanities Speaker Series: Walter Sheidel","author":"ronda-grizzle","date":"2013-04-26 10:46:07 -0400","categories":["Podcasts"],"url":"digital-humanities-speaker-series-walter-sheidel","content":"**Digital Humanities Speaker Series: Walter Scheidel**\n**Redrawing the Map of the Roman World**\n\nIn March, Dr. Walter Scheidel, Dickason Professor in the Humanities, Professor of Classics and History, and Chair of the Department of Classics at Stanford University, spoke about ORBIS, a tool developed at Standord that models the geospatial network of the Roman world.\n\nSummary:\nAncient societies were shaped by logistical constraints that are almost unimaginable to modern observers. “ORBIS: The Stanford Geospatial Network Model of the Roman World” ([orbis.stanford.edu](http://orbis.stanford.edu/)), for the first time, allows us to understand the true cost of distance in building and maintaining a huge empire with pre-modern technology. This talk explores various ways in which this novel Digital Humanities tool changes and enriches our understanding of ancient history.\n\nThe Digital Humanities Speaker Series is co-sponsored by [IATH](http://www.iath.virginia.edu/), [SHANTI](http://shanti.virginia.edu/), and the [Scholars' Lab](http://www.scholarslab.org/).\n\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23529002200/enclosure.mp3\"]\n"},{"id":"2013-04-26-scholars-lab-presentation-using-juxta-commons-in-the-classroom","title":"Scholar's Lab Presentation: Using Juxta Commons in the Classroom","author":"dana-wheeles","date":"2013-04-26 06:18:18 -0400","categories":["Podcasts"],"url":"scholars-lab-presentation-using-juxta-commons-in-the-classroom","content":"**Scholars' Lab Speaker Series: Andrew Stauffer & Dana Wheeles**\n**Using Juxta Commons in the Classroom**\n\n_In February, Andrew Stauffer, Professor of English and Director of NINES, and Juxta Project Administrator Dana Wheeles spoke about using the newly released Juxta Commons in the classroom. Because the presentation was a show-and-tell, Dana has been kind enough to create this blog post as a companion to the podcast of their talk.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619)._\n\n\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23530048967/enclosure.mp3\"]\n\n\n\n\n\n\n[caption id=\"attachment_8058\" align=\"aligncenter\" width=\"300\"][![Juxta Commons work space](http://www.scholarslab.org/wp-content/uploads/2013/04/workspace-300x136.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/04/workspace.jpg) The Juxta Commons Work Space[/caption]\n\nDuring our recent presentation of Juxta Commons in the Scholar's Lab, NINES Director Andrew Stauffer and I showed a number of visualizations of texts collated within the interface of our newly-released application. Because this aspect of the presentation does not translate for an audience listening to the podcast audio, this blog post is meant as a visual companion to our talk.\n\nWe began the presentation with a tour of the Juxta Commons workspace, from the library section at the top of the page (for managing your source files, witnesses and comparison sets) to the visualization pane that dominates the lower portion of the window. Using [a set](http://www.juxtacommons.org/shares/GJm4O9) comparing Lewis Carroll's Alice's Adventures Underground with the more well-know Alice's Adventures in Wonderland, Dr. Stauffer showed how the [heat map](http://juxtacommons.org/guide#visualizations) overlays color over variants - the deeper the color, the more different the passage. He also showed how the [histogram](http://juxtacommons.org/images/histogram.jpg) offers a more global view of the collation, and allows the user to target the regions with the most difference quickly, even for long documents.\n\n[caption id=\"attachment_8062\" align=\"aligncenter\" width=\"300\"][![Alice Underground vs Alice in Wonderland: heat map and histogram](http://www.scholarslab.org/wp-content/uploads/2013/04/alice_set-300x170.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/04/alice_set.jpg) Alice Underground vs Alice in Wonderland: heat map and histogram[/caption]\n\nDr. Stauffer also showed [a comparison](http://www.juxtacommons.org/shares/FF0x40) of two versions of D.G. Rossetti's review, \"[The Stealthy School of Criticism](http://www.rossettiarchive.org/docs/34p-1870.raw.html),\" illustrating how the author toned down his rhetoric in the version published in the Athenaeum.\n\n[caption id=\"attachment_8079\" align=\"aligncenter\" width=\"300\"][![Highlight of variant in Rossetti's text](http://www.scholarslab.org/wp-content/uploads/2013/04/creeping-300x114.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/04/creeping.jpg) Highlight of variant in Rossetti's text[/caption]\n\nWhen I took the podium, I chose to focus on other ways of utilizing Juxta Commons, from authenticating texts found on the web, to exploring the history of news items and Wikipedia articles. For example, a look at [two versions of an article](http://www.juxtacommons.org/shares/Ma6uaV) posted on the New York Times website in November shows that the same article might be drastically different 30 minutes after posting.\n\n\n[![New York Times article](http://www.scholarslab.org/wp-content/uploads/2013/04/nyt-300x157.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/04/nyt.jpg)\n\n\nA full compendium of the sets we shared can be found at the main Juxta blog at [juxtasoftware.org](http://www.juxtasoftware.org/using-juxta-in-the-classroom-scholars-lab-presentation/).\n\nIf you have any questions about Juxta or Juxta Commons, please visit us at our development list, or write us directly at technologies at nines dot org.\n"},{"id":"2013-04-26-scholars-lab-speaker-series-shawn-graham","title":"Scholars' Lab Speaker Series: Shawn Graham","author":"ronda-grizzle","date":"2013-04-26 08:13:59 -0400","categories":["Podcasts"],"url":"scholars-lab-speaker-series-shawn-graham","content":"**Scholars' Lab Speaker Series: Shawn Graham**\n**Practical Necromancy: Simulation and Agent Based Modeling in the Humanities**\n\nIn March, Dr. Shawn Graham, Assistant Professor of Digital Humanities in the Department of History at Carleton University, spoke about using agent based simulations to understand aspects of Greco-Roman antiquity.\n\n\n\nSummary:\nRaising the dead presents certain difficulties, but computation suggests a way forward. In Practical Necromany, Dr. Graham discusses the use of agent based simulations to understand aspects of Greco-Roman antiquity, its perils and potentials, and how all of this fits into a worldview informed by the digital humanities.\n\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23530048996/enclosure.mp3\"]\n"},{"id":"2013-04-29-graduate-fellows-forum-joanna-swafford","title":"Graduate Fellows Forum: Joanna Swafford","author":"ronda-grizzle","date":"2013-04-29 05:04:19 -0400","categories":["Podcasts"],"url":"graduate-fellows-forum-joanna-swafford","content":"**Graduate Fellows Forum: Joanna Swafford**\n**Victorian Songs and Digital Tools: Facilitating Sound Studies Scholarship**\n\nJoanna Swafford\nPhD Candidate, Department of English\nScholars' Lab Fellow 2012-2013\n\nRespondent\nDr. Herbert Tucker\nJohn C. Coleman Professor of English\nUniversity of Virginia\n\nSummary:\nAlthough sound studies and interdisciplinary music and poetry scholarship have increased over the last decade, scholars have not had the digital tools necessary to make their auditory arguments accessible to a wider audience. This talk will present two tools built by Scholars' Lab Fellow Joanna Swafford that will help change that: _Songs of the Victorians_, an archive and analysis of parlor and art song settings of Victorian poems, and _Augmented Notes_, a tool that will let scholars build their own interdisciplinary websites like _Songs of the Victorians_.\n\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23538270765/enclosure.mp3\"]\n\n"},{"id":"2013-04-30-team-praxis","title":"Team Praxis!","author":"cecilia-márquez","date":"2013-04-30 07:42:12 -0400","categories":["Grad Student Research"],"url":"team-praxis","content":"This past week we started a soft (internal) launch of the new Prism so we could begin to work out all of the kinks. This of course meant that we spent _a lot _of time together working out last minute concerns.  This was a really different \"finals\" type experience than the one I am currently experiencing.  Finals in graduate school basically means that I find a quiet place and don't leave until all of the papers have been written.  This crunch time collaboration felt exciting, dynamic, and fun.  It made me realize how much I have come to value the teamwork time that I get in Praxis each week.  Grad school can be isolating and socializing can feel like a distraction.  What was great about Praxis was that we were able to come together weekly in a work-related activity but really enjoy each other's company.\n\nI have always known that I'm a social worker but this experience has confirmed that in order to be successful in my graduate career I will have to build a team of people around me who are willing and able to support me.  I am also increasingly thinking about what type of alt-ac careers would allow me to keep this team experience central to my work life.  I'm going to miss these weekly work/hang sessions.\n"},{"id":"2013-05-03-one-day-of-praxis","title":"One day of Praxis","author":"shane-lin","date":"2013-05-03 11:07:23 -0400","categories":["Grad Student Research"],"url":"one-day-of-praxis","content":"Here is a bunch of photos from our most recent team meeting!\n\n[![20130429-_MG_8502-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8502-Edit-300x200.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8502-Edit.jpg)[![20130429-_MG_8503-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8503-Edit-300x200.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8503-Edit.jpg)[![20130429-_MG_8634](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8634-300x200.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8634.jpg)[![20130429-_MG_8631-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8631-Edit-202x300.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8631-Edit.jpg)[![20130429-_MG_8605-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8605-Edit-300x130.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8605-Edit.jpg) [![20130429-_MG_8558-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8558-Edit-300x140.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8558-Edit.jpg) [![20130429-_MG_8586-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8586-Edit-300x200.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8586-Edit.jpg) [![20130429-_MG_8583-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8583-Edit-300x200.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8583-Edit.jpg) [![20130429-_MG_8596-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8596-Edit-200x300.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8596-Edit.jpg)[![20130429-_MG_8622-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8622-Edit-300x200.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8622-Edit.jpg) [![20130429-_MG_8572-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8572-Edit-300x200.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8572-Edit.jpg)  [![20130502-0030_###83650030_edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130502-0030_83650030_edit-300x198.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130502-0030_83650030_edit.jpg) [![20130429-_MG_8545-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8545-Edit-231x300.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8545-Edit.jpg) [![20130429-_MG_8543-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8543-Edit-200x300.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8543-Edit.jpg) [![20130429-_MG_8528-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8528-Edit-200x300.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8528-Edit.jpg) [![20130429-_MG_8519-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8519-Edit-300x200.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8519-Edit.jpg) [![20130429-_MG_8516-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8516-Edit-300x200.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8516-Edit.jpg) [![20130429-_MG_8515-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8515-Edit-300x165.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8515-Edit.jpg) [![20130429-_MG_8509-Edit-2](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8509-Edit-2-200x300.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8509-Edit-2.jpg) [![20130429-_MG_8511-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8511-Edit-200x300.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8511-Edit.jpg)[![20130429-_MG_8506-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8506-Edit-300x200.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8506-Edit.jpg) [![20130429-_MG_8624-Edit](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8624-Edit-300x200.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/20130429-_MG_8624-Edit.jpg)\n"},{"id":"2013-05-03-random-skills-check","title":"Random Skills: Check","author":"claire-maiers","date":"2013-05-03 12:25:12 -0400","categories":["Grad Student Research"],"url":"random-skills-check","content":"This week Cecilia and took a few hours and completed one of our goals for this semester: creating a tutorial video for using Prism.   Not only did this turn out to be surprisingly fun (due mostly to Cecilia’s amazing antics), but it also added a few more skills to the list of things we have learned through Praxis.\n\n![DSCF1799](http://www.scholarslab.org/wp-content/uploads/2013/05/DSCF1799-300x225.jpg)\n\nFirst---though this might seem trivial-- we got to draw upon out teaching skills, devising a plan for how to teach someone to use Prism in under three minutes.  I authored the tutorial, and thanks to some great ad lib from Cecilia, I think the tutorial manages to capture the playful attitude with which we have approached our work and which we hope Prism engenders.\n\nIn addition, we learned how to use the [iShowU HD](http://www.shinywhitebox.com/ishowu-hd/) software program.  Admittedly, this is a very user-friendly program which allows you to record an area on your screen as a video.  Still, it has left us with one more skill which we can confidently claim for our own.\n\nAnd finally, and perhaps most importantly, we got to use this awesome microphone.   Need I say more?\n\n[![DSCF1789](http://www.scholarslab.org/wp-content/uploads/2013/05/DSCF17891-254x300.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/DSCF17891.jpg)\n\nThe launch of Prism is just around the corner.  In the mean time, check out our [tutorial here](http://www.youtube.com/watch?v=AxHDcW15UBI)!\n"},{"id":"2013-05-06-digital-humanities-speaker-series-alan-liu-rama-hoetzlein","title":"Digital Humanities Speaker Series: Alan Liu & Rama Hoetzlein","author":"ronda-grizzle","date":"2013-05-06 04:37:38 -0400","categories":["Podcasts"],"url":"digital-humanities-speaker-series-alan-liu-rama-hoetzlein","content":"**Digital Humanities Speaker Series: Alan Liu & Rama Hoetlein**\n**The History of Thought as Networked Community: The RoSE Prototype**\n\nIn March, Dr. Alan Liu, Professor in the Department of English at University of California, Santa Barbara,\nand Rama Hoetzlein, Project Scientist, spoke about their work developing the RoSE Prototype.\n\n\nSummary:\nWhat if bibliographies of past authors and works could be modeled as a dynamic, evolving society linked to today's scholars and students?  What if scholars and students could add data about biographical, historical, and intellectual relationships to the bibliographical entries, thus using present-day crowdsourcing to make more socially meaningful the crowds of history?  And what if visualizations could help us actively \"storyboard\" intellectual movements and not just spectate them?  Alan Liu and Rama Hoetzlein present the conceptual framework and some of the discoveries and challenges of the RoSE Research-oriented Social Environment (in beta at the conclusion of a NEH Digital Humanities Start-up grant).\n\nThe Digital Humanities Speaker Series is co-sponsored by [IATH](http://www.iath.virginia.edu/), [SHANTI](http://shanti.virginia.edu/), and the [Scholars' Lab](http://www.scholarslab.org/).\n\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23700755297/enclosure.mp3\"]\n"},{"id":"2013-05-06-on-tasks-large-and-small","title":"On Tasks Large and Small","author":"brandon-walsh","date":"2013-05-06 10:06:03 -0400","categories":["Grad Student Research"],"url":"on-tasks-large-and-small","content":"The biggest issue that I have faced with my work with the Praxis Program has to do with how I judge the difficulty of the tasks before me. I have proven to be singularly inadequate at distinguishing quick fixes with a large payoff from larger problems that would yield only small utility. This issue surfaced early on, when I boldly suggested that I could implement [Omniauth](http://www.omniauth.org/) in a week. Two and a half months later I am still trying to get it to work: the Omniauth code structure works more generally, but each authentication service is its own unique little snowflake, different from all the other snowflakes in the code breaking eccentricities that it brings to the table. We had to jettison Twitter early on because it would not return user emails, which are central to the way that Devise handles logins. Facebook worked on local rails servers, but something whacky is happening on staging (and in deployment, for that matter) so that it throws a redirect uri failure at us that did not appear before. I feel as though I could have pounded away at these problems for ages and gotten nowhere.\n\nBut for every task like Omniauth there have been other jobs that seemed large but turned out to be much easier than expected. I implemented a destroy function on prisms last week in just a couple of hours ([Shane](http://www.scholarslab.org/people/shane-lin/) had already provided a lot of the scaffolding on an older database setup), which meant that I also had to implement user roles and permissions using cancan. Cancan felt like a bigger job than it wound up being, and the whole thing seems to be working in the feature branch now. Now a user can delete a prism that they have uploaded. Great success!\n\nI think this ability to distinguish between large and small tasks is something that can only come with time and experience. We Praxers may be unable to tell how much work a particular feature will require, but the SLab team has been incredibly helpful in that regard. Where we see mountains, they see molehills, and vice versa. And when you get right down to it, Prism seemed like the biggest mountain of all a couple months ago. Prism felt like an insurmountable task until last week when everything came together in a flurry of productivity. I am impressed with what we have done.\n\nNow, without warning, a couple of inspirational [climbing](http://www.youtube.com/watch?v=NG2zyeVRcbs) [videos](http://www.youtube.com/watch?v=EoCPuhhE6dw).\n"},{"id":"2013-05-06-scholars-lab-speaker-series-alan-liu","title":"Scholars' Lab Speaker Series: Alan Liu","author":"ronda-grizzle","date":"2013-05-06 07:11:59 -0400","categories":["Podcasts"],"url":"scholars-lab-speaker-series-alan-liu","content":"**Scholars' Lab Speaker Series: Alan Liu**\n**4Humanities: Values, Strategies, Technologies for Humanities Advocacy in the Digital Age**\n\nIn April, Dr. Alan Liu, Professor in the Department of English at the University of California, Santa Barbara, spoke about ways in which the skills and resources of the DH community can help advocate for the humanities.\n\n\n\nSummary:\nAlan Liu will present an informal talk exploring such issues as assessing values and narrative frames for communicating the worth of the humanities. He will also brainstorm next-generation methods for using digital/networked technologies to create material for public view of scholars' normal research and teaching work.\n\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23697875127/enclosure.mp3\"]\n"},{"id":"2013-05-07-graduate-fellows-forum-david-flaherty","title":"Graduate Fellows Forum: David Flaherty","author":"ronda-grizzle","date":"2013-05-07 04:23:24 -0400","categories":["Podcasts"],"url":"graduate-fellows-forum-david-flaherty","content":"**Graduate Fellows Forum: David Flaherty**\n**Mapping the British Vision of Empire in 1731**\n\nDavid Flaherty\nPhD Candidate, Corcoran Department of History\nScholars' Lab Fellow 2012-2013\n\nRespondent:\nDr. S. Max Edelson\nAssociate Professor, Corcoran Department of History\nUniversity of Virginia\n\nSummary:\nThe British Board of Trade, a bureaucratic body responsible for overseeing the 18th-century Atlantic colonies, had a broad geographic vision of the British Atlantic based on their extensive communication with colonies from Newfoundland to Honduras.  This project maps the Board's correspondence for a single year, showing which points on the map it received information about and where those letters came from, in an attempt to illustrate that vision.\n\n\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23698365867/enclosure.mp3\"]\n"},{"id":"2013-05-08-drum-roll-please","title":"Drum Roll Please.....","author":"claire-maiers","date":"2013-05-08 12:06:32 -0400","categories":["Announcements","Grad Student Research"],"url":"drum-roll-please","content":"After many months of brainstorming, debating, dreaming big, getting down to business, panicking, refocusing, programming, and fine tuning, [Prism](http://prism.scholarslab.org/) is here!\n\n[![LOGO](http://www.scholarslab.org/wp-content/uploads/2013/05/LOGO-300x126.png)](http://prism.scholarslab.org/)\n\nIt has been a great journey, and I think [my fellow Praxers](http://praxis.scholarslab.org/) would agree when I say that we have learned a lot.  Not only have we been introduced to the world of DH and received hands-on training in things like HTML, CSS, Ruby on Rails, database construction, and a smidge of JavaScript and Coffee Script, but we have also learned a great deal about working in a team environment and collaboration.\n\nAlthough I plan to share some of the lessons learned throughout this year with you all in a subsequent blog post, I thought I would take this opportunity to tell you about what we have done with Prism this year.\n\nOauth:  With the new Prism, users are able to sign in through a variety of options.  We have retained the ability for users to create their own account on Prism.  However, users can also sign in through existing accounts with Facebook, Google, and Mozilla Persona.   Props go out to [Brandon](http://twitter.com/walshbr) who tackled OmniAuth early on in the semester and then stuck with it despite many unforeseen hurdles!\n\nRedesign:  From the beginning of our meetings, we discussed creating an online environment that was playful and would invite users to explore and participate.  In tandem with our vision of playfulness, we wanted to design the site with special attention to the user interface.  We pictured a site where the design itself would direct users on how to interact and lead them through the workflow from Prism creation to visualization in a streamlined manner.  Manifesting this vision fell to [Gwen](https://twitter.com/GwenNally/) and [Cecilia](https://twitter.com/cmarque/), and they have done a remarkable job creating a beautiful and functional site.\n\nDatabase Refactoring: On perhaps a less glamorous note, Brandon and [Shane](https://twitter.com/shane_et_al/) (or “Brane” as I call them) have refactored the database for Prism.  This refactoring was crucial to some of the improvements we’ve made to visualizations, allowing for user uploads, and will hopefully allow for others to build upon our code more easily.\n\nUser Uploads: Thanks to Shane and Brandon, Prism now also provides the ability for users to upload their own texts to the site.  Our hope is that this contribution allows for Prism to be deployed in a variety of classroom and scholarly settings.  As users create a new Prism, they have the option to either make their Prism public and available to anyone for highlighting or they can choose to make an unlisted Prism.  Unlisted Prisms are not listed in the Browse page, allowing users to limit participation to desired audiences.  I encourage you to check this feature out for yourself!\n\nA New Visualization:  This version of Prism also includes a second option for visualizing collaborative interpretation, the Winning Facet Visualization, developed by Chris.  This option allows users to see a combination of all the facet categories at once.  In addition, users can interact with the pie chart to see the exact break down of user highlights by category.\n\nThere are many other smaller improvements that I have not listed here, so go take a look at [Prism](http://prism.scholarslab.org/) and experience it for yourself!\n\nBefore I sign off, I want to give a shout out to the [SLab faculty and staff.](https://www.scholarslab.org/people/)  They invested many hours in our team and rescued each of us from a precipice of panic or frustration more than once this year.  None of this would have been possible without them.  I will have more to say on that later, but for now, I will just say THANKS!\n"},{"id":"2013-05-09-prism-for-play","title":"Prism, for Play","author":"bethany-nowviskie","date":"2013-05-09 11:31:20 -0400","categories":["Digital Humanities","Grad Student Research","Research and Development"],"url":"prism-for-play","content":"_[cross-posted from [nowviskie.org](http://nowviskie.org/2013/prism-for-play/)]_\n\nThis week marks the release of a new version of [Prism](http://prism.scholarslab.org/), a web-based tool for \"[crowdsourcing interpretation](http://www.scholarslab.org/digital-humanities/crowdsourcing-interpretation/),\" constructed over the course of two academic years by two separate cohorts of graduate fellows in our [Praxis Program](http://praxis-network.org/praxis-program.html) at the [Scholars' Lab](http://scholarslab.org).\n\n[![prism-logo](http://nowviskie.org/wp-content/uploads/2013/05/LOGO-6747d62e676eadfc92873f25fb82d6af.png)](http://nowviskie.org/wp-content/uploads/2013/05/LOGO-6747d62e676eadfc92873f25fb82d6af.png)\n\n[Praxis fellows](http://praxis.scholarslab.org/people.html) are humanities and social science grad students across a variety of departments at UVa, who come to our library-based lab for an intensive, team-based, hands-on experience in digital humanities project-work, covering as many aspects of DH practice as our practiced [Scholars' Lab staff](https://www.scholarslab.org/people/) can convey. <!-- more --> (By the end of the year, our fellows have negotiated a project charter; learned to create and appreciate robust ontologies and database designs; programmed or at least hacked around in Ruby and Javascript/CoffeeScript; raised up a Rails scaffold and become competent in HTML/CSS; managed the versioning of open source code in GitHub and deployed staging and production instances of a project; made design decisions and analyzed and drawn conclusions about user-experience aspects of a real-world project; communicated the value of their work and grown more comfortable sharing it in iterations and open-access venues; honed their skills at speaking across disciplinary and professional lines; learned hard project-management lessons; expanded their contacts in the DH world; engaged in conversations about funding, academic personnel, professionalization, and broadened career paths for scholars; and had fun and survived it all.)\n\nWhere our 2011-12 cohort of Praxis fellows laid the groundwork (resurrecting an old SpecLab game that evolved into the finest bit of vaporware [never to be produced](http://www.digitalhumanities.org/companion/view?docId=blackwell/9781405103213/9781405103213.xml&chunk.id=ss1-3-4&toc.depth=1&toc.id=ss1-3-4&brand=default) by the humanities computing community at UVa, and creating a [multilingual](http://www.scholarslab.org/grad-student-research/teaching-prism-how-to-speak-spanish-and-french/), prototype system that allowed multiple readers to mark up a pre-set list of texts according to a shared vocabularly), our 2012-13 team had the opportunity to refine the concept into a usable, open-ended tool. Thanks to their work, it's easy to [create a Prism account](http://prism.scholarslab.org/users/sign_in) (including by logging in via existing services) and launch your own markup games, by uploading texts and defining the facets available to readers for the kind of blunt-force, collaborative annotation Prism allows.  Users now have a catalogue of texts they've added to the system or participated in marking up, and can get a sense of the [evolving, shared reading](http://prism.scholarslab.org/prisms) of those texts through two visualization modes -- one new (showing a quantified breakdown of crowdsourced readings), and one (showing the affective frequency of reader agreement) refined. Best of all, Prism has become lovely and light.  A design refresh and [attention to ease-of-entry](http://prism.scholarslab.org/pages/demo) should make it an attractive tool for classroom use, and for experimentation and play.\n\nPlease try it out and let our students know what you think. (They are [Claire Maiers](http://cdm6zf.github.io/), Sociology; [Brandon Walsh](http://bmw9t.github.io/), English; [Gwen Nally](http://egnally.github.io/), Philosophy; [Cecilia Marquez](http://www.ceciliamarquez.org/), History; [Chris Peck](http://artsandsciences.virginia.edu/music/people/graduatestudents/#a25), Music; and [Shane Lin](http://ssl2ab.github.io/), History -- emerging scholars and scholar-practitioners to watch!)  We would be especially interested in pedagogical applications of Prism.  And, since next year's Praxis cohort -- soon to be announced -- will be moving on to a new project (reviving and re-thinking another SpecLab classic, [the Ivanhoe Game](http://nowviskie.org/2009/sketching-ivanhoe/)), we also [encourage developers](https://github.com/scholarslab/prism) to send pull requests for bug fixes and new features.  Much remains possible with the \"crowdsourcing interpretation\" concept at the heart of Prism, which [one early reviewer](http://www.michelepasin.org/blog/2012/06/01/crowdsourcing-interpretation-with-prism-a-new-software-from-the-scholars-lab/) called \"potentially the beginning of a new research field.\"  Further visualizations?  Image-based or non-textual approaches to collaborative markup?  Computational linguistic analysis based on comparison of crowdsourced readings to larger corpora?  The sky is the limit.\n\nFor now, we're just enjoying the way the new, bright, child-like design for Prism matches [the current mood](https://www.scholarslab.org/grad-student-research/one-day-of-praxis/) in the Scholars' Lab grad lounge: \"Look! We made this!\"\n\n"},{"id":"2013-05-13-announcing-neatline-2-0-alpha1","title":"Announcing Neatline 2.0-alpha1!","author":"david-mcclure","date":"2013-05-13 06:38:10 -0400","categories":["Geospatial and Temporal"],"url":"announcing-neatline-2-0-alpha1","content":"![neatline-2.0-alpha1-small](http://www.scholarslab.org/wp-content/uploads/2013/05/neatline-2.0-alpha1-small.png)\n\n_[Cross-posted with [dclure.org](http://dclure.org/logs/announcing-neatline-2-0-alpha1/)]_\n\nIt's here! After much hard work, we're delighted to announce the first alpha release of Neatline 2.0, which migrates the codebase to Omeka 2.0 and adds lots of exciting new things. For now, this is just an initial testing release aimed at developers and other brave folks who want to tinker around with the new set of features and help us work out the kinks. **Notably, this build doesn't yet include the migration to upgrade existing exhibits from the 1.1.x series**, which we'll ship with the first stable release in the next couple weeks once we've had a chance to field test the new code.\n\n\n45 minutes of Neatline 2.0 alpha testing, compressed to 90 seconds, set to Chopin.\n\nIn the interest of modularity (more on this later), the set of features that was bundled together in the original version of Neatline has been split into three separate plugins:\n\n\n\n\n\n\n  * **[Neatline](http://neatline.org/wp-content/uploads/2013/05/Neatline-2.0-alpha1.zip)** - The core map-making toolkit and content management system.\n\n\n\n  * **[NeatlineWaypoints](http://neatline.org/wp-content/uploads/2013/05/NeatlineWaypoints-0.1.zip)** - A list of sortable waypoints, the new version of the vertical \"Item Browser\" panel from the 1.x series.\n\n\n\n  * **[NeatlineSimile](http://neatline.org/wp-content/uploads/2013/05/NeatlineSimile-0.1.zip)** - The SIMILE Timeline widget.\n\n\n\n\nJust unpack the `.zip` archives, copy the folders into the `/plugins` directory in your Omeka 2.x installation, and install the plugins in the Omeka admin. For more detailed information, head over to the [**Neatline 2.0-alpha1 Installation Wiki**](https://github.com/scholarslab/Neatline/wiki/Neatline-2.0-alpha1-Installation), and take a look at the [**change log**](https://github.com/scholarslab/Neatline/blob/develop/CHANGELOG.md) for a more complete list of changes and additions.\n\nWe're really excited about this code. Since releasing the first version last summer, we've gotten a huge amount of incredibly helpful feedback from users, much of which has been directly incorporated into the new release. We've also added a carefully-selected set of new features that opens up the door to some really interesting new approaches to geospatial (and completely _non_-geospatial) annotation. It's a leaner, faster, more focused, more reliable, and generally more capable piece of software - we're excited to start building projects with it!\n\nSome of the additions and changes:\n\n\n\n\n\n\n  * **Real-time spatial querying**, which makes it possible to create _really_ large exhibits - as many as about 1,000,000 records on a single map;\n\n\n\n\n  * **A total rewrite of the front-end application** in [Backbone.js](http://backbonejs.org/) and [Marionette](http://marionettejs.com/) that provides a more minimal, streamlined, and responsive environment for creating and publishing exhibits;\n\n\n\n\n  * **An interactive \"stylesheet\" system** (inspired by projects like Mike Migurski's [Cascadenick](https://github.com/mapnik/Cascadenik)), that makes it possible to use a dialect of CSS - built directly into the editing environment - to synchronize large batches of records;\n\n\n\n\n  * **The ability to import high-fidelity SVG illustrations** created in specialized vector editing tools like Adobe Illustrator and Inkscape;\n\n\n\n\n  * **The ability to add custom base layers**, which, among other things, makes it possible to annotate completely non-spatial entities - paintings, photographs, documents, and anything else that can be captured as an image;\n\n\n\n\n  * **A revamped import-from-Omeka workflow** that makes it easier to link Neatline records to Omeka items and batch-import large collections of items;\n\n\n\n\n  * **A flexible programming API and \"sub-plugin\" system** that makes it easy for developers to extend the core feature set with custom functionality for specific projects - everything from simple JavaScript widgets (legends, sliders, scrollers, etc.) up to really deep modifications that extend the core data model and add completely new interactions.\n\n\n\n\n\nOver the course of the next two weeks, I'll be writing in much more detail about some of the new features. In the meantime - let us know what you think! We're going to be pushing out a series of alpha releases in pretty rapid succession over the course of the next couple weeks, and we're really keen to get feedback about the new features before cutting off a stable 2.0 release. If you find a bug, or think of a feature that you'd like to see included, be sure to file a report on the [issue tracker](https://github.com/scholarslab/Neatline/issues).\n"},{"id":"2013-05-14-arduino-rainbow-hack","title":"Arduino Rainbow Hack","author":"brandon-walsh","date":"2013-05-14 05:58:37 -0400","categories":["Experimental Humanities"],"url":"arduino-rainbow-hack","content":"The following was co-authored and co-hacked with [Claire](http://www.scholarslab.org/people/claire-maiers/).\n\nClaire and I went to the [Arduino](http://www.arduino.cc) Hackday hosted by the Scholars’ Lab on Friday. We had no idea what we were getting into, which made it all the more fun. [Jeremy](http://www.scholarslab.org/people/jeremy-boggs/) brought in a bunch of Arduino kits of all shapes and sizes, and various people went to work to see what they could make out of them.\n\nFirst Claire and I got a blinking light to work. From there we decided to skip several chapters to make a little music player using a piezo speaker component. At that point we went off the map, dreaming big and working off the page. We started out working with this [schematic](http://www.arduino.cc/en/Tutorial/Melody), but we wanted to expand things a little.\n\nI apologize in advance for whatever vocabulary gap there might be in the discussion below; it was my first time coding in C.\n\nFor one, this set-up only uses a single octave major scale. We opened things up by expanding the range of tones the speaker could play, including both a chromatic scale and adding an extra octave. One difficulty here was in the way that way that accidentals would be read by the board. At first we tried to use standard notation – f sharp would be represented by f#. But the code finds the notes in the melody by reading character by character through an array. So when the computer sees “cc#”, it processes it as two ‘c’s and one nonsense syllable that it can’t process. There is probably a more elegant solution, but we got around this by associating the accidentals with new characters entirely\n\nC#/Db             =>       l\n\nD#/Eb              =>       m\n\nF#/Gb              =>       n\n\nG#/Ab             =>       o\n\nA#/Bb             =>       p\n\nNext, we added an extra octave. To extend the range, we hard coded frequencies for the new notes according to the formula given by the original Arduino code:\n\ntimeHigh = period / 2 = 1 / (2 * toneFrequency)\n\nWe probably could have done this in a more dynamic way. Or, as [Eric](http://www.scholarslab.org/people/eric-johnson/) and [Ronda](http://www.scholarslab.org/people/ronda-grizzle/) showed, we could have just downloaded a tone library to do this for us. They were jamming out to the Star Wars theme while we were still trying to get things working.  But eventually, we were able to program in the opening of “Mary Had a Little Lamb,”_ _Mozart’s _Lacrimosa_, John Coltrane’s “26-2,” and “Somewhere Over the Rainbow_._”\n\nHere is where things got really ambitious, we came up with the idea to incorporate a RGB LED light that would change colors each time there was a note change in “Somewhere Over the Rainbow.” Rainbow lights for a song about rainbows.\n\nTo get the light working we worked off a slightly modified version of [this schematic](http://ardx.org/CODE12S), mashed in with the piezo tutiorial. With some quick help from [Eric](http://www.scholarslab.org/people/eric-rochester/), we modified the light arrangement so that each generates a random RGB combination. We then synchronized this with the rhythm of the melody.\n\nIn terms of the actual circuitry, we just split the digital signal so that it went out to two different parts of the bread board simultaneously and then fed information to both pins. Later we added a volume knob to reduce the maddening noise. You can see attached photos below, though you’ll have to meet us halfway with our diagram of the breadboard.\n\nA later, unsuccessful attempt to key each frequency to a particular color resulted in a light that got brighter or darker depending on the note. I think with a little more time I could fix that by hard coding particular RGB values to particular frequencies, but we were trying to do it dynamically by converting the frequency directly into an intensity value.\n\nBehold our final product!\n\n[gist id=5569210]![Arduino Rainbow Circuit Map](http://www.scholarslab.org/wp-content/uploads/2013/05/photo-300x224.jpg)[![Arduino Breadboard Diagram](http://www.scholarslab.org/wp-content/uploads/2013/05/photo-1-300x224.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/photo-1.jpg)\n"},{"id":"2013-05-14-interactive-css-in-neatline-2-0","title":"Interactive CSS in Neatline 2.0","author":"david-mcclure","date":"2013-05-14 08:23:53 -0400","categories":["Geospatial and Temporal"],"url":"interactive-css-in-neatline-2-0","content":"_[Cross-posted with [dclure.org](http://dclure.org/uncategorized/interactive-css-in-neatline-2-0/)]_\n\nNeatline 2.0 makes it possible to work with really large collections of records - as many as about [1,000,000 in a single exhibit](http://dclure.org/logs/neatline-one-million-records/). This level of scalability opens up the door to a whole range of projects that would have been impossible with the first version of Neatline, but it also introduces some really interesting content management challenges. If the map can _display_ millions of records, it also needs utilities to effectively _manage_ content at that scale.\n\nThis often involves a shift from working with individual records to working with groups of records. When there are a million records on the map, it's pretty unlikely that you'll want to change the color of just one of them. More likely, that record will exist as part of a large grouping of related records (eg, \"democratic precincts,\" or \"photographs from 1945\"), all of which should share a certain set of attributes. There needs to be a way to slice and dice records into overlapping clusters of related records, and then apply bulk updates to the individual clusters.\n\nReally, this is a familiar problem - it's structurally identical to the task of styling web pages with CSS, which makes it possible to address groupings of elements with \"selectors\" and apply key-value styling rules to the groups. Inspired by projects like Mike Migurski's [**Cascadenick**](https://github.com/mapnik/Cascadenik), Neatline 2.0 makes it possible to use a Neatline-inflected dialect of CSS to update groups of records linked together with \"tags,\" which can be applied in any combination to the individual records.\n\n**Neatline Stylesheet Basics**\n\nLet's take a look at how this works in practice. Imagine you're plotting results from the last four presidential elections. You load in a big collection of 800,000 records (200,000 precincts for each of the four elections), each representing an individual polling place with a point on the map. Each point is scaled to represent the number of ballots cast at that location, and shared red or blue according to which party won more votes. In this case, there are really seven different nested and overlapping taxonomies in the data. All of the records are `precincts`, but each falls into one of the our election seasons - `2000`, `2004`, `2008`, or `2012`. And each precinct went either `democrat` or `republican`, regardless of which election cycle it belongs to. Each record can be tagged with some combination of these tags:\n\n[![tags-input](http://www.scholarslab.org/wp-content/uploads/2013/05/tags-input.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/tags-input.jpg)\n\nEach of the groupings needs to share a specific set of attributes - and also _not_ share some attributes that need to be assigned separate values on individual records. For example, all of the precincts - regardless of date or party - should share the same basic `fill-opacity` and `stroke-width` styles. All records in each of the groupings for the four election seasons need to share the same `after-date` and `before-date` visibility settings so that the records phase in and out of visibility in unison. And all republican and democratic records should share the same shares of red and blue. Meanwhile, none of the groupings should define a standard point-radius style, which is used on a per-record basis to encode the number of ballots cast at that location.\n\nNeatline-inflected CSS makes it easy to model these relationships. To start, I'll define some basic styles for the top-level `precinct` tag, which is applied to all the records in the exhibit:\n\n\n\nNow, when I click \"Save,\" Neatline instantaneously updates the `stroke-width` and `fill-opacity` styles on all records tagged with `precinct`:\n\n[![precinct-styles](http://www.scholarslab.org/wp-content/uploads/2013/05/precinct-styles-1024x640.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/precinct-styles.jpg)\n\nNext, I'll set the `before-date` and `after-date` properties for each of the for election season tags, which ensure that the four batches of records phase in and out of visibility in unison as the timeline is scrolled back and forth:\n\n\n\nNow, when I open up any individual record, the `before-date` and `after-date` fields will be updated with new values depending on which election the record belongs to:\n\n[![dates-fieldset](http://www.scholarslab.org/wp-content/uploads/2013/05/dates-fieldset.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/dates-fieldset.jpg)\n\nLast, I'll define the coloring rules for the two political parties. First, the Democrats:\n\n\n\nClick \"Save,\" and all democratic precincts update with the new color:\n\n[![democrat-styles](http://www.scholarslab.org/wp-content/uploads/2013/05/democrat-styles-1024x640.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/democrat-styles.jpg)\n\n**Auto-updating stylesheet values**\n\nSo far, we've just been entering hard-coded values into the stylesheet. This often makes sense for properties that have inherently semantic values (eg, dates). For other attributes, though (namely colors), it's much harder to reason in the abstract about what value you want. For example, I know that I want the republican precincts to be \"red,\" but I don't know off-hand that `#ff0000` is the specific hexadecimal value that I want to use. It makes more sense to open up the edit form for an individual record and use the color pickers for the \"Fill Color\" field to find a color that looks good.\n\nAnd even for styles that can be reasoned about in the abstract, it's often easier and more intuitive to use the auto-previewing functionality on one of the record forms to tinker around with different values. Once you've decided on a new setting, though, it's annoying to have to manually propagate the value back into the stylesheets so that all of the record's siblings stay in sync - you'd have to copy the value, close the form, open up the stylesheet, find the right rule, and paste in the new value. To avoid this, Neatline also automatically updates the _stylesheet_ when individual record values are changed, and immediately pushes out the new value to all of the record's siblings.\n\nLet's go back to the election results. For the republican precincts, instead of pasting in a specific hex value for the `fill-color` style, we'll just \"register\" `fill-color` as being one of the properties controlled by the `republican` tag by listing the style and assigning it a value of `auto`:\n\n\n\nWhen I click \"Save,\" nothing happens, since a value isn't defined. Now, though, I can just open up any of the individual `republican` records, choose a shade of red, and save the record. Since we activated the fill-color style for the republican tag, Neatline automatically updates all of the other `republican` records _just as if we had set the value directly on the stylesheet_:\n\n[![republican-record](http://www.scholarslab.org/wp-content/uploads/2013/05/republican-record-1024x640.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/republican-record.jpg)\n\nAnd now, when I go back to the stylesheet, the `fill-color` rule under `republican` is automatically updated with the value that we just set in the record form:\n\n[![updated-stylesheet](http://www.scholarslab.org/wp-content/uploads/2013/05/updated-stylesheet.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/05/updated-stylesheet.jpg)\n\nThis also works for styles that already have concrete values. For example, say I change my mind and want to tweak the shade of blue used for democratic precincts. I can just open up any of the individual `democrat`-tagged records, pick a new value with the color picker, and save the record. Again, Neatline automatically replaces the old value on the stylesheet and propagates the change to all of the other democratic precincts.\n"},{"id":"2013-05-16-neatline-1-1-3-maintenance-release","title":"Neatline 1.1.3 Maintenance Release","author":"david-mcclure","date":"2013-05-16 12:17:57 -0400","categories":["Geospatial and Temporal"],"url":"neatline-1-1-3-maintenance-release","content":"This morning, [Kiyonori Nagasaki](https://twitter.com/knagasaki) noticed that one of the remote API's used by the Neatline 1.x releases went offline, which had the effect of breaking exhibits that included the SIMILE Timeline widget. To fix this, we just posted a [**1.1.3 maintenance release**](http://omeka.org/add-ons/plugins/neatline/) that patches up the timeline problem and also includes a couple of other improvements:\n\n\n\n\n\n  * Disabled animated opacity transitions on WMS tiles, which were causing performance problems in recent builds of Chrome;\n\n\n\n  * Fixed a bug that was causing the map not to focus correctly when a record is selected that has a default focus position/zoom, but no vector geometry.\n\n\n\n\n[**Download Neatline 1.1.3**](http://omeka.org/add-ons/plugins/neatline/).\n\nMeanwhile, lots of activity on the Neatline 2.0 front - we're almost done with a second alpha release, which gets us one step closer to a stable 2.0 release, which will include the migration to update existing installations from the 1.x series.\n\nStay posted!\n"},{"id":"2013-05-17-graduate-fellows-forum-lydia-rodriguez","title":"Graduate Fellows Forum: Lydia Rodríguez","author":"ronda-grizzle","date":"2013-05-17 10:36:28 -0400","categories":["Podcasts"],"url":"graduate-fellows-forum-lydia-rodriguez","content":"**Graduate Fellows Forum: Lydia Rodríguez**\n\n\n\n\n**The Time Has Come: Ethnography, Gesture Research, and Digital Technology**\n\n\n\n\n\nLydia Rodríguez  \n\nPhD Candidate, Department of Anthropology  \n\nScholars' Lab Fellow\n\n\n\n\n\nRespondent  \n\nDr. Eve Danziger  \n\nAssociate Professor, Department of Anthropology  \n\nUniversity of Virginia\n\n\n\n\n\nSummary:\n\n\n\n\nIn Western societies, time is usually perceived as a linear progression of events, but not all cultures think about and experience time in this particular way. In this presentation I analyze the relationship among linguistic, conceptual, and cultural notions of time through ethnographic observation of spoken interactions in Chol, a Maya language spoken in Chiapas, Mexico. In particular, I describe how the concept of time is depicted in the spontaneous gestures which are produced in conversational exchanges among speakers of Chol Maya. I also discuss the role that digital technology has played in the collection and analysis of gestural data, and how digital tools can be used to complement and enhance traditional ethnographic research about temporal conceptualization.\n\n\n\n\n\n\n\n\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.23987868880/enclosure.mp3\"]\n"},{"id":"2013-05-17-testing-asynchronous-background-processes-in-omeka","title":"Testing asynchronous background processes in Omeka","author":"david-mcclure","date":"2013-05-17 06:16:22 -0400","categories":["Research and Development"],"url":"testing-asynchronous-background-processes-in-omeka","content":"_[Cross-posted from [dclure.org](http://dclure.org/logs/testing-asynchronous-background-processes-in-omeka/)]_\n\nI ran into an interesting testing challenge yesterday. In Neatline, there are a couple of controller actions that need to spawn off asynchronous background processes to handle operations that are too long-running to cram inside of a regular request. For example, when the user imports Omeka items into an exhibit, Neatline needs to query a (potentially quite large) collection of Omeka items and insert a corresponding Neatline record for each of them.\n\nJobs extend `Omeka_Job_AbstractJob` and define a public `perform` method:\n\n\n\nAnd can be dispatched asynchronously by getting the `job_dispatcher` out of the registry and passing the job name and parameters to `sendLongRunning`:\n\n\n\nIt's easy enough to directly unit test the `perform` method on the job, but, since actual execution of the process is non-blocking, the jobs can't be tested at the integration level in the ordinary manner. For example, I'd like to just dispatch a request with a mock item query, and check that the correct Neatline records were created. This can't be asserted reliably, though, since there's no guarantee that the job will have completed before the testing assertions are executed.\n\nThe job itself is non-blocking, but the job invocation in the controller code _is_ blocking, and can be tested pretty easily by replacing the `job_dispatcher` with a testing double and spying on the `sendLongRunning` method. Since this is a pattern that needs to be implemented in more than one test, I started by adding a `mockJobDispatcher` method to the abstract test-case class that mocks the job dispatcher and injects it into the registry:\n\n\n\nThen, in the test, we can just call this method to mock the dispatcher, assert that the dispatcher is expecting a call to `sendLongRunning` with the correct job and parameters, and then fire off a mock request to the controller action under test:\n\n\n\nThis is a pretty good solution, but not perfect: The integration test is really asserting an intermediate step in the implementation of the controller action, not the end result - it tests _that_ the job was called with certain parameters, not the final effect of the request. This opens up the door to false positives. For example, in the future, I might make a breaking change to the public API of the `Neatline_ImportItems`. Assuming I've changed the job's unit tests to assert against the new API, the test suite would pass even if I completely forget to update any of the job invocations, since the integration tests are just asserting the structure of the invocation, not the final effects.\n\nI've encountered a version of this problem more than once, and I've never really found a good solution to it. Short of moving up to something like in-browser Selenium tests, or resorting to hacky execution pauses in the integration tests, has anyone ever come across a better way to do this?\n"},{"id":"2013-05-22-neatline-time-1-1-3-release","title":"Neatline Time 1.1.3 Release","author":"jeremy-boggs","date":"2013-05-22 05:00:12 -0400","categories":["Research and Development"],"url":"neatline-time-1-1-3-release","content":"Thanks to Neatline Time user nancymou, we've addressed a bug in Neatline Time where fields added by other plugins to the advanced search form were not recognized by Neatline Time. (In nancymou's case, the other plugin was Exhibit Builder.) So we've put together a bugfix release, [version 1.1.3](http://omeka.org/add-ons/plugins/neatlinetime/), that addresses this issue. [Download Neatline Time 1.1.3](http://omeka.org/add-ons/plugins/neatlinetime/) to upgrade.\n\nWe're also wrapping up work on Neatline Time to make it compatible with [Omeka 2.0](http://omeka.org). There isn't a tagged version yet, but if you're interested in trying it out (and sending us feedback), feel free to [check out the `develop` branch](https://github.com/scholarslab/NeatlineTime/tree/develop/) from our Github repository. You can also [download a zip of the develop branch](https://github.com/scholarslab/NeatlineTime/archive/develop.zip); you'll just need to rename the unzipped folder to \"NeatlineTime\". If you do try it out, and run into any problems, feel free to add a ticket to our [issue tracker](http://github.com/scholarslab/NeatlineTime/issues). There's also plenty of time to [contribute a translation](http://www.scholarslab.org/research-and-development/translating-neatline/) for Neatline Time, or any of the Neatline plugins!\n"},{"id":"2013-05-28-announcing-neatline-2-0-alpha2","title":"Announcing Neatline 2.0-alpha2!","author":"david-mcclure","date":"2013-05-28 09:30:41 -0400","categories":["Geospatial and Temporal"],"url":"announcing-neatline-2-0-alpha2","content":"_[Cross-posted with [dclure.org](http://dclure.org/logs/announcing-neatline-2-0-alpha2/)]_\n\nWe're pleased to announce Neatline 2.0-alpha2, a second developer-preview version that gets us one step closer to a stable 2.0 release! For now, this is still just an testing release aimed at engineers and other folks who want to experiment with the new set of features (for more information, check out the [announcement for the first testing release](http://www.scholarslab.org/geospatial-and-temporal/announcing-neatline-2-0-alpha1/)). Grab the code here:\n\n**[Neatline-2.0-alpha2](http://www.scholarslab.org/wp-content/uploads/2013/05/Neatline-2.0-alpha2.zip) | [NeatlineWaypoints-2.0-alpha2](http://www.scholarslab.org/wp-content/uploads/2013/05/NeatlineWaypoints-2.0-alpha2.zip) | [NeatlineSimile-2.0-alpha2](http://www.scholarslab.org/wp-content/uploads/2013/05/NeatlineSimile-2.0-alpha2.zip)**\n\nThis revision fixes a couple of bugs and adds two new features that didn't make it into the first preview release:\n\n\n\n\n\n\n  1. \n**A user-privileges system**, which makes Neatline much easier to use in collaborative, multi-user settings like classrooms and workshops. In a lot of ways, this feature reflects an expanded focus for Neatline. During the first cycle of development last year, we were mainly focused on building a tool designed for individual scholars and students working on focused projects. In that setting - when just a handful of trusted collaborators are working on a project - it's often not necessary to assign \"ownership\" to individual pieces of content to protect them from being changed or deleted by other users.\n\nOver the course of the last year, though, we've realized that there's a lot of interest in using Neatline in a classroom setting, which introduces a new set of requirements. When 50 students are all building their own Neatline exhibits inside a single installation of Omeka, it would be easy for someone to accidentally edit or delete someone else's work - there need to be guard rails to prevent users from modifying content that doesn't belong to them.\n\nIn Neatline 2.0-alpha2, we've added an ACL (access control list) that makes it possible to enforce a three-level user privileges system:\n\n\n\n    * **Admin** and **Super** users can do everything - they can create, edit, and delete all Neatline exhibits and records, regardless of who they were originally created by.\n\n\n\n    * **Contributor** users can add, edit, and delete their own exhibits, but can't make changes to exhibits or records that they didn't create.\n\n\n\n    * **Researcher** users are denied all Neatline-related privileges - they can't create, edit, or delete any Neatline exhibits or records.\n\n\n\n\nThis is a simple approach, but we think it addresses most of the basic patterns for classroom use that we've encountered here at UVa and elsewhere. If students are working on individual projects, each can be given a separate \"Contributor\" account, which allows them to create and update their own exhibits, but blocks them from changing anyone else's work. If students are working together in groups, each group can be assigned an individual \"Contributor\" account, which allows group members to update each other's work, but prevents them from making changes other groups' exhibits.\n\n\n\n\n  2. \n**An exhibit-specific theming system** that makes it possible to create completely custom \"sub-themes\" for individual Neatline exhibits. Before, it was possible to customize the layout and styling of the Neatline exhibit views by editing the Omeka theme, which would change the appearance of _all_ the exhibits on the site. In many cases, though, individual exhibits have specific requirements. Depending on the content, it might be useful for different exhibits to have different page headers, typography, or viewport layouts; and it's also really useful to be able to load exhibit-specific JavaScript files, which can be used to define custom interactions for individual exhibits.\n\nIn this release, every aspect of an exhibit's public view can be completely customized by adding an \"exhibit theme\" that sits inside of the regular Omeka theme. For example, if I have an exhibit called \"Testing Exhibit\" with a URL slug of `testing-exhibit`, I can define a custom theme for the exhibit by adding a directory in the public theme at `neatline/exhibits/themes/testing-exhibit`. With the directory in place, Neatline will automatically load any combination of custom assets:\n\n\n\n    * If a `template.php` file is present in the directory, it will be used as the view template for the exhibit in place of the default `show.php` template that ships with Neatline.\n\n\n\n    * All `.js` and `.css` files in the directory will be loaded in the public view. This makes it possible to break additional styling and JavaScript functionality across multiple files, which makes it easier to break complex customizations into smaller units.\n\n\n\n\nThis gives the theme developer full control over the appearance and behavior of each individual exhibit, making it possible to build a extremely diverse collection of Neatline projects inside a single installation of Omeka.\n\n\n\n\n\n\nCheck out the [change log](https://github.com/scholarslab/Neatline/blob/develop/CHANGELOG.md#v20-alpha2-commits) for more details. And let us know what you think!\n"},{"id":"2013-05-31-mla14-roundtable-on-the-praxis-network-rethinking-humanities-education-together-and-in-public","title":"MLA14 Roundtable on the Praxis Network: Rethinking Humanities Education, Together and In Public","author":"katina-rogers","date":"2013-05-31 08:53:07 -0400","categories":["Announcements"],"url":"mla14-roundtable-on-the-praxis-network-rethinking-humanities-education-together-and-in-public","content":"_[Cross-posted from [my personal site](http://wp.me/p2CaGd-iB)]_\n\nI’m delighted to announce that our proposed roundtable on the Praxis Network has been accepted for the [2014 MLA Convention](http://www.mla.org/convention). Here are the details:\n\n**Session Proposal**\nHow can humanities programs better equip students for a wider range of careers, without sacrificing the core values or approaches of the disciplines? While not new, the question becomes more urgent as public funding for the humanities shrinks and the proportion of contingent faculty grows. Rather than see these pressures as threats, however, many programs see in them an opportunity to develop vibrant programs that take a broader view of possible methodological approaches, research products, and desirable career outcomes.\n\nThe participants in this proposed roundtable are all members of the [Praxis Network](http://praxis-network.org/), a new international partnership of graduate and undergraduate programs that are making effective interventions in the traditional models of humanities pedagogy and research. They represent programs that are embarking upon collaborative, interdisciplinary, project-based approaches to humanities education.\n\nThe Praxis Network features graduate programs at the University of Virginia, Michigan State University, CUNY Graduate Center, University College London, and Duke University, as well as undergraduate programs at Hope College and Brock University. By bringing together a collection of diverse programs that all aspire to similar goals of increasing the effectiveness of humanities educational practices and making their methodologies more widely applicable, we hope to spark ideas among institutions that are exploring similar initiatives. Each roundtable participant will give brief remarks to introduce their program, leaving substantial time for broader discussion and questions.\n\nThe partnership is one of three complementary projects in the [Scholarly Communication Institute](http://uvasci.org/current-work/graduate-education/)’s latest work on rethinking graduate education. A recent SCI study on the level of career preparation provided by graduate programs makes it clear that most graduates and their employers find that they do not gain many of the skills that are important in their professional environments—such as collaboration, project management, and communication with varied audiences—through their graduate programs.  The Praxis Network provides a closer look at select programs that have taken unusual and effective approaches to addressing some of the issues that the survey uncovered.\n\nBeyond preparing students for a broader range of careers, the Praxis Network programs also provide excellent models for the relevance of humanities scholarship in a changing public landscape. With federal and state funding for higher education facing tremendous pressure, making humanities scholarship meaningful to a much broader audience is critical. Fortunately, scholarly work is becoming increasingly available to a broader and less specialized public, whether through open-access journals, via blogs and personal websites, or as standalone digital projects. The programs in the Praxis Network address these two trends by encouraging students to develop public-facing projects that are accessible to non-specialists, without sacrificing disciplinary rigor. In fact, the students' research output shows that encouraging students to think critically about their intended audience helps them to better grasp not only what is appropriate for the general public, but also what matters to their academic peers.\n\nHumanities programs have the opportunity to better serve their students as well as the public by examining our core values and rethinking the methods we use to teach them. Increased public engagement is not only valuable to general audiences, but also healthy for academic disciplines and for individual graduates. Still, a great deal of work remains before humanities departments will commonly evaluate their success through outcomes other than tenure-track job placement. For wide-scale change to be possible, programs must find it valuable to equip students for varied careers in universities, libraries, cultural heritage organizations, non-profits, government offices, and more.\n\nThe programs in the Praxis Network show the tremendous potential of encouraging students to approach humanistic inquiry in new ways as the discipline moves toward embracing increased collaboration, meaningful public engagement, and an ethos of openness and exploration. Bringing together representatives of each program in a roundtable discussion will provide a fruitful opportunity for others in the humanities community to learn about the developments, to ask questions relevant to the goals and directions of their own institutions, and to spark new ideas for growth and change.\n\n**Participant Bios**\n**David F. Bell**, co-director of the Duke PhD Lab in Digital Knowledge, is Professor of French Studies at Duke University. His research focuses on nineteenth-century French literature and culture, including urban space and technologies of communication. He also works on the concept of tact as a discursive strategy, and on the notion of debt in French literature. He is the co-editor of SubStance.\n\n**Matthew K. Gold** directs the Digital Fellows program at the Graduate Center, City University of New York, and holds joint appointments there and at the New York City College of Technology. Gold is an Associate Professor of English at City Tech, while his roles at the Graduate Center include Director, CUNY Academic Commons; Advisor to the Provost for Master’s Programs and Digital Initiatives; and Acting Executive Officer, MA Program in Liberal Studies. Gold recently edited a collection of essays related to the digital humanities, titled Debates in Digital Humanities. Other projects include “Looking for Whitman”, a multi-campus experiment in digital pedagogy sponsored by two NEH Digital Humanities Start-Up Grants, Commons In A Box, funded by the Alfred P. Sloan Foundation, and JustPublics@365, funded by the Ford Foundation.\n\n**Kevin Kee** is Canada Research Chair of Digital Humanities and Associate Professor in the Department of History and the Centre for Digital Humanities at Brock University, where he also directs the Interactive Arts and Science program. Kee’s research focuses on the intersection of history, computing, education, and game studies, with a particular interest in the use of computing for innovative expressions of culture and history. Part of his innovative approach includes a strong emphasis on partnerships across the university and with the broader community; his development of the business incubator nGen enables a deep level of crossover between the two.\n\n**Cecilia Márquez** is a PhD student at the University of Virginia's Corcoran Department of History, and is one of the 2012-2013 Praxis Fellows. She became interested in the digital humanities through the South Atlantic Studies Fellowship for the Public Humanities, funded by the Virginia Foundation for the Humanities.  Márquez's recent work focuses on the experience of Latina/os in the American South during the Civil Rights Movement.  Her research interests include African American, Latina/o, and Gender and Sexuality Studies.\n\n**Kelli Massa** earned her MSc in the Digital Humanities Program at University College London. As a graduate student in the UCL DH program, Massa had the opportunity to take a variety of stimulating courses connecting the humanities with digital technologies. Entering the program with a humanities background (MA in Literature), she focused primarily on the computer science side of the program and pursued an MSc. Her internship with JISC (Joint Information Systems Committee) sparked an interest in academic software sustainability and repositories, which she plan to research for her dissertation.\n\n**William Pannapacker** is Professor of English at Hope College and directs the Mellon Scholars program. His research and teaching interests include American literature and culture and digital humanities. Pannapacker is also a columnist for the Chronicle of Higher Education and a contributor to The New York Times and Slate, where he writes about a range of issues related to higher education in the humanities.\n\n**Donnie Sackey** is a PhD candidate in the Writing, Rhetoric, and American Cultures department at Michigan State University, and a 2012-2013 Cultural Heritage Informatics Initiative Graduate Fellow. Through his dissertation research on invasive species, Sackey explores the relationship between rhetoric and the environment, as well as ways in which creative applications of information and computing technologies can help map environments and subsequently allow for alternative levels of engagement.\n\n**Katina Rogers** researches graduate education reform and career paths for humanities scholars in her capacity as Senior Research Specialist at the Scholarly Communication Institute. Rogers collaborated on the development of the Praxis Network website and conducted a study of career preparation in humanities graduate programs. She has given invited lectures on her work on graduate education reform and alternative academic career paths at New York University, Stanford University, and the University of Delaware, and her work has been written up in the Chronicle of Higher Education and Inside Higher Ed. She holds a PhD in Comparative Literature from the University of Colorado, Boulder. Effective September 2013, she will begin a new role as Managing Editor of [MLA Commons](http://commons.mla.org/).\n"},{"id":"2013-06-17-reflections-on-project-management-i","title":"Reflections on Project Management I","author":"claire-maiers","date":"2013-06-17 05:35:56 -0400","categories":["Grad Student Research"],"url":"reflections-on-project-management-i","content":"At our final official Praxis meeting, I shared an overview of my experience as project manager with the rest of the team, and I thought I would share some of those same reflections in a short series of blog posts.  This first post reflects on some of the more technical and organizational aspects of project management. \n\n  Early on, I read[ Sharon Leon’s piece about project management](http://mediacommons.futureofthebook.org/alt-ac/pieces/project-management-humanists), and these reflections follow much of her conversation.  As Leon discusses, the PM is responsible for the delegation of roles and tasks, the allocation of resources, establishing a work plan and a time line, and developing a method for tracking progress and the completion of tasks.  In addition, the PM needs to be able to articulate the vision and goals of the project, distinguishing between primary deliverables and those that are secondary.  \n\nRather than deciding on a set of goals and then assigning people to different roles, in the case of Praxis, these processes happened in tandem.  As each of us gravitated toward certain roles, it became more clear which deliverables would become prioritized and which would be secondary.  To help keep myself focused, I drafted a very general [project vision statement](https://github.com/scholarslab/praxis/wiki/Vision-statement) and, at Bethany’s suggestion, collected statements from the other Praxisers about their goals for the semester.  \n\nThis left me with the job of laying out the calendar and work plan for the semester.  As I blogged about earlier, this was a really daunting task for me given that I am still new to the DH world and did not have the skills to conceptualize how all the pieces fit together.  From that process, however, I have learned that PMs do not need to know every detail of how the project will go from the outset.  Rather than feel uncomfortable with my lack of knowledge, I started to see how I was surrounded by a team of knowledgeable consultants who could assist me in figuring out the relevant information needed to coordinate different aspects of our project. \n\nWhen it came to the calendar, I basically just worked backward.  I started by establishing our launch date.  Then, in conversation with Wayne and Bethany, I worked to figure out the approximate dates when each task needed completion.  Leon and others recommend that PMs work with team members to establish deadlines and goals rather than just dictating them.  While this is a good idea (and something which I plan to try again in my next PM gig), it didn’t work so well for Praxis---mostly due to the fact that all of us were novices and not really sure how much time various tasks required.  I did, however, collect information from everyone with regard to their commitments over the semester.  I wanted to know when people would be leaving for the summer, when they would be out of town for a significant period, and if there were other commitments that might take time away from Praxis.  That information helped me to set up the calendar and to know when people were most and least available throughout the last few months. \n\nAlthough we did not have to worry about financial resources or the allocation of equipment or software, I want to say something briefly about time and space.  One of our most important resources has been the graduate lounge.  Often our best work happened when we were in that shared spaced for a good chunk of time.  Although the design team had regular weekly meetings, the rest of us were rarely in the lounge collectively.  Rather, we came in one by one when our schedules allowed.  As the semester quieted down, we found more time to be in the lounge collectively.   I am happily in awe of the amount of work and progress that we accomplished in the last three weeks of Praxis.  Had I known how valuable that shared time and space would be, I would have done more to institute habits of collective work meetings early\n\nTo keep track of progress, we used the issues tracker in github.  Deciding what counted as a task or a milestone was also difficult initially.  Personally, I found that it helps to have the project broken down into small enough parts that you can actually track progress from week to week.  If tasks are too large, there is little to check off of the “to do” list until the project approaches completion.  In the end, each of our core deliverables became a milestone to which a list of issues (or tasks) were attached.   I will have more to say about the articulation of tasks in my next post.\n\n"},{"id":"2013-06-19-reflections-on-project-management-ii-know-your-team","title":"Reflections on Project Management II: Know your Team","author":"claire-maiers","date":"2013-06-19 06:28:45 -0400","categories":["Grad Student Research"],"url":"reflections-on-project-management-ii-know-your-team","content":"In my last post, I talked through some of my experiences with the organizational aspects of project management.  Here, I want to talk about one of the aspects of project management which can be more difficult to articulate and to learn.  While each project needs a timeline, deadlines, and clear goals, a project manager does more than keep track of the organizational aspects of the project.   The way I see it now, a successful project manager is able to facilitate the coordination of her team in a way that maximizes the team’s potential.\n\nThis involves both the ability to be a liaison between various aspects of the project and team members and the ability to learn how each team member works and which kind of guidance (or opinion) is either useful or disruptive.  For example, I found that some Praxisers preferred very specific guidance on tasks, while others preferred just general instruction and to be left (more or less) to their own devices.  In this way, knowing your team members and how they work is an important piece of the puzzle when it comes to identifying milestones and tasks to track.\n\nI have to say that I only figured out this aspect of project management through trial and error.  Most weeks I would send out an email that recapped our Monday meetings and outlined goals for the week.  Sometimes those emails worked to facilitate the work plan for the week, and sometimes they did not.   It was only over time that I began to realize that some of my teammates were not getting the kind of guidance that they expected from a PM and that I was able to start adapting my strategy for thinking about tasks and goals.\n\nHaving said that, I am looking forward to an opportunity to work on multiple projects with the same team.  It is only through time spent together and learning experiences brought on by both failures and successes that you can really get to know your team members.  While it may be true that anyone with decent organizational skills can set up a calendar and track tasks, I think the best project managers know and understand how a team works and can use that knowledge to shape their own approach to leadership.\n\n"},{"id":"2013-07-02-reflections-on-project-management-iii-it-is-all-about-communication","title":"Reflections on Project Management III: It Is All About Communication ","author":"claire-maiers","date":"2013-07-02 08:09:56 -0400","categories":["Grad Student Research"],"url":"reflections-on-project-management-iii-it-is-all-about-communication","content":"I have saved the discussion of my biggest struggle with Project Management for my final post in this miniseries.  Much to my surprise, communication with the rest of the team was one of the more difficult aspects of this semester.  This caught me completely off guard because I actually think that I have pretty good communication skills: I don’t shy away from talking about topics just because they are uncomfortable, I write clearly, and I make an effort to be sure I am hearing and understanding others. And yet, I found communication challenging.\n\nAfter a lot of reflection, there are at least three related issues that contributed to my communication woes.  First, I assumed that others would react to the communication of various sentiments as I would react.  If someone asks me to “do something if I have time,” I just about always get it done.  This is due to my own inability to say “no,” even when I should.  I eventually realized that my team members were not all as overly socialized and prone to guilt as myself.  When I included statements such as “if you have time,” they took me literally (as they should).  If I really needed something done, I had to learn to drop the conditional statement from my emails.   It wasn’t so much that my statements were not clear, but that people did not react to certain statements as I expected.\n\nHowever, this first point is not entirely removed from two other contributions to my communication troubles: insecurity about my own knowledge in DH and a worry over seeming bossy.  I discussed this insecurity about my knowledge and my role in telling my teammates what to do in two earlier posts ([here ](http://www.scholarslab.org/grad-student-research/reflections-on-project-management-i/)and [here](http://www.scholarslab.org/grad-student-research/the-blind-leading-the-blind-a-noob-and-program-management/)).  How could I---with very little DH knowledge---really have the authority to tell other when to do what?  As the semester wore on, this got easier.\n\nThe worry about bossiness has been persistent however.  Worried that statements would be taken the wrong way, I often inserted wishy-washy language and conditional statements into my emails.  As I prepared to talk with the Praxis team about my experience as PM, it occurred to me that this might be a gendered worry.  When I brought this issue up at our final meeting, I was surprised by the amount of conversation that it generated.  I worried that, because I am a woman, certain statements and requests would be seen as me over stepping my grounds or being temperamental and demanding (I have a particular female adjective in mind that is not appropriate for public space, but I hope you get the drift).\n\nAs became clear through our group conversation, others were not prone to actually reading clear and to-the-point statements with this bossy tone simply because of my sex.  But, the fact that I worried so much about it was a problem.  I’ve come to see that the worry itself is actually a gendered aspect. Because of gendered stereotypes of female leaders and bosses, I constantly worried over the perception of my words and decisions.  Men, it seems, might be less prone to this particular type of over analyzing.  As Wayne pointed out, men also worry about striking the right tone and being considerate to their colleagues, but women (or at least this woman), worried a lot about invoking the particular feminine adjective mentioned above in a way that would undermine my own legitimacy and credibility with my teammates.\n\nDue to my concern for seeming too bossy, I actually undermined my abilities all on my own by generating vague and imprecise communication with my team.  I did eventually get better.  As crunch time neared, I learned to just articulate explicitly what needed doing without all the wishy-washy stuff.  I am thankful to Gwen for pointing out during our group conversation both that she was relieved and that our progress improved once I was able to just drop all these concerns and do my job by telling the team what to do.\n\nI cannot thank the SLab staff enough for the opportunity to be a part of the Praxis team this year and for their encouraging us to reflect about the experience as we went along.  The opportunity to blog and converse with the wider DH community has been just as valuable as the experience of working on Prism.  I hope that some of you out there will comment on this miniseries of PM posts.  Although my time with Praxis is wrapping up, I am looking forward to continued dialogue about DH and project management!  Thank you SLab staff and fellow Praxisers---it has been great!\n\n"},{"id":"2013-07-03-are-you-the-new-head-of-scholars-lab","title":"Are you the new Head of Scholars’ Lab?","author":"katina-rogers","date":"2013-07-03 10:42:31 -0400","categories":["Announcements"],"url":"are-you-the-new-head-of-scholars-lab","content":"We are thrilled to announce an exciting job opportunity on the leadership team here in the [Department of Digital Research & Scholarship](http://scholarslab.org). Read on for more details!\n\n**Head of Scholars’ Lab**\nAre you an experienced digital humanities scholar-practitioner with a strong background in project management and public service? The [University of Virginia Library](http://library.virginia.edu) seeks an energetic, adaptable leader for the digital consultation services and intellectual programming of our internationally-respected Scholars’ Lab. The ideal candidate is detail-oriented, eager to work collaboratively with diverse faculty and staff, and able to muster and effectively communicate UVa Library’s deep resources for digital scholarship. This supervisory position is responsible for day-to-day operations in the Scholars’ Lab and, together with the heads of Graduate Programs and Research & Development, completes the leadership team reporting to the [director](http://nowviskie.org/) of Digital Research and Scholarship for UVa Library.\n\nThe Head of the Scholars’ Lab should have a strong service ethic, broad technical knowledge, and ability to collaborate as a true partner with faculty and graduate students, enabling next-generation digital scholarship in a library lab setting. He or she should also be able to take good advantage of the \"20% time\" afforded to all in the Department of Digital Research & Scholarship to pursue professional development and their own (often collaborative) research projects related to the mission of the Scholars’ Lab. This is a full-time, permanent managerial and professional staff position at UVa.\n\n**Primary Responsibilities: **\n_Community-building and Leadership_: strategic leadership of digital humanities services, oversight of day-to-day operations, design of intellectual programming and instruction, and collaboration with local and national/international peers; _Project-based Collaborations_: coordinates development of high-level goals, intake processes, workplans, and MoUs for digital project collaborations, in consultation with departmental, Library, and University partners; _Research and Professional Development_: pursuit of own scholarly R&D agenda related to the humanities or social sciences with publication of results and/or presentation at appropriate venues.\n\n**Specialized Knowledge and Skills: **\nStrong knowledge of digital humanities history, technologies, and intellectual directions. Strong public service orientation and interest in guiding scholarly projects from conceptualization to implementation and audience-building. Excellent communication skills, including the ability to present complex technical information to non-specialists and a clear understanding of the perspectives and needs of scholars. Previous experience in public service in an academic library setting preferred.\n\n**Education: **\nGraduate study (PhD preferred) in a field related to humanities scholarship or humanistic aspects of social or information science.\n\n**Experience: **\n5+ years’ experience with project management and/or hands-on development of digital projects related to digital humanities or cultural heritage. Supervisory experience preferred.\n\n**Salary and Benefits:**\nSalary is commensurate with experience, and expected to range between approximately $60 and $85k per annum. Excellent benefits, TIAA/CREF and other retirement plans along with generous funding for travel and professional development.\n\nTo Apply:\nReview of applications to begin immediately and continue until position is filled. Apply through the University of Virginia [online employment website](http://jobs.virginia.edu/applicants/Central?quickFind=70941). (If you need to search the [Jobs@UVa](http://jobs.virginia.edu) portal, the posting number is 0612479.) Complete application, and attach cover letter and CV, with contact information for three current, professional references.\n\nThe Scholars’ Lab is also seeking a Head of Graduate Programs. Learn more [here](http://www.scholarslab.org/announcements/head-graduate-programs/). Don’t miss a chance to work with our wonderful students and incredible Scholars’ Lab team!\n"},{"id":"2013-07-09-neatline-2-0","title":"Announcing Neatline 2.0.0! A stable, production-ready release","author":"david-mcclure","date":"2013-07-09 10:59:18 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"neatline-2-0","content":"_[Cross-posted from [dclure.org](http://dclure.org/?p=2577)]_\n\n[![neatline-2](http://dclure.org/wp-content/uploads/2013/06/neatline-21-1024x293.png)](http://dclure.org/wp-content/uploads/2013/06/neatline-21.png)\n\nIt's finished! Today we're excited to announce [Neatline 2.0.0](http://omeka.org/add-ons/plugins/neatline/), a stable, production-ready release of the new codebase that can be used to upgrade existing installations. If you’re starting fresh with a new project, just download the new version and install it like any other Omeka plugin. If you're upgrading from Neatline 1.x, be sure to read through the [2.0 migration guide](http://docs.neatline.org/upgrading-to-v2.html) before getting started (**most important - the 2.0 migration runs as a \"background process,\" which means that there could be a 10-20 second lag before your old exhibits are visible under the \"Neatline\" tab**). Then, if you want to use the SIMILE Timeline widget and item-browser panel that were built into the first version of Neatline, download [NeatlineSimile](http://omeka.org/add-ons/plugins/neatlinesimile/) and [NeatlineWaypoints](http://omeka.org/add-ons/plugins/neatlinewaypoints/), the two new sub-plugins that integrate those features seamlessly into the Neatline core. For more information, check out the (all new!) [documentation](https://github.com/scholarslab/Neatline/wiki), which walks through the installation process in detail.\n\nDownload the plugins: **[Neatline](http://omeka.org/add-ons/plugins/neatline/)** | **[NeatlineWaypoints](http://omeka.org/add-ons/plugins/neatlinewaypoints/)** | **[NeatlineSimile](http://omeka.org/add-ons/plugins/neatlinesimile/)**\n\nNeatline 2.0 is a major update that significantly expands the scope of the project. Building on the core set of geospatial annotation tools from the first version, we've turned Neatline into a general-purpose visual annotation framework that can be used to create interactive displays of almost any type of two-dimensional material - maps, paintings, drawings, photographs, documents, and anything else that can be captured as an image. We've also made a series of changes to the user interface and code architecture that are designed to make Neatline more accessible for new users (college undergraduates working on class assignments) and, at the same time, more flexible for advanced users (professional scholars, journalists, and digital artists who want to use Neatline  for complex projects).\n\nSome of the highlights:\n\n\n\n\n\n\n  * **[Improved performance and scalability](http://dclure.org/logs/neatline-one-million-records/)**, powered by a real-time spatial querying system that makes it possible to work with really large collections of records - as many as about 1,000,000 in a single exhibit;\n\n\n\n\n  * **[A more sophisticated set of drawing tools](http://dclure.org/logs/neatline-drawing-svg-on-maps/)**, including the ability to  import high-fidelity SVG documents created in programs like Adobe Illustrator or Inkscape and interactively drag them into position as geometry in Neatline exhibits;\n\n\n\n\n  * **[An interactive, CSS-like stylesheet system](http://dclure.org/logs/interactive-css-in-neatline-2-0/)**, build directly into the editing environment, that makes it possible to quickly perform bulk updates on large collections of records using a simplified dialect of CSS;\n\n\n\n\n  * **[A flexible user-permissions system](http://dclure.org/logs/announcing-neatline-2-0-alpha2/)**, designed to make it easier to use Neatline for class assignments and workshops, that makes it possible to prevent users from modifying or deleting content they didn't create;\n\n\n\n\n  * **Expanded support for non-spatial base layers** that makes it possible to build exhibits on top of any web-accessible static image or non-spatial WMS layer - paintings, drawings, photographs, documents, etc.\n\n\n\n\n  * **A more powerful theming system**, which makes it possible for designers to completely customize the appearance and interactivity of each individual Neatline exhibit. This makes it possible to host completely independent and thematically-distinct projects inside a single installation of Omeka.\n\n\n\n\n  * **A total rewrite of the front-end JavaScript applications** (both the editing environment and the public-facing exhibit views) that provides a more minimalistic and responsive environment for creating and viewing exhibits;\n\n\n\n\n  * **A new programming API and \"sub-plugin\" system** that makes it possible for developers to add custom functionality for specific projects - everything from simple user-interface widgets (sliders, legends, scrollers, forward-and-backward buttons, etc.) up to really extensive modifications that expand the core data model and add totally new interactions.\n\n\n\n\n\nAnd much more! Over the course of the next week, leading up to our panel about Neatline at the [DH 2013](http://dh2013.unl.edu/) conference in Lincoln, Nebraska (\"Circular Development: Neatline and the User/Developer Feedback Loop,\" Wednesday at 10:30), we're going to be fleshing out the new documentation and building a set of Neatline-2.0-powered projects designed to put the new feature set through its paces.\n\nAlso, watch this space later in the week for another code release - we've built an extension called NeatlineTexts that connects Neatline exhibits with word-level annotations in long-format documents, which makes it possible to use Neatline as a publication platform for essays, blog posts, scholarly articles, monographs, etc., and built a special Omeka that's specifically designed to frame these interactive editions.\n\nUntil then - grab the new code, give it spin, and let us know what you think!\n"},{"id":"2013-07-10-summer-of-travel-dh-edition","title":"Summer of travel, DH edition","author":"katina-rogers","date":"2013-07-10 07:32:10 -0400","categories":["Digital Humanities"],"url":"summer-of-travel-dh-edition","content":"_[Cross-posted from [my personal website](http://wp.me/p2CaGd-iZ)]_\n\nThis spring and summer has been the busiest travel season I have ever had. While I won’t deny that I’m happy to be rounding the corner on my last two trips this summer, I’ve learned a tremendous amount as I’ve bounced from city to city, and feel lucky to have had so many outstanding opportunities.\n\nA good portion of the travel has been to share findings from the survey work I did on career preparation among humanities scholars. After doing so much work on the topic, it was incredibly exciting and rewarding to talk with various groups about the results, and to see what questions and ideas arose from my remarks. I’ve [posted a version](http://katinarogers.com/2013/04/23/humanities-unbound-careers-scholarship-beyond-the-tenure-track/) of these talks, so I won’t go into more detail here. Aside from the occasional conference presentation, public speaking is pretty new for me, and it’s been a few years since I’ve even been in front of a classroom. I found myself most comfortable only after extensive preparation: while I admire those who can speak extemporaneously, I learned that I need to craft my talks carefully at this stage. I think that I’ve become a more effective communicator even in the last few months, and I hope that means that the results from SCI’s study will be more likely to reach the right audiences in ways that encourage discussion and action.\n\nAnother unusual trip for me was to attend the [Digital Humanities Summer Institutes](http://dhsi.org/). Those of you who have been to DHSI know the unique atmosphere that I encountered: part conference, part classroom, part summercamp for grown-ups. The welcoming environment that Ray Siemens and his team create suffuses the week, and the backdrop of beautiful Victoria, BC is intoxicating. (I rented a bike my last day there so that I could explore the shoreline, which I highly recommend!) One thing that strikes me as unique about DHSI is the way that it brings together people with similar values but very different backgrounds and goals. I took a course in visual design, and while it’s not a topic that’s of unique interest to digital humanists, the fact that we were all approaching it with goals related to DH meant that we had some strong overlap in terms of audience, anticipated work products, questions to consider, and more. Unlike a conference, you spend the bulk of your time at DHSI with a single group of people—your classmates—and while that’s wonderful in terms of fostering deeper relationships with a small group, I also found it important to make sure to spend time with people that I wasn’t seeing each day. Our conversations tended to be rich and varied, and brought fresh perspectives to the new skills we were learning.\n\nAnd now, the two remaining trips are a small SCI meeting to close out our work on [rethinking humanities graduate education](http://uvasci.org/current-work/graduate-education/), and my very first [DH conference](http://dh2013.unl.edu/). The abstract of the paper I’ll be giving is available [here](http://dh2013.unl.edu/abstracts/ab-145.html). I’m nervous about DH—it’s my first time attending, and I feel like something of an interloper—but also incredibly excited. My talk again report on the survey findings, but from a somewhat different angle, and I expect that the DH community will have some very productive questions, critiques, and feedback to offer. The program is packed with an impressive variety of talks, many of which are outside my areas of expertise, and I’m looking forward to learning a lot and seeing firsthand the kinds of work that the community has been developing.\n\nIn all, the past few months have been a bit of a whirlwind (though I’m pleased to say that I’ve become a pro at packing, unpacking, and grabbing the right number of bins in the airport security line). All this travel has really helped to broaden my perspective, giving me a window into both the day-to-day and long-term questions, goals, and challenges that face individuals in different contexts within higher education. It has helped get me out of my head (a problem sometimes, especially since I’m working from home) and talking with grad students, faculty, and administrators, which has in turn grounded my own perspective. And it has helped me to more accurately assess the value and limitations of my own work. \n"},{"id":"2013-07-18-homers-catalogue-of-ships","title":"Homer's Catalogue of Ships","author":"gwen-nally","date":"2013-07-18 11:53:23 -0400","categories":["Announcements","Digital Humanities","Experimental Humanities","Geospatial and Temporal","Grad Student Research"],"url":"homers-catalogue-of-ships","content":"Ben Jasnow and Courtney Evans (UVA Classics Graduate Students) just presented their findings at DH2013. They hypothesize that their spatial-linguistic analysis of the catalogue of ships could aid in the discovery of ancient sites. Check out their presentation here:  [Mapping the Catalogue of Ships](http://www.scholarslab.org/wp-content/uploads/2013/07/Mapping-the-Catalogue-of-Ships.pdf)\n\nHere's a preview:\n\n[![Mapping the Catalogue of Ships](http://www.scholarslab.org/wp-content/uploads/2013/07/Mapping-the-Catalogue-of-Ships.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/07/Mapping-the-Catalogue-of-Ships.jpg)\n\n[![Mapping the Catalogue of Ships2](http://www.scholarslab.org/wp-content/uploads/2013/07/Mapping-the-Catalogue-of-Ships2.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/07/Mapping-the-Catalogue-of-Ships2.jpg)\n\n[![Mapping the Catalogue of Ships3](http://www.scholarslab.org/wp-content/uploads/2013/07/Mapping-the-Catalogue-of-Ships3.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/07/Mapping-the-Catalogue-of-Ships3.jpg)\n\n[![Mapping the Catalogue of Ships4](http://www.scholarslab.org/wp-content/uploads/2013/07/Mapping-the-Catalogue-of-Ships4.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/07/Mapping-the-Catalogue-of-Ships4.jpg)\n\n[![Mapping the Catalogue of Ships5](http://www.scholarslab.org/wp-content/uploads/2013/07/Mapping-the-Catalogue-of-Ships5.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/07/Mapping-the-Catalogue-of-Ships5.jpg)\n\n[![Mapping the Catalogue of Ships6](http://www.scholarslab.org/wp-content/uploads/2013/07/Mapping-the-Catalogue-of-Ships6.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/07/Mapping-the-Catalogue-of-Ships6.jpg)\n"},{"id":"2013-07-29-scholars-lab-speaker-series-james-smithies","title":"Scholars' Lab Speaker Series: James Smithies","author":"ronda-grizzle","date":"2013-07-29 07:19:10 -0400","categories":["Podcasts"],"url":"scholars-lab-speaker-series-james-smithies","content":"**Speaker Series Brown Bag: James Smithies**\n**The UC CEISMIC Digital Archive: Co-ordinating Libraries, Museums, Archives, Individuals and Government Agencies in a Disaster Management Context**\n\nOn July 22, Dr. James Smithies, Senior Lecturer in Digital Humanities at the University of Canterbury in Christchurch, New Zealand, spoke in the Scholars' Lab about his work designing and developing the CEISMIC Digital Archive.\n\nSummary:\nThe Canterbury region in the South Island of New Zealand has experienced over 11,000 earthquakes since September 2010, including a devastating magnitude 6.3 quake on February 22nd 2011 that resulted in the loss of 185 lives. Only months after the February quake, while university staff were teaching from tents in the approach to winter, a fledgling digital humanities programme was established that had as its first goal the development of a national federated digital archive to preserve the vast quantities of content being produced as a result of the earthquakes. Paul Millar and James Smithies drew together a Consortium of 10 local and national agencies representing New Zealand's libraries, museums, archives and cultural organisations in an effort to ensure a co-ordinated response.They then led technical development of a national federated archive, [ceismic.org.nz](http://ceismic.org.nz/), and a bespoke research archive, [quakestudies.canterbury.ac.nz](http://quakestudies.canterbury.ac.nz/). The CEISMIC archive has recently completed Phase 1 of its technical development, and includes over 20,000 items. Consortium member organisations, local government agencies, and commercial companies provide content next to community groups and individuals. Projections indicate that the archive will hold 100,000 items by the end of 2013. The intention is to remain operational for the 10-15 years it is expected to take to rebuild the region. This talk will describe the current state of the archive, and explain how Millar and Smithies used methods inspired by the digital humanities community to achieve their goals.\n\n[www.ceismic.org.nz](http://www.ceismic.org.nz/)\n\nYou can find Dr. Smithies online on his website ([jamessmithies.org](http://jamessmithies.org/)) and on twitter ([@jamessmithies](https://twitter.com/jamessmithies)).\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.25360114143/enclosure.mp3\"]\n"},{"id":"2013-08-07-announcing-neatline-2-0-2","title":"Announcing Neatline 2.0.2!","author":"david-mcclure","date":"2013-08-07 09:38:14 -0400","categories":["Geospatial and Temporal"],"url":"announcing-neatline-2-0-2","content":"Today we're pleased to announce the release of [**Neatline 2.0.2**](http://omeka.org/add-ons/plugins/Neatline/)! This is a maintenance release that adds a couple of minor features and fixes some bugs we've rooted up in the last few weeks:\n\n\n\n\n\n\n  * Fixes a bug that was causing item-import queries to fail when certain combinations of other plugins were installed alongside Neatline (thanks Jenifer Bartle and Trip Kirkpatrick for bringing this to our attention).\n\n\n\n\n  * Makes it possible to toggle the real-time spatial querying on and off for each individual exhibit. This can be useful if you have a small exhibit (eg, 10-20 records) that can be loaded into the browser all at once without causing performance problems, and you want to avoid the added load on the server incurred by the dynamic querying.\n\n\n\n\n  * Fixes some performance issues with the OpenStreetMap layer in Chrome.\n\n\n\n\n\nAnd more! Check out the [release notes](https://github.com/scholarslab/Neatline/releases/tag/2.0.2) for the full list of changes, and grab the new code from the [Omeka add-ons repository](http://omeka.org/add-ons/plugins/Neatline/). Also, watch this space for a couple of other Neatline-related releases in the coming weeks. Jeremy and I are working on a series of themes for Omeka specifically designed to display Neatline projects, including the NeatLight theme, which is currently used on the [Neatline Labs](http://neatline.dclure.org) site I've started playing around with (still a work in progress). We're also just about ready to cut off a public release of the NeatlineText plugin, which makes it possible to connect records in Neatline exhibits to individual sections, paragraphs, sentences, and words in text documents (check out [this](http://neatline.dclure.org/neatline/show/saturn-v-stage-2) example).\n\nUntil then, give the new code a spin, and let us know what you think!\n"},{"id":"2013-08-08-speaking-in-code","title":"Speaking in Code","author":"bethany-nowviskie","date":"2013-08-08 07:02:33 -0400","categories":["Announcements","Digital Humanities"],"url":"speaking-in-code","content":"We're pleased to announce that applications are open for a 2-day, [NEH-funded](http://neh.gov) symposium and summit to be held at the Scholars' Lab this November 4th and 5th.\n\n[![Speaking in Code](http://www.scholarslab.org/wp-content/uploads/2013/08/Screen-shot-2013-08-08-at-10.35.09-AM.png)](http://codespeak.scholarslab.org/)\n\n\"[Speaking in Code](http://codespeak.scholarslab.org/)\" will bring together a small cohort of accomplished digital humanities software developers. Together, we will give voice to what is almost always tacitly expressed in DH development work: expert knowledge about the intellectual and interpretive dimensions of code-craft, and unspoken understandings about the relation of our labor and its products to ethics, scholarly method, and humanities theory.\n\nOver the course of two days, participants will:\n\n\n\n\t\n  * reflect on and express, from developers' own points of view, what is particular to the humanities and of scholarly significance in DH software development products and practices;\n\n\t\n  * and collaboratively devise an action-oriented agenda to bridge the gaps in critical vocabulary and discourse norms that can frequently distance creators of humanities platforms or tools from the scholars who use and critique them.\n\n\nIn addition to [Scholars' Lab](https://www.scholarslab.org/people/) developers and project managers, facilitators include [Steve Ramsay](http://stephenramsay.us/), [Bill Turkel](http://williamjturkel.net/), [Stéfan Sinclair](http://stefansinclair.name/), [Hugh Cayless](http://philomousos.blogspot.com/), and [Tim Sherratt](http://discontents.com.au/).  The SLab particularly encourages and will prioritize participation of developers who are women, people of color, LGBTQ, or from other under-represented groups. (See \"[You Are Welcome Here](http://codespeak.scholarslab.org/#inclusivity)\" for more info.)\n\nThis will be the first focused meeting to address the implications of tacit knowledge exchange in digital humanities software development. Check out the Speaking in Code website [to apply](http://codespeak.scholarslab.org/)! Deadline September 12th.\n"},{"id":"2013-08-12-alt-ac-report-and-data","title":"Now available: Report and data from SCI’s survey on career prep and graduate education","author":"katina-rogers","date":"2013-08-12 06:05:25 -0400","categories":["Research and Development"],"url":"alt-ac-report-and-data","content":"_[Cross-posted at my [personal website](http://wp.me/p2CaGd-jz)]_\n\nI am delighted to announce the release of a report, executive summary, data, and slides from the [Scholarly Communication Institute](http://uvasci.org)’s recent study investigating perceptions of career preparation provided by humanities graduate programs. The study focused on people with advanced degrees in the humanities who have pursued [alternative academic](http://mediacommons.futureofthebook.org/alt-ac/) careers. Everything is CC-BY, so please read, remix, and share. I’d especially welcome additional analysis on the datasets.\n\nAll of the materials are openly accessible through the University of Virginia’s institutional repository:\n\n\n\n\t\n  * Report, executive summary, and slides (one package): [http://libra.virginia.edu/catalog/libra-oa:3480](http://libra.virginia.edu/catalog/libra-oa:3480)\n\n\t\n  * Or, download the PDFs separately here for easier sharing: [report](http://katinarogers.com/wp-content/uploads/2013/08/Rogers_SCI_Survey_Report_09AUG13.pdf), [executive summary](http://katinarogers.com/wp-content/uploads/2013/08/Rogers_SCI_Survey_Executive-summary_09AUG13.pdf), [slides](http://katinarogers.com/wp-content/uploads/2013/08/Rogers_SCI_HumanitiesUnbound_charts-and-recs_09AUG13.pdf)\n\n\t\n  * Data from the main survey: [http://libra.virginia.edu/catalog/libra-oa:3272](http://libra.virginia.edu/catalog/libra-oa:3272)\n\n\t\n  * Data from the employer survey: [http://libra.virginia.edu/catalog/libra-oa:3500](http://libra.virginia.edu/catalog/libra-oa:3500)\n\n\n\n(Note that the files available for download are listed in the top left-hand corner of each Libra listing.)\n\nHaving worked on this for over a year, I’m more convinced than ever about the importance of incorporating public engagement and collaboration into humanities doctoral education—not only to help equip emerging scholars for a variety of career outcomes, but also to maintain a healthy, vibrant, and rigorous field. It has been fascinating to connect with scholars working in such a diverse range of stimulating careers, and to see some of the patterns in their experiences.\n\nMany, many thanks to everyone who has contributed time and energy to this project—from completing the survey, to reading (or listening to) the preliminary reports, to providing feedback and critique. \n"},{"id":"2013-08-12-displaying-recent-neatline-exhibits-on-your-omeka-home-page","title":"Displaying Recent Neatline Exhibits on your Omeka Home Page","author":"jeremy-boggs","date":"2013-08-11 21:00:56 -0400","categories":["Research and Development"],"url":"displaying-recent-neatline-exhibits-on-your-omeka-home-page","content":"The charismatic [Alex Gil](http://www.elotroalex.com/) submitted a feature request to Neatline asking to be able to browse [Neatline exhibits on your Omeka home page](https://github.com/scholarslab/Neatline/issues/211). Turns out you can already [specify which page you want as your home page in Omeka 2.0](http://omeka.org/codex/Managing_Navigation_2.0#Choose_a_Homepage), so that helped with Alex's original query. But as we discussed the issue, Alex also wondered about putting a list of recent Neatline exhibits on the home page, much the same way Omeka already does with recent items. While we're not sure yet about putting this kind of thing in the plugin itself, I mentioned that it's fairly easy to do in your own them using one of Omeka's hooks, and promised him a blog post explaining more. Here's me making good on that promise.\n\nIn case you didn't know, Omeka has plenty of ways for developers to add new content to an Omeka site or filter existing content using hooks and filters, respectively. To use them, you first need to write a function that adds or changes content to your preference, then you pass that function to the relevant hook or filter in Omeka. Some dummy code to illustrate:\n\n[sourcecode language=\"php\"]\n<?php\n\nfunction my_custom_function() {\n  echo 'Hello world!';\n}\n\nadd_plugin_hook('hook_name', 'my_custom_function');\n\n[/sourcecode]\n\nYou could put this kind of code anywhere that Omeka could run it, particularly a new plugin or your activated theme's `custom.php` file. (An Omeka theme's `custom.php` file is a great place to put some custom code for your Omeka site, without having to go to the trouble of creating and activating a plugin.)\n\nIn our case, we want to append some new content to the home page of an Omeka site, so we'll need to find a hook to let us do that. Fortunately, we have one available—`public_home`—so let's use that to display some recent Neatline exhibits.\n\n(Keep in mind that the following code should work in Omeka 2.0 and Neatline 2.0; you can take a similar approach for earlier versions of each, but some of the functions would be different.)\n\nFirst, we'll need to create a `custom.php` file in your current active theme, if one doesn't already exist. (If it does exist, we'll use that one.) Make sure the file is in the root of your theme: `omeka/themes/your-theme/custom.php`.\n\nNext we'll need to write a function that gets a certain number of Neatline exhibits and lists them out, and put that in our `custom.php` file. We'll name our function `display_recent_neatline_exhibits`, and put all our goodies in there. Let's create the function:\n\n[sourcecode language=\"php\"]\n<?php\n\nfunction display_recent_neatline_exhibits() {\n\n}\n\n[/sourcecode]\n\nAfter we've created the function, we'll go ahead and pass that function to the `public_home` hook:\n\n[sourcecode language=\"php\"]\n<?php\n\nfunction display_recent_neatline_exhibits() {\n\n}\n\nadd_plugin_hook('public_home', 'display_recent_neatline_exhibits');\n[/sourcecode]\n\nWe still shouldn't see any changes on the home page, since our function isn't actually doing anything. But you shouldn't get any errors on the page either. If you do, make sure you have every curly brace and semicolon and all the other characters right; PHP is quite dramatic about syntax errors.\n\nNow lets add some stuff to our function to get some recent Neatline exhibits. First, let's define a variable `$html` and set that equal to an empty string. In the end, we'll echo the value of `$html`, so we want it equal to at least something, in case you actually don't have any Neatline exhibits to display.\n\n[sourcecode language=\"php\"]\n<?php\n\nfunction display_recent_neatline_exhibits() {\n    $html = '';\n\n    echo $html;\n}\n\nadd_plugin_hook('public_home', 'display_recent_neatline_exhibits');\n[/sourcecode]\n\nNext we'll create a variable, `$neatlineExhibits`, and assign it to the results of a query using Omeka's `get_records` function. The [`get_records` function](http://omeka.readthedocs.org/en/latest/Reference/libraries/globals/get_records.html) takes three arguments: the type of record, an array of query parameters, and number to limit results. We'll query for 'NeatlineExhibit' record type, make sure that the `recent` parameter is `true`, and limit our results to five:\n\n[sourcecode language=\"php\"]\n<?php\n\nfunction display_recent_neatline_exhibits() {\n    $html = '';\n\n    // Get our recent Neatline exhibits, limited to five.\n    $neatlineExhibits = get_records('NeatlineExhibit', array('recent' =&gt; true), 5);\n\n    echo $html;\n}\n\nadd_plugin_hook('public_home', 'display_recent_neatline_exhibits');\n[/sourcecode]\n\nNow we'll set the results in `$neatlineExhibits` for a record loop, and check to see if in fact we have exhibits to display in a PHP `if` statement:\n\n[sourcecode language=\"php\"]\n<?php\n\nfunction display_recent_neatline_exhibits() {\n    $html = '';\n\n    // Get our recent Neatline exhibits, limited to five.\n    $neatlineExhibits = get_records('NeatlineExhibit', array('recent' =&gt; true), 5);\n\n    // Set them for the loop.\n    set_loop_records('NeatlineExhibit', $neatlineExhibits);\n\n    // If we have any to loop, we'll append to $html.\n    if (has_loop_records('NeatlineExhibit')) {\n\n    }\n\n    echo $html;\n}\n\nadd_plugin_hook('public_home', 'display_recent_neatline_exhibits');\n[/sourcecode]\n\nInside our `if` statement, we'll update the value of `$html` so that, instead of echoing an empty string, it echos some HTML that includes links to each of our recent Neatline exhibits. Remember that this will only get printed if we actually have Neatline exhibits in the database, otherwise we'll just return an empty string.\n\n[sourcecode language=\"php\"]\n<?php\n\nfunction display_recent_neatline_exhibits() {\n    $html = '';\n\n    // Get our recent Neatline exhibits, limited to five.\n    $neatlineExhibits = get_records('NeatlineExhibit', array('recent' =&gt; true), 5);\n\n    // Set them for the loop.\n    set_loop_records('NeatlineExhibit', $neatlineExhibits);\n\n    // If we have any to loop, we'll append to $html.\n    if (has_loop_records('NeatlineExhibit')) {\n        $html .= '&lt;ul&gt;';\n\n        foreach (loop('NeatlineExhibit') as $exhibit) {\n            $html .= '&lt;li&gt;'\n                   . nl_getExhibitLink(\n                         $exhibit,\n                         'show',\n                         metadata($exhibit, 'title'),\n                         array('class' =&gt; 'neatline')\n                     )\n                   . '&lt;/li&gt;';\n        }\n\n        $html .= '&lt;/ul&gt;';\n    }\n\n    echo $html;\n}\n\nadd_plugin_hook('public_home', 'display_recent_neatline_exhibits');\n[/sourcecode]\n\nAs you can see, we append an opening unordered list tag, `<ul>` to `$html`. (Using `.=` in PHP lets us append additional strings onto an existing variable.) Then, we use Omeka's `loop` function to loop through our set of Neatline exhibits. Inside that loop, we once again adding something to the value of `$html`: A list item wrapping a link to a the current Neatline exhibit in the loop. To help us make that link, we'll use a function provided by the Neatline plugin: `nl_getExhibitLink`. We're passing values for four arguments: The exhibit object (defined in `$exhibit` in the foreach loop); the `action` or route you want the link to take; the text of the link (here we've used Omeka's `metadata` function to give us the title of the exhibit); and an array of attributes for the link (we've added a `class` attribute equal to 'neatline'). Then we end with a closing list item tag.\n\nAnd that should do it. You can see a version more or less the same as what I demonstrate here in a [public gist](https://gist.github.com/clioweb/6178723) I published earlier in the week. If you'd like to display a recent list of Neatline exhibits on your Omeka home page, just grab this code, and put it in your theme's `custom.php` template.\n"},{"id":"2013-08-12-why-do-we-trust-automated-tests","title":"Why do we trust automated tests?","author":"david-mcclure","date":"2013-08-12 05:08:50 -0400","categories":["Research and Development"],"url":"why-do-we-trust-automated-tests","content":"_[Cross-posted from [dclure.org](http://dclure.org/essays/why-do-we-trust-automated-tests/)]_\n\nI'm fascinated by this question. Really, it's more of an academic problem than a practical one - as an engineering practice, testing just _works_, for lots of simple and well-understood reasons. Tests encourage modularity; the process of describing a problem with tests makes you understand it better; testing forces you to go beyond the \"happy case\" and consider edge cases; they provide a kind of functional documentation of the code, making it easier for other developers to get up to speed on what the program is supposed to do; and they inject a sort of refactoring spidey-sense into the codebase, a guard against regressions when features are added.\n\nAt the same time, though, there's a kind of software-philosophical paradox at work. Test are just code - they're made of the same stuff as the programs they evaluate. They're highly specialized, meta-programs that operate on other programs, but programs nonetheless, and vulnerable to the same ailments that plague regular code. And yet we _trust_ tests in a way that we don't trust application code. When a test fails, we tend to believe that the application is broken, not the tests. Why, though? If the tests are fallible, then why don't they need their own tests, which in turn would need their own, and so on and so forth? Isn't it just like fighting fire with fire? If code is unreliable by definition, then there's something strange about trying to conquer unreliability with more unreliability.\n\nAt first, I sort of papered over this question by imagining that there was some kind of deep, categorical difference between resting code and \"regular\" code. The tests/ directory was a magical realm, an alternative plane of engineering subject to a different rules. Tests were a boolean thing, present or absent, on or off - the only question I knew to ask was \"Does it have tests?\", and, a correlate of that, \"What's the coverage level?\" (ie, \"How _many_ tests does it have?\") The assumption being, of course, that the tests were automatically trustworthy just because they existed. This is false, of course [1]. The process of describing code with tests is just another programming problem, a game at which you constantly make mistakes - everything from simple errors in syntax and logic up to really subtle, hellish-to-track-down problems that grow out of design flaws in the testing harness. Just as it's impossible to write any kind of non-trivial program that doesn't have bugs, I've never written a test suite that didn't (doesn't) have false positives, false negatives, or \"air guitar\" assertions (which don't fail, but somehow misunderstand the code, and fail to hook onto meaningful functionality).\n\nSo, back to the drawing board - if there's no secret sauce that makes tests more reliable in principle, where does their authority come from? In place of the category difference, I've started to think about it just in terms of a relative falloff in complexity between the application and the tests. Testing works, I think, simply because it’s generally **easier** to formalize what code should do than how it should do it. All else equal, tests are less likely to contain errors, so it makes more sense to assume that the tests are right and the application is wrong, and not the other way around. By this logic, the value added is proportional to the height of this \"complexity cliff\" between the application and the tests, the extent to which it's easier to write the tests than to make them pass. I've starting using this as a heuristic for evaluating the practical value of a test: The most valuable tests are the ones that are trivially easy to write, and yet assert the functionality of code that is extremely complex; the least valuable are the ones that approach (or even surpass) the complexity of their subjects.\n\nFor example, take something like a sorting algorithm. The actual implementation could be rather dense (ignore that a custom quicksort in JavaScript is never useful):\n\n\n\nThe tests, though, can be fantastically simple:\n\n\n\nThese are ideal tests. They completely describe the functionality of the code, and yet they fall out of your fingers effortlessly. A mistake here would be glaringly obvious, and thus extremely unlikely - a failure in the suite almost certainly means that the code is actually defective, not that it's being exercised incorrectly by the tests.\n\nOf course, this is a cherry-picked example. Sorting algorithms are inherently easy to test - the complexity gap opens up almost automatically, with little effort on the part of the programmer. Usually, of course, this isn't the case - testing can be fiendishly difficult, especially when you're working with stateful programs that don't have the nice, data-in-data-out symmetry of a single function. For example, think about thick JavaScript applications in the browser. A huge amount of busywork has to happen before you can start writing actual tests - HTML fixtures have to be generated and plugged into the testing environment; AJAX calls have to be intercepted by a mock server; and since the entire test suite runs inside a single, shared global environment (PhantomJS, a browser), the application has to be manually burned down and reset to a default state before each test.\n\nIn the real world, tests are never this easy - the \"complexity cliff\" will almost always be smaller, the tests less authoritative. But I've found that this way of thinking about tests - as code that has an imperative to be simpl**er** than the application - provides a kind of second axis along which to apply effort when writing tests. Instead of just writing more tests, I've started spending a lot more time working on low-level, infrastructural improvements to the testing harness, the set of abstract building blocks out of which the tests are constructed. So far, this has taken the form of building up semantic abstractions around the test suite, collections of helpers and high-level assertions that can be composed together to tell stories about the code. After a while, you end up with a kind of codebase-specific DSL that lets you assert functionality at a really high, almost conversational level. The chaotic stage-setting work fades away, leaving just the rationale, the narrative, the _meaning_ of the tests.\n\nIt becomes an optimization problem - instead of just trying to make the tests _wider_ (higher coverage), I've also started trying to make the tests _lower_, to drive down complexity as far towards the quicksort-like tests as possible. It's sort of like trying to boost the \"profit margin\" of the tests - more value is captured as the difficulty of the tests dips further and further below the difficulty of the application:\n\n[![complexity-cliff-web](http://www.scholarslab.org/wp-content/uploads/2013/08/complexity-cliff-web-1024x584.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/08/complexity-cliff-web.jpg)\n\n**[1]** Dangerously false, perhaps, since it basically gives you free license to to write careless, kludgy tests - if a good test is a test that exists, then why bother putting in the extra effort to make it concise, semantic, readable?\n"},{"id":"2013-08-14-parsing-bc-dates-with-javascript","title":"Parsing BC dates with JavaScript","author":"david-mcclure","date":"2013-08-14 09:29:07 -0400","categories":["Research and Development"],"url":"parsing-bc-dates-with-javascript","content":"_[Cross-posted from [dclure.org](http://dclure.org/logs/parsing-bc-dates-with-javascript/)]_\n\nLast semester, while giving a workshop about Neatline at Beloit College in Wisconsin, [Matthew Taylor](https://twitter.com/mptaylor), a professor in the Classics department, noticed a strange bug - Neatline was ignoring negative years, and parsing BC dates as AD dates. So, if you entered \"-1000\" for the \"Start Date\" field on a Neatline record, the timeline would display a dot at 1000 AD. I was surprised by this because Neatline doesn't actually do any of its own date parsing - the code relies on the built-in `Date` object in JavaScript, which is implemented natively in the browser. Under the hood, when Neatline needs to work with a date, it just spins up a new `Date` object, passing in the raw string value entered into the record form:\n\n\n\nSure enough, though, this doesn't work - `Date` just ignores the negative sign and spits back an AD date. And things get even funkier when you drift within 100 years of the year 0. For example, the year 80 BC parses to _1980_ AD, bizarrely enough:\n\n\n\nObviously, this is a big problem if you need to work with ancient dates. At first, I was worried that this would be rather difficult to fix - if we really were hitting up against bugs in the native implementation of the date parsing, it seemed likely that Neatline would have to get into the tricky business of manually picking apart the strings and putting together the date objects by hand. It's always feels icky to redo functionality that's nominally built into the programming environment. But I didn't see any other option - the code was unambiguously broken as it stood, and in a really dramatic way for people working with ancient material.\n\nSo, grumbling at JavaScript, I started to sketch in the outlines of a bespoke date parser. Soon after starting, though, I was idly fiddling around with the `Date` object in the Chrome JavaScript terminal when stumbled across an unexpected (and sort of inexplicable) solution to the problem. In reading through the [documentation for the `Date` object over at MDN](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Date), I noticed that the constructor actually takes three different configuration of parameters. If you pass in a single integer, it treats it as a Unix timestamp; if you pass a single string, it treats it as a plain-text date string and tries to parse it into a machine-readable date (this was the process that appeared to be broken). But you can also pass three separate integers - a year, a month, and a day. Out of curiosity, I plugged in a negative integer for the year, and arbitrary values for the month and day:\n\n\n\nMagically, this works. A promising start, but not a drop-in solution for the problem - in order to use this, Neatline would still have to manually extract each of the date parts from the plain-text date strings entered in the record forms (or break the dates into three parts at the level of the user interface and data model, which seemed like overkill). Then, though, I tried something else - working with the well-formed, BC date object produced with the year/month/day integer values, I tried casting it back to ISO8601 format with the `toISOString` method. This produced a date string with a negative date and...\n\n\n\n...**two leading zeros before the four-digit representation of the year**. I had never seen this before. I immediately tried reversing the process and plugging the outputted ISO string back into the `Date` constructor:\n\n\n\nAnd, sure enough, this works. And it turns out that it also fixes the incorrect parsing of two-digit years:\n\n\n\nI am deeply, profoundly perplexed by this. The [ISO8601 specification](http://dotat.at/tmp/ISO_8601-2004_E.pdf) makes cursory note of an \"expanded\" representation for the year part of the date, but doesn't got into specifics about how or why it should be used. Either way, though, it works in all major browsers. Mysterious stuff.\n"},{"id":"2013-08-14-reprinting-printed-parts","title":"Reprinting Printed Parts","author":"jeremy-boggs","date":"2013-08-14 06:00:12 -0400","categories":["Experimental Humanities"],"url":"reprinting-printed-parts","content":"As some of you know, the Scholars' Lab has a spiffy 3D printer, a [Makerbot](http://makerbot.com) Replicator 2. We've had fun with it, printing all sorts of wonderful things. As time went on and we continued using it, we ran into a problem [plenty of other folks encountered](http://support.makerbot.com/entries/22871743-Replicator-2-stops-extruding-either-starts-clicking-or-silently-stops-), where the plunger that pushes filament against the drive gear was weakening. The first solution we tried was [tightening up the plunger](http://www.youtube.com/watch?v=QOJN_8AAC9U), but we'd have to do this frequently. A better solution was to print a new set of parts—a [spring-loaded arm](http://www.thingiverse.com/thing:53125) and mount—to replace the plunger. So I ordered up the [hardware I'd need](http://store.makerbot.com/drive-block-hardware-kit.html) (spring, bearing, and bolts), and when those arrived in the mail, I printed the arm and other parts, disassembled the drive block on the printer, and replaced the plunger with new spring-loaded arm. You can see the [fully-assembled drive block](http://www.thingiverse.com/make:38003) in my made stuff on [Thingiverse](http://thingiverse.com). After I put the drive block back on the printer, I could tell an immediate difference in the prints. The plastic was extruding more smoothly than before. Loading the filament was a tiny bit trickier, but well worth it.\n\nThen last month, I hauled the printer halfway across the country to use in a workshop on 3D modeling and printing at the [Digital Humanities conference](http://dh2013.unl.edu) in Lincoln, Nebraska. When I started a test print to calibrate the printer, I noticed the printer wasn't extruding plastic as well as it had been. so I took the drive block off again, and noticed the arm was loose, that the spring wasn't actually pushing up on it enough to exert pressure on the filament coming it. I assumed I had just cracked the arm somehow during the drive out to Nebraska, so I just stretched the spring out a bit so it would better push the arm up, and went on with the workshop as planned.\n\nYesterday I finally got around to printing a replacement arm. After taking the old arm and mount off, and comparing the old arm with the new one, it seems like my initial thought that I had broken the arm were incorrect:\n\n[caption id=\"attachment_8603\" align=\"alignnone\" width=\"768\"][![IMG_0334](http://www.scholarslab.org/wp-content/uploads/2013/08/IMG_0334-768x1024.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/08/IMG_0334.jpg) Comparing printed arms for the drive block. The black arm is the replacement, and the orange arm is the old one.[/caption]\n\nUpon further inspection, the old one isn't broken as I had first suspected. Comparing the two (The orange arm is the first one I printed, and the black arm is the replacement I just printed) , it looks like the orange one has warped, very likely due the plastic arm being so close to the heated extruder.  It could also be that the spring was strong enough to warp the plastic arm bit by bit over time. I'm sure the proximity to the heated extruder helped with that too.\n\n[caption id=\"attachment_8604\" align=\"alignnone\" width=\"1024\"][![Assembled drive block with spring-loaded arm.](http://www.scholarslab.org/wp-content/uploads/2013/08/IMG_0338-1024x768.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/08/IMG_0338.jpg) Assembled drive block with spring-loaded arm.[/caption]\n\nI decided to go ahead and use the newly printed arm for now. Needless to say, if you've tried this solution for your Replicator 2, you'll be better off long-term sending for the [machined replacement parts](http://store.makerbot.com/extruder-upgrade.html) as I did. But for now, the new arm is working great, and should hold us over until the machined parts arrive.\n"},{"id":"2013-08-15-problem-solving-with-html5-audio","title":"Problem Solving with HTML5 Audio","author":"wayne-graham","date":"2013-08-15 05:19:16 -0400","categories":["Research and Development"],"url":"problem-solving-with-html5-audio","content":"Several years ago I worked on a project to take recordings made of William Faulkner while he was the Writer-in-Residence at the University of Virginia in 1957 and 1958. The project, [Faulkner at Virginia](http://faulkner.lib.virginia.edu), transcribed the audio and then keyed the main components of the audio to the text using TEI. In order to provide playback of an individual clip, we used a streaming server ([Darwin Streaming Server](https://en.wikipedia.org/wiki/Darwin_Streaming_Server)) that was being managed by another group. This allowed me to provide \"random\" access to the components of the audio, without needing to split up the files. Using the associated API, I could generate a clip of the data with something like this:\n\n[gist id=\"5830678\" file=\"gistfile1.js\"]\n\nWhile this is kind of a nasty bit of JavaScript, it (somewhat) abstracts the Object embed code:\n\n[gist id=\"5830678\" file=\"gistfile2.html\"]\n\nAt the time, the [WHATWG specifications for audio](http://www.whatwg.org/specs/web-apps/current-work/) were still pretty nascent, and didn't have a lot of actual implementation saturation in browsers. At the time (late 2000s), the approach of using a third-party plugin to provide \"advanced\" interaction with a media element was pretty much the only game in town.\n\nAs with any project that relies on web technology, eventually things start to break, or just flat-out not work on devices that can access the Internet (e.g. mobile). Browsers have been in a war with each other for speed, implementation of \"standards\", and market share. This has been a real boon for users as it has allowed developers to really push what the web is capable of as a run-time environment. Unfortunately for the Faulkner audio, the code got to the point where the approach stopped functioning consistently across all desktop browsers (interestingly, Chrome seemed to manifest this issue most consistently), and oh yeah, there are those iOS-based mobile devices that can't play this either.\n\n\n\n## HTML `audio` to the rescue\n\n\n\nYou know that modern browsers (everything but IE < 9) can play audio natively (i.e. without a plugin), right? Really the only really horrible thing is that not every browser handles the same \"native\" format. You can check out a good table for [codec support for audio](http://html5doctor.com/html5-audio-the-state-of-play/#support), but it basically boils down to needing an MP3 and an [Ogg Vorbis](http://www.vorbis.com/) version of the audio files to provide for nearly all the browsers (IE being the outlier, with this working of IE 9+).\n\n[gist id=\"5830678\" file=\"gistfile3.html\"]\n\nThis provides something on your page like this:\n\n\n  \n  \n\n\nThe great thing is that this will work on a mobile device as well. Score one for the Internet! Now to figure out the best way to do this.\n\n\n\n### Split the files\n\n\n\nMy first instinct was to take the files and split them into \"clips\" on the page. This would allow the browser to provide its native playback mechanism, and allow individuals to grab the segments for remixing (still waiting for an auto-tuned remix of \"Spotted Horses\"). In the [TEI source](https://code.google.com/p/faulkneratvirginia/source/browse/trunk/cocoon/data/tei/wfaudio02_1.xml#67) are the start and end times for each of the \"snippets.\" My go-to tool for working with media is [ffmpeg](http://www.ffmpeg.org/), and I knew I could break up the files into components, copying the bitrate into a new mp3. I wrote a quick XSLT to generate a shell script that would generate the `ffmpeg` commands to run.\n\n[gist id=\"5830678\" file=\"convert_mp3.xsl\"]\n\nThis generated a nice file of the commands to run.\n\n[gist id=\"5830678\" file=\"gistfile5.sh\"]\n\nAt this point, all the data has been processed, so I need to see if this this going to actually work. I wrote another XSLT to preview what was going on and make sure this approach was going to work ok. Nothing too fancy, just an HTML wrapper, with most of the \"work\" happening in the `div2` element.\n\n[gist id=\"5830678\" file=\"htmlaudio.xsl\"]\n\nSince the segment file names were derived from their `id` attributes, I was able to just point at the file without a lot of poking around. Now for the test!\n\nI started playing with it and it appeared to work just fine. I then asked one of my colleagues who was working remotely to take a look at it, and she ran into a show stopper. She observed that when she loaded the page, only the first several of the clips were loading.\n\nIn the `audio` element, I had added the `preload=\"auto\"` attribute to allow the file to buffer and play before then entire file had downloaded. When I profiled what was going on, I realized that somewhere around 20Mb of download, the browser was giving up preloading the audio for immediate playback. If you remove that attribute, the browser won't buffer the file and you would have to download the entire file before you could start the playback. Definitely not what I was aiming at. Time to try something else.\n\n\n\n### Audio Time Range\n\n\nIn reading the [MDN docs](https://developer.mozilla.org) on [HTML5 audio](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/audio), I came across a section on [Specifying a playback range](https://developer.mozilla.org/en-US/docs/Web/HTML/Using_HTML5_audio_and_video#Specifying_playback_range). This looks promising! There is one file reference, and I just need to get the playback times in. It is unclear, however, from the description if the browser treats this as a single file transfer, or each segment as it's own download thread. Fortunately it's just a small tweak to the XSLT generating the `audio` elements.\n\n[gist id=\"5830678\" file=\"time_ranges.xsl\"]\n\nAfter checking the browser again, looks like the same issue is there; the browser treats each segment as its own download thread and chokes when it gets around 20Mb. Meh; the Internet. Ok, time to try something different.\n\n\n\n### Audio Sprites\n\n\n\nWhen I was writing my book on developing HTML5 games, I ran across a great article [Audio Sprites (and fixes for iOS)](http://remysharp.com/2010/12/23/audio-sprites/) by Remy Sharpe. The idea draws inspiration from [CSS sprite sheets](http://css-tricks.com/css-sprites/) where you put all your image assets into a single file then display the portion of the image you want on an HTML page. With audio sprites, instead of shifting coordinates of an image around, you shift the playhead of an MP3 file and tell it how long to play. This is really great for games as you can have a single file for players to download with all the audio files. Maybe this technique will work here...\n\nSince I wanted to see how well this would work, and not necessarily write a library to support this, I used the [howler.js](http://badassjs.com/post/41873438322/howler-js-a-cross-browser-javascript-audio-playback) library which has support for audio sprites. Back to the XSLT.\n\nThe `howler.js` API defines sprites by names to allow you to refer to then as variables in your code (again, it's written for developing games). It also wants you to (in milliseconds) tell it where to start playing, and for how long to play. Ugh, my start and end times are in `hh:mm:ss.s` format. I wrote a quick function to explode the timestamps and add them together as milliseconds (actually this is a bit off, but I didn't spend the time to work in the actual conversion units, but I wanted to see if this is going to work before I put that time in).\n\n[gist id=\"5830678\" file=\"timeToMilliseconds.xsl\"]\n\nNow I can set up JavaScript for the Howler object literal for use on the page.\n\n[gist id=\"5830678\" file=\"timeToMilliseconds.xslt\"]\n\nA few notes here, iOS devices require user interaction before an audio asset can be loaded. To handle the mobile devices, I added a button to the page that is hidden with a media query for desktop browsers. When the user clicked on it, they would see a [thumper](http://www.ajaxload.info/) and a notice when the file had been loaded. I also had to add my own \"play\" buttons as this API is really meant for games.\n\nAwesome, it works! But is this really a good idea? This is kind of an exotic (\"clever\" in programming terms) approach. It also relies on an obscure library that may not be maintained in the future. This probably isn't the best path forward...\n\n\n\n## Blended Approach\n\n\nAfter some more thought, maybe what's needed here is a blended approach. I liked the fact that with the timestamps, I only have to create one extra file (and not 2 * n clips for both mp3 and ogg formats), but there was that sticky preload issue. This is where JavaScript can also help. What if there was an obvious mechanism for a user to click and then I could use JavaScript to dynamically construct an `audio` element in the DOM, and only start streaming the \"segment\" the user requested? This just may work :)\n\nWith a little JavaScript, I take a look at the DOM and construct an audio element, passing back to the browser the smallest version of the file (ogg then mp3) the browser can play back natively.\n\n[iframe src=\"http://jsfiddle.net/wsgrah/3EfAD/15/embedded/js,html,css/\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\"]\n\nSo the final product results in this, which has an animation to remove the icon, replacing it with the native audio playbar:\n\n[iframe src=\"http://jsfiddle.net/wsgrah/3EfAD/15/embedded/result/\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\"]\n\n\n\n# Generating Ogg\n\n\n\nNow that I've got a basic system for playing the audio that works on just about every browser, time to take a look at converting these audio files. There are about 60 MP3s that needed to get transcoded for this project. If it were just a handful, I may have just manually done the transcoding in something like [Audacity](http://audacity.sourceforge.net/), but there were a lot of files, and I'm a \"lazy\" developer. Obviously this is a another job for [ffmpeg](http://www.ffmpeg.org/). I had to recompile it from [homebrew](http://brew.sh/) (an OS X package manager) to include the `libvorbis` bindings.\n\n[gist id=\"5830678\" file=\"gistfile4.txt\"]\n\nAfter getting the proper transcoding libraries installed, I wrote a quick bash script to convert all MP3s in a directory to ogg.\n\n[gist id=\"5830678\" file=\"transcode_ogg.sh\"]\n\nAfter this ran (it took a few hours), I had a complete set of MP3 and OggVorbis files for the project.\n\n\n\n## Conclusion\n\n\nAfter rethinking how to address the problem of streaming audio to multiple platforms, with various limitations on how the `audio` specification is implemented, I finally landed on something that is not novel. What it does do, however, is move away from an approach that is no longer widely supported (the use of QTSS), to a single method that leverages the native support of modern browsers to do something reasonably simple...play a bit of sound. I also got rid of a lot of JavaScript (which breaks), the reliance on another server (which breaks), and sped up the delivery of the audio to the client. Additionally, since this isn't an exotic (or complicated) replacement, the next person who has to do something with this code in five years will have a fighting chance at figuring out what is going on!\n"},{"id":"2013-08-26-welcome-new-slab-grad-fellows","title":"Welcome, new SLab grad fellows!","author":"bethany-nowviskie","date":"2013-08-26 05:48:28 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"welcome-new-slab-grad-fellows","content":"The Scholars' Lab is pleased and proud to announce our partnership with **nine new graduate fellows** for the 2013-2014 academic year! They represent seven academic disciplines in the [humanities and social sciences](http://artsandsciences.virginia.edu/home/index.html) at the University of Virginia, and join a distinguished group of [past recipients](http://www.scholarslab.org/graduate-fellowships/) of Scholars' Lab fellowships. (Since 2007,  [UVa Library](http://library.virginia.edu) has offered 44 fellowships to deserving grad students in fields as diverse as History, Archaeology, Computer Music, Anthropology, Economics, English, Ethnomusicology, French, Religious Studies, Art History, Linguistics, and Architecture.)\n\nFirst, we have our three winners of the **UVa Library Graduate Fellowship in Digital Humanities**. They are:\n\n[**Erik DeLuca** ](http://erikdeluca.com/)of the Composition and Computer Technologies Program in UVa's McIntire Department of Music\n\"Community Listening in Isle Royal National Park, a sonic ethnography\"\n\n**[Gwen Nally](http://egnally.github.io/)** of the Corcoran Department of Philosophy\n\"When Socrates Misleads: Falsehood and Fallacy in Plato's Dialogues\"\n\nand\n\n**[Tamika Richeson](http://history.virginia.edu/user/348)** of the Corcoran Department of History\n\"I Know What Liberty Is: Black Motherhood, Labor, and Criminality, 1848-1878\"\n\nThese fellows will have the opportunity to work closely with Scholars' Lab staff over the course of the year, in applying digital methods to their dissertation research and presenting that work online.  P**lease join us on September 10th at noon in the Scholars' Lab**, when we welcome Erik, Gwen, and Tamika with a casual luncheon, and have the opportunity to hear a brief summary from each of them, about what they hope to accomplish this year!\n\nNext, we're getting started with a third year of the [Praxis Program](http://praxis.scholarslab.org/) here at UVa, which is now home base for the new, international [Praxis Network](http://praxis-network.org/)!  Last year saw the refinement and reimagining of [Prism](http://prism.scholarslab.org/) (not _that_ Prism), a tool created by the first Praxis cohort in 2011-12. Prism is a web application for crowdsourcing interpretation, and for thinking through the relationship of humanities inquiry to the methods and motives of crowdsourcing. This year, the 2013-14 Praxis team will rethink, revive, and (we expect) utterly remake [the Ivanhoe Game](http://www.ivanhoegame.org/?page_id=21), another platform for playful, collaborative interpretation of documents and artifacts.\n\n2013-2014 Praxis Fellows include:\n\n**Scott Bailey** (Religious Studies)\n**Elizabeth Fox** (English)\n**Veronica Ikeshoji-Orlati** (Classical Art & Archaeology)\n**Stephanie Kingsley** (English)\n**Francesca Tripodi** (Sociology)\nand** Zachary Stone** (English)\n\nKeep an eye on the [Scholars' Lab blog](http://www.scholarslab.org/archives/) for news throughout the year, on the work of all of our wonderful graduate fellows!\n"},{"id":"2013-09-05-and-so-it-begins","title":"And so it begins...","author":"veronica-ikeshoji-orlati","date":"2013-09-05 13:15:48 -0400","categories":["Grad Student Research"],"url":"and-so-it-begins","content":"This is my first blog post (ever), so I have spent a good deal of time hemming and hawing over an appropriately novel and pithy title to headline my blogging debut. Needless to say, that hasn't happened.\n\nAnyway, I'm Veronica Ikeshoji-Orlati, a 4th year PhD Candidate in Classical Art & Archaeology in the McIntire Department of Art here at UVA. I took a BA in Classics, with minors in Art History and Philosophy, from the University at Buffalo (2003), later returned to UB for an MA in Art History (2010), and am now weaving together the disparate threads of my personal and academic interests into my dissertation, entitled _Music, Performance, and Identity in 4th century BCE South Italian Vase-Painting_. The field of Classical Archaeology, and South Italian Archaeology in particular, offers up many challenges, incredible opportunities, and fascinating methodological questions, so I find my research engaging on many levels.\n\nI am thrilled to be part of the 2013-14 Praxis cohort for a panoply of reasons. The two most important to me are 1. the opportunity to work with, and learn from, people from vastly different backgrounds with diverse personal, professional, and academic interests, and 2. the chance to plunge into the field of Digital Humanities with patient, knowledgeable guides and amicable, resourceful accomplices. Getting to work on the Ivanhoe game is an added benefit, since pedagogy happens to be a topic which occupies a significant corner of my mind and the role of digital spaces and teaching tools in the classroom is a developing interest.\n\nThat just about sums up what an introductory post should say. I look forward to sharing what we're doing, and how it impacts my own research and thinking, here!\n"},{"id":"2013-09-08-a-bit-about-me","title":"A bit about me","author":"francesca-tripodi","date":"2013-09-08 12:30:46 -0400","categories":["Grad Student Research"],"url":"a-bit-about-me","content":"Hello readers! My name is Francesca Tripodi, and I am one of the 2013/2014 Praxis Fellows at UVa. I come to academia from a more circuitous route. Unlike many graduate students that I meet, I didn't realize that I wanted to be in academia until much later in life. As a student at the Annenberg School at the University of Southern California my immediate interests were working in media. But after an extremely fulfilling internship at Fox Cable Networks Group, I caught the travel bug and took off to Australia where I spent six months backpacking \"down under\" followed by a month exploring New Zealand and month in Thailand. When I returned to The States, I yearned for a more global metropolis and spent the better part of my twenties working in Washington, DC.\n\nMy first job was at the United States Telecommunications Training Institution (USTTI). I worked as a liaison between the private sector (Cisco Systems, Bechtel, Qualcomm, and Microsoft) and the public sector (FCC, NTIA) to help deliver low-cost training programs to citizens of developing countries looking to expand and improve their digital infrastructure.  In addition to organizing course content, I worked with USAID offices and the State Department to coordinate the logistics of participants traveling to the US for the training (including visa processing).  After that job, I moved to Georgetown University and eventually became the Program Director of Pathways to Success - an academic immersion program that brings high school students from rural America to Georgetown University in an effort to improve  minority involvement in STEM education. One of my greatest achievements in this position was helping to secure additional funds (1.3million) to continue financing the program through 2012.\n\nAs an employee at Georgetown I also took advantage of their tuition remission benefits and earned my MA in Communication, Culture and Technology. It was there that I learned I ask very \"sociologically oriented\" questions and with the help of my advisor decided to continue my education at the University of Virginia. As a fourth year PhD Candidate in the Sociology Department, I am currently working on data that I collected from an ethnographic study in rural Louisiana on alligator hunting. Some of my more immediate findings are the importance of female hunting in the community and the parallels between the Cajun culture I experienced and the media representations of Cajun life on the show \"Swamp People.\" In the spring I hope to defend my dissertation proposal in an effort to answer the central research question that currently occupies my mind: To what extent does media influence a community’s boundary making process? In what ways do these boundaries shift depending on who controls the mediated narrative?\n\nI am also happily married to a wonderful guy and six weeks ago we welcomed to the world a beautiful baby boy.\n"},{"id":"2013-09-08-greetings-from-stephanie-new-praxis-fellow","title":"Greetings from Stephanie, new Praxis Fellow!","author":"stephanie-kingsley","date":"2013-09-08 15:23:41 -0400","categories":["Grad Student Research"],"url":"greetings-from-stephanie-new-praxis-fellow","content":"I am excited to be a part of the new Praxis cohort and would like to take a few moments to introduce myself before a flurry of--ideally, great and innovative--thoughts populate the blog.  I am a second-year MA student in the English department, specializing in American literature, textual studies, and digital humanities.  My academic interests include Colonial and 19th-century American literature and history, as well as American book history.  My goal is to graduate this Spring and work in publishing or alternate academia.\n\nAlongside my literary studies, a key component of my career at the University of Virginia has been to learn as much as possible about the digital humanities.  I have assisted on Alison Booth's _Collective Biographies of Women_, a database of women grouped in communities based on the 19th-century biographies in which they are featured.  I continued my DH education with Documents Compass's _People of the Founding Era_, an online archive of individuals mentioned in the _Papers of the Founding Fathers_.  Additionally, I took David Seaman's Rare Book School Course \"XML in Action\" this past summer.  These projects have introduced me to digital editing and archiving, while also getting me thinking about other applications of DH.  An area I have yet to break into is crowdsourcing, inviting user participation and contribution, and this is an area which Praxis will plunge me into immediately.  Already, I am blogging... reaching out to people... and excited to be doing it.\n\nWe discussed in our first Praxis meeting a somewhat conflicted relationship to technology which we all share.  For my part, I still have no Smart Phone, forget to check my Facebook, and--lacking a GPS--have been known to use a paper map; notwithstanding all this, a hobbyist dream of mine is to create a digital archive of old family letters and photographs, alongside ancestor profiles.  I believe that DH is the key to preserving and disseminating this sort of material throughout the world.  At the same time, I will never cease to love the smell of a brand-new paperback book, or the feel of one of the many treasures housed in UVA Special Collections.  As a textual-studies and DH scholar, I inhabit both worlds and exist in a constant state of perplexity and wonderment... a state in which I now turn my attention to the work at hand.  To the SLab and Praxis cohort, and all our Praxis-blog followers, I am glad to make your acquaintance and thrilled to begin our work this year!\n"},{"id":"2013-09-09-greetings-and-salutations","title":"Greetings and Salutations","author":"elizabeth-fox","date":"2013-09-09 04:08:28 -0400","categories":["Grad Student Research"],"url":"greetings-and-salutations","content":"Hello all!\n\nMy name is Eliza Fox, and I’m a third-year PhD student in the English Department.  My research focuses on the Victorian novel, along with secondary interests in children’s and young adult literature.  I’m not a total newcomer to the DH world: I spent last year as a NINES Fellow, working on projects that ranged from encoding a manuscript of _Prometheus Unbound _to updating metadata for NINES’s vast collection of aggregated digital objects.  I’ve studied databases at DHSI, and my undergraduate career included the rudiments of HTML.  I’ve also spent an inordinate amount of time enjoying Prism, otherwise known as the fruits of Praxis Past.  (My masterpiece: Prism on Prism, in which I highlighted articles on NSA surveillance for “Uncertainty about the future” and  “Fear.”  Clearly, irony is my medium.)\n\nI recognize, however, that none of this has prepared me for the whirlwind year that lies ahead.  If my past experiences have allowed me to dip my toe in the DH pool, Praxis will force me to jump into the deep end and to figure out – live, in public, and in the moment – how to swim.\n\nBut, strange as it may sound, it’s a jump that I’m immensely excited to take.  Despite my love/hate relationship with technology (I once nicknamed my computer Cher – short for Chernobyl), I’m perpetually intrigued by the opportunities DH offers to represent, reconsider, and reconstitute our studies.  Praxis, in particular, offers the kind of full-throttle, hands-on, learning-by-doing approach that allows for a true appreciation and mastery of the field.  I’ve had many friends go through this program, and I’ve watched them transform, in just a few months, from technological novices into committed DH scholars.  Whatever their introductory levels of experience, they’re now designing archives, teaching coding, and project managing.\n\nI can’t wait to join them.\n"},{"id":"2013-09-11-a-bit-more-medieval","title":"a bit more medieval","author":"zachary-stone","date":"2013-09-11 14:30:38 -0400","categories":["Grad Student Research"],"url":"a-bit-more-medieval","content":"Well. I must admit to some surprise and no small degree of trepidation regarding my presence here (both in the Praxis program and _here _online as a blogger being read by you [blog readers and, I guess, bots]). For example when, in our first meeting, I asked what a ‘wiki’ actually _was_ I found out that they are ‘like Google Docs,’ which of course only begged the question on my part: what is a Google Doc? So. As we can see, this will be a fun year. In truth, I applied to the Praxis program not because of a strong background in the Digital Humanities (hence forth ‘DH’ [incl. the definite article]) but rather on account of my relative lack of any background in DH. Lack of background, however, ought not be equated with lack of interest. As a medievalist, specifically a student of medieval books, manuscripts, and textual cultures, I have both pragmatic and theoretical interests in DH.\n\n\n\n\nPragmatically DH offers new ways to share previously impossible (or at least highly improbable) amounts of data, specifically visual data. As a palaeographer/book historian this allows me to avoid certain compromises forced on previous generations by the exigencies of print. For example, when cataloguing the medieval manuscripts of Wadham College, Oxford I looked to other, printed, manuscript catalogues for guidance regarding the type and amount of information to include.In those catalogues choices regarding how much description of certain facets of a given book to include required consideration of the volume’s overall publishability, especially given the expected low sales volume. Conversely, online cataloguing would allow the inclusion of all the material the cataloguer finds relevant and useful regardless of length. This is an admittedly conservative example, but nevertheless, my experience cataloguing suggested to me the extent to which my scholarly thought, or perhaps more properly ‘imagination’ in its strictest medieval sense, is constrained to the medium in which I am thinking.\n\n\n\n\nSo pragmatic interest is pretty easy to grasp but positing ontological stakes might seem a bit much. Nor can I provide a neat, concrete, example. It’s more of a feeling that the types of textual production and consumption which occur in pre- and post- print environments share a certain resonance that itself poses an ontological challenge- a challenge the very being-ness- of a print-centric intellectual culture. As we begin to think about our charter we, the new Praxis team, have begun to think about credit, i.e. who gets it. In print world this is pretty tidy. Publication represents a convenient end stop to the production process at which juncture credit and the rewards therein may be distributed along traditional lines of authorship, etc. Online, things seem a bit more medieval. By that I mean the lines of authorship and production are blurred to the extant that disentangling them becomes not only impossible but somewhat ludic in principle. Kind of like the manuscripts I spend most of my time buried in.\n\n\n\n\nAnyhow, as usual, I run on. In short, I am excited about the chance Praxis offers to both learn new DH skills and understand how those skills fit in the long, fluid, tradition we conventionally call the ‘Liberal Arts’ or ‘Humanities.’\n"},{"id":"2013-09-11-mapping-crowd-sourced-bicycle-data","title":"Mapping Crowd Sourced Bicycle Data","author":"chris-gist","date":"2013-09-11 06:14:53 -0400","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"mapping-crowd-sourced-bicycle-data","content":"## Background\n\nCharlottesville is not the easiest place to ride a bicycle.  There are obstacles beyond the narrowness of the streets.  Let's take a look at a few of these.\n\n[caption id=\"attachment_5600\" align=\"aligncenter\" width=\"1024\"]![](http://www.scholarslab.org/wp-content/uploads/2012/08/cvilleElevRoad-1024x768.png) Cville street grid overlaid on elevation surface[/caption]\n\nThe above map shows the elevation around Charlottesville with dark green being the lowest areas and bright red being the highest.  The Charlottesville street system is primarily laid out on top of a series of connected ridges.  This fragmented grid leaves only a small number of routes between different \"sections\" of the street network.  Not only that, most of the ridges run in a north/south direction with one going east/west.   In the following map, circles (blue for downtown and orange for UVa Central Grounds) show two \"employment centers\" (those circles will be used as reference in other maps).\n\n[caption id=\"attachment_5602\" align=\"aligncenter\" width=\"1024\"]![](http://www.scholarslab.org/wp-content/uploads/2012/08/cvilleCentersl-1024x768.png) Downtown and UVa reference circles[/caption]\n\nTo make matters worse, there are two rail lines running through town.\n\n[caption id=\"attachment_5601\" align=\"aligncenter\" width=\"1024\"]![](http://www.scholarslab.org/wp-content/uploads/2012/08/rail-1024x768.png) Cville rail corridors[/caption]\n\nThese two lines cross at the train station on W. Main St.  They esentially cut the city into four quads.\n\n[caption id=\"attachment_5637\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/08/quads1-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/08/quads1.png) Cville quadrants[/caption]\n\nThe rail lines further limit the ability to traverse the street network.  Below are all the street crossings (over/under rail lines) from one quad to another.  The crossings marked in red are not, in my opinion, suitable for bicycles in current configurations.\n\n[caption id=\"attachment_5634\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/08/quadsCrossings1-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/08/quadsCrossings1.png) Cville quad passes[/caption]\n\nThat leaves us with some serious bottlenecks for bicycle movement that, not coincidentally, tend to be along busy streets.\n\nHere is a street map to provide context for anyone not familiar with Charlottesville.\n\n[caption id=\"attachment_5654\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/08/referenceMap-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/08/referenceMap.png) Base map uses Open Street Map data[/caption]\n\n## Cville Bike mApp\n\nBack in April of 2012, the [Thomas Jefferson Planning District Commission](http://www.tjpdc.org) (TJPDC) launched a bicycle route data collection project called  [Cville Bike mApp](http://www.cvillebikemapp.com).   The TJPDC  adapted a smart phone apps that allowed users to log all their bicycle trips and share the location data with the TJPDC.  Data collection officially closed on May 18.  The apps are still available and the TJPDC is still collecting data.    The source code can be downloaded [here](https://github.com/tjpdc).\n\nKelly Johnston and I were asked to consult with the TJPDC intern working with the data.  We discussed various visualizations and spatial analyses.  We asked for and were given permission to have the data to see what we could do.  Let the fun begin!\n\n## Data Issues\n\nThe data arrived after being \"cleaned\" of many data points.  This was necessary because there were many obvious cases where people had left their logging app running after they were at home, work or driving.  There were also routes with only a few datapoints over a very large geographic area.  The data was in Excel with each record containing a single latitude/longitude pair with trip ID, date and time.  There was a separate table listing user ID, trip purpose, weather and notes.  There is no personal information in the data we received.  Of the data mapped, there were 1011 trips made by and 118 users.\n\n[caption id=\"attachment_5644\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/08/badTrip1-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/08/badTrip1.png) Someone forgot to turn off their app[/caption][caption id=\"attachment_5284\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/07/badTrip2-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/07/badTrip2.png) Clearly a few points missing along this route[/caption]\n\nThe first idea for visualizing these data was trying to quantify the trips by route.  I thought the idea of using a spatial join to aggregate the trips seemed a useful technique.  However, there were a few issues with the data.  Relying on GPS coordinates from a variety (and quality) of smart phones is problematic.\n\n[caption id=\"attachment_5287\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/07/routesRaw-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/07/routesRaw.png) Main St. Routes[/caption]\n\nThe first technique to try would be to expand the road centerlines using buffers to increase the catchment area for the roads.  Then, spatially join the routes to the roads creating a count of unique trips along any roadway section.  After a little trial and error, I settled on 60 foot buffers around all the road segments.  Of course, this still does not capture all the trips as shown the the image below.\n\n[caption id=\"attachment_5291\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/07/bufferIssue1-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/07/bufferIssue1.png) Red lines indicate edge of 60ft buffer[/caption]\n\n## The Visualizations\n\nI ran the spatial join twice, once to capture the total number of trips and once to capture the unique users.  I then took these data from the buffers and joined them back to the road centerline to associate the trip and user counts with the centerline files in order to create something like the image below.\n\n[caption id=\"attachment_5293\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/07/uniqueTrips-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/07/uniqueTrips.png) Unique Trips[/caption][caption id=\"attachment_5294\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/07/uniqueUsers1-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/07/uniqueUsers1.png) Unique Users[/caption]\n\nThis technique creates some interesting artifacts.  When you spatially join relatively inaccurate data like these, some of the aggregated features will end up being associated with features that were not meant to be.  In other words, side streets that were not or lightly used end up with much higher counts than they should have.  Case in point, W. Main St: is it possible that riders deviated to all the side streets and returned back to Main?\n\n[caption id=\"attachment_5297\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/07/spatialJoinProblem11-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/07/spatialJoinProblem11.png) Side street issue[/caption]\n\nOf course, the answer is no and the problem becomes acutely clear when the trip data is overlaid.\n\n[caption id=\"attachment_5296\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/07/spatialJoinProblem2-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/07/spatialJoinProblem2.png) Lack of accuracy[/caption]\n\nSo, the buffer segments of the side streets nearest to W. Main St. were picking up trips along Main St.  I tried several times to deal with this using various techniques and found nothing very helpful.  Next option?  Move to raster analysis.  I used a tool in ArcMap called Line Density to create a surface raster of all the unique trips.\n\n[caption id=\"attachment_5657\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/08/trip2-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/08/trip2.png) Trip density surface raster[/caption]\n\nThe map above clearly shows the most traveled streets in the study are W. Main St., Rugby Rd./Dairy Rd., Alderman Rd., Water St., Market St., Preston Ave., Rose Hill Dr. and 10th St. NE/Locust Ave.  There is also heavy traffic on the path between Rugby and Emmett adjacent to Lambeth Field.\n\nThe phone app also asked people to log the type of trip.  This is mapable!\n\n[caption id=\"attachment_5382\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/07/tripType-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/07/tripType.png) Trips by type[/caption]\n\nAs you can see in the map, commuting seems to outweigh the other trip types.  What about densities for these types?  I decided to make a map with a series of small multiple maps to demonstrate the densities of the four categories of trips I identified.\n\n[caption id=\"attachment_5383\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/07/tripsByType-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/07/tripsByType.png) A series of small multiples - term coined by E. Tufte[/caption]\n\nClearly, there is a lack of data evident in at least one category.  However, I think there is a definite difference between the four types of rides.  This led me to look at the origination and destination of commuting trips.\n\n[caption id=\"attachment_5438\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/08/commuteOrigDest-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/08/commuteOrigDest.png) Originations/destinations raster[/caption]\n\nOther than the fact that downtown is both a hot spot in morning and afternoon for originations and destinations, I am not sure what else can be gleaned from that map.\n\nI also wanted to see how steep slopes compared to the heaviest used routes.  I created a slope layer to show the steep slopes and overlayed the route density layer.\n\n[caption id=\"attachment_5538\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/08/slopes-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/08/slopes.png) Trip density raster overlaid w/ steep slopes[/caption]\n\nAs you can see, there is not a great deal of overlap between the most heavily used routes and high slope areas.  In those areas where there is overlap, most tend to be where the slope is just off the road and does not represent the road grade as along 5th St. Extended or Market St between 2nd St. NW and McIntire Rd.\n\n## Findings\n\nI don't think there are many surprises in this data.  Cyclists tend to take the flattest routes or at least the ones that don't have steep climbs.  W. Main St. is the highest trafficked area.  Locust Ave., Rose Hill Dr. and Rugby Rd. are north/south collectors for that corridor.  This phenomenon has two causes in my opinion.  First, W. Main is the flat, straight corridor between the two largest population/employment centers in Charlottesville, UVa and Downtown.  Secondly, as mentioned earlier, W. Main is the only east/west oriented ridge in the city.\n\nWhat I do find interesting is some of the alternative paths cyclists are taking.  The 8th St. NW connector (one of my favorite short cuts) under the railroad to the 10th and Page neighborhood connects to a parking lot underpass that bypasses W. Main St.  The extension of the parking lot leads to 7th St. SW which leads to a road within a condominium development (Walker Square).  This route extends to Grove St. and Roosevelt Brown Blvd. (and the UVa Hospital).  This path traverses all four quads, takes advantage of three underpasses, and uses very little in the way of busy streets without bike lanes.\n\n[caption id=\"attachment_5638\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/08/alternatePaths-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/08/alternatePaths.png) The four quad alternative[/caption]\n\nHere is a rather heavily used route along the bike/ped pathway from from Ruby Rd. to Emmett St.  This path connects UVa to Barracks Road Shopping Center.\n\n[caption id=\"attachment_5641\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/08/alternatePaths2-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/08/alternatePaths2.png) Lambeth bike/ped path[/caption]\n\nAnother large barrier is the Rivanna River.  The past few years have seen a good deal of development on Pantops east of town.  The only way to get there is over Free Bridge on the US 250 Bypass.  Neeldess to say, that is not an inviting route for cyclists.  There are paths on both sides of the river.  There needs to be a bridge connecting them.\n\n[caption id=\"attachment_5660\" align=\"aligncenter\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2012/08/alternatePaths3-1024x768.png)](http://www.scholarslab.org/wp-content/uploads/2012/08/alternatePaths3.png) Dream crossing for the Rivanna River[/caption]\n\n## Conclusion\n\nThis type of data collection has inherent issues.  Only people with smart phones can participate, which really limits the sample size and variety.  While always improving, there is a lack of locational accuracy with smart phones.  The phone apps require the user to start and stop the app at the appropriate times.  These issues lead to a small amount of  collected data that has to be further slimmed into \"good data.\"  There is also seems to be a distinctive lack of data around UVa.  The timing of the study did not allow for UVa students to fully participate since they were either in exams or on break for a good portion of the study time.  However given a larger sample of riders over a longer period of time, I think some meaningful results would be forthcoming.  I urge the planning commission and local governments to consider more of this type of survey to gather better data.\n\nIf you have some ideas about better ways to visualize or analyze these data, I would love to hear about it.\n"},{"id":"2013-09-16-greetings","title":"Greetings","author":"scott-bailey","date":"2013-09-16 06:09:11 -0400","categories":["Grad Student Research"],"url":"greetings","content":"Hello! My name is Scott Bailey, and I'm one of the new Praxis Fellows. I am also a Ph.D. student in Religious Studies, writing a dissertation on vulnerability as a locus of dogmatic reflection. Taking a cue from Brené Brown's work on vulnerability, I'm asking what it means to think through the vulnerability of Christ, leading us to think of the vulnerability of God and of humanity. Much of this is done within the context of 20th Century Protestant theology, with a particular focus on Eberhard Jüngel's theology.\n\nI am also avidly interested in technology, though, both inside and outside the classroom. For the past three years, I was the Teaching + Technology Support Partner for the Department of Religious Studies, and helped faculty and grad students learn to use and incorporate different applications into their teaching practice. Of particular interest were applications like WordPress and NowComment. I applied to Praxis in order to keep pushing further, to learn about the what makes some of these applications work, and to learn more broadly about the world of Digital Humanities. From just a bit of exposure to HTML/CSS, I've already found that the concrete character of writing code is a welcome balance to the often abstract and speculative questions in theological ontology with which I am concerned. I look forward to working with the other Praxis Fellows and the rest of the Scholars' Lab in the year to come.\n"},{"id":"2013-09-19-praxis-time-capsule","title":"Praxis Time Capsule","author":"cecilia-márquez","date":"2013-09-19 12:22:44 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"praxis-time-capsule","content":"Obviously this is a little late, but comprehensive exams have a way of stealing whatever time you thought you had.  I wanted to write a post that reflected on my time in Praxis, hopefully share a bit about what I am taking away, suggestions I have for future Praxis generations and an opportunity to share some general gratitude!\n\n\n\n\nWhat I’m Taking Away\n\n\n\n\n\n\n\t\n  * \n\n\nNew level of computer/digital literacy: as I’ve mentioned in other blog posts, the way I frequently dealt with computer problems was to either cry, punch my computer, or take a nap.  While I will miss these fantastic coping strategies, Praxis has actually made me someone that other people ask for help with their computers.\n\n\n\n\n\t\n  * \n\n\nClearer sense of what my interest in digital and alt-ac careers might be: I have learned that I hate ruby and love CSS and design aspects of website development.\n\n\n\n\n\t\n  * \n\n\nFuller understanding of what “digital humanities” actually is and how I might integrate it in my own work\n\n\n\n\n\t\n  * \n\n\nExcitement about alt-ac possibilities\n\n\n\n\n\t\n  * \n\n\nNew network of friends and colleagues\n\n\n\n\n\n\n\nSuggestions for Future Praxi\n\n\n\n\n\n\n\t\n  * \n\n\nDon’t put off the hard conversations--take on questions like what is our goal for this tool? What are the major things we want a user to get out of it? Tackle those early and just force yourself to sit in a room until you hammer them out\n\n\n\n\n\t\n  * \n\n\nLearn to love (or at least tolerate) conflict. As someone who has done a lot of “interdisciplinary” work in the past, that was never as hard (or rewarding) as it was in Praxis.  It becomes a tricky balancing act of maintaining an awareness for what is useful/helpful for your discipline and what the tool might be separate from those disciplinary constraints.\n\n\n\n\n\t\n  * \n\n\nThere really is no such thing as a stupid question\n\n\n\n\n\t\n  * \n\n\nThe Scholars' Lab team are beyond lovable so don’t be scared to ask lots of questions.\n\n\n\n\n\n\n\nGratitude\n\n\n\n\nThanks to everyone in the Scholars' Lab faculty and staff who resisted the temptation to roll their eyes at my one millionth question about html and CSS formatting.  Also thanks for helping me work through all of my insecurities about technology and not judging my borderline obsession with Honey Boo Boo.\n\n\n\n\nThanks to the Praxis team.  I learned more about the practice of “interdisciplinarity” with you 5 than I have ever have.  I also learned a lot about collegial and productive conflict from you all. it also affirmed my love of collaborative and team-based work and gave me hope that there are spaces within (or adjacent to) the academy that can be full of laughter and making mistakes openly.\n"},{"id":"2013-09-25-2013-2014-praxis-charter-ratified","title":"2013-2014 Praxis Charter ratified!","author":"stephanie-kingsley","date":"2013-09-25 09:41:25 -0400","categories":["Grad Student Research"],"url":"2013-2014-praxis-charter-ratified","content":"Last week the new Praxis cohort ratified [its charter](http://praxis.scholarslab.org/charter.html).  This important document ended up demanding much more deliberation than we had anticipated.  Nonetheless, after a couple weeks of thinking about what really mattered to us in commencing our program, we established a set of core beliefs and structuring principles which I believe will help guide us through a very exciting year.\n\nWe took [inspiration](http://praxis.scholarslab.org/topics/toward-a-project-charter/) from the previous cohorts' charters in several respects because in many ways we feel we are continuing in the same tradition.  We, too, will conduct our work in the spirit of open source.  We, too, feel that a key part of this experience will be our all sharing credit for the project.  We also hope to learn programing skills central to DH professions, and we plan to launch a digital tool at the end of the year as an outcome of our participation in the program.\n\nA key tenet which is of primary importance to our particular cohort is that of flexibility, and this ideal influences many aspects of our charter.  For instance, we want the tool we build to be adaptable for various scholarly needs.  As of yet, we are in the early stages of conceptualizing this tool, and the issue of flexibility and utility will no doubt arise as we progress.  (I anticipate many reflective blogs to come on that topic.)  Perhaps even more importantly, we plan to be flexible--understanding, sensitive--with each other.  We all come from different scholarly and professional backgrounds, and we all have personal lives with various demands and responsibilities.  It will be our goal to be supportive of each other personally while working together to make the Praxis experience an enriching one for all.\n\nLast Wednesday, Eric and Wayne showed us how to use a text editor, Github, and a rake task to publish the new charter on the Praxis website.  This was our first lesson in programing.  Our brilliant SLab computer mentors encoded the new charter text in Markdown, committed it to [our code repository](https://github.com/scholarslab/praxis), and then let us do the simple--but no-less-important--step of hitting \"Enter.\"  Upon striking the key, watching a whir of yet incomprehensible code flash across the screen, and thus finalizing our newly forged charter, we felt a rush of glee.  In that moment, we had plunged headfirst into the new and intriguing world of Digital Humanities, and we had a charter to guide our voyage.\n"},{"id":"2013-09-25-neatline-2-1-0","title":"Neatline 2.1.0","author":"david-mcclure","date":"2013-09-25 06:55:46 -0400","categories":["Announcements"],"url":"neatline-2-1-0","content":"We're pleased to announce the release of [Neatline 2.1.0](http://omeka.org/add-ons/plugins/Neatline)! This is a fairly large maintenance release that adds new features, patches up some minor bugs, and ships some improvements to the UI in the editing environment. Some of the highlights:\n\n\n\n\n\n\n  * **A \"fullscreen\" mode** (re-added from the 1.x releases), which makes it possible to link to a page that _just_ displays a Neatline exhibit in isolation, scaled to the size of the screen, without any of the regular Omeka site navigation. Among other things, this makes it much easier to embed a Neatline exhibit as an `iframe` on other websites (eg, a Wordpress blog) - just set the `src` attribute on the `iframe` equal to the URL for the fullscreen exhibit view. Eg:\n\n\n\nThanks [coryduclos](https://github.com/coryduclos), [colonusgroup](https://github.com/colonusgroup), and [martiniusDE](https://github.com/martiniusDE) for letting us know that this was a pain point.\n\n\n\n  * **A series of UI improvements to the editing environment** that should make the exhibit-creation workflow a bit smoother. We bumped up the size of the \"New Record\" button, padded out the list of records, and made the \"X\" buttons used to close record forms a bit larger and easier-to-click. Also, in the record edit form, the \"Save\" and \"Delete\" buttons are now stuck into place at the bottom of the panel, meaning that you don't have to scroll down to the bottom of the form every time you save. Much easier!\n\n[![neatline-2.1](http://dclure.org/wp-content/uploads/2013/09/neatline-2.1-1024x580.jpg)](http://dclure.org/wp-content/uploads/2013/09/neatline-2.1.jpg)\n\n\n\n\n\n  * **Fixes for a handful of small bugs**, mostly cosmetic or involving uncommon edge cases. Notably, 2.1.0 fixes a problem that was causing item imports to fail when the Omeka installation was using the Amazon S3 storage adapter, as we do for our faculty-project installations here at UVa.\n\n\n\n\nCheck out the [release notes](https://github.com/scholarslab/Neatline/releases/tag/2.1.0) on GitHub for the full list of changes, and grab the new code from the [Omeka add-ons repository](http://omeka.org/add-ons/plugins/Neatline). And, as always, be sure to send comments, concerns, bug reports, and feature requests in our direction.\n\nIn other Neatline-related news, be sure to check out Katherine Jentleson's Neatline-enhanced essay \"['Not as rewarding as the North': Holger Cahill's Southern Folk Art Expedition](http://www.aaa.si.edu/essay/katherine-jentleson),\" which just won the Smithsonian's Archives of American Art Graduate Research Essay Prize. I met Katherine at a workshop at Duke back in the spring, and it's been a real pleasure to learn about how she's using Neatline in her work!\n"},{"id":"2013-09-30-are-we-gaming-or-just-simulating","title":"Are we gaming or just simulating? ","author":"francesca-tripodi","date":"2013-09-30 09:38:02 -0400","categories":["Grad Student Research"],"url":"are-we-gaming-or-just-simulating","content":"As we [Praxis Program](http://praxis.scholarslab.org/) fellows embark on just what exactly we want our version of [the Ivanhoe Game](http://nowviskie.org/2009/sketching-ivanhoe/) to accomplish we've been doing some \"research\" - good old fashion game playing. However, I've noticed a sense of frustration within our play, or perhaps it is just me that becomes a bit frustrated with the games at this point. I think part of my frustration comes from limited time to devote to the process of the game. Since the rules of the games we've play thus far are limited or nonexistent, I'm often unclear about what exactly I should be doing and feel like I spend more time trying to figure out what's going on than actually contributing. I'm also unsure if my \"moves\" are being received (and subsequently not enjoyed by the team) or just ignored all together. In the end my take away from our games is that it feels like we're all playing our own game alongside one another without communicating what we are doing, why we're doing it, and most importantly why we think it's fun.\n\nWhat I'm getting at here is that we call Ivanhoe a \"game\" but thus far, our gameplay doesn't have any game-like properties. So one might ask, **what is a game?** Well in his [blog](http://tomchatfield.net/2011/01/12/the-difference-between-games-and-toys/), Digital Scholar Tom Chatfield provides a fairly clear definition:  \"A game means submitting to an external set of rules defining particular things you are supposed to achieve: goals, achievements, points, a certain amount of exploration or action, kills, items, whatever.\"  The use of the word \"game\" gets even more specific when applying it to a pedagogical framework. As Gredler (2004) argues, when we create games for learning they should be competitive exercises with the ability to win. Winning, she notes, should be based on existing knowledge or skill sets.  Perhaps this is where we run into trouble. As an interdisciplinary endeavor we seem to lack a common set of skills by which we could create challenges. Moreover, we seem hesitant to assign a leader over our games to outline what exactly is the goal and how one can achieve it. Finally, there lacks the element of competition that Gredler indicates is essential for differentiating between a game and a simulation.\n\nTherefore, I think that what we've been doing up to this point is playing **simulations**. \"Unlike games, simulations are evolving case studies of a particular social or physical reality. The goal, instead of winning, is to take a bona fide role, address the issues, threats, or problems arising in the simulation, and experience the effects of one’s decisions\" (Gredler 573). While simulations are also fun, we must acknowledge the potential constraints of a simulation - most importantly that of time. If our goal is to use Ivanhoe in a classroom, this is less of an issue. Students could figure out their roles, address problems, and figure out the simulation while subsequently learning the topic at hand. However, if we're wanting to create a tool that is used both for pedagogy and in a research setting among peers, time could be an issue. At an already hectic conference will attendees have time to figure out what's going on, what role they should take, and devote time to contributing when their contributions might not be read or acknowledged?\n\nSo a big thing that I think we need to decide on is do we want to create a simulation or do we want to create a game? Perhpas we could do both - in a \"[Type 2](http://stephenramsay.us/2013/05/03/dh-one-and-two/)\" Platform we could provide links to each resource. But, if we think we want to create a game (as well as a simulation), isn't it time we started playing games?\n\nReferences: Gredler, M. E. (2004). Games and simulations and their relationships to learning. In D. H. Jonassen (Ed.), _Handbook of research for educational communications and technology_ (2nd ed., pp. 571-82). Mahwah, NJ: Lawrence Erlbaum Associates.\n"},{"id":"2013-10-07-experimental-typesetting-with-neatline-and-shakespeare","title":"Experimental typesetting with Neatline and Shakespeare","author":"david-mcclure","date":"2013-10-07 06:45:05 -0400","categories":["Research and Development"],"url":"experimental-typesetting-with-neatline-and-shakespeare","content":"_[Cross-posted from [dclure.org](http://dclure.org/?p=3088)]_\n\n\n\n## [Click here to view the exhibit](http://neatline.dclure.org/neatline/show/by-the-pricking-of-my-thumbs).\n\n\n\n\n\nI've always been fascinated by the geometric structure of text - the fact that literature is encoded as physical, space-occupying symbols that can be described, measured, and manipulated just like any other two-dimensional shapes. There's something counter-intuitive about this. When I look at a letter or a word, I see particles of sound and meaning, transcendental cognitive forms, not things that could be straightforwardly described as a chunks of vector geometry. And there's definitely a truth to this - I do think that texts have a kind of extra-physical cognitive essence that's independent of their visual instantiations on pages or screens, and that it's usually this common denominator that’s most interesting when we talk about literature with other people. And yet, in the context of any individual reading, the physical structure of documents - the set of pragmatic decisions that go into the design, layout, and formatting of text on the page - can have subtle but significant effects on how a text _feels_, on the imaginative dreamscape that surrounds it in your mind when you think back on it days or weeks or years after the fact.\n\nThis is definitely true, for example, at the level of some-thing like font selection, which encodes a kind of \"meaning metadata\" about the text - where it comes from, who it's intended for, how serious it imagines itself to be, etc. But I think it also holds at the level of more incidental, pseudo-random aspects of typesetting. For example, how does the vertical line traced out by the right margin of a paragraph or stanza color the reader's affective reaction to the literary content? Does a jagged, unjustified border make the text feel more tumultuous and Dionysian? Would the same text, printed with a justified margin, become more emotionally controlled and orderly? I think of cases like Whitman's long lines, which often have to be prematurely broken at the right edge of the page, resulting in a kind of clumpy, disorganized visual gestalt. I doubt that Whitman intended this, in the strong sense of the word (although I don't know that he didn't), but it has a kind of symbolic affinity with the poetry itself - sprawling, organic, uncontainable. When I think of Whitman, the image that appears in my mind consists largely of this. I wonder what it would be like to read an \"unbroken\" _Leaves of Grass_, printed on paper wide enough to accommodate the lines? Would it become more metaphysical, detached, ironical? Or would it be just the same?\n\nAnyway, this kind of speculation fascinates me. I've been thinking a lot recently about experimental modes of digital typesetting that would be completely impossible on the analog page - new ways of presenting text on screens to evoke certain feelings or model intuitions about the structure of language. As a quick experiment, I decided to use Neatline to try to capture a certain aspect of my experience of reading Shakespeare. I've always been interested in the notion of language as kind of progressive enveloping of words - they're printed side-by-side on the page as equals, but the meaning of a syntagm grows out of the ordering of the tokens. Each exists in the context of the last and casts meaning onto the next; each word is _contained_, in a sense, inside the sum of its predecessors. I was taken by this idea when I read Saussure and company in college, because it seemed to map onto my own experience of reading poetry - the sensation of scanning a line always felt more like a _descent_ than a left-to-right movement, a shift from the surface (the beginning) to the center (the end).\n\nTo play with this, I built a Neatline exhibit that typesets a single Shakespearean couplet in a kind of recursive, fractal, [Prezi](http://prezi.com/)-like layout in which each successive word is \"physically\" embedded inside of one of the letters of the previous word. Reading the couplet literally becomes a matter of magnification, zooming, _burrowing_ downwards towards the end of the syntagm. To scan the fragment, either pan and zoom the environment manually, as you would a regular, Google-like slippy map, or click on the words in the reference text at the bottom of the screen to automatically focus on individual slices of the text.\n"},{"id":"2013-10-09-chartering-our-project","title":"Chart(er)ing Our Project","author":"veronica-ikeshoji-orlati","date":"2013-10-09 10:18:57 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"chartering-our-project","content":"This post is a couple weeks late, but I would still like to share a few thoughts on the process of drawing up our [2013-14 Praxis charter](http://praxis.scholarslab.org/charter.html). Though we had the eloquent and insightful charters of the previous two cohorts as models, the Scholars' Lab faculty encouraged us to think about what_ our_ goals, principles, and priorities were and how _we_ could best articulate them. Reflecting on what we wanted to accomplish this year – 'accomplish' being itself a difficult term to define – was more challenging than anticipated, but the result is a charter which – in my opinion – lays out a path for all of us to learn, play, think, and work together amicably and productively.\n\n[Stephanie](http://www.scholarslab.org/grad-student-research/2013-2014-praxis-charter-ratified/) has already discussed some of the specific aspects of our charter, but there are a couple which I would like to elaborate on further. Therefore, I'll throw in my two cents on our goals as we did and why 'flexibility' is one of our core values.\n\nWe organized our goals by what we all wanted to get out of our year as Praxers – a solid grounding in DH, an interesting tool for our scholarship, and an improved handle on how to harness the power of our social networks. Personally, _what_ DH is, _how_ it works, and its role in the humanities and social sciences as a whole are my primary questions. Specifically, one of the things which interests me is the role of databases as tools for building knowledge and not just as repositories of 'preformed' knowledge. By learning what computers can actually do – and how their capabilities have been applied within the humanities and social sciences – I hope to get a better grasp on what I can do with data in my own field.\n\nAnd that would have been the end of my charter. Fortunately, however, I am working with a dynamic cohort of Praxers and Scholars' Lab faculty, and so the whole 'outreach' thing is laid out as a top priority, too. To be clear, I believe in working collaboratively, in public, and getting feedback throughout the process. However, I'm terrified of social media and my own insecurities have driven me to a back-seat position on the web, watching the fantastic on-going work of other people and then scurrying back to my private little hole (in the basement, no less) to obsess about my own research until it is perfect (or, more likely, I get completely stuck). Now, talking in person about my ideas and research is easy – I know my audience and there is no permanent record of the ridiculous things I say. Putting ideas out on the internet and accepting that they will never really go away completely...well, that's scary. Hence this blog post is three weeks late.\n\nFinally, a quick note on our core value of 'flexibility.' It is worth noting that we came to this concept through various routes – we wanted a 'flexible' and multidisciplinary tool, we wanted to be 'flexible' with one another, we wanted to be 'flexible' with our definition of the Ivanhoe Game and how we would approach building it. I think this ethos really is fundamental to our group-definition, and I hope it is reflected in our work throughout the year.\n"},{"id":"2013-10-09-forming-norming-storming-performing","title":"Forming, Norming, Storming & Performing ","author":"francesca-tripodi","date":"2013-10-09 10:55:13 -0400","categories":["Grad Student Research"],"url":"forming-norming-storming-performing","content":"This is my first time on a group project of this nature and as the project begins to take shape I am reminded of \"The Tuckman Model.\" Created by Bruce Tuckman in 1965, I can't help but see the relevancy even though his findings are nearly fifty years old. For those unfamiliar with this article, here is a lovely image of his main points borrowed from [SHIFT-IT Coach](http://www.shift-it-coach.com/tag/tuckman-model/) (however the \"adjourning\" stage wasn't part of the original 1965 model, that part wasn't added until the 1970s).\n\n[![tuckman-model](http://www.scholarslab.org/wp-content/uploads/2013/10/tuckman-model-300x177.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/10/tuckman-model.jpg)\n\nRight now I feel like we are definitely in the forming stages. Pleases and Thank Yous still fill our conversations and we seem to be operating as our most polite selves. Rarely do people interrupt one another and I often find myself nodding in appreciation while others speak (even if I might not actually agree with what they are saying...).\n\nI was compelled to write this post because, according to Tuckman, if we are ever going to move forward with our project and start \"performing\" our current formalities must begin to fad. While I'm not suggesting we move toward a path of rudeness, I hope that a leader among us will eventually emerge. When this happens, resistance from the group is bound to occur as it is unlikely that we will all agree on a direction for the project. Moreover, as we begin to figure out just exact it is that we want to build, our individual preferences will also begin to surface and clash with others in the group. This stage of development might not be the most pleasant but I hope my post helps to spur a bit of storming among us. For until we can articulate and battle out our actual desires, and subsequently overcome these obstacles, we won't be able to achieve our goals (whatever they might actually end up being).\n\nReferences: Tuchman, Bruce W. (1965). \"Developmental Sequence in Small Groups.\" _Psychological Bulletin_. 63(6): 384-399\n"},{"id":"2013-10-09-more-fun-with-interactive-typesetting-a-coat-by-yeats","title":"More fun with interactive typesetting: \"A Coat,\" by Yeats","author":"david-mcclure","date":"2013-10-08 20:27:03 -0400","categories":["Experimental Humanities"],"url":"more-fun-with-interactive-typesetting-a-coat-by-yeats","content":"_[Cross-posted from [dclure.org](http://dclure.org/essays/more-fun-with-interactive-typesetting-a-coat-by-yeats/)]_\n\n\n\n## [Click here to view the exhibit](http://neatline.dclure.org/neatline/show/a-coat).\n\n\n\n\n\nAfter spending the weekend [tinkering around with an interactive typesetting of a couplet from Macbeth](http://dclure.org/essays/experimental-typesetting-with-neatline-and-shakespeare/) that tried to model reading as a process of zooming _downward_ towards the end of the phrase, I became curious about experimenting with the opposite analogy - reading as an _upward_ movement, an climb from the bottom (the beginning) to the top (the end), with each word circumscribing everything that comes before it. Really, this is just the flip side of the same coin. Meaning certainly flows \"downhill\" in a phrase - each word is informed by the previous word. But it also flows back \"uphill\" - each word casts new meaning onto what comes before it. What would it would feel like to visualize that?\n\nThis time I decided to work with \"A Coat,\" Yeats' wonderful little ode to simplicity (he renounces what he thinks to be the stylistic affectation of his work from the 1890’s, and announces an intention to write \"naked[ly]\"). Originally, I planned to exactly invert the layout of the Macbeth couplet - start with the \"I\" at the bottom of the stack, and work upwards towards the end with \"naked,\" which, in the final frame, would geometrically contain each of the preceding words. I started to do this, but quickly ran into an interesting computational obstacle, which actually cropped up in the Shakespeare example as well.\n\nTrip Kirkpatrick noticed the problem:\n\n\n\n<blockquote>[@clured](https://twitter.com/clured) Are jaggies on deepest zoom artifacts results of zoomed pixels or aesthetic decisions? Both?\n> \n> -- Trip Kirkpatrick (@triplingual) [October 7, 2013](https://twitter.com/triplingual/statuses/387254660563992576)</blockquote>\n\n\n\n\nIndeed, the last two words - \"way\" and \"comes\" - are pixelated and malformed:\n\n[![way](http://dclure.org/wp-content/uploads/2013/10/way-300x151.jpg)](http://dclure.org/wp-content/uploads/2013/10/way.jpg)\n\n[![comes](http://dclure.org/wp-content/uploads/2013/10/comes-300x93.jpg)](http://dclure.org/wp-content/uploads/2013/10/comes.jpg)\n\nThis wasn't on purpose - I couldn't figure out why it was happening when I was working on the exhibit, but decided against trying to fix it, half out of laziness and half because the visual effect had some satisfying affinities with the content of the line, especially when paired with the \"descending\" motif - a plunge down to hell, where order disintegrates, smooth lines are forbidden, etc. Anyway - after thinking it over, I'm pretty sure I know what's going on, although I'm not certain. At the extremely deep zoom levels (far beyond anything you'd ever need for a regular map), I think that OpenLayers is actually losing the floating point precision that it needs to accurately plot the SVG paths for the letters - the computer is running out of decimal places, essentially.\n\nI squeaked by with the Macbeth couplet, but this turned out to be a showstopper for the Yeats, since I was effectively trying to plot geometry about four and a half times deeper - 45 words versus 11. At that depth, the text becomes completely illegible, so I had to find a way to squeeze more content into fewer zoom levels. In the end, I managed to fit it all in by positioning each line into a geometric “notch” formed by the ascenders of two letters on the following line, which more or less preserves the philosophical rationale of the exhibit (each bit of text \"envelops\" the previous, if somewhat less completely than before) while limiting the zooming to just ten magnification contexts, one for each line.\n\n[![heel](http://dclure.org/wp-content/uploads/2013/10/heel-300x114.jpg)](http://dclure.org/wp-content/uploads/2013/10/heel.jpg)\n\nTo scan the poem, just zoom out by clicking on the \"minus\" button (or scrolling the mouse wheel or pinching the screen, if applicable), or click on the lines in the reference text at the top left to auto-focus on a particular part of the poem.\n"},{"id":"2013-10-10-on-games-that-just-fizzle-a-chronicle-and-reflection","title":"on games that just fizzle - a chronicle and reflection","author":"scott-bailey","date":"2013-10-10 17:43:02 -0400","categories":["Grad Student Research"],"url":"on-games-that-just-fizzle-a-chronicle-and-reflection","content":"For the past few weeks, we've been trying to get a feel for what Ivanhoe is and what one does when playing Ivanhoe. Francesca has already [posted](http://www.scholarslab.org/grad-student-research/are-we-gaming-or-just-simulating/) on the question of whether we are gaming or actually running simulations, so I'd like to focus on something else: the various media through which we have tried to play Ivanhoe games and how those games gently fizzled out.\n\nOur first game was a short-lived chaos of editing and creative additions to Poe's \"The Tell-Tale Heart,\" carried out through a wiki. The medium, I think, determined much of how we played. We had a clean copy of Poe's text, and then with various perspectives and goals we quickly turned the story into a perhaps painfully complex, psycho-analytic thriller, or a deeply tragic therapy session. Or something else. By the end I wasn't sure, as our interventions directly altered and sometimes made incoherent the additions and edits of others. I think it was in part this experience that lead to some of Francesca's questioning, though that concerned our more recent games as well. The wiki, as a platform or tool, or place of engagement, created only a single space within which to work, a single page, a single text which shifted and changed, where there was no sanctity to any move. Any text could be edited, and with every addition or edit the immediate horizon of the text's interpretation changed. And the game fizzled out.\n\nIn meetings shortly after that game gave up the ghost, we discussed other media through which to play Ivanhoe - tumblr, twitter, and the ever-humble email. As well, we looked at reasons why the first game fizzled, especially at the lack of commitment to and interest in Poe's text that some felt. We gathered then to discuss topics or categories for new games that would be played with smaller groups, three or four people only.\n\nFrom there we began four games: an email game related to science-fiction, an email game on \"citizen journalism,\" a twitter game on the second amendment, and a tumblr game on Wallace Stevens' \"Anecdote of the Jar.\" Only the first two truly took off, while the latter two are, I hope, waiting quietly for their time in the spotlight. But those first two _did_ take off, with flurries of posts, enlightening creativity, and clear evidence of **interest** and **commitment**.\n\nThen they fizzled.\n\nAnd I think that's okay, for a couple of reasons. One, as we've been told, many of the early games fizzled after a certain point, where players lost interest, got busy, or just couldn't quite figure out what was left to say or do. No golden move was made that wrapped up all the various interventions from each player in a perfectly elegant fashion. Two, as an experiment, as a endeavor to figure out how different media affect the style of game play, both email games were clear successes. We know now that people are far more likely to play regularly if they receive notifications in their inboxes, places in which they dwell throughout the day. We also know that the discrete, differentiated character of emails encourages moves that perhaps stay more coherent and consistent within a single role per person and provides for some degree of protection for each intervention. An edit or intervention in someone else's move shows up necessarily as another move, another intervention, while in a wiki it could, unless one looks at the version control, show up not at all.\n\nWe've learned. We've gained some knowledge about what features, like a notification system, might need to be built into our own project. As we move forward in the next few weeks, storyboarding and wire-framing, I look forward to seeing what else reveals itself about the Ivanhoe game.\n"},{"id":"2013-10-11-song-of-wandering-aengus","title":"\"The Song of Wandering Aengus,\" Neatline, and negotiation with the machine","author":"david-mcclure","date":"2013-10-11 07:43:03 -0400","categories":["Experimental Humanities"],"url":"song-of-wandering-aengus","content":"_[Cross-posted from [dclure.org](http://neatline.dclure.org/neatline/show/song-of-wandering-aengus)]_\n\n\n\n## [Click here to view the exhibit](http://neatline.dclure.org/neatline/show/song-of-wandering-aengus).\n\n\n\n\n\nOne last little experiment with Neatline-powered interactive typesettings - this time with the ending of Yeats' endlessly recitable \"The Song of Wandering Aengus,\" which, like many great poems, seems to somehow signify the entire world and nothing really in particular. I chose to use just the last three lines so that it would be possible to play with a more dramatic type of geometric nesting that, with more content, would quickly run up against the technical limitation that I mentioned in [Wednesday's post about \"A Coat\"](http://dclure.org/essays/more-fun-with-interactive-typesetting-a-coat-by-yeats/) - the vector geometry used to form the letters starts to degrade as the Javascript environment runs out of decimal precision at around 40 levels of zoom, making it impossible to continue the downward beyond a certain point.\n\nWith just three lines, though, I was able to place each consecutive line completely inside of one of the dots above an \"i\" in the previous line. So, the \"silver apples of the moon\" are inscribed into the dot over the \"i\" in the \"till\" of \"till time and times are done,\" and the \"golden apples of the moon\" are then contained by the dot on the \"i\" in \"silver.\" Since the nested lines are placed literally inside the shaded boundaries of letters (as opposed to the empty spaces delineated by the \"holes\" in letters, as was the case with the [first](http://dclure.org/essays/more-fun-with-interactive-typesetting-a-coat-by-yeats/) [two](http://dclure.org/essays/experimental-typesetting-with-neatline-and-shakespeare/) experiments), the color of the text has to alternate in order to be legible against the changing color of the backdrop. What I didn't expect (although in retrospect I guess it's obvious) is that this shift in the color palette completely modulates the visual temperature of the whole environment - the backdrop swerves from bright white to solid black back and then back to white over the course of the three lines, with the last transition mapping onto the night-to-day, moon-to-sun thematic movement in the final couplet.\n\nInterestingly, this effect was almost thwarted by another unexpected quirk in the underlying technologies, although I managed to maneuver around it with a little hack in the exhibit theme. The problem was this - it turns out that OpenLayers will actually stop rendering an SVG geometry ones the dimension of the viewport shrinks down below a certain ratio relative to the overall size of the shape. So, in this case, as the camera descends down into the black landscape of the dot over the first \"i,\" the black background supplied by the vector shape would suddenly drop away, as if the camera were falling through the surface, which of course had the effect of making the second-to-last line - typeset in white - suddenly invisible against the default-white background of the exhibit.\n\nI thought this was a showstopper, but then I realized that I could programmatically \"fake\" the black background by directly flipping the CSS `background` color on the exhibit container. So, I just fired up the Javascript console, inspected the `zoom` attribute on the OpenLayers instance to get the zoom thresholds where the color changes needed to happen, and then dropped a little chunk of custom code into the [exhibit theme](https://github.com/davidmcclure/neatlight/tree/master/neatline/exhibits/themes/song-of-wandering-aengus) that manifests the style change in response to the zoom events triggered by the map:\n\n\n\nWeird, but effective. Whenever I work on projects like these I'm fascinated by the wrinkles that arise in the interaction between what you want to do and what the technology allows you to do. It's very different from analog scholarship or art practice, where you have a more complete mastery over the field of play - you have a much more direct and unmediated control over the sound of your words, the shape of a line in a physical sketch, the pressure of a brush stroke. With digital objects, though, you're building on top of almost unimaginably huge stacks of technology - the millions of man-hours of work that it took to create the vast ecosystem of Javascript and PHP libraries that Neatline depends on, the whole set of lower-level technologies that shape the underlying browser rendering engines and Javascript runtimes, which in turn are implemented in still lower-level languages, which eventually brush up against the dizzying rabbit hole of physical hardware engineering, which to my mind is about as close to magic as anything that people have produced.\n\nThat kind of deep, massively-distributed collaboration can definitely exist offscreen (eg., all of intellectual history, in a sense), but it's more loosely coupled, and certainly less fragile - if I write an essay about Yeats, Yeats can't _break_ in the way that a code dependency literally can (will). At first this really bothered me, but I've come to peace with it - digital work is by definition a relinquishing of control, a give-and-take with the machine, a negotiation with our current little slice of modernity about what's possible.\n"},{"id":"2013-10-14-more-musings-on-tuckman","title":"More Musings on Tuckman...","author":"stephanie-kingsley","date":"2013-10-14 13:49:46 -0400","categories":["Grad Student Research"],"url":"more-musings-on-tuckman","content":"Last week [Francesca posted](http://www.scholarslab.org/grad-student-research/forming-norming-storming-performing/) on Bruce Tuckman's model of group work on a collaborative project.  To recap, a group passes through four phases: Forming, Storming, Norming, Performing, and Adjourning.  I find this an intriguing model, and I agree with Francesca that we are currently in a sort of \"Forming\" stage, but I also find myself revisiting our work on the charter a few weeks ago and thinking, \"Wasn't there a some Storming going on then?\"  It was relatively courteous storming, but I think in working to establish our deepest desires for this project and the experience of the coming year, frustrations did occur and personalities clashed.  Encouraged by our SLab mentors, we determined to hastily conclude work on the charter and thus enabled ourselves to focus on game play, which we did for a spell with a vigor which I would consider... Formative Performing.  Now that we spend more time in the less emotionally charged world of command line and CSS, we have relaxed, are peaceful, take turns speaking, and have probably slipped into more casual Forming; but there was disagreement in the charter-writing days, and I wonder if Tuckman's model might not require some rethinking.\n\nI envision a model of group work where we can view the team as passing through Forming, Storming, Norming, Performing, and Adjourning; but perhaps a group may need to work through the various stages more than once depending on what it is doing.  I think one of the virtues of our group is that we are all opinionated and feel strongly.  On the one hand, that can lead to the sorts of clashes we experienced with the charter, but on the other, we have a fabulous team of six passionate scholars whose cumulative ideas are certain to birth greatness by the end of the year!... To recover from that moment of enthusiastic wordiness, in short, I think our group will likely be revisiting the Storming stage when we get into determining what our Ivanhoe game will be.  Then we will make a decision, settle down, see how our various strengths will help the project as a whole and define individual roles, and create a plan (Norming?). Then we can begin Performing once again.\n\nThis is a wonderful model to consider, and I applaud Francesca for bringing it to bear and encourage Storming.  It is easy to look on conflict as negative, but Tuckman incorporates it as a healthy part of group growth and development, and I look forward to the stormy days ahead as the forerunners of brilliant productivity.\n"},{"id":"2013-10-14-nitle-presentation-on-geotemporal-storytelling-with-neatline","title":"NITLE Presentation on Geotemporal Storytelling with Neatline","author":"jeremy-boggs","date":"2013-10-14 06:20:58 -0400","categories":["Digital Humanities","Geospatial and Temporal"],"url":"nitle-presentation-on-geotemporal-storytelling-with-neatline","content":"About this time last year, [David McClure](http://www.scholarslab.org/people/david-mcclure/) and I had a great conversation with the folks from the [National Institute for Technology in Liberal Education (NITLE)](http://www.nitle.org/) about geotemporal storytelling with [Neatline](http://neatline.org). We had lots of great questions and comments from the audience, too. Video for the talk is now available on NITLE's YouTube channel:\n\n\n"},{"id":"2013-10-16-a-review-of-the-suffragette-game","title":"A review of the suffragette game","author":"stephanie-kingsley","date":"2013-10-16 03:55:51 -0400","categories":["Grad Student Research"],"url":"a-review-of-the-suffragette-game","content":"I think [Scott's review](http://www.scholarslab.org/grad-student-research/on-games-that-just-fizzle-a-chronicle-and-reflection/) of our Ivanhoe game play is insightful and timely, and it has inspired me to consider the once-thriving-but-now-dormant \"citizen journalism\" game.  I think it's time to really look into the successful games and, aside from the fact that we could see what was going on more immediately via the email medium, examine what else contributed to their being taken up with such vim.  I think an element of fun is paramount.  The sci-fi people (Bethany, Eliza, Eric, Scott, and Veronica all contributed) played a game on _[Galaxy Quest Chompers](http://www.youtube.com/watch?v=gqRdT8m1Suo) _entitled \"Sci-fi Fun,\" and the journalism people (Eliza, Francesca, Zach, Veronica, and I) began playing around with--and even revising--a major moment in history: c. 1909 women's suffrage in England.  As the beginner--and as of right now, the finisher--of this game, I would like to review the major moves made and directions our game took in anticipation of building on them in Round 2 of game play.\n\nThe premise: a mysterious first-person narrator discovers a news clipping with [this photograph](http://www.xtimeline.com/__UserPic_Large/6851/ELT200805050915472469693.JPG) in her mother's trunk.  The narrator says, \"She had always fallen strangely silent when asked about the movement; perhaps now I might finally learn more about that momentous day in her life.  I began to read.\"  This was all presented in email form, with the photograph pasted into the email.  The idea was to prompt reflections on the photograph, contributions to what the article might have said, and explanations for why the mother wouldn't want to talk about the movement.  I figured that such a setup was open-ended enough to allow for creativity, topically engaging and therefore open to critical evaluations regarding feminism and history, and **focused--**which the Poe game was not.\n\nEliza rapidly authored a wonderful article on the suffragette plot to assassinate Prime Minister Asquith--which I thought was a colorful fabrication until she informed me that it was a true event.  Because I had framed the game in fiction, I assumed that subsequent moves would be fictional; from the get-go, the line between history and alternative history had been blurred.  In the spirit of fun, I concocted a secondary plot line in which Asquith had an affair with suffragette Mary Leigh--totally fictitious.  I constructed this story in the form of telegrams and letters the narrator, Mary Leigh's daughter, discovers in her mother's trunk.\n\n[![telegraph_H to M](http://www.scholarslab.org/wp-content/uploads/2013/10/telegraph_H-to-M-300x175.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/10/telegraph_H-to-M.jpg)\n\nThen, Mary's daughter by Asquith (Etta and the narrator) falls ill, and she writes him about her illness (fictional); Mary gets thrown in prison, engages in a hunger strike, and gets force-fed (contributed by Zach and true); and Asquith's wife and his daughter Violet get involved in the suffrage movement (contributed by Veronica and probably fiction?).  Francesca contributed an [interview with Mrs. Banks,](http://www.youtube.com/watch?v=Kvk1NZDFvZU)) a delightful homage to _Mary Poppins_; Mary Leigh seeks Poppins' services for Etta; and all the women in Asquith's life--wife, mistress, and daughters--band together in the end for the sake  of Women's Suffrage (my move and meant to be a Golden Move of sorts, as the game by that point was indeed \"fizzling\").\n\nThe game clearly was a blend of delving into real history and absurdity.  A debate took place in one of our Praxis meetings shortly after the game had begun: was there a use to engaging in alternative history in this way?  The conclusion was that it depended on your purpose: if the goal is to establish fact in a historical research project, then sticking to fact is best; if, however, you are attempting to think in more outside-the-box ways about history, then breaking into alternative history could inspire new ways of interpreting a particular historical moment.\n\nIn analyzing my game play, I realize that I was consistently attracted to ironies: the irony of Mary Leigh trying to influence Asquith to give women the vote by seducing him, the irony of her leaving her daughter every day and seeking Mary Poppins' assistance while she protested for that same daughter's rights (\"Our daughters' daughters will adore us...\"), etc.  This was not to impose a particular criticism on this moment in history, but rather to parse out experiential elements--hypothetical effects these women's public lives could have had on their personal lives and vice versa.  After all, even in _Mary Poppins, _Mrs. Banks asks her maid to hide their suffrage ribbons as she hears Mr. Banks at the door, saying, \"You know how they infuriate him.\"\n\n__Francesca's contribution of _Mary Poppins _was lighthearted and fun, but it got me to thinking about Mrs. Banks, so enthused about the movement that Katie Nanna's giving notice (where the video breaks off) and the need to find a new governess to care for the children is an interruption.  We could have stayed more grounded in our Ivanhoe, but I think that by blending historical with alt-historical moves we ended up with a provocative game which, for me at least, prompted some interesting and troubling musings on gender politics.  Altogether, I think it was a very successful Ivanhoe, and I look forward to the ideas new games may produce.\n"},{"id":"2013-10-17-welcoming-purdom-lindblad-laura-miller","title":"Welcoming Purdom Lindblad & Laura Miller!","author":"bethany-nowviskie","date":"2013-10-17 04:33:40 -0400","categories":["Announcements"],"url":"welcoming-purdom-lindblad-laura-miller","content":"With great pleasure, I write to announce two wonderful additions to the Scholars' Lab team!\n\nPurdom Lindblad will join us on December 30th as our much-anticipated Head of Scholars' Lab Graduate Programs (including, among other programs, [Praxis](http://praxis.scholarslab.org/) and the [Praxis Network](http://praxis-network.org/)), and Laura Miller will shortly take on a dual position coordinating public programs in the SLab and assisting me with strategic planning for digital humanities in the UVa Provost's office.\n\n[caption id=\"attachment_8964\" align=\"alignleft\" width=\"300\"]![Purdom Lindblad](http://www.scholarslab.org/wp-content/uploads/2013/10/purdom-300x200.jpg) Purdom Lindblad[/caption] [caption id=\"attachment_8963\" align=\"alignleft\" width=\"300\"]![Laura Miller](http://www.scholarslab.org/wp-content/uploads/2013/10/laura-300x225.jpg) Laura Miller[/caption]\n\n[Purdom](https://twitter.com/Purdom_L) comes to us from Virginia Tech, where she works as the College Librarian for Humanities and Digital Humanities. There, she has radically expanded DH project support and programming, organized graduate student open-access events, and helped to establish Port, Tech's digital research commons. Purdom has taught at both the undergraduate and graduate level. Her current course on Religion in America involves students in an interactive exhibit in the library. She is active at the Byron Society of America, is co-PI of an ACH micro-grant project to produce [dm4dh](http://dm4dh.org), a blog and podcast on data management in the humanities, and studied at Doshisha University in Kyoto as the winner of a distinguished Monbukagakusho scholarship. Purdom holds an MA in American Studies from Michigan State University (where she worked at MATRIX, MSU's digital humanities center) and an MS in Information from the University of Michigan. Purdom describes herself as an open access advocate, an \"abuser of exclamation points, and a knitter (stereotype!).\"\n\nLaura Miller comes to the Department of Digital Research & Scholarship from a central role as Assistant to the Director in UVa's [Brown Science and Engineering Library](https://www.facebook.com/pages/Charles-L-Brown-Science-and-Engineering-Library/368675533563), where she provided public service, outreach, and research instruction, project management, and user needs assessment for both space and interface design. Last year, she coordinated a successful Learning Spaces Renewal Project across four of UVa's libraries. Laura is also co-creator of the Library's [data management portal](http://dmconsult.library.virginia.edu/research-and-development-initiatives/) for UVa graduate students, and recently presented on the subject at a J-term Data Management Bootcamp for Grad Students (a partnership with Virginia Tech). Laura holds a BA in English from William and Mary and an MS in Library and Information Science from Florida State. She has a past life in science-fiction book publishing and her research interests include usability and iterative, user-centered design. Laura also reports an interest in \"quirky podcasts, negotiating with a three year-old, and live music of (almost) any sort.\"\n\nStaff, students, and collaborating faculty in the SLab are hugely excited about the transformative impact Laura and Purdom will have on our work. I hope you will join me in welcoming them! \n"},{"id":"2013-10-21-2013-gis-day-wednesday-november-20th-2013","title":"2013 GIS Day - Wednesday, November 20th, 2013","author":"chris-gist","date":"2013-10-21 09:18:33 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"2013-gis-day-wednesday-november-20th-2013","content":"In honor of [GIS Day 2013](http://www.gisday.com/), the Scholars’ Lab at UVa Library would like to invite you to participate in our celebration on November 20th.\n\nStarting at 1:30PM in the Scholars’ Lab, there will be a round of lightning talks followed by the cutting of the GIS Day cake.  We encourage everyone, including students (UVa, PVCC and high school), in the Charlottesville GIS community to contribute.  If you have never seen lightning round talks, they can be pretty entertaining: a rapid fire succession of speakers given a set, short amount of time and PowerPoint slides.  In previous years, we've had many great presenters giving a powerful message by showing the breadth of disciplines and fields in which GIS is used.\n\nIn this year’s round, each speaker will be given five minutes with a maximum of ten slides.  It is a fairly easy task to create and give a lighting round talk (assuming you can deal with time constraints).  Help make this year’s event special by participating in the talks.  You can present on anything spatially related you like.  It could be about a project you have worked on, things going on at your office or just something of personal interest.\n\nIf you wish to participate in the lightning round talks, please email [cgist[at]virginia.edu](mailto:cgist@virginia.edu).  If you cannot participate, please come to enjoy the talks and GIS Day cake and see what we concocted this years’ cake.  [Previous years’ cakes](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/) are documented on our blog.\n\n![2010 cake](http://www.scholarslab.org/wp-content/uploads/2011/11/2010-cake-300x225.png)\n\nWe will wrap up the event by 3:30PM.\n\nPlease let [Chris Gist](mailto:cgist@virginia.edu) know if you have any questions or thoughts.\n"},{"id":"2013-10-21-building-dwelling-coding","title":"Building, Dwelling, Coding","author":"scott-bailey","date":"2013-10-21 06:55:37 -0400","categories":["Grad Student Research"],"url":"building-dwelling-coding","content":"Over the past week, I've become rather occupied with customizing Sublime Text 2, the editor in which I've been writing code. I've been adding packages to make it work more efficiently, to cut down on the number of keystrokes, to make it so my fingers rarely have to leave the keyboard. With [Jeremy's](http://clioweb.org/) help, I created a symbolic link between the terminal and Sublime, so I can open files in Sublime from the terminal itself. I've spent several hours watching workflow videos, trying to figure out what habits I need to cultivate to write better code more efficiently. And some it is wonderful. Why write vendor prefixes yourself when you can run your CSS through [Prefixr](http://prefixr.com/) with a simple keyboard shortcut? Why type out the basic html structure, when you can use [Emmet](http://emmet.io/), type !, and just hit tab to have it created? There are so many shortcuts to take advantage of, built in to Sublime and through the rich environment of plugins available, so many ways to be that much more efficient.\n\n\n\n* * *\n\n\n\nIn his well-known 1951 essay, \"Building, Dwelling, Thinking,\" Martin Heidegger, with a mix of etymological work and analysis of everyday life, draws our attention to the fundamental interconnection of the spaces we create through building and the way in which those built spaces reflect and determine how we exist and think within those spaces. Without stepping into the long-running and difficult debate on what precisely Heidegger means by the fourfold, the gods, mortals, earth, and sky, we can still take a few quick lessons from Heidegger's insights.\n\nIn acts of building, we affect and determine spaces themselves, physically and semantically. We intervene creatively in our landscape, adding material objects which become part of the web of meaning that any person enters when existing within that space. Taking the space itself as a phenomenon, we could say that acts of building alter the structure of the phenomenon, such that it presences differently and its semantic field is shifted. As we enter into this built space, we exist correspondingly within it, and if we can _dwell_, if we can let be what is, giving that which is the case the space in which to be, our own existence will be shaped toward allowing the freedom of all that is.\n\nIf this seems a bit mystical, it is. Much of Heidegger's late work, especially on _Gelassenheit_, bears the trace of Meister Eckhart and other Christian mystics, so focused on a type of being and thinking that runs counter to what will later be called the dominating subjectivity of the transcendental ego. It must be remembered, though, that Heidegger, from the beginning to the end, set out to describe concrete human existence.\n\n\n\n* * *\n\n\n\nWhenever I've taught, especially working in constructive theology and in ethics, I've encouraged my students to take time to think, to give thinking time, as much time as it takes, and to keep giving the thought time as it shows itself in writing. My philosophical and theological heritage is one of thinkers who tended to have one really good thought that they remained with for decades, describing its contours and connections again and again, circling back around repeatedly, building an ever thicker description of some fundamental reality. I've taken the first step on that road myself, for over a year now trying to think one thought about vulnerability as a fundamental structure, ontic and ontological, in human existence.\n\nBut.\n\nIn modifying Sublime Text differently this week, building out the structure itself and learning the new signs of this environment, this place, I have been building a space that allows one to dwell in it through the quick tapping of a few keys. A space that encourages a form of dwelling suited more to the finishing of a product, toward thinking that thinks in short deadlines, in concrete results, in operability. We are, in Praxis, building a tool to be used after all. It is also a space, though, that will hopefully become something more like a home, something like Heidegger's Black Forest hut, in which he found himself free, with the space to let himself be himself. It is hopefully the case that the careful habituation of fingers to keyboard shortcuts, the learned muscle memory that navigates the signs of this built space, will give me space and freedom to write code simply as myself, to let myself think in code, and, as an [upcoming conference](http://codespeak.scholarslab.org/) here at the Scholars' Lab puts it, to let myself speak in code.\n\nIf this does turn out to be the case, as I'm sure I'll report on some months down the road, then it will have been the case that these two forms of dwelling, these two forms of thinking, will not have been so very different after all.\n"},{"id":"2013-10-21-shut-up-legs-or-jens-voight-and-why-i-applied-for-praxis","title":"Shut up Legs! or: Jens Voight and Why I Applied for Praxis","author":"zachary-stone","date":"2013-10-21 07:05:17 -0400","categories":["Grad Student Research"],"url":"shut-up-legs-or-jens-voight-and-why-i-applied-for-praxis","content":"So. In [our charter](http://praxis.scholarslab.org/charter.html) we committed to a goal we called ‘Outreach.’  As part of that commitment I pushed for a firm statement of two blog posts per person per month. This seemed reasonable at the time, and still does. And yet, here we find ourselves on October 16 and, discounting my [introductory post](http://www.scholarslab.org/grad-student-research/a-bit-more-medieval/) I have yet to surface online.  This would be the moment to play the grad student ‘woe is me I am so busy grading student papers reading for orals having existential crises etc. etc.’ card but, unfortunately in my case it simply isn’t true.\n\nDeeper still, I try to convince myself (and thought about trying it on you kind folks) that I really need to be careful about what I say online as who knows what job committee Googling might turn up.  Again, though, this masks my deeper anxieties about not only blogging and DH, but also the entire practice of the interwebs. The reason I know I have time to blog is that I have time to read not 1, or even 3, but 6 cycling related blogs religiously.1\n\nWhile I have enjoyed the first fruits of the DH revolution (or whatever DH’ers call the last fifteen odd years) in my professional work, the very same technologies that enable those ventures imperil my own work.  I am not being over dramatic.  Nor am I casting aspersions on the Internet as an institution. No, like Eliot, I am distracted from distraction by [Jens Voigt](http://www.tumblr.com/tagged/jens-voigt) memes (or [facts](https://twitter.com/JensVoigtFacts)).2 The fault is all mine but the effect is real. Nor do I think I am alone. Judging by the number of cat/Ryan Gosling memes I see on the Facebook, I fear that the scholarship of an entire generation is being threatened by anything.GIF.\n\nIn my fear of my PowerBook (or really any of Apple’s techno-crack that brings custom bike pictures straight to my lustful gaze in so many cunning ways), though, lies the root of my interest in the Praxis program.  I am not dumb. I cannot turn back the clock to a predigital age (though the Wifi in many of the medieval libraries I haunt is refreshingly antique), nor do I want to. What I need to learn, what I want to learn, is how to be both digitally engaged and human. How to make the machines work for me and how to think along with emerging technology and not against it.\n\nThree plus years deep in the [_Hors Catégorie _climb](http://en.wikipedia.org/wiki/Hors_catégorie) that is grad school (yes... I think \"WWJVD?\" [What Would Jens Voigt Do?] all the time, and the answer is always the same: [Shut up Legs](http://teamjva.com/jens-voigt-soundboard/) [or metaphorically whatever in me is wanting to quit at that day/time]) I realized that while I will learn many wonderful things from my colleagues and mentors ensconced traditional humanities departments, I will not learn how to be a humanist in an increasingly post human world. And by mean I mean _in_, or online.  This is not in any way a criticism of the specific people and departments I know and have known, rather it is recognition of a persistent condition in the academy as it has been broadly configured. Praxis, for me, offered the chance to learn not just how to program or how to manage DH projects, but how to thrive in world in which I have no choice but to live. Most of my posts will explore my evolving relationship with not just DH but the Internet as cultural force in my entire life.  This may seem outside the remit of ‘DH’ but I may just be foolish enough to think that DH and old-fashioned ‘H’ finally aim at the same target: how to live, and live well, in the world in which you live.\n\nOh yah. I ride bikes some times. And really love Jens Voigt. And you should too.\n\n\n1. [Prolly is not Probably](http://prollyisnotprobably.com), [Bike Snob NYC](http://bikesnobnyc.blogspot.com), [Bike Rumor](http://bikerumor.com), [Rapha](http://www.rapha.cc/blogs/), [Road.cc](http://road.cc), [VeloNews.com](http://velonews.competitor.com)\n\n\n\n\n\n\n2. TS Eliot, \"Burnt Norton,\" Four Quartets (Harcourt, 1943), loosely.\n\n\n"},{"id":"2013-10-24-podcast-introducing-our-2013-2014-graduate-fellows","title":"Podcast: Introducing Our 2013-2014 Graduate Fellows","author":"ronda-grizzle","date":"2013-10-24 05:48:14 -0400","categories":["Podcasts"],"url":"podcast-introducing-our-2013-2014-graduate-fellows","content":"**Graduate Fellows Forum**\n**Introducing Our 2013-2014 Graduate Fellows**\n\nOn September 10, 2013, the Scholars' Lab hosted a lunch talk to welcome our new Digital Humanities Graduate Fellows, who gave an introduction to their research to be undertaken with the Scholars' Lab during the next year.\n\n**Eric DeLuca**\nComposition and Computer Technologies Program\nMcIntire Department of Music\n\n\"Community Listening in Isle Royal National Park, a sonic ethnography\"\n\nSounds not only change physically as they travel across and through spaces and places, but they also change, and shape, dense webs of relationships between people and things across socio-cultural contexts. Within this space, what can we learn from individualized listeners? And what can we learn by listening to how these people listen? My work with the Scholars’ Lab focuses on one of these relationships. Blurring the line between soundscape composition, audio documentary, and sonic ethnography, the work documents how I became part of a dialogue between wolf researchers of the world’s longest running wildlife study, and a community of park-exploring people. In short, the wolf researchers have tapped into a network of park visitors and employees scattered across the island, listening. The scientists collect listening reports from this network that leads to the determination of wolf reproductive success in the summer season. In the process, this network of listeners gains, among other things, a deep listening relationship with the place. During extensive fieldwork in the US National Park system as a composer/researcher through their Artist-in-Residence program I have become interested in how these ways of listening exists within, and are tied to complicated, interconnected environments. In order to understand this listening relationship it is necessary to understand an array of issues and topics directly related: noise, silence, chance, the relationship between science and the public, global climate change, National Park policy and politics, species reintroduction, togetherness, the romanticization and dramatization of the wolf, spirituality, and socio-aesthetics of the place. The work is framed by the intrinsic relationship between this listening network and the ecological well-being of the park, which is currently at risk of major change because the wolves, who play a vital role in maintaining this health, are on the brink of “blinking-out”. My time in the Scholars’ Lab will be spent experimenting with different forms of research transmission/creative expression geared toward connecting the research/story to people. I am interested in preparing an interactive version of the project that would grow into, or become a platform for, other community-based sound and listening projects that may include sound mapping using GIS, online sound databases, and data scraping.\n\n**Gwen Nally**\nCorcoran Department of Philosophy\n\n\"When Socrates Misleads: Falsehood and Fallacy in Plato’s Dialogues\"\n\nMy dissertation challenges a powerful orthodoxy; scholars generally assume that the Plato’s dialogues portray Socrates as a sincere thinker, arguing in good faith and in search of the truth. It is my view, however, that Socrates often misleads his interlocutors, sometimes for sport and sometimes in the service of educating those who are resistant to philosophical argumentation. In particular, I have identified a number of conversations, in the Phaedo, Meno, and Phaedrus, where Socrates misrepresents his own convictions in order to convince an interlocutor of an important philosophical truth. These instances provide contextual and linguistic evidence not only that Socrates misleads but that he is aware of having done so.\n\nMy digital research will focus on identifying other dialogues in which Socrates knowingly misleads his interlocutors. Because many of my arguments are philological, I purpose to use computational linguistic techniques, like topic modeling, to compare the vocabularies of the Phaedo, Meno, and Phaedrus to other dialogues of with similar themes, including the Gorgias, Symposium, Parmenides, and Theaetetus. Comparing these dialogues through statistical analysis will, I hope, reveal further linguistic indications of deceptive rhetorical practices.\n\nIn addition to showing interesting results for my own research, I hope to demonstrate the extent to which topic modeling might be used as an aid to close reading. In recent years, those interested in topic modeling have tended toward corpus-based approaches, examining large swaths of text, across multiple authors and genres, in order to determine sweeping thematic and structural shifts.[1] I hope that this project will demonstrate the extent to which topic modeling might also be useful to scholars who are interested in detailed textual analysis.\n\n[1] Clay Templeton (August 1, 2011). \"Topic Modeling in the Humanities: An Overview.\" MITH: Maryland Institute for Technologies in the Humanities. UMD. http://mith.umd.edu/topic-modeling-in-the-humanities-an-overview. See also: http://mith.umd.edu/corporacamp/tool.php.\n\n\n**Tamika Richeson**\nCorcoran Department of History\n\n\"SURVIVAL AND SURVEILLANCE: RECOVERING NARRATIVES OF BLACK FEMALE CRIMINALITY DURING THE CIVIL WAR\"\n\nA critical component of her dissertation titled, “Black Sass: A Social and Cultural Examination of Black Female Criminality in Civil War Era Washington, D.C. 1850-1880,” Tamika Richeson’s digital humanities project applies the police precinct data of over 450 arrests of black women to develop a digital narrative of the lived experiences of “lower-class” black women across space. Applying crime as a focal point, her study of the spatial relationship between black women’s law-breaking activity and police surveillance in the nation’s capital, offers a window into the lives and labors of lower-class black women during an era of national conflict and fortified race-based legal restrictions. The nation’s capital was a shared space, occupied by local black and white inhabitants, immigrants, politicians, and soldiers. Her project involves a detailed examination of criminal data from 1861 and 1862 to capture the tensions and conflicts within that shared space and at centers of power. Her research demonstrates that black women strategically navigated wartime Emancipation in their quest for independence, stability, and survival. This case study at the heart of national politics and culture resituates lower-class black women from the margins to the center to examine the racialized and gendered context in which American criminal law took shape.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.26842728989/enclosure.mp3\"]\n"},{"id":"2013-10-24-podcast-sukanta-chaudhuri","title":"Podcast: Sukanta Chaudhuri","author":"ronda-grizzle","date":"2013-10-24 05:49:18 -0400","categories":["Podcasts"],"url":"podcast-sukanta-chaudhuri","content":"**Institute of the Humanities & Global Cultures Speaker: Sukanta Chaudhuri**\n**Many Tagores: Travels through a Variorum Website**\n\nOn September 12, 2013, Professor Sukanta Chaudhuri, Professor of English Literature at Jadavpur University and a Digital Humanities scholar, spoke in Alderman Library at the conclusion of his residence at UVa as an Institute of the Humanities & Global Cultures Clay Distinguished Visiting Professor. Dr. Chaudhuri is the Principle Investigator of the _Bichitra: Online Tagore Variorum_.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.26844676557/enclosure.mp3\"]\n"},{"id":"2013-11-01-thinking-through-doing-while-losing-my-marbles","title":"Thinking Through Doing While Losing My Marbles","author":"veronica-ikeshoji-orlati","date":"2013-11-01 13:17:53 -0400","categories":["Grad Student Research"],"url":"thinking-through-doing-while-losing-my-marbles","content":"Last Monday, at the suggestion of our Scholars' Lab mentors, the Praxis cohort divided into two teams to start wire-framing some of our ideas for the Ivanhoe Game. The initial thought was to divide into groups along a theoretical Type I/Type II DH divide, as identified by [Stephen Ramsay](http://stephenramsay.us/2013/05/03/dh-one-and-two/). Many of us Praxers, however, are not absolutely committed to one or the other DH type, thereby throwing a bit of a wrench into that plan. So instead, we decided that we'd try wire-framing two versions of the Ivanhoe Game with no particular constrictions – not even feasibility.\n\nYes, we've had the opportunity to design what we would build in an ideal world with unlimited time, resources, and abilities.\n\nAnd so the fun began.\n\nIn the past ten days or so, Francesca, Zach, and I have met a couple of times to discuss how we visualize our version of Ivanhoe playing out (pun intended). Jeremy ran us all through the essentials of wire-framing and user interface design, and at his suggestion we've consulted J.J. Garrett's [_The Elements of User Experience: User-Centered Design for the Web and Beyond_](http://www.jjg.net/elements/) to figure out how, exactly, we want to go about building our game and conceptualizing the user experience.\n\nSimultaneously, the entire Praxis fellows group has been engaged in a rather entertaining and informative e-mail-based game on the history of the Parthenon marbles. Stephanie chose the Parthenon marbles for us because of the wealth of information available, the extensive debate surrounding them, and their 2,500 year history touching upon questions close to many of our disciplines. We've taken carefully-defined roles (I myself am arguing a position I personally find rather unpalatable and untenable, other players taking on roles from history and mythology) and we have worked towards building a moderately-coherent narrative of the Parthenon marbles through our various game personae.\n\nOne of the reasons we decided to start another communal game was to experiment with adding more structure to the format of game play (e.g. a points system, a game/move time-line, and explicitly-stated roles) to see if it would improve the quality of the game and, perhaps, [better sustain game-play](http://www.scholarslab.org/grad-student-research/on-games-that-just-fizzle-a-chronicle-and-reflection/). Additionally, we thought it would be helpful to simultaneously play and design a game so that we could think more critically about what type of interface would enable the various moves, connections, and conversations we would like to foster but simply cannot in the platforms available to us.\n\nUpon further reflection, I realized what this whole process reminded me of: dissertation research and writing. Last Fall, I read Joan Bolker's _[Writing Your Dissertation in Fifteen Minutes a Day](http://us.macmillan.com/writingyourdissertationinfifteenminutesaday/JoanBolker) _in which the author advocates for writing each and every day to improve one's productivity and quality of research. Additionally, Bolker insists that, while research and thought are (obviously) fundamental to a dissertation, writing is often the only way to think through ideas, questions, and problems. In essence, to push beyond the vague generalities and variably-substantiated theories which float around in our heads as we conduct research, it is necessary to articulate the arguments in concrete form to expose the strengths and weaknesses of their logical structures and underlying suppositions. Likewise, we have reached a point with Ivanhoe at which designing _something_, no matter how amateurish, incomplete, and flawed, is absolutely necessary to determine the next step in creating a workable game. We are, in theory, thinking by doing, while still pushing our 'research' forward with the Parthenon marbles game.\n\nAs a bit of a post-script, I seem to be finding Ivanhoe-esque objects and ideas everywhere, such as [J.J. Abrams and D. Dorst's ](http://www.buzzfeed.com/summeranne/peek-inside-jj-abrams-absurdly-beautiful-new-project)[_S._](http://www.buzzfeed.com/summeranne/peek-inside-jj-abrams-absurdly-beautiful-new-project), an annotated version of a book, the _Ships of Theseus_, written by the fictional author V.M. Straka. Though the game Francesca, Zach, and I are designing is more of an open-ended space for creating and visualizing the connections between various data-sets and less of a resource for annotating text (like our first game on Edgar Allen Poe's _The Tell Tale Heart_), I'm eager to get my hands on a copy of _S._ soon – and maybe I can call reading that research.\n"},{"id":"2013-11-11-stephen-covey-intervenes-in-wire-framing-ivanhoe","title":"Stephen Covey intervenes in wire-framing Ivanhoe","author":"stephanie-kingsley","date":"2013-11-11 07:57:23 -0500","categories":["Grad Student Research"],"url":"stephen-covey-intervenes-in-wire-framing-ivanhoe","content":"When the SLab folks recommended we split into groups and begin creating wireframes for our Ivanhoe games, my first thought was, \"How can we start building when we don't really know what we're doing yet?\"  However, talking about what we were doing in the abstract had been generating somewhat circular discussion, so I took a leap of faith and began drawing.  Jeremy handed out colored pencils, and we all started sketching individually.\n\nIt occurred to me in that moment that what we were doing was analogous to stream-of-consciousness writing—imagining in the very moment of creation.  So, inspired, I took up a violet pencil and commenced.  To get the creative juices flowing, I drew a rectangle to represent a computer screen, and the empty spaces suddenly demanded, \"Fill me with something brilliant!\"  Ideas began to arise spontaneously.  I mused first about what our game-play space would look like—which led to more musings on what Ivanhoe was all about.  This time, however, rather than just spinning my wheels, I drew out two visualizations to follow the two trains of thought arising concurrently: one, a linear view of Ivanhoe which would represent moves much the way Facebook posts pop up in the Newsfeed, and the second, a nonlinear model which displayed moves as a web representing them in relation to one another.  Simply by sketching out the tension between linear and non-linear Ivanhoe concepts, I had a couple concrete visualizations in hand—already a step forward!\n\nWe split into groups of three to envision two possibilities for Ivanhoe: Eliza, Scott, and me; and Francesca, Veronica, and Zach.  Our goal was to create wireframes and present them to the SLab folks in our meeting today, so both groups met separately over the past week to prepare.  I realized that not only did sketching out my ideas help me delineate specific problems to address (such as the issue of linearity), but it helped us to collaborate more efficiently, as well.  We were able to communicate in concrete ways as opposed to trying to speak about abstract concepts.\n\nAt this point, I could speak about a lot of aspects of wire-framing; it was a rich experience in a variety of ways.  What strikes me the most, however, is the way the simple act of having to get our ideas on paper helped us communicate better with one another—an unanticipated benefit.  For those familiar with Stephen R. Covey's _[The 7 Habits of Highly Effective People](https://www.stephencovey.com/7habits/7habits.phphttp://), _I understand this phenomenon best in light of [Habit 5](https://www.stephencovey.com/7habits/7habits-habit5.php): \"Seek first to understand, then to be understood.\"  The premise is self-explanatory: if you are trying to make another person understand your point of view, first try to understand his.  The reason I bring up this concept is that I saw drawing out our ideas together as a facilitator for understanding one another.  If Scott had an idea, he drew it on the board, and Eliza and I both could immediately understand it and then _build on it _by adding our own ideas to Scott's drawing.  The space of time in which something was being drawn created a moment in which discussion ceased, and everyone waited to continue until the idea was fully formed on the board and could be considered.  In this way, Eliza, Scott, and I were able to lay out a solid plan for what our Ivanhoe might look like.\n\nI find group interaction fascinating.  This being my first major collaborative project in a working environment, I see it as an opportunity to grow socially in a professional sense.  Working with other people is not easy.  It is rewarding, fun, exhilarating and inspiring, but not necessarily easy.  We are a group of confident high-achievers, and that means that every one of us has excellent ideas that could stand alone.  The challenge is to meet each other in the middle, and by taking the best of each, to fashion something even better.  Covey's [Habit 6](https://www.stephencovey.com/7habits/7habits-habit6.php) is \"Synergize,\" which means combining the strengths of individuals to collaboratively create what no one person could create on his or her own.  This is what we're doing now, and I look forward to seeing what we'll build together.\n\nSources:\n\n_Stephen R. Covey.  _Web.  Accessed 6 Nov 2013 <https://www.stephencovey.com>.\n"},{"id":"2013-11-11-two-ivanhoes-one-direction","title":"Two Ivanhoes, One Direction ","author":"francesca-tripodi","date":"2013-11-11 07:59:33 -0500","categories":["Grad Student Research"],"url":"two-ivanhoes-one-direction","content":"Over the past few weeks our team of six divided into two subgroups to try and wireframe out our respective visions of Ivanhoe (see [Veronica's](http://www.scholarslab.org/grad-student-research/thinking-through-doing-while-losing-my-marbles/) excellent post for more details on how these groups were organized). After coming together and presenting each of our ideas, I was immediately struck by the similarities between our projects (and a bit relieved to see that we won't really need to make too many compromises in either direction). We both envisioned a central space or environment where users would come to collaborate (similar to [Prism](http://prism.scholarslab.org/pages/about) all of the play would happen in one place). We both mapped out a profile page where users can easily access games in progress, chat with other players, and have the ability to edit  their profile. Yet we also had some important differences:\n\n1. Team II (Stephanie, Eliza, and Scott) had a role journal, a \"contact us\" feature, an option to sign in to the game by using Facebook/Twitter as well as the ability to link our environment to these social networking sites.  They also had a demo feature for people new to the space, the ability to star moves, activate move notifications, and assign points. Overall, I think most of these functions are good ones - especially the ability to star moves and have the option for players to receive notifications (they suggested an e-mail notification but perhaps we could also make a text option to facilitate more smartphone usage). Given smartphone proliferation, I also really like the idea of linking up to FB/Twitter because I think it would increase participation and frequency of use. I also think their idea of a demo is essential because it could help new users navigate the space but also serve as a marketing tool! However, I'm not sure I see the benefit of a role journal. I understand creating a restraint so players aren't just making moves haphazardly, but our team presented a \"rational\" section whereby players had to justify why they made moves instead of a \"role journal\" which I believe is a bit excessive. For me, I want most of the time players spend in the environment to be devoted to playing/making moves and not writing. I'm unclear of what the benefits of a role journal might be and I'm concerned that by creating more work for users our environment will become a less fun place. I'm also a bit weary of a \"contact us\" section because it requires constant monitoring and after this academic year comes to a close I wonder who would maintain this function. Perhaps a FAQs might be a good alternative?\n\n2. Team I (me, Veronica, Zach) - one big difference of our presentation was the look and feel of the environment. While Team I envisioned a space that looks similar to WordPress, our team was thinking more of a Google Maps interface - where users could zoom in and out. We also want to have a function similar to a tag cloud whereby the images with more connections would grow in size as the game progresses (or if you have a team game, the team with the larger images is the one making more connections...aka \"winning\").  The benefit of our layout is mainly aesthetic, but Team II might be on to something with navigation. It's possible our layout could get too messy as more and more people make moves. We also included a taxonomy feature - a necessary component if we want users to have the ability to search for and find games they aren't directly invited to. Finally, we proposed an **image only game** (in the form of pictures, music, videos) and this is something that I really want to push for. While I realize that the original Ivanhoe was manipulation of texts, I think that Prism essentially does this (although in a less playful way). SO, if we want to have the ability to manipulate a text, perhaps we could embed some of the Prism tools into our gaming environment? Is this possible? Do those familiar with the old Ivanhoe feel that a text based game is essential or do you like the idea of image only?\n\nIn general, I was impressed by all of our ideas and doing this exercise gave me a renewed energy for our project.  But as we move forward in figuring out which additions we want to include and ultimately what direction we end up taking, I think the final question we all need to ask ourselves is what else is missing?\n"},{"id":"2013-11-12-sticky-situations-lessons-group-cohesion","title":"Sticky Situations: Lessons in Group Cohesion","author":"veronica-ikeshoji-orlati","date":"2013-11-12 10:12:40 -0500","categories":["Grad Student Research"],"url":"sticky-situations-lessons-group-cohesion","content":"Over the past couple of weeks, we Praxers have been wire-framing two distinct visions of the Ivanhoe game in two groups – Eliza, Scott, and Stephanie in one, Francesca, Zach, and I in the other. On Wednesday we presented our visions to each other and our dedicated Scholars' Lab mentors. (Francesca has written an [excellent summation](http://www.scholarslab.org/grad-student-research/two-ivanhoes-one-direction/) of both, highlighting some of the key similarities and differences.) As we await the second databurst from the people who know how to actually build our fuzzy visions, I thought I would share a couple of brief reflections on how our first stab at creating something together went.\n\n**First Lesson: Ivanhoe = Archaeological Excavation.**\n\nAs an archaeologist, I am accustomed to – and very much relish – working with other people towards building a coherent interpretation of archaeological data. Indeed, one person has never and will never be able to collect, organize, and analyze all of the data which comes out of an excavation, not only due to the sheer quantity of information, but also because of the years required to develop sufficient expertise in any one sub-field of archaeology to properly contextualize finds.\n\nIvanhoe is a bit like an archaeological site, and since it's still in the early stages of development, it presents many of the same questions and problems. To add to the chaos, we Praxers do not yet have defined specialties, and our overall interests and goals are, for the most part, strikingly amorphous. So as of last week, we were effectively a group of first-time DH archaeologists looking at a gigantic field armed only with the knowledge that there was _something _out there _somewhere_ for us to find. By wire-framing some of our visions of Ivanhoe we executed a field survey and, with the interpretative assistance of the SLab R&D people, we're finally going to figure out where to 'dig' and what sorts of tools and specialties we're going to need to accomplish our project.\n\nYes, that _is_ a terribly convoluted metaphor. Nevertheless, I think it is helpful in conceptualizing how to work on a collaborative DH project. No one of us will hold 'The Key' to making Ivanhoe work, and our final product will be more than the sum of our individual efforts. (Or so one would hope.)\n\n**Second Lesson: Thinking Brilliant Thoughts Means Nothing If You Don't Articulate Them.**\n\nEvery once in a while, an absolutely Brilliant Idea enters into my head. (Allow me the illusion for the moment.) I tell myself that I won't forget this Brilliant Idea and continue to go about the rest of my day, basking smugly in the radiant glow of my own genius.\n\nThe end of the day arrives, and I finally sit at my computer to write the paper/essay/dissertation chapter which will elucidate for the world my Brilliant Idea.\n\nAnd it's gone.\n\nI spend that evening trying to recreate the moment the Brilliant Idea struck me, combing my memory for any detail which might trigger the Brilliant Idea to reappear. But instead of suddenly recalling the Brilliant Idea in full, with additional layers of nuance added from my efforts in remembering it, I pass the evening writing laboriously around the topic of the Brilliant Idea and hoping it will manifest again if I just...write...enough.\n\nI suspect I am not the only person to have suffered the loss of a Brilliant Idea. I did not, however, realize that the very same thing happens in groups. Francesca, Zach, and I had a number of excellent conversations about our vision of Ivanhoe (greatly facilitated by our SLab mentors!). The problem, however, came after the conversations, when we had to figure out how to articulate everything we discussed in a coherent manner. Not until Jeremy finally forced us to diagram things out did we understand just what we were doing, and at that point we started to realize how many holes we had in our vision.\n\nThis lesson reiterates what I blogged about [a couple of weeks ago](http://www.scholarslab.org/grad-student-research/thinking-through-doing-while-losing-my-marbles/), but I think it is absolutely crucial to making sure we actually have a concrete product by the end of the year which reflects the quality conversations and vibrant energy of our group.\n\n**Third Lesson: Disagreement Happens.**\n\nThis is another statement of the obvious, but making disagreements fruitful and productive is something that, in my opinion, we are struggling with as a group. [Francesca's](http://www.scholarslab.org/grad-student-research/forming-norming-storming-performing/) blog post last month on the evolution of group dynamics and [Stephanie's](http://www.scholarslab.org/grad-student-research/more-musings-on-tuckman/) subsequent evaluation of our group based on Tuckman's Forming-Norming-Storming-Performing model are indicative of our general concern with being positive and productive as a group.\n\nThe fact is that academia can be terribly isolating and often [values individual, field-specific contributions](http://www.insidehighered.com/news/2013/10/31/study-finds-phds-who-write-interdisciplinary-dissertations-earn-less) by far more than collaborative, interdisciplinary ones. As graduate students in the humanities and social sciences, we often talk about _my_ research, _my_ contribution, _my_ interests. But perhaps, as we work through the conflicts between our individual ideas, we can continue to better _our_ project.\n"},{"id":"2013-11-15-role-journals-texts-pedagogy-and-pragmatism","title":"Role Journals, Texts, Pedagogy, and Pragmatism","author":"scott-bailey","date":"2013-11-15 04:33:26 -0500","categories":["Grad Student Research"],"url":"role-journals-texts-pedagogy-and-pragmatism","content":"In [her recent post](http://www.scholarslab.org/grad-student-research/two-ivanhoes-one-direction/) on the Scholars' Lab, Francesca gave a quick rundown on some of the similarities and differences between the approaches of our two wire-framing teams. I have to confess that I was surprised by a couple of her concerns, and I'd like here to clarify the reasoning behind our strong focus on the role journal and our sense of Ivanhoe as a textually centered game with significant multimedia capabilities. I'll do so with two concerns in mind: teaching goals and practice and pragmatic concerns.\n\n**Role Journals and Pedagogy**\nFrancesca makes a distinction between the \"rationale\" section that her own group built into their version, and the role journal which our group included. She understands the rationale section to be a place where \"players had to justify why they made moves,\" which might create a \"restraint so players aren't just making moves haphazardly.\" This will be less \"excessive\" than the role journal, and will allow players to spend most of their time \"devoted to playing/making moves and not writing.\" As well, there is a concern that creating this extra bit of work for players will make the environment \"less fun.\"\n\nI am certainly sympathetic with many of these concerns, but I think clarification of our idea of the role journal in terms of its pedagogical use will ameliorate these significantly. I and a few others have thought of Ivanhoe especially in terms of pedagogy. As such, Ivanhoe is an application that allows players to intervene in a discursive space (this will be familiar language to those acquainted with it from a decade ago) and facilitates critical reflection upon acts of interpretation. But what does that mean?\n\nI'll drop back to my own discipline in the hopes of giving a clear illustration. Suppose you are teaching a course in Reformation history and theology, and were teaching Martin Luther's \"On the Babylonian Captivity of the Church.\" In groups, you have your students engage this text through Ivanhoe. Each chooses a role. Perhaps one decides to be Luther himself, another Pope Leo X. Someone else chooses to write as Elector Frederick. All of these would be significant figures in the historical controversy. But someone else decides to write as Pope Francis, the current Pope, another as the Nazi jurist Carl Schmitt, and yet another as the Daily Show's Jon Stewart. Each of these roles will bring a distinct and interesting approach to the historical, political, and theological controversies which find expression in Luther's text. As the game begins and then progresses, each player, maintaining his or her role, begins to make interventions. Perhaps in the guise of writing a history of the papacy, \"Pope Francis\" lays out in successive moves the critical development of the papacy throughout the medieval and early modern periods. Perhaps \"Luther\" submits as his or her moves autobiographical reflections which reveal Luther's deep anxiety and distress in his own spiritual life. \"Jon Stewart\" provides short, witty, incisive engagements all the relevant figures, reacting especially to the moves of others, pointing out inconsistencies and moments of hypocrisy, expressing a voice of moral indignation at the faults of Luther, Leo X, and others. As the game progresses, the players attempt to respond more significantly to each other, all for the goal of understanding the initial text through the fusion of numerous horizons (do understand this in Gadamer's terms).\n\nAs a teacher, while I want my students to be engaging creatively, analytically, and critically with Luther's text, I want even more for them to understand and engage the way in which they think and interpret and form judgments. I want them to reflect on the interventions or moves of other players, to think about how those moves open up dimensions of the text which they now can engage, to consider how those moves of others capacitate their own critical and creative interventions. And I want them thinking about how the roles that they have chosen come with certain hermeneutic positions with which they must reckon in order to understand how to coherently embody that role in its engagement with the discursive space of the text.\n\nTo think about the text. To think about thinking about the text.\n\nAs a tool, Ivanhoe allows us to engage an object of concern, and while I've used a text as an example, it could be an image, a video, a sound file, a physical object, or almost anything else. Around that object of concern we engage in acts of interpretation which could take a vast number of forms and styles. And then we can think about what we're doing when we're playing. We can raise awareness of how we ourselves think and perhaps how others think as well.\n\nThis is the benefit of the role journal, to give our students the space to reflect consciously on what is happening in the game and what is happening in their own thinking. It is this second part, I think, which really takes us to the point of deep engagement with critical thinking. In the design my group offered, we placed a button on every page that had game content on it, and when you click that button it pops out a little no-frills text editor for the role journal. The idea was that while reading the interventions of others or one's own interventions, the player could write quick notes and reflections. This method of quick entry was paired with a dedicated role journal page, with a more substantial WYSIWYG editor that would allow for more significant entries into the role journal. Could much of this be done with pen and paper, in a student's notebook? Sure. But the idea of building it in is to give players a single environment in which the game and their production of content exists, and the idea of the pop out editor is to make critical reflection a common part of the experience of the game.\n\nThere are other details, of course, like whether the role journals are public or private. There are reasons for either, and I think it would be an option for each game to choose one or the other. I could certainly see an interesting activity where at the end of a game, the participants share their private role journals and reflect on others reflecting on the game as it progressed.\n\nIs this a bit more work? Yes. But I think it is pedagogically worth it, and not so very different from the idea of a move \"rationale.\" I even think that for quite a few players, the critical, reflective activity of the role journal, and the sharing of them, might rather heighten the fun of the game.\n\n**Texts and Pragmatism**\nThe example above and even my language of \"reading\" someone's else moves undoubtedly reveal that I think of Ivanhoe in terms of texts. Working in philosophical theology, texts are my mainstay. Yet, I think Ivanhoe should be a game wherein we can play with any number of objects, whether they be texts or images, video or audio-clips, or anything else. I see Ivanhoe being incredibly flexible here and open to so many types of games. So, I was surprised to see the other group, and Francesca in her blog post, push for a non-textual game. I'll focus my response on Francesca's blog post.\n\nShe writes there that they proposed, \"an image only game (in the form of pictures, music, and videos),\" which she \"really want[s] to push for.\" Francesca notes that the original Ivanhoe game was primarily \"manipulation of texts,\" but suggests that Prism, the product of our forebears in Praxis, \"essentially does this (although in a less playful way).\" She suggests that we consider \"embedding Prism tools in our gaming environment,\" but asks whether those \"familiar with the old Ivanhoe feel that a text based game is essential or do you like the idea of image only?\"\n\nI don't know whether I count as one of those familiar with the old Ivanhoe, but as one of those building the new one, I have to say that I think the two options given are not the only ones. As I pointed out above, I think we need to build an application flexible enough to facilitate games of many varieties, some of which may involve a great deal of text and others which involve only a bit and others which involve not a whit. I'll return to some of this reasoning in terms of pragmatism shortly.\n\nI want take a moment, though, to address Prism. Francesca interestingly asked if we could embed Prism within our gaming environment, and, also interestingly, within my group's presentation, Stephanie had built a mockup of how something very close to this could be done as an alternate way to play the Ivanhoe game, an alternate way to make textual interventions. But here's the rub, it was an alternate way, meaning not the only way, nor perhaps even the primary way. What Prism allows one to do is \"collaborative interpretation\" through highlighting a text according to different categories, or \"facets.\" If we look at our most successful games of Ivanhoe as our cohort has experimented with Ivanhoe (primarily textually, I will admit), the most interest moves have involved writing text and including media with that text at times. This is a different thing than Prism though, and doesn't involve the same sort of highlighted interpretation of a text through determined categories. Rather, textual moves in Ivanhoe are often products, literary creations of different forms, interwoven with different media. So I have to say that I don't think embedding Prism is sufficient to grapple with the ways in which we might engage texts through Ivanhoe. The two tools are simpy different, with different goals and different means.\n\nBut why include text at all? Why not just build an image (very broadly conceived) game? Selfishly, I would want to play this game myself, to use it in thinking through theological and philosophical texts, and perhaps play it now and then with the science-fiction and fantasy literature I love. For me, that will mean writing text and incorporating others texts, as well as linking in images, videos, and sound clips. Disregarding my own interests, though, there is a significant pragmatic reason for making sure that Ivanhoe handles texts well. One of the tenets of the Praxis Program is to build a functioning tool or application that will actually be used. To do that latter part, we need to have an idea of our audience and what they might do with our tool. At the moment, while digital humanities is spreading, and while universities and colleges are pushing for innovative work in digital pedagogy, many of those who are incorporating digital tools in their teaching work within English, literature, and history departments. While all of these people probably engage with different media in their work and teaching, and would play games that incorporate different media, many of them substantively work with texts, and teach classes that are in a deep and abiding way about texts. These are the people who might be most open to embracing a tool such as Ivanhoe, that could enrich and enliven their teaching through productive, ludic engagements among students, especially if we can build a version of Ivanhoe that has a low-bar of entry to use. Understanding pragmatically who might buy in and use the tool we're building more quickly and more thoroughly, then, I think it is necessary to make sure that Ivanhoe is text inclusive, just as it is inclusive of others forms of media.\n\nDespite this rather lengthy engagement with Francesca's concerns, I actually agree with her when she writes that she, \"was struck by the similarities between our projects (and a bit relieved to see that we won't really need to make too many compromises in either direction).\" The differences I've addressed here are important, and I've tried to offer cogent arguments regarding both. But I don't think either difference is enormous or prohibitive of our team, all six of us along with the folks of the Scholars' Lab, moving forward. I certainly look forward to the days ahead.\n"},{"id":"2013-11-18-tongue-tied-in-css","title":"Tongue-tied in CSS","author":"francesca-tripodi","date":"2013-11-18 07:15:14 -0500","categories":["Grad Student Research"],"url":"tongue-tied-in-css","content":"Learning programming languages is both exciting, fascinating, _and _a bit overwhelming for me. While I find that I'm typically pretty good at learning new languages I'm having difficulty getting energized about learning the back end of building what we see on the web. Ironically, one of the main reasons I applied for Praxis was to learn how to program but the more I become familiar with it, the less I want to do it! Don't get me wrong, I see the value and importance in the skill, I think I'm just way behind on the learning curve and the mountain in front of me seems daunting. Moreover, I don't think I'm ever going to get to  \"computer guru\" status so I wonder if having a mediocre set of skills will be particularly useful on the job market.\n\nThat being said, I'm happy that our group has decided to work within WordPress and develop a game PlugIn instead of building an Ivanhoe from scratch. Not only are there cost benefits to this model, it seems like knowing how to use WordPress in a more meaningful way will be an important skill for me to have and something I will be able to take with me after my fellowship is over. Even though I originally thought I would be building something from the ground up I'm coming to realize that a year would never be enough time for me to do this well. I also have a newfound appreciation for the IT professionals at offices everywhere.\n"},{"id":"2013-11-21-gis-day-2013","title":"GIS Day 2013","author":"chris-gist","date":"2013-11-21 06:22:44 -0500","categories":["Geospatial and Temporal"],"url":"gis-day-2013","content":"November 20, 2013 was [GIS Day](http://gisday.com/).  In our annual tradition here in the Scholars' Lab, we hosted a round of lighting talks with a variety of speakers including several groups from the [Shenandoah Valley Governor's School](http://svgsstudentnews.wordpress.com/).\n\n[![Capture](http://www.scholarslab.org/wp-content/uploads/2013/11/Capture-793x1024.png)](http://www.scholarslab.org/wp-content/uploads/2013/11/Capture.png)\n\n\n\nAs always, the had a great mix of disciplines and uses of GIS.  Thanks again to all the participants.  A slide stack of the presentations is available [here](http://teaching.scholarslab.org/courses/2013_GIS_Day/Scholars_Lab_GIS_Day_2013_Presentations.pdf).  Oh, I almost forgot the other part of our GIS Day tradition, GIS Day cake!\n\n[![2013-11-20 12.45.53](http://www.scholarslab.org/wp-content/uploads/2013/11/2013-11-20-12.45.53-1024x768.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/11/2013-11-20-12.45.53.jpg)\n\n\n\nFor images of previous years' cakes, look [here](http://www.scholarslab.org/announcements/the-mappy-goodness-that-is-gis-day-in-the-scholars-lab/).\n\nVideo of presentations to come.\n"},{"id":"2013-11-25-better-focus","title":"Better :focus","author":"jeremy-boggs","date":"2013-11-25 08:48:52 -0500","categories":["Research and Development"],"url":"better-focus","content":"Whenever I've taught folks how to do some basic HTML and CSS, the first thing they want to change are the styles for links on the page. And who can blame them? The default colors for links are pretty lame, as you can see in my [first example page on CodePen](http://codepen.io/clioweb/pen/wtBCo).\n\nFor those who don't know, there are a few states of a link you can style independently, using CSS [pseudo-class selectors](http://www.w3.org/TR/CSS2/selector.html#pseudo-class-selectors). These pseudo-class selectors include:\n\n\n\n\t\n  * `:link` - The default state of the link\n\n\t\n  * `:visited` - Allows you to style links to addresses you've already visited.\n\n\t\n  * `:hover` - Allows you to update link styles when a user moves their pointer over a link.\n\n\t\n  * `:focus` - Allows you to update link styles when a user tabs or otherwise changes their focus to a link.\n\n\t\n  * `:active` - Used to update link styles when the link is activated (when clicked or when a user hits enter on a focused link).\n\n\n\nYou'll notice that, if you use your \"Tab\" key to navigation through the links on the example CodePen page, you can move between links. The link you're current on will be outlined, depending on your browser's settings. (You can do the same for forms, too. This is a nice way,if the form is marked up correctly, to move between fields when filling out a form, instead of moving your cursor to the next field and clicking on it.) The styles of these focused elements can be updated using the :focus pseudoclass selector.\n\nNow, earlier versions (and some current versions) of CSS resets would set the \"outline\" property for everything to \"0\", thus getting rid of the outline on focus and any other state. In their defense, they also recommended resetting this for accessibility, but I'll admit that I rarely remembered to do this in my younger days. The result is that there are no indications when a user has focused on a link or form element.\n\nSimilarly, but perhaps even more foolishly, I was for a while inclined to think that the act of hovering over a link or form element was equivalent to focusing, so I would very often style these two pseudoclasses in the same CSS rule:\n\n[code language=\"css\"]\na:hover,\na:focus {\n   color: orange;\n}\n[/code]\n\nYou can actually seen an example of this right now on our [Speaking in Code](http://codespeak.scholarslab.org) site. Using the tab key to move between links on the page uses the same styles as hover, but they're not very noticeable, certainly not noticeable enough to know where you've focused when the page scrolls down. Part of the problem here is that the hover/focus styles aren't really different enough from the default link styles anyways, which is something I should address. But with focus, the way a user navigates the page is inherently different, and assuming that hover and focus are equivalent leads to a pretty inaccessible page, even for folks who just tab between links for convenience.\n\nLong story short: I need to do better focus styles. This is a very different method for navigating web pages, and the styles should help facilitate that navigation, not obscure it.\n\nAs a quick example of exploring more prominent styles for link :focus, I created a [new CodePen with some updated styles](http://codepen.io/clioweb/pen/ArtEq):\n\n[code language=\"css\"]\na:focus {\n  outline: thin dotted #60c;\n  color: #60c;\n  background: #fff09e;\n  text-decoration:none;\n  padding: 2px 0;\n  font-weight:bold;\n}\n[/code]\n\nNow if you tab through the links on the page, you'll notice they stand out a bit more. They're intentionally ugly—to show you how different you can actually make these look—but you can change the styles to balance matching your own aesthetic with making the links noticeable enough to see where you've focused on the page.\n\nA few lessons to take away here:\n\n\n\n\n\t\n  1. _If you use a CSS reset, actually read through it and see what it does._ They're meant to be just that—a reset—and still expect you to set styles appropriately. Most of them are designed to give you a consistent starting point across browsers, but they still don't supersede your responsibility to set styles appropriately for all your users. They're also meant to be tinkered with and changed for your own preferences and design goals.\n\n\t\n  2. Take particular care to style the :focus pseudoclass for both links and form elements. It's a handy way to navigate parts of a page, but without noticeable style updates, users can quickly lose track of where they've actually focused.\n\n\n\nNow I'm off to go update the styles on Speaking in Code, and just about every other project I've had my hands on around here!\n  *[HTML]: Hypertext Markup Language\n  *[CSS]: Cascading Style Sheets\n"},{"id":"2013-12-09-podcast-open-access-week-speaker-gail-mcmillan","title":"Podcast: Open Access Week Speaker - Gail McMillan","author":"ronda-grizzle","date":"2013-12-09 06:07:17 -0500","categories":["Podcasts"],"url":"podcast-open-access-week-speaker-gail-mcmillan","content":"**UVa Library Open Access Week Speaker: Gail McMillan**\n**Graduate Student Publishing and Open Access: Understanding the Digital Landscape of Open Access Publishing for Theses and Dissertations**\n\nOn October 29, 2013, Gail McMillan, Director of Digital Libraries and Archives at Virginia Tech, spoke in Alderman Library about her research on the effects of open access for theses and dissertations on publishing opportunities for graduate students.\n\nJoanna Swafford and Brandon Walsh, Ph.D. candidates in the Department of English and former Scholars' Lab Fellows, served as respondents for Ms. McMillan's talk.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.28074082867/enclosure.mp3\"]\n"},{"id":"2013-12-11-neatline-san-francisco","title":"Neighborhoods of San Francisco","author":"david-mcclure","date":"2013-12-11 07:14:03 -0500","categories":["Geospatial and Temporal"],"url":"neatline-san-francisco","content":"_[Cross-posted from [dclure.org](http://dclure.org/essays/neighborhoods-of-san-francisco/)]_\n\n\n\n## [View the Exhibit](http://neatline.dclure.org/neatline/show/neighborhoods-of-san-francisco)\n\n\n\n[![neighborhoods-of-san-francisco](http://dclure.org/wp-content/uploads/2013/12/neighborhoods-of-san-francisco.jpg)](http://neatline.dclure.org/neatline/show/neighborhoods-of-san-francisco)\n\nBuilt on the [Stamen Toner layer](http://maps.stamen.com/#toner/12/37.7706/-122.3782).\n\nBack in October, about a month after moving from Scholars' Lab HQ in Virginia out to Menlo Park (my partner started a PhD program at Stanford), I drove up the peninsula to San Francisco on a Saturday morning and set out on a long, rambling, 10-mile trek along the northwest shoulder of the city. It was a fantastic day - I walked west through Golden Gate Park, north along the Richmond beach, past the Cliff House, into the fog over Lincoln Park, through the mansions in Sea Cliff, past the abandoned artillery nests on the western coast of the Presidio, and finally out onto the Golden Gate bridge. From there, I headed south through the trails in the Presidio, into Richmond, and eventually back to where I started, near the top right corner of the park.\n\nFrom the middle of the Golden Gate Bridge, you can look out to the east over a large swath of the city - the skyscrapers of the financial district, the new span of the Bay Bridge hanging over Treasure Island, Alcatraz, and the faded outline of the East Bay, the Berkeley campus a little smudge at the base of the ridge line. But, scanning my eyes over the rest of the city, I realized that I had very little sense of what I was actually looking at. I could attach labels onto all the touristy landmarks, but I didn't have any kind of intuitive mental geography of the place - the names of all the little hills and neighborhoods, what connects to what, how to string the pieces of the city together into workable routes and itineraries.\n\nSo, over the course of the next few weeks, I slowly cobbled together a [Neatline exhibit](http://neatline.dclure.org/neatline/show/neighborhoods-of-san-francisco) that plots out each of the neighborhoods in the city - 87 of them, by my count, although it's somewhat a matter of interpretation as to how they should be sliced and diced. Working mainly from [this image](http://www.reocities.com/mwarren_us/sf-neighborhoods/SFNeighborhoods.gif) as reference, I started by tracing rough outlines of the boundaries (using Neatline's standard-issue \"Draw Polygon\" tool) on top of Stamen's Toner layer. Then, once the borders were in place, I used Neatline's SVG import tool to place vector-geometry text labels on top of each of the individual neighborhoods, inspired by other spatial-wordcloud experiments like [this](http://sfsgeography.wikispaces.com/file/view/SF_neighborhhods.jpg/184285923/SF_neighborhhods.jpg) and [this](http://www.californiatravel.eu/travel/NeighborhoodsofSanFrancisco.jpg).\n\n**Adventures in geospatial typesetting**\n\nThis was great fun, and, interestingly, it ended up overlapping in unexpected ways with the [interactive](http://dclure.org/essays/experimental-typesetting-with-neatline-and-shakespeare/) [typesetting](http://dclure.org/essays/more-fun-with-interactive-typesetting-a-coat-by-yeats/) [projects](http://dclure.org/essays/song-of-wandering-aengus/) that I was playing with earlier in the semester. The process of positioning the labels becomes a kind of textual jigsaw puzzle, a game of trying to wrangle the raw, geometric instantiations of words into a coherent organizational scheme - except, this time, against the backdrop of actual geospatial coordinates and locations, not the abstract, featureless voids of the poetry experiments. Often, this is pretty straightforward - Noe Valley and the Inner Mission, for example, just get tagged as such:\n\n[![noe-valley-inner-mission](http://dclure.org/wp-content/uploads/2013/12/noe-valley-inner-mission.jpg)](http://dclure.org/wp-content/uploads/2013/12/noe-valley-inner-mission.jpg)\n\nIn other places, though, the names of the neighborhoods overlap with one another in ways that make it possible to find interesting \"economies\" in how the words can be laid out on the map - when adjacent neighborhoods share the same words, it's sometimes possible to essentially atomize the names into their component parts, and then rebuild them according to their own spatial logic, in a sense, by visually stringing together the pieces on the map. Take Richmond, for example, which is divided into three side-by-side segments: Outer Richmond, Central Richmond, and Inner Richmond. Instead of cluttering things up by repeating \"Richmond\" for each of the three sections, I just dragged out a single, all-encompassing \"Richmond\" across the entire width of the three sub-neighborhoods, and the blocked in the three modifiers as smaller words on top of the corresponding sections:\n\n[![richmond](http://dclure.org/wp-content/uploads/2013/12/richmond.jpg)](http://dclure.org/wp-content/uploads/2013/12/richmond.jpg)\n\nThis worked much the same way for the Sunset and Parkside neighborhoods, which share the same cleanly partitioned spatial organization:\n\n[![sunset](http://dclure.org/wp-content/uploads/2013/12/sunset.jpg)](http://dclure.org/wp-content/uploads/2013/12/sunset.jpg)\n\nWith the exception of the \"middle\" portion of Parkside, which is just the un-prefixed \"Parkside,\" meaning that the center piece doesn't get a separate modifier:\n\n[![parkside](http://dclure.org/wp-content/uploads/2013/12/parkside.jpg)](http://dclure.org/wp-content/uploads/2013/12/parkside.jpg)\n\nIn other cases, though, it gets much trickier, and much more interesting. Take the little cluster of neighborhoods at the southwest corner of the Presidio, the big park at the base of the Golden Gate Bridge. It's a hodgepodge of repeated names, but in a much more scrambled and overlapping way - Presidio, Presidio Heights, Pacific Heights, Lower Pacific Heights. In this case, I had to take a bit more care to place the little black arrows in ways that didn't connote incorrect linkages among the names. For example, the relationship between Presidio and Presidio Heights moves in just one direction - Presidio Heights (labelled with just \"Heights\" on the map) needs to \"inherit\" the \"Presidio\" modifier from the Presidio, but not the other way around, since the Presidio ceases to be the Presidio when \"Heights\" is tacked onto it:\n\n[![presidio](http://dclure.org/wp-content/uploads/2013/12/presidio.jpg)](http://dclure.org/wp-content/uploads/2013/12/presidio.jpg)\n\nTo encode these relationships, I settled on a rule of thumb that the arrows would always be _contained inside the neighborhoods that they modify_. So, the arrow pointing from \"Presidio\" to \"Heights\" is fully contained inside of the geographic boundaries of Presidio Heights, in the sense that it pulls \"Presidio\" downward into the \"Heights,\" without also pushing \"Heights\" back in the other direction (which would effectively mislabel the Presidio). Likewise the link between \"Pacific\" and \"Heights\" is contained within the Pacific Heights outline, since otherwise Presidio Heights would be implicitly but incorrectly prefixed by \"Pacific.\"\n\nAnyway, this is all completely useless as actual cartographic practice, but great fun as a kind of abstract _étude_ of information design. It's also incredibly useful as a mnemonic device - after untold hours in Palo Alto coffee shops sketching out all the outlines and positioning the labels, they're all thoroughly burned into my mind. This is an interesting aspect of digital mapping projects that doesn't get a lot of attention - we tend to focus on the final products, the public-facing visualizations and interactions (for good and obvious reasons), but much less on the _process_ that goes into creating them, the personal acquisition of knowledge that takes place when you force yourself to spend dozens or hundreds of hours painstakingly positioning and repositioning things on maps. It gives you an incredible sense of cognitive intimacy with the space, the ability to load a little chunk of the world into working memory and reason about it in really complex and interesting ways.\n"},{"id":"2013-12-14-turning-points-in-praxis-new-roles-wire-frames-and-programming-languages","title":"Turning points in Praxis: new roles, wire-frames, and programming languages","author":"stephanie-kingsley","date":"2013-12-14 04:33:56 -0500","categories":["Grad Student Research"],"url":"turning-points-in-praxis-new-roles-wire-frames-and-programming-languages","content":"The last couple of weeks have been exciting ones in our program.  Our team has now specified our individual roles for the year.  Eliza, Scott, and Veronica will be our coders; Francesca and Zach will be the design team; and I will be performing project management duties, with assistance from Francesca.  I am excited to see these groups forming, people beginning to specialize, and a greater sense of direction inspiring everyone.\n\nAfter weeks of theorizing what the Ivanhoe Game SHOULD be, our talks about what it actually COULD be based on time, money, skills, etc., inspired some new questions:\n\nWhat platform should we use to build our tool?\n\n\n\n\t\n  * Which type of tool will be the most accessible and inviting to users?\n\n\n\n\t\n  * Which type of programing language will we be able to master in this time frame?\n\n\n\n\t\n  * What kind of tool is most likely to prompt open-source contributions by other academics?\n\n\n\n\t\n  * What is the easiest to maintain time-wise (and time is, after all, money) for the SLab once the fellows leave the program?\n\n\nOur end decision has been to build our Ivanhoe as [a WordPress plugin](http://wordpress.org/plugins/) which users can download and use on their own WP sites, rather than as a stand-alone, hosted service.  We saw WP as familiar and widely-used enough that it would make start-up as easy as possible for users.  (See [Francesca's thoughts](http://www.scholarslab.org/grad-student-research/tongue-tied-in-css/) on our decision to use WordPress.)  It also means that other programers can play with our code and contribute to Ivanhoe's development and maintenance.\n\nEliza, Jeremy, Zach, and I met Wednesday to draw up some wire-frames for our revised Ivanhoe.  After a couple weeks of discussion about what our Ivanhoe would be, we have a more concrete--but still highly viscous--vision of what our game will look like.  We plan to have a game which will allow players to make moves which link to other moves--a key feature.  Players will be able to incorporate text, image files, sound files, and video files into their moves.  We also have plans for a role journal which will aggregate the \"Rationale\" entries made every time a player makes a move.  Ivanhoe strongly resembles blogging platforms such as Tumblr and WordPress blogs, but we hope our particular setup with linking possibilities will inspire new ways of posting.  We also hope, through our design strategies, to encourage a more multimedia approach to making posts.\n\nOur journey now takes us forth into the strange world of PHP--which, if you had asked me about a month ago, I'd have guessed was some sort of chemical.  To embrace our multimedia tenet, I will conclude with a not-so-subtle _homage _to our new best friend:\n\n![File:PHP-logo.svg](http://upload.wikimedia.org/wikipedia/commons/thumb/2/27/PHP-logo.svg/450px-PHP-logo.svg.png)\n\nAs we ring in the new year, here's to magic... only a few queries away: <?php {endless possibilities} ?>\n"},{"id":"2013-12-16-podcast-dot-porter","title":"Podcast: Dot Porter","author":"ronda-grizzle","date":"2013-12-16 04:10:47 -0500","categories":["Podcasts"],"url":"podcast-dot-porter","content":"**Scholars' Lab Speaker: Dot Porter**\n**Ceci n'est pas un manuscript: How Digitization and Presentation Practices Ignore and Obscure the Physicality of the Object**\n\nOn October 30, 2013, Dot Porter, Curator of Digital Research Services at the Schoenberg Institute for Manuscript Studies in the Kislak Center for Special Collections at the University of Pennsylvania, spoke in the Scholars' Lab about the ways in which current manuscript digitization and presentation practices ignore and obscure the physicality of the object and offered some ideas for how to deal with it.\n\nMs. Porter joined us as part of the _Global Digital Libraries Symposium_, co-sponsored by Rare Book School, the Scholars' Lab, and the Buckner W. Clay Endowment for the Humanities at the Institute of the Humanities & Global Culture.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.28089203644/enclosure.mp3\"]\n"},{"id":"2013-12-16-podcast-meg-stewart","title":"Podcast: Meg Stewart","author":"ronda-grizzle","date":"2013-12-16 04:28:41 -0500","categories":["Podcasts"],"url":"podcast-meg-stewart","content":"**Scholars' Lab Speaker: Meg Stewart**\n**A Fulbright Scholar Talks About Participatory GIS, the Caribbean, Google Earth and How a Fulbright Could Be in Your Future**\n\nOn December 3, 2013, Meg Stewart, Academic Technology Consultant and Fulbright Ambassador, spoke in the Scholars' Lab about her experiences as a geospatial technologist in the Caribbean and about the Fulbright Scholar program and encouraged applications for a Fulbright grant.\n\nMs. Stewart's presentation slides can be [viewed by clicking here](http://www.scholarslab.org/wp-content/uploads/2013/12/Stewart_slides_Fulbright_ScholarsLab.pdf).\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://deimos3.apple.com/WebObjects/Core.woa/FeedEnclosure/virginia-public-dz.5154837759.05154837761.28089203756/enclosure.mp3\"]\n"},{"id":"2013-12-24-breaking-things-over-winter-break","title":"Breaking Things Over Winter Break","author":"veronica-ikeshoji-orlati","date":"2013-12-24 05:47:01 -0500","categories":["Grad Student Research"],"url":"breaking-things-over-winter-break","content":"It has been over a month (!) since my last post, but the long delay is certainly not due to lack of activity in the Praxis-Ivanhoe world. Our PM, Stephanie, has written a [great post](http://www.scholarslab.org/grad-student-research/turning-points-in-praxis-new-roles-wire-frames-and-programming-languages/) summing up the progress we've made as a group towards defining and building Ivanhoe. I'd like to underscore her point that, now that our roles within the team have started to crystallize, it feels like we're finally all moving in the same direction.\n\nDuring the weeks between Thanksgiving and the end of the semester, Wayne et al. presented us with some lessons in [PHP](http://php.net/) and have now sent us off for the winter break with the task of practicing what we learned. Again. And again. And again.\n\nAnd again.\n\nAnd that is exactly what I'm doing this winter break. I am committing to memory the syntax of foreach loops and if...elseif...else statements, and I'm stumbling my way through the (rather entertaining) process of translating what I want to do into a logical flow my fatal-error-loving computer can comprehend. But to tell the truth, I've also spent an inordinate amount of time breaking things and banging my head on my keyboard (figuratively and, on more than one occasion, literally). And no matter how many times our Scholars' Lab mentors say it's okay to break a piece of code, a wave of panic overcomes me every time I do so. (Sample internal monologue: “Oh no! It's the end of the world and my computer will self-destruct in 5 seconds! How could I forget that semicolon? Noooo!”)\n\nFumbling around with PHP has dredged up many of the anxieties which Francesca articulated in one of [her posts](http://www.scholarslab.org/grad-student-research/tongue-tied-in-css/), namely that I am so far behind on the learning curve that becoming adequately competent (and confident) with computer programming is beyond my reach. I suppose I knew that a magical panacea for my technophobia wouldn't exist, and I even asked to work on the development side of Ivanhoe so that I would be forced to face my fears directly, but that hasn't kept me from hoping to simply one day wake up and be okay with breaking things.\n\nAfter a brief reflection on the nature of my programming anxiety, I did a few Google searches. What I found surprised me. Books such as [Gender and Computers: Understanding the Digital Divide (2003)](http://books.google.com/books?isbn=141060893X), [Gender Codes: Why Women are Leaving Computing (2011)](http://books.google.com/books?isbn=1118035135), [Recoding Gender: Women's Changing Participation in Computing (2012)](http://books.google.com/books?isbn=0262018063); articles such as [“Caring About Connections: Gender and Computing” (1999)](http://www.cs.cmu.edu/afs/cs/project/gendergap/www/papers/IEEE99.html), [“Why So Few Women Enroll In Computing? Gender and Ethnic Differences in Students' Perception” (2009)](http://www.unm.edu/~varma/print/CSE_Few%20Women.pdf), [“The Effect of Tangible Artifacts, Gender and Subjective Technical Competence on Teaching Programming to Seventh Graders” (2010)](http://link.springer.com/chapter/10.1007/978-3-642-11376-5_7); [long-term, multi-part studies on women in CS](http://www.cs.cmu.edu/afs/cs/project/gendergap/www/); clearly I need to think a bit more about the roots of my anxieties.\n\nI'll save the topic of gender and coding for another post. In the meantime, for the rest of winter break, you'll find me breaking code, breaking down over broken code, and trying to break through some of my anxieties and insecurities about computer programming.\n"},{"id":"2013-12-24-lessons-for-christmas-a-sawzall-solves-all","title":"Lessons for Christmas: A Sawzall Solves All","author":"zachary-stone","date":"2013-12-24 05:32:54 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"lessons-for-christmas-a-sawzall-solves-all","content":"[![](http://i21.geccdn.net/site/images/n-picgroup/DWA_DCS380B.jpg)](http://i21.geccdn.net/site/images/n-picgroup/DWA_DCS380B.jpg)\n\nChristmas, this year, is for building stuff. Today’s work orbits around three projects: building my new dining room table, finishing off a bike I am building for someone, and building some stuff in CSS.  <!-- more -->As we move into next year with our Praxis roles a bit more solidified, it looks as if I will be focusing on design.  Which seems kind of fun and kind of scary and kind of like building a table or a bike. Or at least I go about all three roughly the same way: if stuff doesn’t fit, hit with a hammer until it does and always remember: A Sawzall solves all. In the case of CSS that means forking stuff over to Wayne.  Anyhow, all three balance out The Other Thing (Orals on 30 Jan…gulp) and allow me to make small, tangible advances on something (anything, really) while slogging away at the mountain of textual criticism reading. In the course of all three projects, similar patterns, or at least aesthetics, emerge. I dislike clutter. If given the choice to excise or supplement I always opt for the Sawzall. I love the clean lines of a perfectly assembled bike, a well-balanced tabletop, and a clutter free website.  Thankfully the ghosts of Praxis past set a pretty good aesthetic precedent in the latest iteration of [Prism](http://prism.scholarslab.org/pages/index?locale=en). Hopefully, come the new year, I’ll have a new table, a happy relative on a bicycle, and some flashing dinosaurs in CSS.\n"},{"id":"2013-12-24-praxis-holidays","title":"Praxis Holidays","author":"elizabeth-fox","date":"2013-12-24 17:42:38 -0500","categories":["Grad Student Research"],"url":"praxis-holidays","content":"Just before we departed for the break, Stephanie and I met with Jeremy to talk over some of our wireframes for Ivanhoe.  (Stephanie discussed our wireframing process in [her post](http://www.scholarslab.org/grad-student-research/turning-points-in-praxis-new-roles-wire-frames-and-programming-languages/).)  “Right,” Jeremy told us.  “You’re going to want to clone the files that I’ve already created and start from there.”\n\nWe blinked at him.  “Cloning?” we said.  “Is that like forking?  Do we need another repo?  Can we _have _another repo?”\n\nThus began an epic Git recap, in which Jeremy heroically explained version control to us once again.  The result was not only a much-needed refresher on the workings of [Github](https://github.com/), but a reminder of just how much we’ve learned this semester.   In true Praxis soup-to-nuts form, we’ve considered quite a few topics—we’ve discussed Git, CSS, wireframes, and [WordPress](http://wordpress.org/) as well as PHP.  With only two meetings a week and a wealth of other commitments, we’ve all struggled with the act of learning and retaining information as we move from one subject to the next.\n\nFor me, then, this winter break represents a time to catch up on things.  Like Veronica, I’ll be spending most of it working on PHP.  (As she has rightly noted in [her post](http://www.scholarslab.org/grad-student-research/breaking-things-over-winter-break/), the process represents equal parts studying and banging my head against the desk.)  In addition, though, I’ll take some time to review the many other skills that we have learned.  By keeping up my proficiency in various fields, I’ll be able to understand the design team as well as the development team, to discuss wireframes in addition to PHP.  I’ll also gain a better understanding of DH as a whole—what better holiday gift for a Praxer?\n\nHappy holidays to all, and have fun cloning in the new year!\n"},{"id":"2013-12-24-we-wish-you-a-merry-cssmas","title":"We wish you a merry CSSmas! ","author":"francesca-tripodi","date":"2013-12-24 05:37:50 -0500","categories":["Grad Student Research"],"url":"we-wish-you-a-merry-cssmas","content":"While I plan on taking some time off over Christmas and New Year's to be with family and friends, I also plan on fine-tuning my programing skills over the break. At the suggestion of Jeremy the first step is diving into tutorials (which involves equal part determination and motivation).\n\n[![Screen shot 2013-12-23 at 2.35.10 PM](http://www.scholarslab.org/wp-content/uploads/2013/12/Screen-shot-2013-12-23-at-2.35.10-PM-300x177.png)](http://www.scholarslab.org/wp-content/uploads/2013/12/Screen-shot-2013-12-23-at-2.35.10-PM.png)\n\nAs you may have gathered from [my last post ](http://www.scholarslab.org/grad-student-research/tongue-tied-in-css/) I plan on starting with the very first lesson: [What is CSS?](http://docs.webplatform.org/wiki/tutorials/learning_what_css_is)  ;)\n\nSee you all in the New Year!\n"},{"id":"2013-12-25-a-little-bit-of-everything-christmas-for-the-praxis-project-manager","title":"A little bit of everything: Christmas for the Praxis project manager","author":"stephanie-kingsley","date":"2013-12-25 10:14:46 -0500","categories":["Grad Student Research"],"url":"a-little-bit-of-everything-christmas-for-the-praxis-project-manager","content":"Merry Christmas!  As you can see from [Zach](http://www.scholarslab.org/digital-humanities/lessons-for-christmas-a-sawzall-solves-all/), [Francesca](http://www.scholarslab.org/grad-student-research/we-wish-you-a-merry-cssmas/), [Veronica](http://www.scholarslab.org/grad-student-research/breaking-things-over-winter-break/), and [Eliza's](http://www.scholarslab.org/grad-student-research/praxis-holidays/) posts, we're all equally busy preparing ourselves for the thrilling upcoming semester of building Ivanhoe.  Like my fellow Fellows, I too will be practicing my HTML, CSS, and PHP, because as project manager I need to understand the challenges the group faces in coding and designing Ivanhoe.  (I also find these computing languages fun--so why not?)  Besides this practice, I am currently in the process of coordinating a group post for New Year's Day--perhaps with flashing dinosaurs.  This post will show off some of our new CSS skills--and hopefully some PHP.  So stay tuned.\n\nAlthough things have calmed down a bit now, before leaving Charlottesville for Winter Break, I had several very busy days in the Scholars' Lab meeting with Bethany, Jeremy, and Wayne about various project management issues.  Bethany schooled me in being an administrator on the SLab [WordPress](http://wordpress.org/) site--a skill I'm now wielding in publishing Praxers' blog posts.  [Eliza described our meeting with Jeremy to wire-frame Ivanhoe in HTML.](http://www.scholarslab.org/grad-student-research/praxis-holidays/)  To do this, we needed a tutorial on collaborating in GitHub--cloning, pulling, pushing, forking, etc.  Wayne and Jeremy both advised me in laying out a schedule for Ivanhoe construction next semester (more on this in the Spring).  Finally, I compared Doodle Polls for all Scholars' Lab and Praxis people, sent a flurry of emails to coordinate meeting times for next semester, and--laptop in tow--drove home to Charlotte, where I now sit contentedly drinking a glass of eggnog.\n\nI will be working to familiarize myself with WordPress, as well as gain proficiency in setting up GitHub repositories and coordinating collaboration on Git.  Our team's New Year's post will be the result of my first self-organized Git project! Setting that up took quite a while, involving a combination of recall from Jeremy's lessons and trial-and-error before I successfully created a repository that would display as a webpage which our whole team could work on.  (Veronica mentioned that learning PHP involved [breaking things](http://www.scholarslab.org/grad-student-research/breaking-things-over-winter-break/); well, I set up and deleted three different repos before achieving success.)  I also needed to describe the process of cloning a Git repo to a teammate who had been absent for Jeremy's lesson but would need to clone over break. Teaching something is truly the best way of solidifying your own learning, which is convenient, as I have a number of friends who want me to teach them HTML over the holidays for the purposes of building their own professional web pages. To all coders everywhere, teach your friends, and spread the joy in this festive time of year!\n\nSo ultimately, my break is about learning a multitude of _bits_ of computer/coding knowledge so that I can be competent in prioritizing, scheduling, and reporting back on our progress next semester.  I look forward to cracking CSS with my chestnuts, HTML with my hazelnuts, PHP with my pecans, and WordPress with my walnuts as I work through my mixed bag of skills--and nuts!--this winter.\n"},{"id":"2013-12-31-happy-new-year-and-a-few-thoughts-to-begin-it-with","title":"Happy New Year! -- and a few thoughts to begin it with","author":"stephanie-kingsley","date":"2013-12-31 12:12:34 -0500","categories":["Grad Student Research"],"url":"happy-new-year-and-a-few-thoughts-to-begin-it-with","content":"To those who read [my post last week](http://www.scholarslab.org/grad-student-research/a-little-bit-of-everything-christmas-for-the-praxis-project-manager/), we do not have a flashy New Year's post to show you; however, I have drawn some useful insights from this circumstance which I would like to share as we kick off another year.\n\nMy first insight has to do with learning new computing skills and the [SLab](http://www.scholarslab.org/) folks.  The plan was for our Praxis fellows to contribute some HTML, CSS, and PHP to a [GitHub](https://github.com/) repository to create a web page which would read \"Happy New Year!\"  We would then display this page or include a link to it in a blog post on New Year's Day.  The idea was to demonstrate the new skills we learned in Praxis this past semester which we will be using next semester to construct Ivanhoe.  I failed, however, to account for a few pitfalls along the way.  For instance, as Wayne pointed out to me over email, GitHub will not support PHP; I would instead need to use [Heroku](http://heroku.com/) or [AppFog](https://www.appfog.com/) to deploy it.  So my battle with Heroku commenced.  Setting up the account was easy enough, but problems began when I attempted to set up remote repositories and apps.  Despite my having set up several apps with names like \"vast ocean\" and \"aqueous fountain,\" in actually trying to verify the correct remote repository or add files to the app, either the repo would \"already exist\" or the push would fail for one reason or another.  This post will not be a particularly tech-savvy one, but the point of all this is rather to say how grateful I am for the wonderful support and patience of Eric, Jeremy, and Wayne (our development and design SLab mentors) in assisting us Praxers in tackling the most elementary of computing problems.  This was the day after Christmas, and all our SLab friends were probably relaxing or celebrating, as I should have been doing--so no assistance via chat, either.  I spent close to four hours waging war on Heroku when one of the SLab folks could have set me up in five minutes.  I stubbornly relish figuring things out for myself, but sometimes, that really can result in--like [Veronica](http://www.scholarslab.org/grad-student-research/breaking-things-over-winter-break/) and [Eliza](http://www.scholarslab.org/grad-student-research/praxis-holidays/)--banging my head on my keyboard.  Thank you, Eric, Jeremy, and Wayne, for your teaching and patience.\n\nMy second insight relates to project management.  I must admit, I concocted this plan myself.  As an optimist, I tend to see creative endeavors as solely fun and easier to execute than they actually are in the end.  I failed to anticipate the obvious: that no one wants to or should be badgered into doing more work--however fun--between December 24th and January 1... period.  I underestimated how much effort this thing would take to pull off (as my Heroku efforts demonstrate), and I got overly enthusiastic about this new creative endeavor, expecting others to feel the same.  I ended up calling off the project, and we all went off to revel with family and friends, just as we should.  To my fellow Praxis teammates, I apologize for my unseasonable exuberance.  The project manager's job should be to figure out what work needs to be done and how it can be executed most efficiently--not to create more work for the team.  So even though we do not have a flashy New-Year's post, I am happy to say that we do have a more sensible project manager.\n\nA Happy New Year to our Praxis team, to the entire SLab, and to all our followers!  I look forward to continuing work in January and keeping everyone posted on Ivanhoe and all our Praxis experiences along the way.\n"},{"id":"2014-01-05-building-a-website-and-pulling-apart-wordpress-plugins","title":"Building a Website and Pulling Apart WordPress Plugins","author":"scott-bailey","date":"2014-01-05 08:02:03 -0500","categories":["Grad Student Research"],"url":"building-a-website-and-pulling-apart-wordpress-plugins","content":"For the first time in all my years as a graduate student, I decided to take some of Christmas break as an actual break from school work. So, for the first two weeks of break, I refused to read school related emails, read mostly neuroscience instead of theology or philosophy, only thought about my dissertation a couple of times a day, and only glanced at code here and there. After those restful two weeks, it's time to get back to work though.\n\nBefore school starts back up, I've got a few things I'm trying to do. Foremost, I'm finishing up building my own website, which will be hosted through [Github Pages](http://pages.github.com/) and run there with [Jekyll](http://jekyllrb.com/). I've been playing around with different grid frameworks, such as [Skeleton](http://www.getskeleton.com/), but have decided finally to just write it from scratch with no grid given the simplicity of what I'm trying to do. A surprisingly hard part of the whole process, I've found, is picking good typography. As writing focused websites and blogging platforms, such as [Medium](https://medium.com/), become more popular, more and more designers seem to be moving toward clear, minimal layouts and designs which serve as backdrops to more interesting typographical choices. As I am myself drawn toward sites with a lot of white space, minimal navigational elements, and larger text, I've been designing my own site similarly, and trying to choose a good font stack to really set off my own written content, as the primary focus of the website will be blog posts. Doing so, though, has led to hours of perusing fonts on [Google Fonts](http://www.google.com/fonts) and [Font Squirrel](http://www.fontsquirrel.com/) and reading articles on typography in modern web design.\n\nMore immediately relevant to [Praxis](http://praxis.scholarslab.org/), I'm reviewing our lessons on PHP from December, continuing to practice through resources provided by the SLAB staff, and beginning to pick through [WordPress Plugins](http://wordpress.org/plugins/). With the last of these, I'm paying a lot of attention to the structure of the plugin, and the structure of the code. One of the most important things we can do in building the Ivanhoe plugin is to rigorously stick to best practices, including clear and consistent commenting, to make our code easily readable for future groups and/or outside developers who might want to work on it in the future.\n\nAs we get closer to our official return to school, I look forward to hitting the ground running with the other Praxers and building something not just working but sustainable!\n"},{"id":"2014-01-06-are-you-our-new-digital-humanities-developer","title":"Are you our new Digital Humanities Developer?","author":"wayne-graham","date":"2014-01-06 08:56:56 -0500","categories":["Announcements"],"url":"are-you-our-new-digital-humanities-developer","content":"Are you an enthusiastic software developer with an interest in the humanities or cultural heritage? The internationally-recognized [Scholars' Lab](http://scholarslab.org) is seeking a digital humanities software engineer to join its innovative Research and Development group. At the [University of Virginia](http://www.virginia.edu/)-based Scholars' Lab, you'll work on projects like [Neatline](http://neatline.org) and collaborations with UVa faculty and students, mentor graduate fellows, and help teach in our [Praxis Program](http://praxis.scholarslab.org/). In addition to contributing to all of these facets of digital humanities work at Virginia, you will also be eligible for \"20% time,\" where you are encouraged to pursue your own (often collaborative) R&D project or scholarship.\n\nAs a Digital Humanities software engineer reporting to the Head of R&D for the Scholars' Lab, you will be responsible for building, testing, and debugging code, developing documentation, and helping to manage our server infrastructure. You should possess a fine attention to detail and a high level of accountability and responsibility. We're looking for someone who enjoys technical challenges, likes to figure out how things work, and stays involved in the latest web and digitial humanities technologies. You will need to fit in with a creative and collaborative group of software engineers to help create the next generation of scholarly interfaces. We particularly encourage applications by women developers and members of other under-represented groups.\n\nDemonstrated ability with one (or more) of the following:\n\n  * Your Github/Bitbucket/other repository\n  * Your Open Source Work\n  * Your awesome blog\n  * Code samples from side projects\n  * Your production website (handling real traffic)\n  * Ability to work with technical and non-technical collaborators\n\n## Duties and Responsibilities\n\n  * Build, test, and debug open source software\n  * Estimate effort for software projects\n  * Brainstorm and prototype new concepts and approaches into real things\n  * Server and service deployments, server and database installations and configuration management\n  * Ability to draft and communicate design concepts\n  * Writing and updating internal documentation of systems and processes\n  * Knowledge of systems and network security issues and trends\n  * Maintain distinct environments such as development, staging, and production\n\n## Qualifications\n\n  * Experience with configuration management systems and concepts (e.g. puppet, chef, Ansible, cfengine)\n  * Experience building web applications\n  * Knowledge (or ability to learn) our technology stack, which includes:\n    * Ruby/Ruby on Rails/Sinatra\n    * PHP/Omeka\n    * MySQL/PostgreSQL/PostGIS\n    * JavaScript/CoffeeScript (Backbone, Ember, jQuery)\n    * Testing frameworks (PHPUnit, RSpec, Cucumber, Jasmine)\n    * git\n    * Cocoon\n    * JSP\n    * Capistrano/Vagrant\n  * Passion for growing your skills, tackling interesting work and challenging problems\n  * Ability to design and write well-structured, maintainable, well-documented code that balances beauty and pragmatism\n  * Strong communication skills\n  * Experience in the digital humanities is a plus\n\n## To Apply\n\nFor full details, and to apply for this position, see the [official posting](http://jobs.virginia.edu/applicants/Central?quickFind=72340) (posting number 0613484 if you search [Jobs@UVa](https://jobs.virginia.edu)). The University of Virginia is an Equal Opportunity/Affirmative Action employer, strongly committed to achieving excellence though cultural diversity. The University actively encourages applications and nominations from members of underrepresented groups.\n"},{"id":"2014-01-07-faulty-format-or-user-error","title":"Faulty Format or User Error?","author":"francesca-tripodi","date":"2014-01-07 07:00:39 -0500","categories":["Grad Student Research"],"url":"faulty-format-or-user-error","content":"As you've already learned from [Stephanie's post ](http://www.scholarslab.org/grad-student-research/turning-points-in-praxis-new-roles-wire-frames-and-programming-languages/) team delegates (Stephanie & Eliza) have been wireframing ideas for Ivanhoe since we've decided on creating a plugin for [WordPress](http://wordpress.org/). But before this happened, the entire team brainstormed the features that we would like to see included in our final product. As we became animated about our \"must haves\" (uploading visuals, URL linking, text options) - Bethany brought up a good point - WordPress already does all these things (Plugin not required!) Her statement got me thinking - for all of our desires to create \"visual connections\" - none of us do this! Perhaps some of the problem comes with the way WordPress is set up - for those of us who rarely notice here is a screen shot (click to make it larger):\n\n\n[![Screen shot 2013-12-23 at 2.54.29 PM](http://www.scholarslab.org/wp-content/uploads/2013/12/Screen-shot-2013-12-23-at-2.54.29-PM-300x157.png)](http://www.scholarslab.org/wp-content/uploads/2013/12/Screen-shot-2013-12-23-at-2.54.29-PM.png)\n\n\nPerhaps part of the reason people don't link or use media is that the primary space is geared toward writing. Even the symbols most prominent mimic those found on Word (a text-based tool). Think about how different it would be when you clicked on the button to create a new post, this is what came up (yes, it's a bit ugly...but go with me here)\n\n[![Screen shot 2013-12-23 at 3.35.13 PM](http://www.scholarslab.org/wp-content/uploads/2013/12/Screen-shot-2013-12-23-at-3.35.13-PM-300x277.png)](http://www.scholarslab.org/wp-content/uploads/2013/12/Screen-shot-2013-12-23-at-3.35.13-PM.png)\n\nWhile these are essentially the same features that already exist in basic WordPress the emphasis on the visual makes it seem like something else entirely (a point [McLuhan](http://www.youtube.com/watch?v=ImaH51F4HBw) has been making since the seventies). So what does this mean for us? Well for starts I think we all need to look more carefully at what WordPress already offers and then think about how the organization/visualization of what we create will influence user application in ways we might not intend. Doing so will help us create a product that people want to use but also an environment that fosters our intended ideas.\n"},{"id":"2014-01-07-map-sleuthing-in-africa","title":"Map Sleuthing in Africa","author":"kelly-johnston","date":"2014-01-07 04:36:20 -0500","categories":["Geospatial and Temporal"],"url":"map-sleuthing-in-africa","content":"One of the many fun things we do in the [Scholars’ Lab](http://guides.lib.virginia.edu/gis) is help people find geographic datasets.\n\n[caption id=\"attachment_9186\" align=\"alignnone\" width=\"344\"][![srh.noaa.gov](http://www.scholarslab.org/wp-content/uploads/2013/12/gislayers.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/12/gislayers.jpg) Yes, these are geographic datasets via srh.noaa.gov[/caption]\n\nFolks use geographic datasets to make maps and for spatial analysis using geographic information systems software.  Finding detailed local-scale datasets can be hard.  And finding local-scale geographic datasets for areas outside the United States is even harder, often approaching impossible, which takes a little longer.\n\nCharlottesville, the University of Virginia’s home town, is a [sister city](http://www.charlottesville.org/index.aspx?page=2985) with [Winneba, Ghana](http://goo.gl/maps/mNTPQ).  So it follows that the UVa School of Architecture chose Winneba as one of the cities for its interdisciplinary Resilient Communities project focusing on the challenge of climate change and rapid urbanization for coastal communities. Local-scale mapping and analysis for Resilient Communities will require detailed datasets describing both natural and man-made features.\n\nDuring the summer of 2013 our Scholars' Lab colleague Matt West invested a good bit of time searching for Ghana datasets.  But the datasets found from afar were coarse in resolution, better suited for regional analysis than the kind of local-scale work planned for Resilient Communities.  Seemed our best option for success was to find local experts in Ghana with access to the best local datasets.\n\n[caption id=\"attachment_9299\" align=\"alignnone\" width=\"1024\"][![MV Explorer via semesteratsea.org](http://www.scholarslab.org/wp-content/uploads/2014/01/20131202-MV-Explorer-Kite-Aerial-Photography-sailing-into-Rio-1024x768.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/01/20131202-MV-Explorer-Kite-Aerial-Photography-sailing-into-Rio.jpg) Semester At Sea on the MV Explorer via semesteratsea.org[/caption]\n\nOn the [Fall 2013 voyage of Semester At Sea](http://www.semesteratsea.org/voyages/fall-2013/), a ship-based study abroad program, I was fortunate to visit Ghana with [Nancy Takahashi](http://www.arch.virginia.edu/people/directory/nancy-takahashi), distinguished lecturer of Landscape Architecture at UVa.  Nancy is leading the Resilient Communities effort in Winneba.  Joining us was Semester At Sea faculty member [Sian Davies-Vollum](http://www.semesteratsea.org/faculty-and-staff/sian-davies-vollum/), an expert on coastal geology.\n\nTo kick-off the Ghana project, Nancy arranged a series of meetings with Ghanaian experts.  Throughout our time in Ghana, city planner and chair of the Winneba-Charlottesville Sister City Commission, Joe Baami gave us excellent guidance.\n\n[caption id=\"attachment_9187\" align=\"alignnone\" width=\"1024\"][![20131016 Winneba 03 welcome ceremony, Joe Baami](http://www.scholarslab.org/wp-content/uploads/2013/12/20131016-Winneba-03-welcome-ceremony-Joe-Baami-1024x789.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/12/20131016-Winneba-03-welcome-ceremony-Joe-Baami.jpg) Joe Baami introduces Winneba elders[/caption]\n\nIn Winneba we met with local government leaders to learn about the community and discuss plans for collaboration. We conferred with local school officials, visited the site where a new public library is planned, walked the downtown waterfront lined with traditional fishing boats, visited coastal and inland neighborhoods, and generally became more familiar with the lay of the land and local culture.  One goal was to lay the groundwork for future visits by Semester At Sea students who will contribute to Resilient Communities through fieldwork and research.\n\n[caption id=\"attachment_9201\" align=\"alignnone\" width=\"1024\"][![Winneba Waterfront](http://www.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-197-boats-beach-1024x768.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-197-boats-beach.jpg) Winneba Waterfront[/caption]\n\n[caption id=\"attachment_9189\" align=\"alignnone\" width=\"565\"][![20131017 Winneba 206 Sean Davies-Vollum Kelly Iddrisu Shani](http://www.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-206-Sean-Davies-Vollum-Kelly-Iddrisu-Shani-e1387847553766-565x1024.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-206-Sean-Davies-Vollum-Kelly-Iddrisu-Shani-e1387847553766.jpg) Kelly Johnston with Iddrisu Shani - Winneba Environmental Health Officer[/caption]\n\nTo better understand Winneba we needed maps.  Ideally we would find locally produced maps showing roads, streams, and elevation contours along with point locations for important sites like schools, clinics, and drinking water sources.\n\nBut early in our conversations with Winneba government officials we learned the creation of maps built on local-scale GIS datasets was planned but had not yet happened.  So as we toured Winneba we used a printed Google Map (with minimal detail) to orient ourselves.\n\n[caption id=\"attachment_9190\" align=\"alignnone\" width=\"1024\"][![20131017 Winneba 240](http://www.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-240-1024x768.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-240.jpg) Joe Baami, Iddrisu Shani, and Winneba Assembly Representative[/caption]\n\nHelping us in Winneba was our Ghanaian colleague Benjamin (Benjie) Akuetteh with the [Centre for Remote Sensing and Geographic Information Services](http://cersgis.org/home.html) at the University of Ghana, Legon.  Benjie focused our discussion on the importance of understanding the spatial relationships at work in the community.  He believes maps should start every such discussion and invariably serve to bring folks from across disciplines more quickly to a common understanding.  Benjie's many successes working with localities and geographic datasets in Ghana lent credibility to his views.\n\n[caption id=\"attachment_9192\" align=\"alignnone\" width=\"1024\"][![20131017 Winneba 176 Kelly Benjamin Akuetteh (Benji) GIS guys](http://www.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-176-Kelly-Benjamin-Akuetteh-Benji-GIS-guys-1024x768.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/12/20131017-Winneba-176-Kelly-Benjamin-Akuetteh-Benji-GIS-guys.jpg) Kelly Johnston with Benjamin Akuetteh, Principal Applications Specialist, Centre for Remote Sensing and Geographic Information Services at the University of Ghana, Legon[/caption]\n\nWe traveled to Legon in the northern suburbs of Ghana's capital, Accra, to visit the campus and tour the Centre for Remote Sensing and Geographic Information Services where we met Benjie's colleagues.  It's a very mappy place.\n\nLater, magic happened when Benjie made some calls and found a lead on some Winneba data.  So we set off on an adventure to the Survey Office of Ghana in Accra.\n\n[caption id=\"attachment_9193\" align=\"alignnone\" width=\"1024\"][![](http://www.scholarslab.org/wp-content/uploads/2013/12/20131018-Accra-288-Survey-Office-1024x768.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/12/20131018-Accra-288-Survey-Office.jpg) One of the many buildings at the Survey Office of Ghana[/caption]\n\nA series of meetings starting with the Director of Survey at the Ghana Survey Office lead us into a cartographers' paradise.  Detailed datasets for Winneba may already exist!  Then at the desk of  Charles Nanzo of the Survey Office we saw for the first time those very detailed Winneba GIS datasets flash up on the screen!  The appearance of street centerlines, political boundaries, contour lines, and building footprints generated excitement among all the map nerds.\n\n[caption id=\"attachment_9200\" align=\"alignnone\" width=\"1024\"][![GIS Datasets!](http://www.scholarslab.org/wp-content/uploads/2013/12/20131018-Accra-290-survey-office-1024x768.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/12/20131018-Accra-290-survey-office.jpg) Charles Nanzo displays GIS datasets for Winneba[/caption]\n\nUnfortunately we were not able to access the data to examine it in detail but we believe these datasets can jumpstart the process of establishing a detailed basemap for Resilient Communities in Ghana.  Since we were not prepared for a large data purchase on the spot, we made some notes with contact information, snapped a few photos, and opted temporarily for a less-detailed paper map purchased from the nearby analog “Map Sales Office”.\n\n[caption id=\"attachment_9197\" align=\"alignnone\" width=\"1024\"][![The Sign Says It All](http://www.scholarslab.org/wp-content/uploads/2013/12/20131018-Accra-293-survey-office-map-sales-office-1024x768.jpg)](http://www.scholarslab.org/wp-content/uploads/2013/12/20131018-Accra-293-survey-office-map-sales-office.jpg) The Sign Says It All[/caption]\n\nNow back in Charlottesville we’re working with the University of Virginia Library to acquire the digital datasets from the Ghana Survey Office to make the best use of their content for the Resilient Communities project.\n\nOur experience tracking down these digital datasets points to the importance of local knowledge and local networks.  Without the help of our local experts in Ghana, these detailed local datasets would not have come to light.  Of course, this is only a small first step.  We look forward to a long and productive relationship working in collaboration with our friends in Ghana.\n"},{"id":"2014-01-14-on-community-listening-1","title":"On Community Listening: 1","author":"erik-deluca","date":"2014-01-14 07:02:03 -0500","categories":["Digital Humanities","Experimental Humanities","Grad Student Research","Research and Development"],"url":"on-community-listening-1","content":"During the Fall semester, Scholars Lab Design Architect, Jeremy Boggs and I spent time conceptualizing and sketching a web interface to house a perusable version of my ethnographic composition, _Community Listening in Isle Royale National Park_. Check out the project abstract below. This blog post is the first of many posts that will track our developments.\n\nMeanwhile, check out past Scholars' Lab Fellow, [Wendy Hsu](http://beingwendyhsu.info/)'s recent series in Ethnography Matters, [On Digital Ethnography](http://ethnographymatters.net/category/series/on-digital-ethnography/). This four-part series provides a groundwork for discussing and making ethnography that exists beyond text and print. My project _Community Listening _falls nicely_ _into this category.\n\nCompletely unrelated: I've been listening to [THIS](http://www.youtube.com/watch?v=0_Q-1u6sNgQ) and [THIS](http://www.youtube.com/watch?v=EKydgctnEDM).\n\n---\n\nCommunity Listening in Isle Royale National Park traces how I became part of a dialogue among a team of wolf biologists and a community of park-explorers who share a unique deep listening relationship. The scientists involved in this project are the primary investigators in the five-decade long wolf/moose project, the longest continuous wildlife study in the world. The primary way these researchers determine clues of wolf reproduction is to listen for the sounds of group howling during the summer months when excitement at den sites erupts during pup feeding time. For these clues, the researchers ears, or “antennas” as they put it, tap into a network of visitors and employees who are scattered across the island, listening. This listening network is directly tied to the ecological well-being of the park, which is currently at risk of major change because the wolves, who play a vital role, are on the brink of “blinking-out” due to global climate change. The ethnographic composition weaves together several different types of sound data collected during fieldwork: soundscape recordings of the place, interviews with the wolf/moose researchers, interviews with park visitors and employees, audio diaries, an audio essay derived from field notes, and archival recordings. The work will exist in three formats: a 25-minute radio work, an online web environment that mirrors the listening network as an interactive form, and a concert piece. Issues of access are important to this project because of its interdisciplinary nature. Therefore, the narrative frame of each format is meant to guide the listeners through the specific project content while also creating a space for self-perusal and discovery.\n"},{"id":"2014-01-14-when-expectations-meet-reality","title":"When expectations meet reality ","author":"francesca-tripodi","date":"2014-01-14 05:45:06 -0500","categories":["Grad Student Research"],"url":"when-expectations-meet-reality","content":"As with the beginning of any new year, 2014 brings with it high hopes. When the clock struck 12:01, I turned to my husband and said that one of my professional new years resolutions is to be \"more productive.\" But as I reflect a bit more on this goal, it got me thinking, how much more productive can I actually be? This year in addition to taking on this Praxis Fellowship and TAing for the Media Studies Department (approximately 80 students each semester) I am trying to complete/defend my dissertation proposal. Since I'm finally done with coursework, I thought this year was going to be a breeze! Of course, I didn't quite consider the _reality _of my situation - in addition to all of these professional responsibilities I was taking on another large undertaking...raising my son who was born on July 23, 2013. _Insert on-going (and extremely convoluted) debate on [ if women can really have it all.](http://www.theatlantic.com/magazine/archive/2012/07/why-women-still-cant-have-it-all/309020/)  _\n\nBeing a new mom and a graduate student is quite frankly bizarre. In some respects I live a semi-charmed life. Unlike most of the other working moms that I know, I am able to spend most days with my son (in part this stems from the fact that my modest income does not begin to cover the ridiculous price of childcare). Luckily for me I work for amazing people who let me do a great deal of my work at home (both in Media Studies and Scholars' Lab). Thanks to Bethany in particular, who doesn't believe in the [Yahoo approach to creativity,](http://www.nytimes.com/2013/03/06/technology/yahoos-in-office-policy-aims-to-bolster-morale.html?_r=0) and an amazing mother-in-law who comes up once a week to help with childcare, I'm just barely making it work. However, even with all of this extra help I basically have no free time and spend most nights and weekends working.  I feel like I'm in a constant state of catching up. Basically, this is my life:\n\n\n\nWhat could this post possibly do with Praxis? Well, for one it helps to explain why I'm so lost when it comes to learning new programming languages! But it mostly reaches out as a thank you to the SLAB team for helping me with my schedule (Wayne for Web-conferencing me in and Jeremy for making himself available on IRC over the weekends). And, a shout out to the other members of my team: five highly skilled and motivated individuals who are juggling their own eccentricities of graduate life. Finally - I hope my post can serve as a reminder to all of us to set realistic expectations for ourselves. Perhaps my new years goal is not to be more \"productive\" but to be more \"realistic\" about my capabilities and do the best that I can.\n\nI'll wrap this up here with a bit of humor[ (](http://www.youtube.com/watch?v=4YIj4rLYo0c)[courtesy of PhD comics](http://www.phdcomics.com/comics/archive.php?comicid=1500))\n\n[![phd060112s](http://www.scholarslab.org/wp-content/uploads/2014/01/phd060112s-300x130.gif)](http://www.scholarslab.org/wp-content/uploads/2014/01/phd060112s.gif)\n"},{"id":"2014-01-15-neatline-2-2-0","title":"Neatline release-apalooza: Neatline 2.2.0, Neatscape, Astrolabe","author":"david-mcclure","date":"2014-01-15 07:28:07 -0500","categories":["Announcements"],"url":"neatline-2-2-0","content":"Today we're excited to announce the release of Neatline 2.2.0! This is a big update that ships out a cluster of features and fixes that address a couple of rough spots identified by users over the course of the last couple months. 2.2.0 focuses on improvements in two areas - first, we've overhauled the workflows that connect Neatline records with Omeka items to make them more intuitive, flexible, and feature-rich, with the goal of making the overall integration between the two environments feel more seamless and low-friction. Second, we've added a system of interactive documentation to the editor that builds reference materials and tutorials directly into the interface, which should make it easier for new users to find their way around.\n\nWe're also pushing out maintenance releases of the two extensions, NeatlineSimile and NeatlineWaypoints, which add compatibility for Neatline 2.2 and deal with a couple of minor bugs. As always, grab the code from the Omeka addons repository:\n\n**[Neatline 2.2.0](http://omeka.org/add-ons/plugins/neatline/)** | **[NeatlineSimile 2.0.1](http://omeka.org/add-ons/plugins/neatlinesimile/)** | **[NeatlineWaypoints 2.0.2](http://omeka.org/add-ons/plugins/neatlinewaypoints/)**\n\nWhat's more, we're also making release candidates available for two new Omeka themes designed to showcase Neatline exhibits: [Astrolabe](http://omeka.org/add-ons/themes/astrolabe/) and [Neatscape](http://omeka.org/add-ons/themes/neatscape/). Loyal readers may recall that a while back, we [ran a theme naming contest](http://www.scholarslab.org/announcements/neatline-omeka-theme-name-winners/), and we're finally making good on our word! These are just release candidates, but we wanted to get them out in the open for testing and feedback before cutting off stable releases. Give them a spin, and be sure to file a ticket on the respective issue trackers ([Astrolabe](https://github.com/scholarslab/astrolabe/issues) and [Neatscape](https://github.com/scholarslab/neatscape/issues)) if you find quirks or need new features.\n\nSome highlights in Neatline 2.2:\n\n\n\n\n\n\n  * **Adds a new interface for linking Neatline records to Omeka items** that makes it possible to browse the entire collection of items, run full-text searches, and instantaneously preview the Omeka content (metadata, images, etc.) as it will appear in the public Neatline exhibit.\n\n[![item-binding](http://dclure.org/wp-content/uploads/2014/01/item-binding.gif)](http://dclure.org/wp-content/uploads/2014/01/item-binding.gif)\n\n\n\n\n\n  * **Frees up the \"Title\" and \"Body\" fields for modification on Neatline records linked to Omeka items**. Previously, these fields were automatically populated with the item content imported from Omeka, making it impossible to add custom information not contained in the Omeka metadata. Now, Neatline leaves these fields open for editing and displays them above the content synced in from Omeka, making it possible (though not required) to add exhibit-specific headings and text descriptions for imported items.\n\n\n\n  * **Makes it possible to import raw data from the Dublin Core \"Coverage\" field**. When Omeka items are imported into Neatline exhibits, existing values in the Dublin Core \"Coverage\" field (either KML or WKT strings) are now automatically imported into Neatline and displayed on the map. Previously, this only worked if the coverage on the item was created with the Neatline Features plugin. With this functionality in place, it's much easier to bulk-import existing spatial data sets - use the [CSV Import](http://omeka.org/add-ons/plugins/csv-import/) plugin to populate a collection of items, and then push the new items to a Neatline exhibit.\n\n\n\n\n  * **Adds interactive documentation to the editor** that builds reference materials for each individual control directly into the interface. Now, the heading for each input is followed by a little \"**?**\" button that, when clicked, overlays a document with information about what the control does, how to use it, and how it interacts with other functionality. The goal is to make the editor effectively self-documenting, so that it's unnecessary to find separate documentation and toggle back and forth between different tabs as you work.\n\n[![interactive-docs](http://dclure.org/wp-content/uploads/2014/01/interactive-docs.gif)](http://dclure.org/wp-content/uploads/2014/01/interactive-docs.gif)\n\n\n\n\n\n\nLast semester was a busy one for Neatline - we were supporting twelve classes here at UVa that are using Neatline for research assignments, and had the good fortune to collaborate with a number of folks at Harvard, Stanford, Northeastern, Duke, Indiana, and elsewhere who were using Neatline or gearing up for upcoming projects in the new year. We've also got a couple of exciting ideas brewing here in the lab for new, Neatline-powered projects - keep an eye on this space over the course of the next couple months.\n\nAs always, don't hesitate to file bug reports on the [issue tracker](https://github.com/scholarslab/Neatline/issues), post questions to the [forums](http://omeka.org/forums/forum/plugins), or [contact us directly](mailto:neatline@collab.itc.virginia.edu). Happy new year!\n"},{"id":"2014-01-15-on-community-listening-2","title":"On Community Listening: 2","author":"erik-deluca","date":"2014-01-15 07:05:30 -0500","categories":["Grad Student Research"],"url":"on-community-listening-2","content":"[Don't miss the [first part](http://www.scholarslab.org/digital-humanities/on-community-listening-1/) of Scholars' Lab graduate fellow Erik DeLuca's series on his project, continued here.]\n\nCheck out this “trailer” I made for _Community Listening_:\n\n[soundcloud url=\"http://api.soundcloud.com/tracks/119822657\" params=\"color=ff6600&auto_play=false&show_artwork=true\" width=\"100%\" height=\"166\" iframe=\"true\" /]\n\nThe excerpts poetic opening of place is meant to link the listeners to the island environment and contextualize the content to come. This opening is composed of field recordings that I made in 2011 while I was the Artist-In-Residence in the park. We hear the high-frequency sounds of tiny invertebrates in tidal pools, underwater sounds of Lake Superior, fog horns, bell buoys, and the beeps of Rolf's wolf radio collar receiver. These recordings are mixed with selections from a string quartet that I wrote for the park which was composed from spectral information of the fog horn recordings. To understand the diverse perspective of this network I conducted interviews with park visitors and employees. In addition, I gave several small sound recorders to visitors and asked them to keep audio diaries of their experience listening in the park. Using a digital audio workstation and a selection of this sound data, I composed the next section of the excerpt, a sonic representation of the community listening network. Using the technology I was able to poetically portray the polyvocality of the network in sound. Meanwhile, to maintain narrative continuity, a layer of environmental recordings continues, transitioning into a brief introduction of how the Peterson’s wolf/moose study benefits from the network. There is a considerable amount of noise in this recorded interview because I reverted to using my lo-fi, unobtrusive camera-sized compact sound recorder for interview tasks. I packed up my pro sound gear midway through fieldwork because it was acting as physical and cultural barrier between myself and the Peterson's.  They seemed to be lumping me into the media category of journalists who were flocking to the island at the time to report on the low wolf numbers. When I switched to the small lo-fi recorder I noticed a significant difference in the Peterson's attitude towards me and decided that I would much rather deal with lo-fi sound quality in exchange for a comfortable and communicative environment. In fact, I have come to like these lo-fi recordings because they remind the listener of the recordist in the field due to audible disturbances and artifacts. My goal for this excerpt is to create a narrative frame to guide listeners through which allows for optimal room for self-perusal and discovery.\n"},{"id":"2014-01-16-building-from-scratch-then-scratching-that-build","title":"Building From Scratch Then Scratching That Build","author":"scott-bailey","date":"2014-01-16 06:00:25 -0500","categories":["Grad Student Research"],"url":"building-from-scratch-then-scratching-that-build","content":"Over the past couple of weeks I've been working hard on building my own website. I actually began this work last summer, in the midst of a code camp for humanities graduate students being run at U.Va. with the great help of [Scholars' Lab](http://www.scholarslab.org/) staff. There, I had gotten an intro to git, found out about [Github Pages](http://pages.github.com/) and [Jekyll](http://jekyllrb.com/), and had learned enough PHP to build a couple different WordPress themes throughout July and August. But, I was never happy with what I had produced and when school started back, my priorities shifted. When the holiday break began, I knew I needed to go ahead and get something up and running.\n\nI decided to host the site with Github Pages using Jekyll for a couple of different reasons. One, it's free and I'm a humanities graduate student. Two, I wanted my site to be focused around blog content, and Jekyll will auto-build a blog from your markdown files. I have written nearly everything over the past six months in markdown, so this fits well with my current habits and preferences. And three, hosting through Github Pages has made using git and the terminal a comfortable habit, all to the betterment of my future.\n\nThis still left the question of what to build and how to build it. Throughout the year, I had spent time looking at different frameworks and boilerplates. I checked out the [HTML5 Boilerplate](http://html5boilerplate.com/), browsed through [Bootstrap](http://getbootstrap.com/), and looked at a number of different grid systems, such as [Skeleton](http://www.getskeleton.com/). A number of these different frameworks and systems fit the design aesthetic I like - minimal clutter, clear navigation, focus on text and typography and the like. As I said in [my previous blog post](http://www.scholarslab.org/grad-student-research/building-a-website-and-pulling-apart-wordpress-plugins/), I decided to build it from scratch, thinking that the design I had in mind and had sketched out was pretty simple to code. And it was, mostly. I built a blog site with a left sidebar navigation menu taking roughly 2/5 of the page with the rest displaying the blog content. I used [Adobe Kuler](https://kuler.adobe.com/create/color-wheel/) to help generate a color scheme I liked. I spent quite a bit of time looking at typography. Wanting to keep up with the times, I made the whole thing responsive and wrote media queries to shift the navigation menu to the top at an appropriate breakpoint, and though I still needed to iron out some positioning and spacing kinks, it all worked. But there was something missing, some bit of polish and clarity that you find in great websites today. That's when I realized that minimalism isn't necessarily easier to code. The basics of the layout and style were straightforward, but when there is less on the page, every detail matters, and to code those details, those little bits of spaces, calculating the proper ems and such, that would take time.\n\nUnfortunately, time is not necessarily what I have in abundance. With the spring semester starting, my dissertation looms especially large, as does planning for the course I'm TAing and preparing for our work in [Praxis](http://praxis.scholarslab.org/) on Ivanhoe. In the interest of pragmatism, I decided near the end of break to scratch my built-from-scratch build. I want my site to show a bit of my coding ability, but I also want it to meet a certain standard. My own from scratch build isn't quite there, though given some time it will be. I decided, though, to rebuild using the [PureCSS Framework](http://purecss.io/). It's a set of CSS modules that can be used all together or pulled apart. To get the layout and appearance I wanted, and still be responsive, I decided to use one of their layouts similar to my own. I worked off of the framework and adapted it to the templates that Jekyll needs, done with the [Liquid](http://docs.shopify.com/themes/liquid-basics) templating language, to get content to display where and how I wanted it. I then started changing things a bit with my own CSS, using the color scheme I had decided on to style the links, to change a bit in the menu, to change spacing, and then put in my own font choices. I hooked in [Font Awesome](http://fontawesome.io/) to get icons for links to my Github and Twitter pages. The whole time, I used git to track the changes I was making (as my [commit history](https://github.com/csbailey5t/csbailey5t.github.com/commits/master) on Github shows) and ran Jekyll locally to know what was going on before pushing to Github.\n\nThere are still things here and there I want to tweak, such as spacing, but the website is up and running at [csbailey.org](http://csbailey.org/).\n\nAnd I learned this lesson, which I think has been dwelling in the back of my head for some months. The website that is up and running is better than the built from scratch website that sits on your computer, never to be seen by anyone else.\n"},{"id":"2014-01-20-look-its-a-game-its-a-simulation-no-its-gamification","title":"Look, it's a game, it's a simulation, no...it's GAMIFICATION! ","author":"francesca-tripodi","date":"2014-01-20 04:00:01 -0500","categories":["Grad Student Research"],"url":"look-its-a-game-its-a-simulation-no-its-gamification","content":"Back in October [I wrote a blog post](http://www.scholarslab.org/grad-student-research/are-we-gaming-or-just-simulating/) pushing back on the idea that we were actually creating a game.* Using Gredler's (2004) work on games vs simulations I was arguing that while we were calling Ivanhoe a game we weren't incorporating essential elements of game-play (i.e. rules, goals, the ability to win, etc). [As we begin wire-framing](http://www.scholarslab.org/grad-student-research/turning-points-in-praxis-new-roles-wire-frames-and-programming-languages/), these same questions are coming into play again: should we have a role journal? what about points?\n\nSuch a conversation got me thinking again about what we're actually making. To help me with this struggle, I ordered a copy of [The Gamification of Learning and Instruction Fieldbook ](http://www.amazon.com/gp/product/111867443X/ref=oh_details_o01_s00_i00?ie=UTF8&psc=1) written by Karl Kapp, Lucas Blair, and Rich Mesh (2014). By reading their book I came to realize that we're not producing a game _or _a simulation...we are actually incorporating gamification. Gamification means that we are using elements of games to create a more engaging pedagogical environment.\n\nAs Kapp et al write, \"Gamificiation is using game-based mechanics, aesthetics, and game-thinking to engage people, motivate action, promote learning, and solve problems\" (2014: 54). Moreover, I think Ivanhoe is using more _content gamification _vs _structural gamification_. While these are not mutually exclusive ideas, it seems that we want to create a product that \"makes the content more game-like but doesn't turn the content into a game\" (pg 55).\n\nSuch a revelation about what we were making gave me a renewed sense of energy about our product.  By working with the elements of gamification we can \"encourage learners to progress through content, motivate action, influence behavior, and drive innovation\" (pg 56). In doing so, we can make something that enhances learning and hopefully adds a bit more enjoyment to our classroom.\n\n\n*Side note, my original post was selected by [Digital Humanities Now for \"Editors Choice\" -](http://digitalhumanitiesnow.org/editors-choice-archive/) if you've got some time, you should peruse their other selections. It's a great way to see what else is going on in the DH world.\n"},{"id":"2014-01-23-spring-2014-scholars-lab-gis-workshop-series","title":"Spring 2014 Scholars’ Lab GIS Workshop Series","author":"chris-gist","date":"2014-01-23 10:48:14 -0500","categories":["Announcements","Geospatial and Temporal"],"url":"spring-2014-scholars-lab-gis-workshop-series","content":"All sessions are one hour and assume attendees have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials with expert assistance.  All sessions will be taught on **Wednesdays from 10AM to 11AM in the Alderman Electronic Classroom, ALD 421** (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community.\n\nFebruary 5th\n**Making Your First Map with ArcGIS\n**Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa. \n\nFebruary 12th\n**Getting Your Data on a Map\n**Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map.\n\nFebruary 19th\n**Georeferencing a Map\n**Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS.\n\nFebruary 26th\n**Easy Demographics\n**Need to make a quick demographic map or religious adherence?  This workshop will show you how easily navigate Social Explorer.  This powerful online application makes it easy to create maps with contemporary and historic census data and religious information.\n\nMarch 5th\n**Historic Census Data\n**Would you like to map the poverty in Philadelphia around the turn of the 20th Century?  How about a racial breakdown by state in the 1860s?  This workshop will focus on how to download historic census boundary and tabular data to make historic demographic maps.\n\nMarch 19th\n**Collecting Your Own Spatial Data\n**Research projects often rely on fieldwork to build new datasets.  In this workshop we’ll focus on tools for spatial data collection. First we’ll take a quick look behind the curtain to see how GPS really works and how to use that knowledge to our advantage.  Then we’ll evaluate free or low-cost options to gather locations and associated attributes using handheld GPS devices, smartphones, and apps.  This workshop will introduce you to a range of devices and methods for mobile spatial data collection.\n\nMarch 26th\n**Making Your First Map with QGIS\n**Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This one-hour workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular open source GIS software (QGIS) and a gentle introduction to cartography.  You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa.\n\nApril 2nd\n**Quantum GIS – Adding Remote Data Services\n**Would you like to show the live weather radar on your map?  How about other live and/or free data?  This workshop will show you how to add open web service (OWS) layers to Quantum GIS and use them in a map.\n\nApril 9th\n**Working With Rasters Using Open Source Tools\n**QGIS is a popular open source software for working with spatial data.  The Geospatial Data Abstraction Library (GDAL) is an open source utility library for raster processing that’s integrated with QGIS.  In this workshop we will use QGIS and GDAL with freely available datasets to create elevation contour lines and identify the highest and lowest elevation points inside a study area.  Gain the skills you need to perform your own terrain analysis using free open source tools.     \n\nApril 16th\n**Learning Old-School Mapping Techniques\n**How did folks make maps before GPS and satellite imagery?  In this workshop we’ll focus on plane table mapping.  Using just a flat surface, a sheet of paper, a straight edge, and a pencil we’ll learn techniques to create accurate maps for large geographic areas.   With plane table mapping, if you can see it, you can map it.  \n"},{"id":"2014-01-29-dh-speaker-series-presents-adeline-koh-roopika-risam","title":"DH Speaker Series: Adeline Koh & Roopika Risam on DHpoco","author":"laura-miller","date":"2014-01-29 05:08:13 -0500","categories":["Announcements"],"url":"dh-speaker-series-presents-adeline-koh-roopika-risam","content":"### _Theories and Practices of Postcolonial Digital Humanities_\n\n\nFriday, **January 31 at 10:00 am**\nin Alderman Library, Room 421\n\n[![tumblr_mlsjq6Oq0U1s79o1mo1_1280](http://www.scholarslab.org/wp-content/uploads/2014/01/tumblr_mlsjq6Oq0U1s79o1mo1_1280-300x186.png)](http://www.scholarslab.org/wp-content/uploads/2014/01/tumblr_mlsjq6Oq0U1s79o1mo1_1280.png)\nBoth postcolonial studies and the digital humanities have gained currency within the academy but are subject to strident critiques from interlocutors. Postcolonial studies has been criticized for being overly reliant on jargon and apolitical, whereas the digital humanities have been taken to task for failing to interrogate questions of race, power, and identity fully. The [Postcolonial Digital Humanities](http://dhpoco.org/) (#dhpoco) intervenes in these gaps through theory and praxis. #dhpoco engages postcolonial studies to address global issues relating to race, gender, class, sexuality, and disability within cultures of technology while bringing the activist praxis of the digital humanities to the work of postcolonial studies. Co-founders Adeline Koh and Roopika Risam will discuss the theoretical underpinnings of #dhpoco and outline the tactics that #dhpoco employs.\n\nAdeline Koh is the director of DH @ Stockton and an assistant professor of literature at Richard Stockton College. Koh is the co-founder of Postcolonial Digital Humanities with Roopika Risam and directs Digitizing Chinese Englishmen, a digital archival project, and The Stockton Postcolonial Studies Project, an online magazine of postcolonial studies. She is the designer of Trading Races, a historical role-playing game for undergraduates,  and is a core contributor to the Profhacker column at the Chronicle of Higher Education.\n\nRoopika Risam is an Assistant Professor of English at Salem State University. Her research interests include postcolonial studies and minority discourse in the United States, and the role of technology in mediating between the two. She is the co-founder of Postcolonial Digital Humanities with Adeline Koh and is co-director of the open-access public domain critical edition of Claude McKay's Harlem Shadows with Chris Forster of Syracuse University.\n\nThis event is co-sponsored by [The Carter G. Woodson Institute](http://artsandsciences.virginia.edu/woodson/index.html) and the Scholars’ Lab.  As always, it is free, open to all, and requires no advance registration.\n\nImage courtesy of [http://dhpoco.tumblr.com/tagged/comics](http://dhpoco.tumblr.com/tagged/comics)\n"},{"id":"2014-01-30-potential-ivanhoe-users","title":"Help us design the Ivanhoe Game!","author":"stephanie-kingsley","date":"2014-01-30 12:00:47 -0500","categories":["Grad Student Research"],"url":"potential-ivanhoe-users","content":"Attention, game players and digital-pedagogy enthusiasts!\n\nThis year's [Praxis Program](http://praxis.scholarslab.org/) is rebooting SpecLab's [Ivanhoe Game](http://www.ivanhoegame.org/?page_id=21) as a [WordPress](http://wordpress.org/) [theme](http://wordpress.org/themes/) that users can install on their own websites and style and configure according to their individual needs.\n\nWe would greatly appreciate feedback on the ways potential users already employ WordPress in their course websites or personal homepages.  If you are interested in installing Ivanhoe and hosting games for students or friends, please help us by answering a few questions.\n\n\n\nThank you!\n"},{"id":"2014-01-30-spring-2014-scholars-lab-tech-workshop-series","title":"Spring 2014 Scholars' Lab Tech Workshop Series","author":"ronda-grizzle","date":"2014-01-30 11:32:21 -0500","categories":["Announcements"],"url":"spring-2014-scholars-lab-tech-workshop-series","content":"We're pleased to announce our Spring 2014 Tech Workshop series roster.\n\nAll workshop sessions are free, open to the public, and require neither previous experience nor registration.\n\n**February 12**\n_**Introduction to Neatline**_\nJoin us for a hands-on introduction to Neatline, a set of plugins for Omeka developed by the Scholars’ Lab. With Neatline, anyone can create beautiful, complex maps and narrative sequences from collections of archives and artifacts, and connect maps and narratives with timelines that are more-than-usually sensitive to ambiguity and nuance. See [neatline.org](http://neatline.org/) for more information.\n\n**Time:** 3:00-4:00pm\n**Location:** Alderman Library Electronic Classroom (ALD 421)\n**Instructor:** Ronda Grizzle\n  \n\n\n**February 26**\n**_Neatline Timelines_**\nYou say you’ve got the basics of the tools available in Neatline, but want to know more about how to use timelines to illustrate your data? This is the class for you. Please join us for this 1-hour, hands-on workshop. Prior experience with Neatline or previous attendance at the Introduction to Neatline workshop is helpful, but not required.\n\n**Time:** 3:00-4:00pm\n**Location:** Alderman Library Electronic Classroom (ALD 421)\n**Instructor:** Ronda Grizzle\n  \n\n\n**March 5**\n**_Introduction to Screen Scraping_**\nHave you ever found data on the web that you need to use, but when you look for the “download” button, it’s not there? If so, we can help you. In this workshop, we’ll start with a quick introduction to Python. We’ll also talk about several libraries that are commonly used to download pages and pull information out of them. And by the end of the session, we’ll have downloaded a web page and extracted data from it into a format that we can easily load into Excel, a database, or a statistical package.\n\n**Time:** 1:00-2:00pm\n**Location:** Alderman Library Electronic Classroom (ALD 421)\n**Instructor:** Eric Rochester\n  \n\n\n**March 19**\n**_Version Control with git_**\nIf you have trouble keeping track of changes you’ve made in projects, then git can help you. It’s a *version control* program that remembers all the changes you’ve make to a project, and it also helps you collaborate with others on that project. Git’s great for managing websites, computer program source code, and even papers. In this workshop, we’ll learn about how git views the world so that we can use it more effectively, and we’ll get hands-on practice using git to track changes in a small project.\n\n**Time:** 3:00-4:00pm\n**Location:** Alderman Library Electronic Classroom (ALD 421)\n**Instructor:** Eric Rochester\n  \n\n\n**March 26**\n**_Web Site Design and Development_**\nThis workshop will help demystify the process of creating a web site by introducing some basic concepts and methods for web site design and development. By the end of the workshop, students will understand rudimentary HTML for web page markup and Cascading Style Sheets (CSS) for web site presentation and design. Students should walk away from the class with a simple but tasteful “About Me” page they can publish to the web, and use as a foundation for building a larger web site. Comfort with a computer and web browser will make the workshop easier; prior experience with web design or development could be useful, but is not required.\n\n**Time:** 3:00-4:00pm\n**Location:** Alderman Library Electronic Classroom (ALD 421)\n**Instructor:** Jeremy Boggs\n  \n\n\n**April 2**\n**_Data Management for Graduate Students I_**\n\n**Time:** 3:00-4:00pm\n**Location:** Alderman Library Electronic Classroom (ALD 421)\n**Instructors:** Purdom Lindblad and Sherry Lake\n  \n\n\n**April 9**\n**_Data Management for Graduate Students II_**\n\n**Time:** 3:00-4:00pm\n**Location:** Alderman Library Electronic Classroom (ALD 421)\n**Instructors:** Purdom Lindblad and Sherry Lake\n  \n\n\n**April 16**\n**_Introduction to Omeka_**\n\n**Time:** 3:00-4:00pm\n**Location:** Alderman Library Electronic Classroom (ALD 421)\n**Instructor:** Ronda Grizzle\n"},{"id":"2014-02-03-wireframing-fun","title":"Wireframing fun","author":"francesca-tripodi","date":"2014-02-03 08:50:50 -0500","categories":["Grad Student Research"],"url":"wireframing-fun","content":"Over the last week Stephanie, Eliza, and I (with significant help from Jeremy) have been putting together a wireframe of our product. Partaking in this process has been so exciting and I've learned a few key concepts:\n\n\n\n\t\n  1. I've become familiar with using [GitHub](https://github.com/). By building a repository we've been able to collaborate on the same files and build tickets in the system to help us organize what else we need to create as well as assign people on the team to each task.\n\n\t\n  2. I feel more confident using HTML, CSS, and Terminal. While I'd still qualify myself as a novice, I am starting to feel a little less lost/scared.\n\n\nBelieve it or not I've actually had a great deal of fun working on this mini-team. Using HTML provides an instant-gratification that most academic work lacks. While I might never see the returns on my dissertation, I immediately see the fruits of my labor with HTML and could not help but smile when I would refresh my browser and see my changes appear. Jeremy's suggestion to actually build out the wireframes was a great one. While our end product isn't pretty, it definitely is a step in the right direction and I'm looking forward to seeing how our programming team ends up building out this blueprint.\n\nTo see our wire-frames to get a preview of what Ivanhoe will do, visit [http://scholarslab.github.io/ivanhoe-mockups/.](http://scholarslab.github.io/ivanhoe-mockups/)\n"},{"id":"2014-02-04-neatline-gettysburg-address","title":"The \"Nicolay copy\" of the Gettysburg Address","author":"david-mcclure","date":"2014-02-04 05:13:47 -0500","categories":["Experimental Humanities"],"url":"neatline-gettysburg-address","content":"_[Cross-posted from [dclure.org](http://dclure.org/logs/nicolay-copy-gettysburg-address/)]_\n\n\n\n## [Launch the Exhibit](http://neatline.dclure.org/neatline/show/gettysburg-address)\n\n\n\n[![nicolay-fullscreen](http://dclure.org/wp-content/uploads/2014/02/nicolay-fullscreen1.jpg)](http://neatline.dclure.org/neatline/show/gettysburg-address)\n\n\n\nThis is a project that I've been hacking away at for some time, but only found the time (and motivation) to get it polished up and out the door over the weekend - a digital edition of the [\"Nicolay copy\" of the Gettysburg Address](http://prod.myloc.gov/Exhibitions/gettysburgaddress/exhibitionitems/ExhibitObjects/NicolayCopy.aspx), with each of the ~250 words in the manuscript individually traced out in Neatline and wired up with a plain-text transcription of the text on the right side of the screen. I've tinkered around with [similar interfaces](http://neatline.dclure.org/neatline/show/saturn-v-stage-2) in the past, but this time I wanted to play around with different approaches to formalizing the connection between the digitally-typeset words in the text and the handwritten words in the manuscript. Your eyes tend to dart back and forth between the image and the text, and it's easy to lose your place - how to reduce that cognitive friction?\n\nTo chip away at this, I wrote a little sub-plugin for Neatline called [WordLines](https://github.com/davidmcclure/nl-widget-WordLines), which automatically overlays a little visual guideline (under the hood, a [d3](http://d3js.org/)-wrapped SVG `` element) on top of the page that connects each pair of words in the two viewports when the cursor hovers on either of the instantiations. So, when the mouse passes over words in the transcription, lines are automatically drawn to the corresponding locations on the image; and vice versa. From a technical standpoint, this turns out to be quite easy - just get the pixel offsets for the `` element in the transcription and the vector annotation on the map (for the latter, OpenLayers does the heavy lifting with helpers like `getViewPortPxFromLonLat`, which maps spatial coordinates to document-space pixel pairs), and then draw a line connecting the two points. The one hitch, though, is that this involves placing a large SVG element directly on top of the page content, which, by default, will cover all of the underlying elements (shapes on the map, words in the text) and block them from receiving the cursor events that drive the rest of the UI - including, very problematically, the `mouseleave` event that garbage-collects old lines and prevents them from getting stuck on the screen.\n\n[![wordline](http://dclure.org/wp-content/uploads/2014/02/wordline.jpg)](http://dclure.org/wp-content/uploads/2014/02/wordline.jpg)\n\nThe work-around is to put `pointer-events: none;` on the SVG element, which causes the browser to treat it as a purely visual veneer over the page - cursor events drop through to the underlying content elements, and everything else behaves normally. This is [just barely and only very recently cross-browser](http://caniuse.com/pointer-events), but I'm not sure if there's actually any other way to accomplish this, given the full set of constraints.\n\n**Modeling intuitions about scale**\n\nOriginally, I had planned to just leave it at that, but, as is almost always the case with these projects, I ended up learning lots of interesting things along the way, and I ended up going back and adding in another set of annotations that make note of some of the more historically noteworthy aspects of the manuscript. Namely, I was interested in the different types of paper used for the two different pages (Lincoln probably wrote the first page in Washington before departing, the second page after arriving in Gettysburg) and the matching fold creases on the pages, which some historians have pointed to as evidence that the Nicolay copy was perhaps the actual \"reading copy\" that Lincoln used when delivering the speech, since eyewitness accounts describe Lincoln pulling a folded piece of paper out of his coat pocket.\n\nThe other candidate is the [Hay draft](http://prod.myloc.gov/Exhibitions/gettysburgaddress/exhibitionitems/ExhibitObjects/HayDraft.aspx), which includes lots of changes and corrections in Lincoln's hand, giving it the appearance of working draft that was prepared just before the event. One problem with the Hays draft, though, is its size - it's written on larger paper and has just a single fold down the center, which would seem to make it an unlikely thing to tuck into coat pocket. When I read about this, I realized that I had paid almost no attention to the physical size of the manuscript. On the screen, it's either extremely small or almost infinitely large - a tiny speck when you zoom far back, and an endless plane of beige-and-black when you zoom in. But, in this case, size turns out to be of great historical significance - the Nicolay copy is smaller than the Hays copy, especially when folded along the set of matching creases clearly visible on the pages.\n\nSo, how big is it? This involved a bit of guesswork. The resource page for the manuscript on the Library of Congress website doesn't include dimensions, and direct Google searches didn't turn up an easy answer, so I started poking around the internet to see if I could find other Lincoln manuscripts written on the \"Executive Stationery\" used for the first page. I rooted up a couple of documents for sale by rare book sellers, and in [both](http://www.baumanrarebooks.com/rare-books/lincoln-abraham/autograph-letter-signed/63126.aspx) [cases](http://www.robertedwardauctions.com/auction/2006/1204.html) the dimensions are listed at about 5 inches in width and 7-8 inches in height, meaning that the Nicolay copy - assuming the stationery was more or less standardized - would have folded down to a roughly 5 x 2.5-inch rectangle, which seems reasonably pocket-sized. (Again, this is amateur historical conjecture - if I'm wrong, please let me know!)\n\nI sketched out little ruler annotations labeling the width of the page and the height of the fold segment, but, zooming around the exhibit, I realized that I still didn't any intuitive sense of the size of the thing. Raw numerical measurements, even when you're beat across the head with them, become surprisingly abstract in the a-physical, point-of-reference-less netherlands of deeply-zooming digital landscapes. I dug out a ruler and zoomed the exhibit back until the first page occupied five real-world inches, and then held my hand up to the screen, imagining the sheet of paper in my hand. And then I thought - why not just bake some kind of visual reference directly into the exhibit? I hunted down a CC-licensed SVG illustration of a handprint, and, using the size of my own hand as a reference, used Neatline's import-SVG feature to position the outline in the whitespace to the right of the first page of the manuscript:\n\n[![hand2](http://dclure.org/wp-content/uploads/2014/02/hand2.jpg)](http://dclure.org/wp-content/uploads/2014/02/hand2.jpg)\n"},{"id":"2014-02-04-scholars-lab-grads-partner-with-washington-lee-university","title":"Scholars' Lab Grads partner with Washington & Lee University","author":"purdom-lindblad","date":"2014-02-04 10:46:33 -0500","categories":["Announcements","Digital Humanities"],"url":"scholars-lab-grads-partner-with-washington-lee-university","content":"[caption id=\"attachment_9521\" align=\"alignleft\" width=\"300\"][![quinn_anya_photo](http://www.scholarslab.org/wp-content/uploads/2014/02/quinn_anya_photo-e1391521680507-300x179.jpeg)]( http://www.flickr.com/photos/quinnanya/7744258278/sizes/z/ ) Photo by Anya Quinn[/caption]\n\nThe Scholars’ Lab is pleased to [partner with Washington and Lee University](http://news.blogs.wlu.edu/2014/01/30/wl-announces-digital-humanities-partnership-with-uva/ ) on a grant from the Associated Colleges of the South. The ASC grant furthers the [W&L Digital Humanities Working Group’s](http://digitalhumanities.wlu.edu ) efforts to create an Introduction to Digital Humanities course and explore a future Digital Humanities (DH) certificate at Washington and Lee. The Working Group strives to integrate digital tools and methods into humanities pedagogy.\n\nW&L; Working Group members will consult with Scholars' Lab and other [UVa Library](http://library.virginia.edu) staff throughout their planning process. The innovative [Praxis Program](http://praxis.scholarslab.org ) at the Scholars’ Lab (also the heart of our international [Praxis Network](http://praxis-network.org/)) is the model to explore what might form the core curriculum for a W&L DH certificate. The Praxis Program fosters a highly collaborative approach to scoping and building of  Digital Humanities tools, engaging in digitally-inflected public humanities work, and advancing humanities scholarship. Partnering with Washington and Lee faculty, UVA graduate students will contribute their experiences and research areas to the introductory course as they gain valuable experience working in a small liberal arts environment.\n\nKeep an eye out for future updates on this exciting partnership!\n"},{"id":"2014-02-05-podcast-adeline-koh-roopika-risam-2","title":"Podcast: Adeline Koh & Roopika Risam","author":"ronda-grizzle","date":"2014-02-05 05:26:07 -0500","categories":["Podcasts"],"url":"podcast-adeline-koh-roopika-risam-2","content":"**Digital Humanities Speakers: Adeline Koh and Roopika Risam**\n**Theories and Practices of Postcolonial Digital Humanities**\n\nBoth postcolonial studies and the digital humanities have gained currency within the academy but are subject to strident critiques from interlocutors. Postcolonial studies has been criticized for being overly reliant on jargon and apolitical, whereas the digital humanities have been taken to task for failing to interrogate questions of race, power, and identity fully. The Postcolonial Digital Humanities (#dhpoco) intervenes in these gaps through theory and praxis. #dhpoco engages postcolonial studies to address global issues relating to race, gender, class, sexuality, and disability within cultures of technology while bringing the activist praxis of the digital humanities to the work of postcolonial studies.\n\nCo-founders Adeline Koh and Roopika Risam discussed the theoretical underpinnings of #dhpoco and outlined the tactics that #dhpoco employs.\n\nThis event was co-sponsored by The Carter G. Woodson Institute and the Scholars’ Lab.\n\nDr. Koh and Dr. Risam's presentation slides are available from Slideshare: [Theories and Practices of Postcolonial Digital Humanities](http://www.slideshare.net/roopsi1/theories-and-practices-of-postcolonial-digital-humanities-roopika-risam-and-adeline-koh)\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"http://a270.phobos.apple.com/us/r30/CobaltPublic/v4/9f/e5/5c/9fe55cfd-0579-873a-03d4-87531a950e8c/332-4848115438543626250-risam_koh.mp3\"]\n"},{"id":"2014-02-05-praxis-weekly-digest-1","title":"Praxis Weekly Digest #1","author":"stephanie-kingsley","date":"2014-02-05 04:00:52 -0500","categories":["Grad Student Research"],"url":"praxis-weekly-digest-1","content":"First, if you missed our post last week about redesigning [Ivanhoe](http://www.ivanhoegame.org/?page_id=21), we are now in the process of building the game as a [WordPress](http://wordpress.org/) [Theme](http://wordpress.org/themes/).  If you are a potential Ivanhoe user, [help us out by giving some feedback about the way you use WordPress](http://www.scholarslab.org/grad-student-research/potential-ivanhoe-users/).\n\nCongratulations to Zach for passing his orals!  Now he has read even more books and will be that much better qualified to wage a fierce Ivanhoe war.  Way to go, comrade!\n\nFrancesca and I dusted off our artistic abilities the other day to brainstorm a logo.  Watch out for a [post from Francesca](http://www.scholarslab.org/grad-student-research/websites-media-buttons-and-logos-oh-my/) on this first step to designing Ivanhoe.\n\nAnd last but certainly not least, many thanks to our Development Team for putting in hours and hours of learning and wielding PHP, led by the tireless and endlessly patient Jeremy.  They now have a page built which allows a user to enter a move and which then displays the moves in a series of posts.  Great job, guys!  (Stay tuned for a [post on this from Veronica](http://www.scholarslab.org/grad-student-research/foreign-languages-and-ivanhoe-progress/).)\n\nAs we plunge headfirst into building and designing the Ivanhoe WordPress Theme, I reflect on the vast change which has occurred in our group dynamics between this semester and last.  In the fall, we spent most of our time deciding what Ivanhoe was about.  Several posts mused on teamwork (see [\"Forming, Norming, Storming & Performing,\"](http://www.scholarslab.org/grad-student-research/forming-norming-storming-performing/) \"[More Musings on Tuckman,\"](http://www.scholarslab.org/grad-student-research/more-musings-on-tuckman/) [\"Stephen Covey intervenes in wire-framing Ivanhoe\"](http://www.scholarslab.org/grad-student-research/stephen-covey-intervenes-in-wire-framing-ivanhoe/), and [\"Sticky Situations: Lessons in Group Cohesion\"](http://www.scholarslab.org/grad-student-research/sticky-situations-lessons-group-cohesion/)), and, indeed, we had difficulties coming together on some points.  In retrospect, several elements contributed to this.  As I mentioned then in \"Stephen Covey,\" we all have strong personalities and fervent opinions.  Apply that to the attempt to rethink an already abstract idea such as Ivanhoe (which I would then have described as a game where you make moves on a text or concept to intervene in that text and generate criticism), and rabbit holes get deeper and deeper.  Furthermore, as a group we didn't know how we all fit into the mix.  Once we took our places in Design (Francesca and Zach), Development (Eliza, Scott, and Veronica), and as project manager (me), we each had an individual purpose and set of responsibilities.  Now that the wire-frames are finished ([see Francesca's post](http://www.scholarslab.org/grad-student-research/wireframing-fun/)), we all feel energized and are moving forward.\n\nAs project manager, I realize how much I enjoy talking with the team and seeing what challenges everyone is taking on.  I enjoy getting updates on people's progress and being a sounding board for ideas.  I also thrive on creating to-do lists and getting word out about what is going on in Praxis.  That being said, I see it as part of my job to share our progress with you, as well.\n\nThis will be the first of a series of weekly check-in posts I will be writing as we proceed in building and designing Ivanhoe this spring.  Each post will provide a space to acknowledge what our group has accomplished that week.  Our [charter](http://praxis.scholarslab.org/charter.html) emphasized equal credit.  Something which I believe is critical to all members receiving that equal credit, however, is to acknowledge individually what the teams and members have done from week to week.  I plan for these posts to provide that acknowledgement.  Only in that way will those interested in our activities see how each member's contribution fits into the whole.  My weekly updates will also uphold our charter's emphasis on outreach by letting you all know what is going on in our group.  Now that we are actively building, this is more vital than ever.\n\nSo welcome to my Weekly Praxis Digest.  Keep an eye out for the upcoming posts I mentioned from Francesca and Veronica, don't forget to fill out our [WordPress questionnaire](http://www.scholarslab.org/grad-student-research/potential-ivanhoe-users/) if you're interested in Ivanhoe, and wish us luck as we move into a new week of building!\n"},{"id":"2014-02-10-websites-media-buttons-and-logos-oh-my","title":"Websites, Media Buttons, and Logos - oh my! ","author":"francesca-tripodi","date":"2014-02-10 05:00:01 -0500","categories":["Grad Student Research"],"url":"websites-media-buttons-and-logos-oh-my","content":"Last week I took a different approach to design. Instead of wireframing in html, Stephanie and I decided to break out the colored pencils and tap into our creative side.\n\n[![photo 1-1](http://www.scholarslab.org/wp-content/uploads/2014/01/photo-1-1-300x224.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/01/photo-1-1.jpg)\n\nFirst, we worked on creating an Ivanhoe logo. Inspired by the[ original website ](http://www.ivanhoegame.org/) - Stephanie and I modified the header by focusing in on the \"I\" and \"V\" only. We also incorporated a tool (or I suppose the way he's holding it the pen looks more like a weapon of sorts... after all, \"the pen is mightier than the sword\"!). We also decided to use the same color scheme as [PRISM](http://prism.scholarslab.org/) and we hope that by doing so it creates a sense of unity among Scholars Lab products.\n\n[![IvanhoeLogo](http://www.scholarslab.org/wp-content/uploads/2014/01/IvanhoeLogo-300x224.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/01/IvanhoeLogo.jpg)\n\nNext, we drew up a modified \"Add Media\" button. Why you may ask? Well even though WordPress already has an \"Add Media\" button - I believe the drabness of the feature ends up fostering a text-first space in WordPress rather than working with, manipulating, and linking other forms of media besides text (for more on my logic of why a modified \"Add Media\" button is necessary - [click here](http://www.scholarslab.org/grad-student-research/faulty-format-or-user-error/)). We hope that by incorporating this feature it will draw students into our game as we intend - fostering the possibility of a pedagogical space that includes all forms of media. _Even though in the end the button might just be a modified version of what already exists (think make it red vs grey) it was still fun envisioning how our game might look in the end. _\n\n[![photo 2-2](http://www.scholarslab.org/wp-content/uploads/2014/01/photo-2-21-300x224.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/01/photo-2-21.jpg)\n\nStephanie and I also thought that it would be good to use WordPress for our informational site since we are building a WordPress plugin.... however we were \"convinced\" that the better way to go would be building the site ourselves. While I now see why it is important to go this route, building a site from the ground up brought with it a wave of anxiety. I still feel so uneasy about my CSS/HTML skills and I was concerned that not relying on WordPress would ultimately mean roadblocks for the team. In addition to the information site, I also need to be working on designing the actual game and was really worried that my skill level would impede progress. Luckily for me we have an awesome support team and Jeremy and Wayne sat down with me (and my son Lev!) to help Zach and I begin building our site. While we still have a great deal of work to go on it, I was once again surprised at my capabilities (and also satisfied/proud to see the fruits of our labor). I think in the end it's just about making the time to come into SLAB for help and being a bit more confident in myself.\n\n[caption id=\"attachment_9560\" align=\"alignnone\" width=\"300\"][![Wayne teaching me CSS--and Lev too!](http://www.scholarslab.org/wp-content/uploads/2014/02/Lev-300x225.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/02/Lev.jpg) Wayne teaching me CSS--and Lev too![/caption]\n\nFinally - we are in the process of [crowdsourcing](http://www.scholarslab.org/grad-student-research/potential-ivanhoe-users/)! We'd love to hear from you on how we can make Ivanhoe great.  We're still interested in [how you use WordPress](http://www.scholarslab.org/grad-student-research/potential-ivanhoe-users/), so fill out our questionnaire if you'd like to help.  Thanks!\n"},{"id":"2014-02-13-foreign-languages-and-ivanhoe-progress","title":"Foreign Languages and Ivanhoe Progress","author":"veronica-ikeshoji-orlati","date":"2014-02-13 05:00:45 -0500","categories":["Grad Student Research"],"url":"foreign-languages-and-ivanhoe-progress","content":"Eight years ago, I sat staring at my Latin prose composition homework. The assignment was to translate a few sentences and a couple of short, not-particularly-complex paragraphs from English into Latin. In that precise moment, however, it would have been equally effective to ask me to go find and slay a fire-breathing dragon, since the task of translating not just words, but entire thoughts into a language I had spent most of my life just reading seemed absolutely insurmountable. How did Cicero make his speeches so persuasive..._and_ _eloquent_? How did Martial write such astringent, sharp-witted satires with so few words..._in meter_? Clearly a lot of talent was involved. Or, perhaps, magic.\n\nOver the past couple of weeks, we have started to build the core features of our [Ivanhoe](http://www.ivanhoegame.org/?page_id=21) [WordPress](http://wordpress.org/) [Theme](http://wordpress.org/themes/). We have created the 'Games' and 'Moves' custom post types and have a (mostly) functional template page for making new moves with the WordPress WYSIWYG editor hooked in. We're currently working on how to relate moves to one another and display those relationships for each game, and it seems as though we're making progress. (Or so Jeremy, Eric, and Wayne insist.) Here is some photographic evidence of a couple of us looking very serious and typing things:\n\n[![VandSfocused](http://www.scholarslab.org/wp-content/uploads/2014/02/VandSfocused-300x220.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/02/VandSfocused.jpg)\n\nAnd of what we've gotten done thus far:\n\n[![ivanhoe_screenshot_02_11_14](http://www.scholarslab.org/wp-content/uploads/2014/02/Ivanhoe_Screenshot-300x168.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/02/Ivanhoe_Screenshot.jpg)\n\nNevertheless, working on Ivanhoe sometimes feels a bit like a Latin prose comp throwback. Some five-line blocks of code have taken hours to get right, and the [WordPress Codex](http://codex.wordpress.org/) seems to be written in an impenetrably-dense, grammar-book style. But in my current attempt to write and think in another language, I feel somewhat less bewildered. I am certainly no more prepared to write PHP functions and read WordPress's Codex than I was to tackle the English-Latin exercises of Bradley's Arnold with Gildersleeve's grammar in hand. But I am not being left to my own devices to figure things out this time, and it has made a huge difference.\n"},{"id":"2014-02-14-praxis-weekly-digest-2","title":"Praxis Weekly Digest #2","author":"stephanie-kingsley","date":"2014-02-14 06:00:02 -0500","categories":["Grad Student Research"],"url":"praxis-weekly-digest-2","content":"This week, [Praxis](http://praxis.scholarslab.org/) has made some very exciting progress.  Eliza, Scott, and Veronica continue to work on our [WordPress](http://wordpress.org/) [Theme](http://wordpress.org/themes/).  As Veronica mentioned in her post, [\"Foreign Languages and Ivanhoe Progress,\"](http://www.scholarslab.org/grad-student-research/foreign-languages-and-ivanhoe-progress/) the challenge the Development team faces this week is figuring out how to create links between moves which respond to other moves.  For instance, if in the Suffragette Game, a telegram from Lord Asquith is found in Mary Leigh's trunk, and \"Mary Leigh\" made this move in response to the newspaper article posted by \"Anonymous Suffragette,\" then we want those two moves to be linked together in the WordPress database.  Then, when a user views one of those moves on the front end, each move needs to display links to its associated move.  This system of networking moves is a central feature of our Ivanhoe, so Development's successful execution of this complex task will be a major breakthrough in the programming of the Ivanhoe Theme overall.  Take a look at an [extract of the Suffragette Game](http://scholarslab.github.io/ivanhoe-mockups/game.html) on our mockups, hosted by [GitHub](https://github.com/), to see examples of this network.\n\nOur Design team, Francesca and Zach, will be working on the logo and overall aesthetic for our Ivanhoe informational website, which will eventually extend to the Theme itself.  We chose to construct Ivanhoe as a WordPress Theme so that we would be able to control the look of the game, and now our designers are working to decide what that look should be.  What should the colors, fonts, and graphics of the website communicate about our interpretation of the [Ivanhoe Game](http://www.ivanhoegame.org/?page_id=21)?  Jeremy met with Francesca, Zach, and me this week to instruct us in tackling this step.  He stressed the fact that each design choice should have special significance to Ivanhoe—meaning, we can't just choose a font or color because we like it.  With Jeremy's help, Design began making progress in brainstorming a logo.  They want to incorporate the [Prism ](http://prism.scholarslab.org/)colors and aesthetic to stress the continuity between the projects, while adapting Prism's design in such a way as to emphasize Ivanhoe's focus on interactive play.  The thought is yet in its nascent stages, but it compelled me to visit some of the design blogs from previous Praxis cohorts, and a quote from designer Chris Peck struck me as useful as we begin to consider Ivanhoe with respect to Prism:\n\n\n<blockquote>There’s something very compelling about refraction as a metaphor for collective interpretation of text. The crowd is a prism that reveals facets of the text, and the text is a prism that reveals facets of the crowd. The rainbow splayed across the text is an apt image for Prism’s aspirations for bringing text to life through accumulated interpretations. ([\"Prism on Spring Break\"](http://www.scholarslab.org/research-and-development/prism-on-spring-break/))</blockquote>\n\n\nChris wrote this while playing with the idea of [displaying multiple colors of highlighting across text](http://www.scholarslab.org/grad-student-research/gradient-highlights/); he eventually changed this to showing only one at a time.  Still, his comments show his design ideas to be metaphors for how he viewed Prism theoretically, and this thought process is one which I think we should try and emulate in moving forward on Ivanhoe design.\n\nOn the Project Management front, I continue to meet with the separate teams, see how they're doing, take pictures, and publicize.  In this vein, I would like to introduce the #Ivanhoe hashtag, because Ivanhoe is a phenomenon unto itself... can become a whole new movement in critical thought... and is being revitalized for pedagogical and scholarly use as we speak!  What better reasons to create a hashtag?  Look for it in tweets from @PraxisProgram and @scholarslab.\n\nI would also like to announce a talk which Praxis will be giving on Prism and Ivanhoe.  Members of both this and last year's cohorts will be speaking on the use of their projects for textual interpretation at the UVa Graduate English Conference, \"Reading Then and Now,\" to be hosted in Charlottesville on the UVa campus the weekend of April 4-6; our panel will be that Friday.  I will announce this again with further details when the conference schedule is established, but if you're planning to be in Charlottesville that weekend, mark your calendars.  We'd love to hear your thoughts on Prism and Ivanhoe.\n"},{"id":"2014-02-17-apply-for-praxis-2014-2015","title":"Apply for Praxis 2014-2015!","author":"purdom-lindblad","date":"2014-02-17 07:46:50 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"apply-for-praxis-2014-2015","content":"![praxis](http://www.scholarslab.org/wp-content/uploads/2012/08/praxis-300x168.png)\n\nUVa grad students! **Apply by March 10th** for a unique, funded opportunity to work with an interdisciplinary group of your peers, learning digital humanities skills from the experts, and collaboratively designing and executing an innovative digital project.\n\nEach year, the Scholars’ Lab [Praxis Program](http://praxis.scholarslab.org) provides six UVa graduate students with hands-on experience in digital humanities collaboration and project creation. Team members work to take a shared project from conception to design and development, and finally to publication, community engagement, and analysis.\n\nOur fellows [blog about their experiences ](http://www.scholarslab.org/archives/)and develop increased facility with project management, collaboration, and the public humanities, even as they tackle (most for the first time, and with the mentorship of our wonderful [faculty and staff](http://www.scholarslab.org/people/)) new programming languages, tools, and digital methods.\n\nThe [2013-14 Praxis cohor](http://praxis.scholarslab.org)t is in full swing, thanks to a generous support by the [University of Virginia Library](http://library.virginia.edu/).  Recently, the Scholars’ Lab joined with like-minded institutions to create a [Praxis Network](http://praxis-network.org/), made up of allied but differently-inflected humanities education initiatives from around the world, mainly focused on graduate training, and all engaged in rethinking pedagogy and campus partnerships in relation to the digital humanities.\n\n**We will welcome six new, competitively-selected Praxis students in late August 2014**. Each will be awarded $8000 in fellowship funds, and will be expected to devote approximately 10 hours per week in the fall and spring semesters, to learning together and building a collaborative digital humanities project in the Scholars’ Lab. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, and can make use of our dedicated grad lounge.\n\nAll _University of Virginia graduate students_ working within or committed to humanities disciplines are eligible to apply to join the 2014-15 cohort. We particularly encourage applications from women, LGBT students, and people of color, and will be working to put together an interdisciplinary and intellectually diverse team.\n\n**Application deadline: Monday, March 10th.**\nThe application process is simple: direct an email to [Purdom Lindblad](mailto:jpl8e@virginia.edu). Please indicate why you’re interested in the Praxis Program, what you think you will gain from it, and what you feel you would bring to a collaborative digital humanities project. A small committee, consisting of Scholars’ Lab faculty and staff and past Praxis Program participants, will evaluate expressions of interest and schedule group interviews with finalists.\n"},{"id":"2014-02-17-call-for-graduate-fellowship-in-the-digital-humanities","title":"Call for Graduate Fellowship in the Digital Humanities","author":"purdom-lindblad","date":"2014-02-17 07:46:16 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"call-for-graduate-fellowship-in-the-digital-humanities","content":"[![Photo by ericskiff](http://www.scholarslab.org/wp-content/uploads/2014/02/ericskiff-236x300.jpg)](http://www.flickr.com/photos/ericskiff/2925603321/sizes/l/)The [Scholars' Lab](http://www.scholarslab.org) is proud to announce that applications for our prestigious [Graduate Fellowship in the Digital Humanities](http://www.scholarslab.org/graduate-fellowship-in-digital-humanities/) are being accepted for the 2014-2015 academic year.\n\nThe fellowship supports ABD graduate students doing innovative work in the digital humanities at the University of Virginia. The Scholars’ Lab offers Grad Fellows advice and assistance with the creation and analysis of digital content, as well as consultation on intellectual property issues and best practices in digital scholarship and DH software development.\n\nFellows join our [vibrant community](http://www.scholarslab.org/graduate-fellowships/), have a voice in intellectual programming for the Scholars’ Lab, make use of our dedicated grad lounge, and participate in one formal colloquium at the Library per semester.\n\nSupported by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew & Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities, the Graduate Fellowship in Digital Humanities is designed to advance the humanities and provide emerging digital scholars with an opportunity for growth.\n\nPlease see the [DH Fellowship page](http://www.scholarslab.org/graduate-fellowship-in-digital-humanities/) for more information about the program, eligibility requirements, and application information. Deadline: March 10th!\n\nPlease contact[ Purdom Lindblad](mailto: jpl8e@virginia.edu), Head of Graduate Programs at the Scholars' Lab, with questions.\n"},{"id":"2014-02-20-software-development-for-the-ma-humanities-student","title":"Software Development for the MA Humanities Student","author":"eric-rochester","date":"2014-02-20 06:27:10 -0500","categories":null,"url":"software-development-for-the-ma-humanities-student","content":"This is _not_ a transcript of a brief panel talk I gave for the UVa [Graduate English Student Association](http://graduate.engl.virginia.edu/gesa/) Career Panel. It's based on what I hope to say, but I'm actually writing this before the event so it (and its links) can be available beforehand.\n\n\n\n\n\n# About me\n\n\n\n\n\nI've been interested in two things for about as long as I can remember: computers and literature. These intersected a little in science fiction and fantasy, but largely, the two obsessions remained strangely separate. I'd spent a lot of time reading, both \"literature\" and \"trash\"; but I'd also enjoyed playing computer games and trying to program my own.\n\n\n\n\n\nIt wasn't until half-way through my PhD program at [The University of Georgia](http://www.uga.edu/) that my interests started to come together. Initially, I just had a reputation for being able to help people format columns in Word. Then I got involved in digital humanities, then called humanities computing. I also created a website for a professor, and later I started doing web development and systems administration for the [Linguistic Atlas Projects](http://www.lap.uga.edu/).\n\n\n\n\n\nAlthough these jobs weren't my primary focus in graduate school, I did take them seriously. I learned best practices, including version control and testing. This was good for the project, but it was also good for me: doing things right up front saved me pain and sweat later.\n\n\n\n\n\nAnd this was how my two interests finally found common ground.\n\n\n\n\n\nWhen I graduated I took a job doing a combination of corpus linguistics and software development. This was good, but when I needed to look for another job, I found that there were fewer options for corpus linguistics than for web development.\n\n\n\n\n\nSo I made web sites for a few years. I had a lot of fun, and I learned a lot, both about the work itself and about interacting with clients and stakeholders.\n\n\n\n\n\nFor the last almost three years, I've been senior developer here at the [Scholars' Lab](http://www.scholarslab.org/). What does that entail?\n\n\n\n\n\n\n\n  * **Software development**: True to my title, a lot of what I do involves developing and maintaining computer systems and web sites.\n\n\n\n  * **Mentoring and education**: Our biggest focus is education and mentoring. Sometimes that means helping someone who walks in with a digital project. More often it involves helping one of the Scholars' Lab's fellows or one of the students in the [Praxis Program](http://praxis.scholarslab.org/).\n\n\n\n  * **Documentation**: An important--but often overlooked--aspect of software projects is their documentation. We don't spend enough time on this.\n\n\n\n\n\nThat's not all, but those three is probably how I spend most of my time.\n\n\n\n\n\n# What Kind of Work are we Talking about?\n\n\n\n\n\nI'm a software developer, so I've necessarily focused on that in talking about my personal journey. However, software projects are large, sprawling, complex behemoths, and there are a lot of different tasks that need to be done and a lot of different specialties that are required to contribute. So even if writing code doesn't appeal to you, other things might.\n\n\n\n\n\n\n\n  * **Project management**: Keep everyone on track.\n\n\n\n  * **Community outreach**: Publicize the project and be an active member of the project's community.\n\n\n\n  * **Design**: Make the product usable.\n\n\n\n  * **Documentation**: A different way to make the product usable.\n\n\n\n  * **Testing**: Check that the product works and works correctly.\n\n\n\n\n\n<blockquote>\n\n> \n> Gina Trapani has an excellent post talking about how crucial--but also how under-valued--many tasks are in a software project, especially in the open source world. You can read about it at [Designers, Women, and Hostility in Open Source](http://smarterware.org/7550/designers-women-and-hostility-in-open-source).\n> \n> \n</blockquote>\n\n\n\n\n\n# What Advantages do you Have?\n\n\n\n\n\nTypically, people expect those in any technical job to have a STEM background. This is false, and in fact, a humanities background can be a great asset in almost any job in software development.\n\n\n\n\n\nLet me count the ways.\n\n\n\n\n\n\n\n  * **Communication** This point is trite, but it's true. At a fundamental level programming involves communicating. Your code must communicate to the computer, to other developers, and even to your future self. You'll also need to communicate effectively to clients, to your boss, and to co-workers.\n\n\n\n  * **Education and Negotiation** An important part of software development involves educating and negotiating with others. For example, adding a feature may involve dropping another one. This doesn't make clients happy, and you'll need to explain why and argue your case.\n\n\n\n  * **Research** Learning new technologies as well as using ones you're already familiar with both involve a lot of research. Knowing how to learn and how to research is an important asset here.\n\n\n\n  * **Reading and interpretation** Most programmers work from specification documents. Being able to interpret them appropriately is crucial.\n\n\n\n  * **Multi-level focus** I'm not sure what to call this point, but it may be the most important one. When you analyze literature you must command details from a variety of texts and sources and synthesize them to make a larger point. This involves paying attention to both the forest and the trees. Writing software involves the same split focus: on the one hand, you spend a lot of time in the weeds thinking about semicolons; on the other hand, you must keep the big picture in mind to stay on track and on schedule.\n\n\n\n\n\n<blockquote>\n\n> \n> For some of the same points, plus some others, see Shelby Switzer's post on [How my “impractical” humanities degree prepared me for a career in programming](http://shelbyswitzer.com/humanities_degrees_help_programmers/).\n> \n> \n</blockquote>\n\n\n\n\n\n# What Can you be Doing Now?\n\n\n\n\n\nObviously, finish your degree. This is the most important thing you can do.\n\n\n\n\n\nBut in your spare time (ha!), there are some other things you can do, both now and in the future. (Again, apologies: this list is for software programmers, especially web developers.)\n\n\n\n\n\n\n\n  * **Learn the basics** For web development, design, etc., this means learning HTML, CSS, and JavaScript. [CodeAcademy](http://www.codecademy.com/), [Code School](https://www.codeschool.com/), and [tuts+](http://code.tutsplus.com/) are all good. The main thing is to type along yourself.\n\n\n\n  * **Learn a Web Language** Essentially, you want something you can use to interact with a database and dynamically generate web pages. This could be [JavaScript using NodeJS](http://nodejs.org/), [Ruby](https://www.ruby-lang.org/), or [Python](http://www.python.org/). Any of these are good. If there's one available for your language of choice, the [Learn Code the Hard Way](http://learncodethehardway.org/) series is excellent.\n\n\n\n  * **Learn a Web Framework** Find one based on the language you picked in the previous point. For JavaScript, that means [Express](http://expressjs.com/) or [Sails](http://sailsjs.org/). For Ruby, [Ruby on Rails](http://rubyonrails.org/) or [Sinatra](http://www.sinatrarb.com/). For Python, [Django](https://www.djangoproject.com/) or [Flask](http://flask.pocoo.org/).\n\n\n\n\n\nIf you're just getting started, don't worry about getting a broad knowledge of different technologies. All of them are similar. You'll be better served by going deep into one choice. What you learn will apply to the other systems, and you can learn them later when required.\n\n\n\n\n\nAlso, learn the tools you'll use to work in these languages. Learn them thoroughly and learn them well. You're going to live in them.\n\n\n\n\n\n\n\n  * **A text editor** This is probably the single-most important tool for a software developer. Know it inside and out. Know all of its tricks. [Sublime Text](http://www.sublimetext.com/) is a popular choice right now.\n\n\n\n  * **Version control** Programmers use version control systems to track the changes they make to their code. [Git](http://git-scm.com/) is a very popular choice, and [Github](https://github.com/) allows you to share your code and collaborate with others.\n\n\n\n  * **Online documentation** Find the documentation for your programming language, its libraries, and the web framework you're using. Also [StackOverflow](http://stackoverflow.com/) is a popular site for asking questions related to software development.\n\n\n\n\n\nFinally, **get your work out there**. There's never been a better time for this than right now. You can put your code online for others to see on [Github](https://github.com/). You can also run web apps quickly and easily using [Heroku](http://www.heroku.com/). Having code up on these makes it easy for potential employers to see your skills. It also lets them know that you're active and learning and capable. They won't replace a good portfolio that directs potential employers' attention and highlights your best work, but they are a good start, and they'll set up above most other applicants.\n\n\n\n\n\nIn general, this is a great time to go into software development and other technical jobs. Hopefully this post tells you what you need to think about and plan for.\n\n\n\n"},{"id":"2014-02-22-digest-3-project-mutability-shifting-identities-and-changing-roles","title":"(Digest #3) Project mutability: shifting identities and changing roles","author":"stephanie-kingsley","date":"2014-02-22 10:18:47 -0500","categories":["Grad Student Research"],"url":"digest-3-project-mutability-shifting-identities-and-changing-roles","content":"This week was full of excitement.  Our Development Team continued working on getting [Ivanhoe](http://www.ivanhoegame.org/?page_id=21) up and running; they will be presenting the working [WP Theme](http://wordpress.org/themes/) to our Praxis team this Tuesday.\n\nThe Design Team started thinking more about how we want the name, logo, font, and overall aesthetic to reflect our game.  The name \"Ivanhoe,\" in particular, has long been a subject of some contention in our group.  To address this concern, Francesca and Zach will be giving two separate identity pitches for the game on March 4.  In the next couple of weeks, they will think through why we would retain the name \"Ivanhoe,\" why we might call it something else if we were to change the name, and what aesthetics we would use to reflect those decisions.  Stay tuned for a post from Francesca on this turbulent but essential process.\n\nIn addition to the activities of the two teams, the roles of our individual members have begun to shift, as well.  Eliza started out solely on development, and although she is still primarily a member of that team, she has become the primary wielder of CSS and HTML in the Praxis group.  Eliza is now actively applying the CSS of our wireframes to the Theme our developers have been creating in PHP; she thus has become somewhat of a development-design liaison.  This will prove invaluable when the Design team has given its identity pitches and it becomes time to bring the chosen identity to life in CSS.\n\nFrancesca will also begin to take on responsibilities beyond her role on Design.  After the identity pitches have been given, Francesca will spend more time writing the content of our informational website.  Part of the goal of the website will be to explain the decisions behind the game's aesthetic, and Francesca will be well equipped to address them as a designer.  Thus, as Eliza shifts from Development to Design in March, Francesca will move into more of a Support role.\n\nI think these developments are exciting.  As members become interested in different parts of the project, they can move into new roles.  This ability to adapt on the part of the entire group is in accord with our [charter](http://praxis.scholarslab.org/charter.html) goal of flexibility.  It also reflects an attention to the ever-changing needs of the project itself.  As we move through the various stages of conceiving, building, and designing Ivanhoe, different types of work will become necessary.\n\nI continue to sit in on the separate teams' meetings and assign issues and milestones in [GitHub](https://github.com/).  The shifting roles within our team and changing requirements of the project have required me to continually adjust the assignments and reorganize workflow.  I also constantly re-conceive how I myself fit into the group as project manager.  That, however, is meat for another post.\n"},{"id":"2014-02-24-on-community-listening-3","title":"On Community Listening: 3","author":"erik-deluca","date":"2014-02-24 04:30:20 -0500","categories":null,"url":"on-community-listening-3","content":"Check out my previous posts, [On Community Listening 1](http://www.scholarslab.org/grad-student-research/on-community-listening-2/) and [2](http://www.scholarslab.org/grad-student-research/on-community-listening-2/) for context.\n\nThe visual aesthetic of the web environment is minimal because I want listeners to focus on the sounds that the interface holds and not on the overstimulation of visual content. Think [Sol Lewitt](http://en.wikipedia.org/wiki/Sol_LeWitt). [This](http://www.massmoca.org/lewitt/grid.php) is my favorite series by Mr. Lewitt.\n\nThe form of the interface is inspired by [Alexander Calder's](http://en.wikipedia.org/wiki/Alexander_Calder) [kinetic sculptures](http://en.wikipedia.org/wiki/Mobile_%28sculpture%29) and [Earl Brown's](http://en.wikipedia.org/wiki/Earle_Brown) “[Open Form](http://en.wikipedia.org/wiki/Open_Form)” works. I like these structures because the art is composed of a network of reconfigurable vignettes that can be perceived from many different perspectives. Calder's sculptures are reconfigured by environmental factors like the wind and light. The conductor and/or the musicians reconfigure most of Brown’s [music](http://www.youtube.com/watch?v=vCSK5FiBNAA).\n\nThe web interface for “Community Listening” will be different from these two types of works. The form will allow listeners the ability to directly reconfigure and interact with the ethnographic composition. In a sense, the listeners will also become the “conductor” of the work. This type of user flexibility confronts issues of access. The form of digital scholarship can “speak” to many different people from many backgrounds. Right?\n\nUnrelated:\n\nI'm listening to [this](http://www.youtube.com/watch?v=KhOsOb_iBOs) and [this](http://www.youtube.com/watch?v=13WGthWZink&feature=kp).\n"},{"id":"2014-02-26-dh-speaker-series-ted-underwood","title":"DH Speaker Series: Ted Underwood on CS and the Humanities","author":"laura-miller","date":"2014-02-26 09:41:11 -0500","categories":["Announcements","Podcasts"],"url":"dh-speaker-series-ted-underwood","content":"March 10 update: Due to technical difficulties, we are not able to post the podcast of this event.  Apologies for any inconvenience!\n\n\n### ___Beyond Tools: The Shared Questions about Interpretation that Link Computer Science to the Humanities___\n\n\n**Thursday, March 6 at 10:00 am**\nin Alderman Library, Room 421\n\n[![underwood_teacup](http://www.scholarslab.org/wp-content/uploads/2014/02/underwood_teacup.png)](http://www.scholarslab.org/wp-content/uploads/2014/02/underwood_teacup.png)\n\nThe phrase \"digital humanities\" suggests an encounter with digital technology itself -- which might seem to involve departments of computer science only insofar as they build tools for us to use. But as collaborations between humanists and computer scientists grow more common, it's becoming clear that these disciplines are working on shared, surprisingly fundamental questions. For instance, computer scientists want to understand how we learn to generalize about latent categories from limited evidence, which is a good part of what humanists do when we \"interpret an archive\" or \"develop a theory.\" Instead of treating CS as a source of tools, some humanists are starting to approach the discipline as a theoretical interlocutor, analogous to linguistics or anthropology. What might this conversation look like concretely? I'll flesh out some possibilities, briefly describing a collaboration with David Bamman at Carnegie Mellon (we're attempting to model character in nineteenth-century novels), and reflecting more generally on the humanistic value of model-building. I'll also acknowledge some of the social divisions that make this conversation risky.\n\n[Ted Underwood](http://www.english.illinois.edu/people/tunder) is Associate Professor of English and Liberal Arts and Sciences Centennial Scholar at the University of Illinois, Urbana-Champaign.  He is the author of two books on eighteenth- and nineteenth-century literary history, including Why Literary Periods Mattered (Stanford, 2013). He is currently developing models of genre in eighteenth- and nineteenth-century books, supported by a Digital Humanities Start-Up Grant from the NEH and an ACLS Digital Innovation Fellowship. A collaborative essay with Andrew Goldstone, topic-modeling the history of literary scholarship, is forthcoming in New Literary History.\n\nAll Scholars' Lab events are free, open to the public, and require no advance registration.\n\n\n###### Image courtesy of [http://tedunderwood.com/](http://tedunderwood.com/)\n\n\n\n"},{"id":"2014-03-03-on-stemmatics","title":"On Stemmatics","author":"zachary-stone","date":"2014-03-03 05:00:10 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"on-stemmatics","content":"[![Screen Shot 2014-03-01 at 10.40.56 AM](http://www.scholarslab.org/wp-content/uploads/2014/03/Screen-Shot-2014-03-01-at-10.40.56-AM-300x87.png)](http://www.scholarslab.org/wp-content/uploads/2014/03/Screen-Shot-2014-03-01-at-10.40.56-AM.png)\n\nSo this is a git network graph. Specifically, [it is the network graph for Ivanhoe](https://github.com/scholarslab/ivanhoe/network) from c. 20 February to 1 March. The blue line is our Develop branch and the various branches are features, projects, etc. The first little pink dot is [my first branch](https://github.com/scholarslab/ivanhoe/tree/feature/zachcss).  While programming development is far from complete, I forked off a branch to begin styling completed features with CSS. The next little pink dot was my introduction to the world of merge conflict. [Gross](https://github.com/scholarslab/ivanhoe/commit/732aae83d3b67a182daad60a357ef9e927ea61dc).\n\n[caption id=\"attachment_9656\" align=\"aligncenter\" width=\"300\"][![Stemma](http://www.scholarslab.org/wp-content/uploads/2014/03/Stemma-300x111.png)](http://www.digitalmedievalist.org/journal/9/hall/) © Alaric Hall and Katelin Parsons, 2013. Creative Commons Attribution-NonCommercial licence[/caption]\n\nThis is a [stemma](http://en.wikipedia.org/wiki/Textual_criticism#Stemmatics). It is a stemma of _Konráðs saga keisarasonar.**[1]** _Up at the top we have our putative archetype, hyparchetype, or possibly even authorial holograph.[2] The myriad of branches represent various copies and what have you. I believe the similarity of these images is both obvious and salutary.\n\nBoth images map out textual iterations created by humans. Both bear witness to merge problems and variations. Both require diverse groups of human beings to work together to iterate a text. While the technological means of iteration have evolved considerably, the problem of reliable iteration across time and space remains.\n\nI am not implying that coding is just like manuscript copying. It isn’t, and digital textuality is not exactly analogous to manuscript textuality. I guess what I am saying is a bit more Eliotean. The end of all exploration will be to return home and to know it for the first time.[3] Mediums change, and as a bibliographer and book historian I certainly avow inseparability of medium and message, but it seems problems remain the same. How to meaningfully pass information from one human being to another and not to fail.\n\n\n  \n\n\n\n\n* * *\n\n\n\n\n\n\n\n[1] I’ve never read _Konráðs saga keisarasonar_; this is just the best stemma picture I found on the interwebs. Full credit it is from a [post by Alaric Hall Katelin Parsons](http://www.digitalmedievalist.org/journal/9/hall/) over at [_Digital Medievalist_](http://www.digitalmedievalist.org). Full citation at the foot of this note. If you want to know more about this specific text/stemma or just want to see a really rad discussion of online editing and connect with one of the best online places for manuscript-y folks, hit it up. Hall and Parsons, _Digital Medievalist _9 (2013). ISSN: 1715-0736.\n\n\n\n\n\n\n\n\n[2] If you are George Kane\n\n\n\n\n\n\n\n\n[3] Loosely [\"Little Gidding: V,\"](http://www.columbia.edu/itc/history/winter/w3206/edit/tseliotlittlegidding.html) ll. 26-9 ish.\n\n\n\n\n\n"},{"id":"2014-03-06-digest-4-on-managing-projects-not-people-reflections-after-a-project-management-crisis","title":"(Digest #4) On managing projects, not people","author":"stephanie-kingsley","date":"2014-03-06 05:00:20 -0500","categories":["Grad Student Research"],"url":"digest-4-on-managing-projects-not-people-reflections-after-a-project-management-crisis","content":"This digest comes a bit late, because in the interim I have been going through a mild project management crisis.  Now that the crisis is past, I see the experience as the perfect opportunity for a post.\n\nI mentioned in my last post that the nature of our project—and thus our team—is changing.  Development has a working game up and running.  They now are adding features and troubleshooting bugs in the program.  Design has given the identity pitches and is moving forward with our new identity: Ivanhoe, designed in such a way as to emphasize the web-like network of moves which our game will inspire.  (Watch for more posts on the design plans from Francesca and Zach.)\n\nEliza is learning [SASS](http://sass-lang.com/), which—very roughly speaking—lets her make her design life easier by building templates of [CSS](http://www.w3schools.com/css/).  These will enable her to not have to enter every little bit of CSS for complex but commonly used formatting.  Zach is becoming more familiar with CSS, and the two of them will start applying the design plan to our website and theme within the  week.\n\nFrancesca is refining design on our logo and will be transitioning to writing web content this week.  Francesca has reflected before on the nature of our tool as a game (see, for instance, [\"Are we gaming or just simulating?\"](http://www.scholarslab.org/grad-student-research/are-we-gaming-or-just-simulating/) and an [earlier post on gamification](http://www.scholarslab.org/grad-student-research/look-its-a-game-its-a-simulation-no-its-gamification/)), and I think she is the perfect person to write web content on Ivanhoe for this reason.  She will be writing a brief history of the Ivanhoe Game, so anyone who wishes to see what our game has grown out of may do so.\n\nThat leaves me.  For a while, I was heavily focused on what everyone was working on.  I knew that my job was to keep up with that and make sure that everything got done.  This quickly became problematic, however.  Development's understanding of coding tasks far outstripped my own.  They have also been very organized and have had very little need of managing outside of their own team.  Design also established a system of meeting, dividing up tasks—each member taking a separate design plan for the identity pitch—and plunged into the design.  This left me feeling a bit bewildered.  I felt that I had little to do but blog on what everyone else was doing.\n\nThis particular circumstance is essentially a good thing; a project manager needs the team members to take initiative and plan their own parts of the project.  Once that's going smoothly, however, a new style of project management is called for.  I was not prepared for this, and in my frustration, I fear I may have teetered on the brink of MICROMANAGEMENT.  This was the very demon I had striven to avoid in my Praxis involvement, and in a climactic meeting with my team, I discovered that I may have begun this downward spiral.  I didn't want people to feel like I was looming.  But how to do my job?\n\nThis is where a project manager needs to remember that she is managing a _project, _not people—a confusing point, as people are responsible for everything that gets done on a project.  Still, with some coaching from Bethany and Purdom, and after putting inquiries to my teammates as to how I could best be helpful, I emerged from my confusion with a very good idea of what my role would involve in the latter half of this semester.\n\nI figure primarily in the role of Support now.  I am responsible for publicity—blogging and looking for opportunities to get the word out about Ivanhoe.  I am also the primary steward of The Timeline (all caps because it's so important).  This quite simply means knowing what our deadlines are and keeping people aware of them so they can plan their own work accordingly.  I am also involved in testing Ivanhoe—a very fun job where I enter games on our [Heroku](https://www.heroku.com/) testing site and then tell Development what's broken.  I create a new issue on [GitHub](https://github.com/scholarslab/ivanhoe/issues?labels=bug&state=open) for each bug, give it a big red \"Bug\" label, and go look for more.  It's satisfying in the way that taking a red pen to an essay is satisfying.  And I know that when it's all done, our game will be the better for it.\n\n[![GitIssues](http://www.scholarslab.org/wp-content/uploads/2014/03/GitIssues-300x102.png)](http://www.scholarslab.org/wp-content/uploads/2014/03/GitIssues.png)\n\nLastly, my job is to maintain our vision for Ivanhoe.  This means keeping people on track if they start wanting to add features which veer from our essential goals.  This part of my job is a bit amorphous and abstract at this point, but for all that, it is inspiring.  I look forward to seeing where Ivanhoe will go, and to gaining greater project management experience and insight as the semester progresses.\n"},{"id":"2014-03-06-on-community-listening-4","title":"On Community Listening: 4","author":"erik-deluca","date":"2014-03-06 03:19:43 -0500","categories":null,"url":"on-community-listening-4","content":"Check out my previous posts for context:\n\n[On Community Listening 1](https://www.scholarslab.org/digital-humanities/on-community-listening-1/)\n\n[ On Community Listening 2](https://www.scholarslab.org/grad-student-research/on-community-listening-2/)\n\n[On Community Listening 3](https://www.scholarslab.org/uncategorized/on-community-listening-3/)\n\n[THIS](http://www.latimes.com/nation/nationnow/la-na-nn-isle-royale-wolves-climate-change-20140228,0,2461367,full.story#axzz2udznJ96m) article published in the LA Times today (2/28) is about the researchers I'm working with on Community Listening,  the wolves they study, and global climate change. Lots of great stuff here, including this quote from Michael Nelson: “To preserve a healthy ecosystem with climate change, we at times are going to have to intervene, and that’s a hard thing to wrap our heads around...” This quote may seem trivial but the wolves in question live in a National Park that has a \"let nature take its course\" directive.\n\nMy project is not so much about the wolves that live on Isle Royale but on the deep listening network that exists there. However, this network, I believe, is very dependent on the wolves that live in this place (an issue that I will address in my project). If these wolves go extinct, will this deep listening network also vanish? How do environmental fluctuations, like global climate change and its residual effects, dictate how we listen to the world?\n"},{"id":"2014-03-17-project-gemini-over-baja-california","title":"Project Gemini over Baja California","author":"david-mcclure","date":"2014-03-17 06:32:01 -0400","categories":["Geospatial and Temporal"],"url":"project-gemini-over-baja-california","content":"_[Cross-posted from [dclure.org](http://dclure.org/logs/project-gemini-over-baja-california/)]_\n\n\n\n## [Launch the Exhibit](http://neatline.dclure.org/neatline/show/gemini-over-baja-california)\n\n\n\n[![gemini-screenshot](http://dclure.org/wp-content/uploads/2014/03/gemini-screenshot-1024x615.jpg)](http://neatline.dclure.org/neatline/show/gemini-over-baja-california)\n\nA couple weeks ago, somewhere in the middle of a long session of free-association link hopping on Wikipedia, I stumbled into a cluster of articles about [Project Gemini](http://en.wikipedia.org/wiki/Project_Gemini), NASA's second manned spaceflight program. Gemini, I quickly discovered, produced some spectacular photographs - many of them pointed downward towards the surface of the earth, capturing a dizzying opposition between the intelligible scale of the foreground (the 20-foot capsule, 100-foot tethering cords, 6-foot human bodies floating in space) and the completely unintelligible scale of the massive geographic entities below (peninsulas, continents, oceans).\n\nAs I started to click through the pictures, I found myself reflexively alt-tabbing back and forth between Chrome and Google Earth to compare them with the modern satellite imagery of the same geographic locations. Which made me think - why not try to actually combine the two into a single environment? Over the course of the next few days, I sketched out a little Neatline exhibit that plasters two photographs of Baja California Sur - taken about a year apart on Gemini 5 (August 1965) and Gemini 11 (September 1966) - around the Mapbox satellite imagery of the peninsula. Instead of lining up the coastlines to make the images overlay accurately on top of the satellite tiles, I just plunked them down on the map off to the side at a scale and orientation that makes it easy to compare the two. (We've [played around with this before](http://hotchkiss.neatline.org/neatline-exhibits/show/my-dear-little-nelly/fullscreen), and I like to think of it as faux - or just especially humanistic! - georectification.)\n\n[![faux-georectification](http://dclure.org/wp-content/uploads/2014/03/faux-georectification-1024x616.jpg)](http://dclure.org/wp-content/uploads/2014/03/faux-georectification.jpg)\n\nThen, using the drawing tools in Neatline, I blocked in some simple annotations that visually wire up the two sets of imagery - outlines around the four islands along the eastern coast of the peninsula, and arrows between the different instantiations of La Paz and San José del Cabo. I also wanted to find a way to visually formalize the difference in perspective between the Gemini photographs (oblique, wide-angle, deliberate) and the Mapbox tiles (flat-on, uniform). Using Illustrator, I created a long, ruler-like vector shape to label the ~200-mile distance between La Paz and the approximate positon of the Gemini 5 capsule when the picture was taken, and then used the \"Perspective Grid\" tool to render the shape in three dimensions and place it on top of the Gemini photograph, as if the same shape were physically positioned in front of the lens. In Illustrator:\n\n[![200-miles-illustrator](http://dclure.org/wp-content/uploads/2014/03/200-miles-illustrator-1024x564.jpg)](http://dclure.org/wp-content/uploads/2014/03/200-miles-illustrator.jpg)\n\nAnd placed in the Neatline exhibit, first to match the shallow angle of the Gemini shot:\n\n[![200-miles](http://dclure.org/wp-content/uploads/2014/03/200-miles-1024x619.jpg)](http://dclure.org/wp-content/uploads/2014/03/200-miles.jpg)\n\nAnd then to match the perpendicular angle of the Mapbox tiles:\n\n[![200-miles-mapbox](http://dclure.org/wp-content/uploads/2014/03/200-miles-mapbox-1024x602.jpg)](http://dclure.org/wp-content/uploads/2014/03/200-miles-mapbox.jpg)\n\nI was also fascinated by the surreal opposition in scale between the Agena Target Vehicle (an unmanned spacecraft used for docking practice in orbit) and Isla San José, which sits serenely in the dark blue of the Gulf of California hundreds of miles below, but occupies almost exactly the same amount of space in the photograph as the 7-foot boom antenna on the Agena. In the space between the two, I dragged out two little shapes that map the sizes of things onto recognizable objects - a 6-foot person in the foreground, Manhattan in the background:\n\n[![manhattan-person](http://dclure.org/wp-content/uploads/2014/03/manhattan-person-1024x617.jpg)](http://dclure.org/wp-content/uploads/2014/03/manhattan-person.jpg)\n\n**Perspective and Perspectivelessness**\n\nThese images fascinate me because they roll together two types of imagery - both ubiquitous on the web - that are almost exact opposites of one another. On the one hand, you have regular pictures, taken by regular (non-astronaut) people. These photographs freeze into place one particular _perspective_ on things. In a literal sense, the world recedes from the lens in three dimensions - walls, buildings, bridges, mountains, valleys, clouds. Close things are big, distant things are small. Some are in focus, others aren't. And unlike other forms of art like painting, poetry, sculpture, or music, which can claim (overconfidently, maybe) to graft completely new material onto the world, photographs innovate at the level of _stance_ and _viewpoint_, the newness of the perspective on things that already exist. It's less about what they add, more about what they subtract in order to whittle the world down to one particular frame. Why that angle? Why that moment? Why _that_, and not anything else?\n\nOn the other hand you have spatial photography - the satellite imagery used in [Google Maps](https://www.google.com/maps), [Mapbox](https://www.mapbox.com/), [Bing Maps](http://www.bing.com/maps/), etc. - which is almost completely _perspectiveless_, in just about every sense of the word. The world becomes a flat, depthless plane, photographed from a distance at a perpendicular angle. Instead of trying to find interesting new ways to crop down the world, spatial tiles try to be comprehensive and standardized. Instead of showing one thing, in one way, at one moment in time, they try to show everything, in the exact same way, at the exact same moment - now. The companies that source and assemble the satellite imagery race to keep it as current as possible, right at the threshold of the present. Last year, Google announced that its satellite imagery had been [purged of all clouds](http://google-latlong.blogspot.com/2013/06/only-clear-skies-on-google-maps-and.html). No doubt this makes it more functional, but it also does away with those wispy, bright-white threads of cloud that used to hang over the rainforests in Peru and Brazil, which were lovely. What's gained, of course, is the intoxicating grandeur of it all, the ability to hold in a single view a photograph of the entire world - which, if nothing else, is some sort of crazy affirmation of human willpower. I always imagine Whitman, scratching out \"[Salut au Monde](http://www.bartelby.com/142/74.html)\", panning around Google Maps for inspiration.\n\nPhotographs taken by astronauts, though, collapse the distinction in interesting ways. They're literally \"satellite\" photography, but they're also drenched in subjectivity and historical stance. The oceans and continents spread out hundreds of miles below, just like on Google Maps or Mapbox - but the pictures were snapped with regular cameras by the hands of actual people, stuffed into little canisters falling around the world at thousands of miles an hour, which were only up there in the first place due to a crazy mix of socio-political ambitions and anxieties that were deeply characteristic of that particular moment in history. The Gemini imagery is haloed with little bits of space-race technology that instantly historicizes the frame - the nose cone of the capsule blocks out a huge swath of desert and ocean, the Agena vehicle hangs just a couple of hundred feet from the camera, tethered by a slight, ribbon-like cord that twists for hundreds of miles across the Gulf of California.\n"},{"id":"2014-03-19-an-ivanhoe-design-idea","title":"An Ivanhoe Design Idea","author":"zachary-stone","date":"2014-03-19 08:54:45 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"an-ivanhoe-design-idea","content":"So last week Francesca and I each pitched design for our informational website. While the bulk of our pitches focused on the look of the website, I formulated [my website design](https://zacharyestone.github.io/ivanhoefont/index.html) (ps, as this was just a mock up it isn't cross browser tested, sorry) to be as transferable as possible (or desired).  Had we gone this direction, certain elements of my design--colors, font, iconography--could have been exported to the finished game so as to give an aesthetic continuity to the entire project. Whether or not this is a good idea--i.e. using the aesthetics of the finished game to make an active and explicit intervention into the game itself--is a topic we are still working through (more anon).  In the end, we went more Francesca's way than mine--for reasons that may become clear below and in consultation with her post on the design pitches--but the exercise in coming up with a design was generative and opened up new ways to think about the game. Below is a verbatim copy of my rough pitch. Please pardon spelling , grammar, gross generalizations, etc. I will say that the major strength of my design was also its weakness. We all thought (myself included) that it overdetermined the nature and use of the game and might discourage potential (non medieval [well what did you expect?]) users.\n\n1. Summary\n\nMy design consists of three major elements: a color scheme, a set of three icons, and a specific font used for all text.  These design elements, which will be discussed briefly below, all derive from medieval traditions of book making and reading. I turned to medieval book history for inspiration for two major reasons. First, the name Ivanhoe has an inevitably medieval hue to it. Even if people don’t know the novel, it just sounds medieval. As such, a design that embraced the reception of _Ivanhoe_ grew naturally out of my desire to embrace the history of Ivanhoe in my design. Furthermore, the production of meaning in a manuscript culture—in which books were visual and oral as well as textual—has remained in my mind as we have developed the game. Their method of iteration, bespoke copying, requires close engagement with and modulation of meaning by individuals and groups endemic to my Ivanhoe experience.\n\n2. Consider an opening from a copy of Gatian’s _Decretals_:\n\n[caption id=\"attachment_9677\" align=\"alignnone\" width=\"199\"][![Bologna, s. xiii Cesena, Biblioteca Malatestiana, Pluteo II sin. cod. 1, fol. 2r](http://www.scholarslab.org/wp-content/uploads/2014/03/glossed-199x300.jpg)](http://classes.maxwell.syr.edu/his311/Lecture%20Three/manuscript_of_gratian.htm) Bologna, s. xiii  \nCesena, Biblioteca Malatestiana, Pluteo II sin. cod. 1, fol. 2r[/caption]\n\nThe commentary wraps the text, which is in turn illuminated by an illustration depicting oral composition.  If they could have put in movies and music, they would have!\n\n3. Color Scheme\n\nFrom this tradition I gleaned a color scheme that both harkens back to medieval books, but more importantly, reflects their hard-earned insights into how to make messy text legible and navigable. Light-grey text contrasts with a light-cream writing support, and muted reds and blues highlight and punctuate the text.\n\n4. Iconography\n\n-The pilcrow ¶ (modern sign for paragraph ) evolved out of capitula, or chapter markings, and was used to signify a new train of thought. Throughout my design pilcrows mark out trains of thought regarding our Ivanhoe narrative. What is the game. Where did come from. Why. These sorts of things.\n\n-The hedra ❦ is perhaps the oldest form of punctuation and has been used to mark off major sections of texts up to present times. In my system hedras signal moments of transfer in which information is being passed between users. In our website this is the “Install” section. The leafy, organic, vine implies notions of planting, growing, and connectivity while at the same time marking discreet units.\n\n-The manicule, ☝ or pointing hand, is a frequent marginal notation deployed by readers to simply say “look here” “important.” I deploy the manicule for similar functions on the website. It points to documentation and demos. Places we want users to LOOK.\n\n5. Font Choice\n\nI make exclusive use of Caudex through out my design. I choose Caudex for three reasons: style, legibility, and versatility. In style it echo aspects of various Gothic bookhands, but, crucially it remains legible in both is minuscule and majuscule forms. This dual legibility allows me to deploy majuscule scripts for logos and headers but use the miniscule for content. Thus my logos, rubrication, and content maintain a basic continuity.\n\n6. Applicability for the Game\n\nWhile I formulated my design identity around the info website, I believe its major elements could translate to gameplay with minimal difficulty. The icons were chosen in part for their symbolic valence, but also for their general utility as bullet points, section markers, and possible buttons in the game. All are text ornaments and not images thus they are easier to work with. The basic color scheme is minimal but useful (light grey for most text, red to flag attention, blue to ornament) and the merits of the font discussed above apply to the game as well. Thus I believe this identity allows for a strong aesthetic sense of what our game is that both relates to the history of Ivanhoe and to possible future experiences of it. While the manuscript book might seem over determined, the Book was and remains the western icon for information transfer and the dynamic tension produced by echoing medieval practice in a post-modern digital environment might produce interesting result.\n"},{"id":"2014-03-21-dh-speaker-series-micki-kaufman","title":"DH Speaker Series: Micki Kaufman on Quantifying Kissinger","author":"laura-miller","date":"2014-03-21 08:28:24 -0400","categories":["Announcements","Visualization and Data Mining"],"url":"dh-speaker-series-micki-kaufman","content":"### [_**Everything on Paper Will Be Used Against Me: Quantifying Kissinger**_](http://www.mickikaufman.com/qk/)\n\n\n**Thursday, April 3 at 10:00 am**\nin Alderman Library, Room 421\n\n[![BombingCorrelation-small](http://www.scholarslab.org/wp-content/uploads/2014/03/BombingCorrelation-big-300x177.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/03/BombingCorrelation-big.jpg)\n\nClick for a larger image.  See [www.mickikaufman.com/qk](http://www.mickikaufman.com/qk/) for a detailed description.Scarcity  of  information  is  a  common  frustration  for  many  historians.  However,  for  researchers  of  twentieth-­ and  twenty-­first  century  history  the  opposite  problem  is  also  increasingly  common.  In  contrast  to  scholars  of ancient  history,  who  base  much  of  their  analyses  on  rare  and  unique  relics  of  antiquity,  historians  studying the  ‘Age  of  Information’  (and  the  even  more  recent  period  of  ‘Big  Data’)  increasingly  confront  a  deluge  of information,  a  vast  field  of  haystacks  within  which  they  must  locate  the  needles  -­  and  presumably,  use them  to  knit  together  a  valid  historical  interpretation.\n\nAs  larger  and  larger  volumes  of  human  cultural  output are  accumulated,  historians  will  continue  to  adapt  and  innovate  new  and  existing  tools  and  methods  -­ especially  those  developed  in  other  fields,  including  computational  biology,  literary  studies,  statistics  and psychology  -­  to  overcome  the  ‘information  overload’  and  facilitate  new  historical  interpretations  of challengingly  massive  digital  archives.    The  declassification  of  Former  Secretary  of  State  Henry  Kissinger's correspondence  by  the  State  Department  and  the  hosting  of  that  material  on  the  Digital  National  Security Archive  (DNSA)’s  Kissinger  Collection  web  site  presents  just  such  a  challenge  and  concomitant  opportunity. Given  the  role  Henry  Kissinger  played  in  first  ‘computerizing’  the  State  Department  in  the  late  1960s  it  is perhaps  not  surprising  that  the  continuing  declassifications  of  large  volumes  of  material  have  made historians  of  the  Kissinger/Nixon  era  dubious  ‘beneficiaries’  of  the  'big  data'  era,  inheritors  of  countless government  mainframes’  worth  of  text.\n\nWhile  simply  having  such  a  large  volume  of  information  online  in digital  form  for  researchers  is  valuable,  the  usual  restriction  to  a  web-­based  ‘search’  form  interface  often renders  it  of  limited  use  and  approachability.  As  detailed  on  [the  project's  web  site](http://www.mickikaufman.com/qk/)  my  work  involves  the  application  of  a  host  of  quantitative  text  analysis methods  like  word  frequency/correlation,  topic  modeling  and  sentiment  analysis  (as  well  as  a  variety  of  data visualization  deisgns  and  methods)  to  historical  research  on  the  DNSA’s  Kissinger  Collection,  comprising approximately  17600  meeting  memoranda  (‘memcons’)  and  teleconference  transcripts  (‘telcons’)  detailing the  former  US  National  Security  Advisor  and  Secretary  of  State’s  correspondence  during  the  period  1969-­ 1977.  This  application  of  computational  techniques  to  the  study  of  twentieth-­century  diplomatic  history  has generated  useful  finding  aids  for  researchers,  provided  essential  testing  grounds  for  new  historical methodologies,  and  prompted  new  interpretations  and  questions  about  the  Nixon/Kissinger  era.\n\n[Micki  Kaufman](http://www.mickikaufman.com/)  is a doctoral  student  in  US  history  at  the  Graduate  Center  of the  City  University  of  New  York  (GC-­CUNY).  She  received  her  B.A.  in  US  History  from  Columbia University  summa  cum  laude,  Phi  Beta  Kappa  in  2011  and  her  M.A.  in  US  History  from  GC-­CUNY  in 2013.  A  GC-­CUNY  Digital  Fellow  and  recipient  of  GC-­CUNY’s  Provost’s  Digital  Innovation  Grant  in 2012–2014,  [her  current  research](http://www.mickikaufman.com/qk/)  involves  the  use  of  computational  text  analysis  and  visualization techniques  in  the  study  of  the  DNSA’s  (Digital  National  Security  Archive’s)  Kissinger  Memcon  and Telcon  collections.  She  is  a  co-­author  of  “General,  I  Have  Fought Just  As  Many  Nuclear  Wars  As  You  Have,”  published  in  the  December  2012  American  Historical Review,  has  served  as  a  digital  humanities  consultant  for  a  number  of  institutes  and  projects,  lectures and  leads  workshops  in  Text  Analysis  and  Visualization,  and  has  taught  US  History  Since  the  Civil War  at  Hunter  College  and  the  Digital  Praxis  Seminar  at  the  CUNY  Graduate  Center.  She  is  also  a platinum-­award-­winning  recording  engineer,  and  a  featured  Sundance/ASCAP  film  score  composer.\n\nAll [Scholars’ Lab events](http://www.scholarslab.org/events/) are free, open to the public, and require no advance registration.\n"},{"id":"2014-03-24-9770","title":"Praxis Program Panel: \"Reading Digitally with Prism and Ivanhoe\"","author":"stephanie-kingsley","date":"2014-03-24 06:00:41 -0400","categories":["Grad Student Research"],"url":"9770","content":"On April 4 at 4:30 PM in the Scholars' Lab, members of current and previous [Praxis](http://praxis.scholarslab.org/) cohorts will give a presentation on how their projects, [Prism](http://prism.scholarslab.org/) and Ivanhoe, can inform textual study and reading in a digital environment.  The presentation will also feature demonstrations of both projects.\n\n\n\n\nPresenters include:\n\n\n\n\n\n\n\t\n  * Scott Bailey (Religious Studies, Praxis '13-14)\n\n\t\n  * Veronica Ikeshoji-Orlati (Classical Archaeology, Praxis '13-14)\n\n\t\n  * Stephanie Kingsley (English, Praxis '13-14)\n\n\t\n  * Sarah Storti (English, Praxis '11-12)\n\n\t\n  * Brandon Walsh (English, Praxis '12-13)\n\n\nThe panel follows the opening masterclass of the Graduate English Student Association conference \"Reading Then and Now,\" which will run April 4-6, 2014.  The conference will also feature a masterclass by UVa English faculty member [Rita Felski](http://www.engl.virginia.edu/people/rf6d) (RSVP only); keynote speaker [Andrew Piper](http://www.mcgill.ca/german/faculty/piper), from McGill University; graduate student papers on reading practices across cultures and time periods; and a workshop by the [Rare Book School](http://www.rarebookschool.org/).  For a complete schedule, visit [http://graduate.engl.virginia.edu/gesa/conference/schedule/](http://graduate.engl.virginia.edu/gesa/conference/schedule/).\n\nThe Praxis panel on Friday is open to the public.\n\n\n\n\n\n\n"},{"id":"2014-03-24-whats-in-a-name","title":"What's in a name? ","author":"francesca-tripodi","date":"2014-03-24 06:00:30 -0400","categories":["Grad Student Research"],"url":"whats-in-a-name","content":"The week before spring break Zach and I pitched two different design strategies for Ivanhoe. [See [Zach's post](http://www.scholarslab.org/digital-humanities/an-ivanhoe-design-idea/) on a Medieval-themed design.] As part of the pitch we each designed an informational website that we could apply to the game,  but I also was trying to convince our team that we should change our name. Why? Well, even though Ivanhoe has positive associations with an existing community, I felt that the name did not convey how our game is designed to function. Also, by changing our name we were engaging in a meta-learning experience - essentially playing Ivanhoe with the name Ivanhoe! By giving Ivanhoe a new name I did not want us to abandon the old, but rather taking the original spirit of Ivanhoe, make new connections and create something new. _\n_\n\n[![Rhyzome Logo](http://www.scholarslab.org/wp-content/uploads/2014/03/Screen-shot-2014-03-18-at-1.49.39-PM.png)](http://www.scholarslab.org/wp-content/uploads/2014/03/Screen-shot-2014-03-18-at-1.49.39-PM.png)\n\nThe design team came up with the name \"Rhyzome\" as a play on the word Rhizome - a biological term used to describe types of vegetation connected by a series of tubes.  The Rhyzome logo (see above) was designed around this same logic - the nodes in the logo symbolize connections users make during the game and the two sizes of circles represent first and second level moves. Green shows the connection between our game and the hard sciences, conveying our goal of cross-discplinary application.\n\nAfter my pitch our team agreed that Rhyzome was not the best alternative. While the name better conveys what Ivanhoe does, Rhyzome is already associated with an existing[ community and journal](http://rhizome.org/) in the digital humanities. So, we decided that what is best for the team is to stay with the name that evokes recognizability within the DH community but apply the Rhyzome aesthetic to our logo/game design. Below is a first iteration of a logo that combines the two ideas. As you can see there are still incorporating two circle sizes to represent different levels of connections as well as connecting all the letters to represent a network of ideas. This logo also leaves lines open inside the A and the O to represent the possibility for new connections as the game progresses. Question to our readers - is this logo legible? Thoughts on this design? Would really love feedback as I continue to modify it.\n\n[![Ivanhoe Logo](http://www.scholarslab.org/wp-content/uploads/2014/03/Screen-shot-2014-03-18-at-2.01.02-PM-300x174.png)](http://www.scholarslab.org/wp-content/uploads/2014/03/Screen-shot-2014-03-18-at-2.01.02-PM.png)\n\nWhile my \"pitch\" for a new name was unsuccessful, in the end it was a win for our team. We now have a great design strategy that matches the theoretical logic of Ivanhoe!\n"},{"id":"2014-03-25-development-design-and-the-distance-in-between","title":"Development, Design, and the Distance in-between","author":"veronica-ikeshoji-orlati","date":"2014-03-24 21:05:14 -0400","categories":["Grad Student Research"],"url":"development-design-and-the-distance-in-between","content":"In the past week, Scott and I have been chipping away at the various issues Stephanie has uncovered in her testing of proto-Ivanhoe. There are [a LOT of issues](https://github.com/scholarslab/ivanhoe/issues?labels=&page=1&state=open), and there are still a handful of core features we have left to build by our April 1 deadline. Specifically, we need to work through some bugs with the display and assignment of roles to users, give a bit more thought to the role journal (which took a backseat to primary feature development earlier this semester), and set up a couple of other pages to make navigating the theme a bit more manageable.\n\nBuilding things is fun, but the challenge as we push towards a beta version of Ivanhoe is refining what we have built (our 'something that works' version) into a properly useable and playable game. Our code is still quite messy, since we sacrificed cleanliness for functionality as we learned to deal with PHP and [WordPress](http://wordpress.org/), and some of the bugs we need to fix in the next week or so require that we grapple with what we have already written and think more critically about the logic of what we actually want to happen.\n\nAnd this is where things get interesting.\n\nUp until about a week or two ago, the tasks of the development and design teams seemed to be fairly well circumscribed. Scott and I would read WordPress documentation, write functions, break some stuff, then have Jeremy/Eric/Wayne point out where our logic failed. Eliza/Zach/Francesca would theorize, design, and write the code to make the look-and-feel of the theme coherent, navigable, and visually appealing to users on all sorts of devices. But as part of the design team's trouble-shooting, Zach has recently been inputting WordPress test content to make sure that different types of data show up as expected, and it seems that some of the problems he has encountered are not issues with the CSS or Susy grid but, rather, problems with the consistency of the data the underlying code puts out.\n\nAs graduate students in the humanities and social sciences, it is very easy to throw ourselves wholly into our respective disciplines and revel in the tangible progress we make there. On the development team, we have likewise had the singular focus of getting features working and have made good progress thus far. It seems, however, that we have arrived at a point at which all of us need to regroup a bit to figure out what tasks each team should prioritize to move the entire project along, because a coherent game is the goal, not a glorious laundry-list of features which nobody can access or use.\n"},{"id":"2014-03-26-call-for-ivanhoe-testers","title":"Call for Ivanhoe Testers!","author":"stephanie-kingsley","date":"2014-03-26 09:45:00 -0400","categories":["Announcements","Grad Student Research","Research and Development"],"url":"call-for-ivanhoe-testers","content":"This year's [Praxis](http://praxis.scholarslab.org/) fellows are in the last couple weeks of programming before our release of the new Ivanhoe Game (rebuilt as a [WordPress Theme](http://wordpress.org/themes/)), and we are looking for people to test the program.\n\nThe Ivanhoe Game is a pedagogical and critical tool which enables scholars or students to generate discussion and criticism on a subject through role-play.  The original Ivanhoe Game was conceived in the early 2000s by Jerome McGann and Johanna Drucker; they were then joined by Bethany Nowviskie, Nick Laiacona, and others to develop the game in UVa's SpecLab/ARP--one of several precursors to the [Scholars' Lab](http://www.scholarslab.org/).  You can learn more at [http://www.ivanhoegame.org/?page_id=21](http://www.ivanhoegame.org/?page_id=21)\n\nPotential uses of the Ivanhoe Game WP Theme include:\n\n\n\n\t\n  * Critical back-and-forth game play among scholars interested in applying different theoretical schools of thought to the same work.  For instance, in a game on _The Tempest, _one scholar might elect to 'be' Freud, another a Postcolonialist, and another a feminist critic; and the 'moves' might constitute asides written by these players in response to action in the play.\n\n\t\n  * Students in a course on textual editing or book history might take on roles as different people involved in the publication of a hypothetical or real book: author, editor, compositor, illustrator, printer, binder, bookseller, etc.!\n\n\t\n  * Law students can take on roles in a digital mock trial, making moves for various motions, documentary evidence, arguments, witness statements, testimony, etc.\n\n\t\n  * History students in a course on the American Revolution might take on roles as King George II, the Founding Fathers, generals and soldiers in various armies, and others.\n\n\nThis is just to name a few; the possibilities are endless!  Ivanhoe's construction as a WP Theme means that it can be easily incorporated into already existing WP pages, including course pages.  For a completely bare-bones idea of what Ivanhoe will be, [visit our testing app Games page](http://ivanhoe-staging.herokuapp.com/?post_type=ivanhoe_game), hosted on [Heroku.](https://www.heroku.com/)\n\nWe hope to start testing on April 8.  We are looking for varying levels of commitment.  For a larger number of testers, 20-30 minutes a week would be helpful for us.  For a few testers who have the time and interest, we hope for 2-5 hours a week.\n\nIf you are interested, please fill out the form below.  I will contact you within a week to make arrangements for testing.\n\n\n"},{"id":"2014-03-27-solrsearch-2-0","title":"SolrSearch 2.0","author":"david-mcclure","date":"2014-03-27 06:20:19 -0400","categories":["Announcements"],"url":"solrsearch-2-0","content":"![](https://camo.githubusercontent.com/c0f1aee75ff273b3a715ad185d1aa4e7e1958008/68747470733a2f2f6c7563656e652e6170616368652e6f72672f696d616765732f736f6c722e706e67)\n\nToday we're pleased to announce version 2.0 of the [SolrSearch plugin](http://omeka.org/add-ons/plugins/solrsearch/) for Omeka! SolrSearch replaces the default search interface in Omeka with one powered by [Solr](https://lucene.apache.org/solr/), a blazing-fast search engine that supports advanced features like hit highlighting and faceting. In most cases, [Omeka's built-in searching capabilities](http://omeka.org/codex/Managing_Search_Settings_2.0) work great, but there are a couple of situations where it might make sense to take a look at Solr:\n\n\n\n\n\n\n  * When you have a _really_ large collection - many tens or hundreds of thousands of items - and want something scales a bit better than the default solution.\n\n\n\n\n  * When your metadata contains a lot of text content and you want to take advantage of Solr's hit highlighting functionality, which makes it possible to display a preview snippet from each of the matching records.\n\n\n\n\n  * When your site makes heavy use of content taxonomies - collections, tags, item types, etc. - and you want to use Solr's [faceting](http://en.wikipedia.org/wiki/Faceted_search) capabilities, which make it possible for users to progressively narrow down search results by adding on filters that crop out records that don't fall into certain categories. Stuff like - show me all items in \"Collection 1\", tagged with \"tag 2\", and so forth.\n\n\n\n\n\nTo use SolrSearch, you'll need access to an installation of Solr 4. To make deployment easy, the plugin includes a preconfigured \"core\" template, which contains all the configuration files necessary to index content from Omeka. Once the plugin is installed, just copy-and-paste the core into your Solr home directory, fill out the configuration forms, and click the button to index your content in Solr.\n\nOnce everything's up and running, SolrSearch will automatically intercept search queries that are entered into the regular Omeka \"Search\" box and redirect them to a custom interface, which exposes all the bells and whistles provided by Solr. Here's what the end result looks like in the \"Seasons\" theme, querying against a test collection that contains the last few posts from this blog, which include lots of exciting Ivanhoe-related news:\n\n[![solr-search](http://www.scholarslab.org/wp-content/uploads/2014/03/solr-search2-749x1024.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/03/solr-search2.jpg)\n\nOut of the box, SolrSearch knows how to index three types of content: (1) Omeka items, (2) pages created with the Simple Pages plugin, and (3) exhibits (and exhibit page content) created with the Exhibit Builder plugin. Since regular Omeka items are the most common (and structurally complex) type of content, the plugin includes a point-and-click interface that makes it easy to configure exactly how the items are stored in Solr - which elements are indexed, and which elements should be used as facets:\n\n[![solr-config](http://www.scholarslab.org/wp-content/uploads/2014/03/solr-config-857x1024.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/03/solr-config.jpg)\n\nMeanwhile, if you have content housed in database tables controlled by other plugins that you want to vacuum up into the index, SolrSearch ships with an \"addons\" system (devised by my brilliant partner in crime Eric Rochester), which makes it possible to tell SolrSearch how to index other types of content just by adding little JSON documents that describe the schema. For example, registering Simple Pages is as simple is this:\n\n\n\nAnd the system even scales up to handle more complicated data models, like the [parent-child relationship between \"pages\" and \"page blocks\" in ExhibitBuilder](https://github.com/scholarslab/SolrSearch/blob/master/addons/exhibits.json), or between \"exhibits\" and \"records\" in Neatline.\n\nAnyhow, grab the built package from the Omeka addons repository or clone the repository from GitHub. As always, if you find bugs or think of useful features, be sure to file a ticket on the [issue tracker](https://github.com/scholarslab/SolrSearch/issues?state=open)!\n"},{"id":"2014-03-28-fedoraconnector-2-0","title":"FedoraConnector 2.0","author":"david-mcclure","date":"2014-03-28 08:28:40 -0400","categories":["Announcements"],"url":"fedoraconnector-2-0","content":"[![fedora](http://www.scholarslab.org/wp-content/uploads/2014/03/fedora.png)](http://www.scholarslab.org/wp-content/uploads/2014/03/fedora.png)\n\nHot on the heels of yesterday's update to the [SolrSearch](http://www.scholarslab.org/announcements/solrsearch-2-0/) plugin, today we're happy to announce version 2.0 of the [FedoraConnector](http://omeka.org/add-ons/plugins/fedoraconnector/) plugin, which makes it possible to link items in Omeka with objects in [Fedora Commons](http://www.fedora-commons.org/) repositories! The workflow is simple - just register the location of one or more installations of Fedora, and then individual items in the Omeka collection can be associated with a Fedora PID. Once the link is established, any combination of the datastreams associated with the PID can be selected for import. For each of the datastreams, FedoraConnector proceeds in one of two ways:\n\n\n\n\n\n\n  * If the datastream contains **metadata** (e.g., a Dublin Core record), the plugin will check to see if it can find an \"importer\" that knows how to read the metadata format. Out of the box, the plugin can import Dublin Core and MODS, but also includes a really simple API that makes it easy to add in new importers for other metadata standards. If an importer is found for the datastream, FedoraConnector just copies all of the metadata into the item, mapping the content into the Dublin Core elements according to the rules defined in the importer. This creates a \"physical\" copy of the metadata that isn't linked to the source object in Fedora - changes in Omeka aren't pushed back upstream into Fedora, and changes in Fedora don't cascade down into Omeka.\n\n\n\n\n  * If the datastream delivers **content** (e.g., an image), the plugin will check to see if it can find a \"renderer\" that knows how to display the content. Like the importers, the renderers are structured as an extensible API that ships with a couple of sensible defaults - out of the box, the plugin can display regular images (JPEGs, TIFs, PNGs, etc.) and JPEG2000 images. If a renderer exists for the content type in question, the plugin will display the content _directly from Fedora_. So, for example, if the datastream is a JPEG image, the plugin will add markup like this to the item show page:\n\n\n\nUnlike the metadata datastreams, then, which are copied from Fedora, content datastreams pipe in data from Fedora on-the-fly, meaning that a change in Fedora will immediately propagate out to Omeka.\n\n\n\n\n\n(See the image below for a sense of what the final result might look like - in this case, displaying an image from the Holsinger collection at UVa, with both a metadata and content datastream selected.)\n\nFor now, FedoraConnector is a pretty simple plugin. We've gone back and forth over the course of the last couple years about how to model the interaction between Omeka and Fedora. Should it just be a \"pull\" relationship (Fedora -> Omeka), or also a \"push\" relationship (Omeka -> Fedora)? Should the imported content in Omeka stay completely synchronized with Fedora, or should it be allowed to diverge for presentational purposes? These are tricky questions. Implementations of Fedora - and the workflows that intersect with it - can vary pretty widely from one institution to the next. The current set of features was built in response to specific needs here at UVa, but we've been talking recently with folks at a couple of other institutions who are interested in experimenting with variations on the same basic theme.\n\nSo, to that end, if you're use Fedora and Omeka and interested in wiring them together - we'd love to hear from you! Specifically, how exactly do you use Fedora, and what type of relationship between the two services would be most useful? With a more complete picture of what would be useful, I suspect that a handful of pretty surgical interventions would be enough to accommodate most use patterns. In the meantime, be sure to file a ticket on the [issue tracker](https://github.com/scholarslab/SolrSearch/issues) if you find bugs or think of other features that would be useful.\n\n[![fedora](http://www.scholarslab.org/wp-content/uploads/2014/03/fedora-352x1024.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/03/fedora.jpg)\n"},{"id":"2014-03-31-digest-5-managerial-musings-preceding-the-launch","title":"(Digest #5) More Reflections on Project Management","author":"stephanie-kingsley","date":"2014-03-31 06:00:22 -0400","categories":["Grad Student Research"],"url":"digest-5-managerial-musings-preceding-the-launch","content":"It has been a couple weeks since my last digest, but as you can see from other posts, the Praxis goings-on are many! Development scurries to fix the multitude of bugs that seem to keep crawling out of the WordPress woodwork, while Design continues to apply CSS, SASS, and SUSY to bring our vision of Ivanhoe to fruition. Meanwhile, Francesca and I draft, revise, and reorganize content for our informational website, which will go up next week.\n\nAs Project Manager, I have been busy getting word out about Ivanhoe, preparing for the [Praxis panel and demo on April 4](http://www.scholarslab.org/grad-student-research/9770/), gathering testers, and constantly rethinking how we want to present our new game to the world.\n\nI also continue to reflect on my own role as project manager. A few weeks back, I chronicled my [project-management crisis](http://www.scholarslab.org/uncategorized/digest-4-on-managing-projects-not-people-reflections-after-a-project-management-crisis/), in which I reached a point where I wasn't entirely sure what my purpose was because my team needed very little actual management. As we rapidly approach our soft launch (April 8), I realize how very much I have to do in terms of promoting Ivanhoe and organizing the next major phase of the project: testing.  In writing my post on testing, sending emails to prospective testers, and preparing Ivanhoe web content, I have to consider what people unfamiliar with Ivanhoe need to know about it. I also find myself revisiting the question which our team debated through the entirety of the first semester: what is Ivanhoe?\n\nI am not going to speculate on what Ivanhoe is in this post; for my presentation of the game, check out my [\"Call for Testers\" post](http://www.scholarslab.org/announcements/call-for-ivanhoe-testers/). What I do wish to take a moment to marvel at is how having to present Ivanhoe to others has made me see new aspects of it which I hadn't considered before. While showing the game to a friend just last night in a sort of informal think-tank session, I discovered just how useful it is to be able to view a string of posts containing YouTube videos, image files, text, and web links, one after another. Our Theme also allows users to view a list of all the moves a player has made in a given role, which makes it easy to see how the player has developed that role over the course of the game. Essentially, we have streamlined multimedia game play and greatly facilitated role review. When Ivanhoe comes out, and after we kill all the bugs, of course, it will be easy to use for collaborative multimedia game play.\n\nI see even better now how my role as project manager is both coherent and vital.  By being at the forefront of publicity, I constantly revisit the question of what Ivanhoe is, and I can bring those insights back to team meetings and use them to guide us as we continue to make decisions about features and design details. At the end of [\"On Managing Projects, Not People,\"](http://www.scholarslab.org/uncategorized/digest-4-on-managing-projects-not-people-reflections-after-a-project-management-crisis/) I mentioned that the task of maintaining our vision for Ivanhoe was at that point \"a bit amorphous and abstract.\"  I now have a much better idea of how very real that job is.  I look forward to presenting that vision to the world, incarnate, in a couple weeks.\n"},{"id":"2014-03-31-neatline-text","title":"NeatlineText: Connect Neatline exhibits to documents","author":"david-mcclure","date":"2014-03-31 08:00:11 -0400","categories":["Announcements"],"url":"neatline-text","content":"## [Download the plugin](http://omeka.org/add-ons/plugins/neatlinetext)\n\n\n\n[![nltext-detail](http://www.scholarslab.org/wp-content/uploads/2014/03/nltext-detail-1024x442.jpg)](http://omeka.org/add-ons/plugins/neatlinetext)\n\nToday we're pleased to announce the first public release of **[NeatlineText](http://omeka.org/add-ons/plugins/neatlinetext)**, which makes it possible to create interactive, Neatline-enhanced editions of text documents - literary and historical texts, articles, book chapters, dissertations, blog posts, etc. - by connecting individual paragraphs, sentences, and words with objects in Neatline exhibits. Once the associations are established, the plugin wires up two-way linkages in the user interface between the highlighted sections in the text and the imagery in the exhibit. Click on the text, and the exhibit focuses around the corresponding location or annotation. Or, click on the map, and the text scrolls to show the corresponding sections in the text.\n\nWe've been using some version of this code in internal projects here at the lab for almost two years, and it's long overdue for a public release. The rationale for NeatlineText is pretty simple - again and again, we've found that Neatline projects often go hand-in-hand with some kind of regular text narrative that sets the stage, describes the goals of project, or lays out an expository thesis that would be hard to weave into the more visual, free-form world of the Neatline exhibit proper. This is awesome combination - tools like Neatline are really good at displaying spatial, visual, dimensional, diagrammatic information, but nothing beats plain old text when you need to develop a nuanced, closely-argued narrative or interpretation.\n\nThe difficulty, though, is that it's hard to combine the two in a way that doesn't favor one over the other. We've taken quite a few whacks at this problem over the course of the last few years. One option is to present the text narrative as a kind of \"front page\" of the project that links out to the interactive environment. But this tends to make the Neatline exhibit feel like an add-on, something grafted onto the project as an after-thought. And this can easily become a self-fulfilling prophecy - when you have the click back and forth between different web pages to read the text and explore the exhibit, you tend to write the text as a more or less standalone piece of writing, instead of weaving the narrative in with the conceptual landscape of the exhibit.\n\nAnother option is to chop the prose narrative up into little chunks and build it directly into the exhibit - like the numbered waypoints we used in the [the](http://hotchkiss.neatline.org/neatline-exhibits/show/chancellorsville-may-2-1863-132/fullscreen) [Hotchkiss](http://hotchkiss.neatline.org/neatline-exhibits/show/battle-of-chancellorsville/fullscreen) [projects](http://hotchkiss.neatline.org/neatline-exhibits/show/my-dear-little-nelly/fullscreen) back in 2012, each waypoint containing a small portion of a longer interpretive essay. But this tends to err in the other direction, dissolving the text into the visual organization of the exhibit instead of presenting it as a first-class piece of content.\n\nNeatlineText tries to solves the problem by just plunking the two next to each other and making it easy for the reader (and the writer!) to move back and forth between the two. For example, NeatlineText powers the interactions between the text and imagery in [these](http://neatline.dclure.org/neatline/show/gemini-over-baja-california) [two](http://neatline.dclure.org/neatline/show/saturn-v-stage-2) exhibits of NASA photograph from the 1960s:\n\n[![nltext-gemini](http://www.scholarslab.org/wp-content/uploads/2014/03/nltext-gemini-1024x619.jpg)](http://dclure.org/logs/project-gemini-over-baja-california/)\n\n[![nltext-saturn-v](http://www.scholarslab.org/wp-content/uploads/2014/03/nltext-saturn-v-1024x615.jpg)](http://neatline.dclure.org/neatline/show/saturn-v-stage-2)\n\n(Yes, I know - I'm a space nerd.) NeatlineText is also great for creating interactive editions of primary texts. An earlier version of this code powers the [Mapping the Catalog of Ships](http://ships.lib.virginia.edu/neatline-editions/271) project by Jenny Strauss Clay, Courtney Evans, and Ben Jasnow (winner of the Fortier Prize prize at DH2013!), which connects the contingents in the Greek army mentioned in Book 2 of the Iliad with locations on the map:\n\n[![nltext-ships](http://www.scholarslab.org/wp-content/uploads/2014/03/nltext-ships-1024x614.jpg)](http://ships.lib.virginia.edu/neatline-editions/271)\n\nAnd NeatlineText was also used in this interactive edition of the [first draft of the Gettysburg Address](http://neatline.dclure.org/neatline/show/gettysburg-address):\n\n[![nltext-gettysburg](http://www.scholarslab.org/wp-content/uploads/2014/03/nltext-gettysburg-1024x611.jpg)](http://dclure.org/logs/nicolay-copy-gettysburg-address/)\n\nAnyway, grab the code from the [Omeka add-ons repository](http://omeka.org/add-ons/plugins/neatlinetext) and check out the [documentation](https://github.com/scholarslab/nl-widget-Text#neatlinetext) for step-by-step instructions about how to get up and running. And, as always, be sure to [file a ticket](https://github.com/scholarslab/nl-widget-Text/issues) if you run into problems!\n"},{"id":"2014-04-01-theming-neatline-exhibits","title":"Creating themes for individual Neatline exhibits","author":"david-mcclure","date":"2014-04-01 09:16:12 -0400","categories":["Geospatial and Temporal"],"url":"theming-neatline-exhibits","content":"**tldr:** Neatline makes it possible to create separate themes for individual exhibits, which is useful if you want to host a collection of self-contained Neatline projects on a single site. To get started, fork the [exhibit starter theme](https://github.com/scholarslab/neatline-theme-template), which abstracts out the style, layout, and UX of the [Project Gemini over Baja California](http://dclure.org/logs/project-gemini-over-baja-california/) exhibit.\n\nOne of the coolest but most under-documented features in Neatline is the ability to create separate themes for each individual exhibit. Since Neatline exhibits are just one particular type of \"view\" inside of Omeka, it's always been possible to customize the styling and layout at the level of the Omeka theme. Changes the the Omeka theme, though, propagate to all the exhibits on the site. In many cases, this is ideal - if you have a collection of closely-related Neatline projects, all part of the same thematic umbrella, it makes sense that they should all look more or less the same. For examples of this, check out Jeremy's beautiful [Astrolabe](https://github.com/scholarslab/astrolabe) and [Neatscape](https://github.com/scholarslab/neatscape) themes for Omeka, which were designed with Neatline projects in mind.\n\nIn other cases, though, this can be a real hindrance. Sometimes it can make sense to host a number of self-contained Neatline exhibits in the same installation of Omeka. For example, imagine you're using Neatline in a big lecture course, and you split the class up into 10-15 groups of students, all working on separate exhibits. As the semester draws to an end, some of the groups want to use the [NeatlineText](http://www.scholarslab.org/announcements/neatline-text/) plugin, and need a layout that positions the exhibit narrative on the side of the screen, flush with the edge of the window. But other groups are just threading the text content into the record bodies, and don't want a big, empty container element taking up space on the screen. How to handle both at once? Or, for a concrete example, take a look at the [Neatline Labs](http://neatline.dclure.org/) site, which I use a sandbox for little Neatline-powered experiments and feature demos. By design, these projects are all totally different - different content, different layouts, different Javascript interactions, etc:\n\n[![panorama](http://www.scholarslab.org/wp-content/uploads/2014/03/panorama-1024x88.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/03/panorama.jpg)\n\nIt would be annoying to have to spin up a completely new instance of Omeka for each of these projects. To get around this, Neatline implements its own \"sub-theming\" system, piggybacking on top of the capabilities provided by Omeka, that makes it possible to customize part or all of the appearance, layout, or behavior of each exhibit on an individual basis. This is an opt-in system that can be mixed with the regular, site-wide theming system - if you have 10 Neatline exhibits on your site, you could write exhibit-specific themes for three of them, and leave the other seven unchanged, allowing them to continue to inherit the generic Omeka theme. And, within the three exhibit-specific themes, you have full control over which parts of the theme you override - for one, you could leave the layout unchanged, but modify the CSS; for another, you could leave the CSS the same but change the layout and add some custom Javascript interactions. Exhibit-specific themes are also highly portable - once you've built one to your liking, it can be adapted for new exhibits just by copying and renaming the directory.\n\nI've held off on documenting this publicly because I wanted to be sure that the file structure and Javacsript APIs used in the themes worked well at scale - but at this point it's all pretty battle tested, and I'm curious to see what other folks can come up with!\n\n\n\n## Getting started: Creating the theme directory\n\n\n\nNeatline themes are created as directories that sit inside of the Omeka theme. For any given exhibit, Neatline will look for a theme directory that has the same name as the \"URL slug\" of the exhibit, the unique, plain-text identifier used to form the end of the exhibit's public-facing URL. So, imagine you've got an exhibit called \"Test Exhibit,\" with a URL slug of **`test-exhibit`**. To create a theme for the exhibit, create a directory called **`test-exhibit`** at this location relative to the root of your Omeka theme:\n\n**`[omeka-theme]/neatline/exhibits/themes/test-exhibit`**\n\nFor example, here's the layout of [my fork of the Neatlight theme](https://github.com/davidmcclure/neatlight), with the theme directories for a handful of exhibits at [neatline.dclure.org](http://neatline.dclure.org/):\n\n[![nltheme-tree](http://www.scholarslab.org/wp-content/uploads/2014/03/nltheme-tree.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/03/nltheme-tree.jpg)\n\n\n\n## Anatomy of a Neatline theme\n\n\n\nNeatline themes consist of just four files: **`style.css`**, **`script.js`**, **`show.php`**, and **`item.php`**.\n\n\n\n# `style.css`\n\n\n\nUse **`style.css`** to add custom CSS to the exhibit. Neatline loads this as the last stylesheet on the page, after the Omeka CSS and after the CSS provided by the Neatline core (which, if you want, can be omitted from the page by providing a custom **`show.php`** template - see below). **`style.css`** can be anything from a handful of simple rules to change fonts or colors up to a complete redesign of the page.\n\n\n\n# `script.js`\n\n\n\nUse **`script.js`** to add custom Javascript interactions to the page. Again, these can be as simple or complex as needed. The Neatline front-end application is a big chunk of code, and it's a bit beyond the scope of this article to really dive into the API in detail. The gist of it, though, is that Neatline is structured as a bunch of little mini-applications, called \"modules,\" that communicate with one another using a [pub-sub](http://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern) messaging system, powered by the superb [EventAggregator component in the Marionette framework](https://github.com/marionettejs/backbone.wreqr#event-aggregator).\n\nThe cool thing about this architecture is that snippets of code in the **`script.js`** file can hook directly into this messaging system and interact with Neatline just as if they were included in the core codebase - Neatline literally won't know the difference. There's really no limit to what you could do here - the entire Neatline editing environment, for instance, is implemented as a single module (containing lots and lots of nested sub-modules), and could theoretically be grafted onto Neatline completely inside of an exhibit theme. This makes it possible to wrap up a Neatline exhibit in pretty much any kind of interface without having to modify the internals.\n\nThat said, in most cases you'll probably just need a few little snippets to add in some visual bells and whistles, or to manage complex layout tasks that are tough to accomplish in CSS. For example, here are a few snippets I used in the [Project Gemini over Baja California](http://dclure.org/logs/project-gemini-over-baja-california/) project:\n\n\n\n\n\n  * Position the text container on the left side of the screen and fill the height of the window:\n\n\n\n\n\n  * Add an [NProgress](http://ricostacruz.com/nprogress/)-powered loading bar to the page:\n\n\n\n\n\n  * Implement custom zooming buttons:\n\n\n\n\n\n  * Add a \"Loading Tiles...\" spinner that displays when WMS imagery is being loaded from Geoserver:\n\n\n\n\n\nFor an example of a fully-fledged module, which follows the file layout conventions of the Neatline core, take a look at the [`Lines` module in the Gemini theme](https://github.com/davidmcclure/neatlight/tree/master/neatline/exhibits/themes/gemini-over-baja-california/assets/javascripts/lines), which intercepts events broadcast by NeatlineText and draws the little yellow lines between the text and the map.\n\n\n\n# `show.php`\n\n\n\nBy default, all Neatline exhibits are displayed using the **`show.php`** that ships with the plugin. If you create a file called **`show.php`** in the exhibit theme, though, Neatline will use that file in place of the default. This makes it possible to completely customize the structure of the markup in any way you want. For example, if you look closely at some of the Jacascript examples above, you'll notice that in a couple of places the code is selecting elements (things like `$('#wms-loader')` and `$('div.narrative')`) that aren't actually templated anywhere in the default **`show.php`**, which looks like this:\n\n\n\nThis works, though, because I'm providing a custom **`show.php`** template that provides those elements (e.g., see the `#wms-loader` element down near the bottom):\n\n\n\n\n\n# `item.php`\n\n\n\nLast but not least, Neatline makes it possible to override the template that's used to generate the metadata output for items displayed inside Neatline exhibits. By default, Neatline uses a simple template that pretty much just follows the layout of the regular item \"show\" pages in Omeka:\n\n\n\nBut, imagine you had an exhibit that was filled with items that represent photographs, and, for the sake of cleanliness and visual economy, you _just_ want to display the item title and the image thumbnail. Just drop in a new **`item.php`** template that does exactly what you need:\n\n\n\nAnd Neatline will automatically use that template instead of the default. What if you need different templates for different items, though? For example, imagine that you actually have two types of items in the exhibit - the images, which just need the title and thumbnail, but also a set of letters, which are structured as \"Text\" type items with the transcriptions of the documents entered into the \"Text\" field. So, how to display both types of items in the exhibit, without resorting to a weird, Frankenstein template that accommodates both?\n\nFirst, add tags to the records in Neatline:\n\n[![nltheme-tag](http://www.scholarslab.org/wp-content/uploads/2014/03/nltheme-tag-300x214.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/03/nltheme-tag.jpg)\n\nThen, just create two template in the exhibit theme - one called **`item-image.php`** (the same as above), the other called **`item-letter.php`**. In the letter template, just display the title and text:\n\n\n\nNeatline will automatically use the tag-specific templates for any records tagged with `image` or `letter`, and fall back to the unadorned **`item.php`** template for records that aren't tagged.\n\n\n\n## Starter theme\n\n\n\nSo far, we've just been entering all of our custom CSS and Javascript directly into the **`style.css`** and **`script.js`** and  files. This works fine for simple themes, but it can start to get a little clunky as the theme grows more complex - nobody likes to see a big heap of Javascript snippets, all doing different things, crammed into the same file. So, how to decompose **`style.css`** and **`script.js`** into separate files? One good solution is to use a task runner like [Grunt](http://gruntjs.com/) to concatenate multiple source files into the **`style.css`** and **`script.js`** files, which, instead of being edited directly, become compiled payload files that are updated automatically by the task runner.\n\n[![file-concatenation](http://www.scholarslab.org/wp-content/uploads/2014/03/file-concatenation.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/03/file-concatenation.jpg)\n\nTo make it easy to get started, I've created a little starter theme, based on the theme used for the [Project Gemini over Baja California](http://dclure.org/logs/project-gemini-over-baja-california/) project, with all of the configuration and file structure in place to build out themes for exhibits that use the NeatlineText extension. This includes all of the layout, styling, and UX interactions from the Gemini project, like the little yellow lines that wire up the text with the map.\n\n[https://github.com/scholarslab/neatline-theme-template](https://github.com/scholarslab/neatline-theme-template)\n\nJust fork the repo, clone it into your Omeka theme, and theme into the sunset!\n"},{"id":"2014-04-08-all-together-now","title":"All Together Now","author":"veronica-ikeshoji-orlati","date":"2014-04-08 18:36:21 -0400","categories":["Grad Student Research"],"url":"all-together-now","content":"Recently, Bethany shared a [twitter conversation](https://storify.com/nowviskie/collective-authorship-and-the-praxisprogram-charte) about why “Praxis Program Team” has been listed among the authors of publications and presentations on Prism. As a member of the 2013-14 Praxis cohort, I can attest to the fact that we haven't yet given as much thought to the question of publication and presentation as previous years have. In [our charter](http://praxis.scholarslab.org/charter.html), we adopted the \"Equal Credit\" clause from the [2012-13 cohort's charter](http://praxis.scholarslab.org/charter-2012-2013.html), which was itself a thoughtful reiteration of the same clause in the [2011-12 cohort's charter](https://github.com/scholarslab/praxis/blob/3bf01121aff5e57172d9a2d998098a8c34b26bab/charter.md). But since ratifying our charter in mid-September, we haven't taken the time to think critically about what the \"Equal Credit\" clause means in the long run.\n\nAs the end of our fellowship year together looms on the horizon, it seems prudent to reflect on what 'equal credit' means in our re-imagination of the [Ivanhoe Game as a WordPress theme](http://ivanhoe.scholarslab.org/). It seems especially important given that [collaborative work is a concept not entirely at home in the academic world](http://www.insidehighered.com/advice/2013/02/20/essay-issues-related-what-digital-scholarship-counts-tenure-and-promotion) in which we, as humanities and social sciences graduate students, have been well-steeped.\n\nWe started the year with a gargantuan task - to redefine, redesign, and build anew the Ivanhoe Game. To accomplish that goal, however, we had the even greater challenge of attempting to become a cohesive, functional group. [We struggled](https://www.scholarslab.org/grad-student-research/sticky-situations-lessons-group-cohesion/) (or, as [Francesca more thoughtfully wrote](http://www.scholarslab.org/grad-student-research/forming-norming-storming-performing/), we took our time) to draw upon all of the intellectual and methodological strengths represented in our cohort. Tensions eased as soon as we broke into development, design, and support teams to start building Ivanhoe, but then we had to deal with [communication breakdowns along the way](http://www.scholarslab.org/grad-student-research/development-design-and-the-distance-in-between/). One thing, however, has become patently clear: the whole of what we have accomplished thus far is certainly more than the sum of our individual contributions.\n\nSince I appear obsessed with metaphors in writing about Praxis, I offer one from my previous life as a chamber musician. I played Baroque cello and was often asked 'why would you ever choose to play basso continuo and not just focus on being a solo concert cellist?' Of course, one can always produce an appropriately snide response to such a question ('have you _seen_ the number of solo pieces for Baroque cello?'). But the fact is, my motivations for choosing Baroque cello as my instrument were different. I thought it was absolutely fantastic the way that the basso continuo formed the foundation of all (well, most) Baroque music, and the way individual performers melded into a single instrument in Baroque ensembles was breathtakingly powerful.\n\nEvery member of a musical ensemble is responsible for practicing and developing his or her individual technique and musicality. But it is when those individuals come together as an ensemble that the performance happens, that stories are told. This, to me, reflects the principle underlying our \"Equal Credit\" clause, and it is why the re-imagined Ivanhoe belongs to the Scholars' Lab and Praxis Program Team as a whole.\n\nAnd look how much fun it is to work together! When you close an issue, Eric might even draw you a unicorn fighting Pegasos before crossing off the issue from our whiteboard:\n\n[![Eric_Pegasos_Unicorn](http://www.scholarslab.org/wp-content/uploads/2014/04/Eric_Pegasos_Unicorn-169x300.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/04/Eric_Pegasos_Unicorn.jpg)\n"},{"id":"2014-04-08-day-of-dh-2014","title":"Day of DH 2014","author":"purdom-lindblad","date":"2014-04-08 16:26:04 -0400","categories":["Digital Humanities","Grad Student Research","Research and Development"],"url":"day-of-dh-2014","content":"[caption id=\"attachment_10129\" align=\"alignleft\" width=\"300\"][![Gratuitous shot of the laptop stand prototype Jeremy and I printed. ](http://www.scholarslab.org/wp-content/uploads/2014/04/kraken-300x225.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/04/kraken.jpg) Gratuitous shot of the laptop stand prototype Jeremy and I printed.[/caption]\n\nYesterday morning, I arrived at the Lab early, grabbed a cup of coffee, and began to work on a series of slides for an upcoming data management for humanities and social science graduate students [workshop](https://www.google.com/calendar/render?eid=OWJvYnRwNnJiYWJhajlhMDg2Z3RiNTBwOTAgOGlrOHBpcGQ2cjY1Z24zaHVyZWM2YWxpMWNAZw&ctz&sf=true&output=xml). As I worked, the Lab slowly, then more quickly, filled with my incredible colleagues, students, and one very adorable baby. Over the course of the day, there was an ebb and flow of questions, answers, experiments, consultations, and a few random cat gifs.\n\nToday, the official Day of DH mirrored the feel, if not the same events, of yesterday. I realize this post is in danger of sounding overly-sentimental (and extremely privileged), but I have been thinking a great deal about Miriam Posner’s excellent [Commit to DH People, not Projects post](http://miriamposner.com/blog/commit-to-dh-people-not-dh-projects/), particularly as the Praxis team readies for the Ivanhoe launch. So, I thought I would contribute a few of today’s observed moments of the Scholars’ Lab cultivating people.\n\nWe are very fortunate to have Spandana Bhowmik visiting the Lab from [Jadavpur University in Calcutta](http://www.jaduniv.edu.in). Spandana opened the morning with a presentation of her work. As she walked us through her project, several good questions were raised despite the fact none of us were very familiar with the texts she studies. Engaged listening is something I have noticed in my short four months in the Lab.\n\nIn the afternoon, the [Praxis team](http://praxis.scholarslab.org/people.html) gathered as they [ready for the launch of Ivanhoe](http://www.scholarslab.org/grad-student-research/development-design-and-the-distance-in-between/). In the midst of talking out strategies to support the launch, the possibility of Ivanhoe crashing after 3 minutes was raised. Immediately, the response: “Maybe, but what a glorious 3 minutes it will be.” A casual moment reflecting failure was possible, but the discussion then turned to what must be done to release the best working game we could—something Miriam points to in her post, sometimes failure is good, but steps can be taken to protect against failure. I am consistently impressed with how the R&D team set our fellows up to succeed, to gain confidence, and to laugh at and then understand the broken bits. The flip side of this is I am also impressed with how hard our fellows work.\n\nAll of this is not to sidestep the important discussions around the implicit dangers of the imperative to “[do what you love](http://www.slate.com/articles/technology/technology/2014/01/do_what_you_love_love_what_you_do_an_omnipresent_mantra_that_s_bad_for_work.html);” this post is not really about doing what one loves, but more about seeking out people who are committed to cultivating environments of experimental play, collaborative learning, and fellowship.\n"},{"id":"2014-04-09-announcing-the-ivanhoe-information-website-and-beginning-of-testing","title":"(Digest #6) Announcing the Ivanhoe Information Website and Beginning of Testing","author":"stephanie-kingsley","date":"2014-04-09 10:00:31 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"announcing-the-ivanhoe-information-website-and-beginning-of-testing","content":"Yesterday we celebrated the Day of DH by preparing for internal testing of the Ivanhoe Game WP Theme.  The entire team is now scurrying to make some finishing touches to the theme, info site, and documentation before our testers begin their games.  Development has been working to stabilize roles and the role journal features, and they have reported success.  The following image indicates Development's glee at this and similar recent successes:\n\n[![wheee](http://www.scholarslab.org/wp-content/uploads/2014/04/wheee-225x300.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/04/wheee.jpg)\n\nDesign continues to reflect on and tweak the aesthetics of Ivanhoe, while Support (now consisting of Francesca and myself) work on the website.  Francesca just finished drafting documentation; see her recent post on the process of walking through Ivanhoe and imagining where users will need guidance.  Also see [Veronica's post](http://www.scholarslab.org/grad-student-research/all-together-now/) reflecting on equal credit, a tenet from our charter; now that we have a product we can begin taking credit for, figuring out how to apportion that credit will start to become a bigger deal, so it's a good time to start thinking about it.\n\nIn addition to the typical weekly digest material where I give an update on each team's activities, this week's digest brings extra-exciting news: we have an informational website.  To learn about Ivanhoe and start thinking about what it will do, [visit our website](http://ivanhoe.scholarslab.org/).  You can also visit our [testing space](http://ivanhoe-staging.herokuapp.com/) (a Heroku staging app) to view game play as testing (which begins tomorrow) proceeds.\n\nAlthough the Ivanhoe Game WordPress Theme is still in its development, we want you all to be aware that it will very soon be available for download.  Once we've spent a few weeks working to improve Ivanhoe based on feedback from our testers, then we will officially launch Ivanhoe version 1.0, and you can start customizing it and playing your own games on it.  (Stay tuned; the official launch date will be announced within the week.)\n\nSo keep an eye on Heroku to see what's going on with our testers' games, and start thinking of ideas for your own.  It will not be long before the Ivanhoe Game returns and is available for everyone to enjoy.\n\nHappy Belated Day of DH!\n"},{"id":"2014-04-09-documenting-ivanhoe","title":"Documenting Ivanhoe","author":"francesca-tripodi","date":"2014-04-09 10:00:22 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"documenting-ivanhoe","content":"In preparation for our launch, Stephanie and I spent some time documenting Ivanhoe to help users navigate our site. For inspiration we visited [Neatline's website](http://docs.neatline.org/) because we felt that their documentation process was spectacular! Doing this was much more difficult than anticipated because I was trying to see the game from a person who had never heard of it before. While the technology is fairly intuitive (i.e. filling out fields is rather easy for most people familiar with WordPress) _why _they want to fill out the fields gets back to the theoretical underpinnings of our game.\n\nFor example - Starting a New Game is pretty easy. Click \"New Game\" and fill in the blanks (very similar to writing a post for those familiar with Word Press). But when you start a new game one of the fields is \"Add Media/Content.\" Now _what you do with this field _is much more complicated than a simple \"how to\" might entail. Therefore, we felt it was important to add more than just \"fill in the fields.\" Here is some supplemental information regarding Add Media/Content field that we created.\n\n\n_**Add Media/Content: **Arguably one of the most important aspects of the game, the Add Media/Add Content feature allows you to shape the structure of your game. In this field we suggest listing the objective of your game (What do you want players to achieve? Do you want to have rules? Is there a way to win?). There are many ways you can play Ivanhoe. You can start with just the objective of the game and see where players take it. You can insert an image, a video, a music recording, or text and have people make new connections. You can start with a suggested list of roles or have players invent them on their own! The sky is the limit in terms of creativity and functionality so be sure to take some time to think about how you want to play the game before completing this section. _\n\n\n\n\nHopefully our documentation eases users into Ivanhoe and facilitates an environment that is easy to feel comfortable in! That being said, we plan on providing access to our GitHub account so that users can constantly provide feedback for how our documentation process could improve.\n"},{"id":"2014-04-11-more-better-breaking","title":"More Better Breaking","author":"scott-bailey","date":"2014-04-11 05:00:35 -0400","categories":["Grad Student Research"],"url":"more-better-breaking","content":"Over the course of the last few months, those of us in the development team have been hard at work writing the code to make the Ivanhoe game function. I thought I'd give a (very) brief look into our development workflow.\n\n1.) Each week, sometimes several times a week, check Github for issues assigned to development. ([Current list here](https://github.com/scholarslab/ivanhoe/issues?labels=&milestone=&page=1&state=open))\n2.) Rank said issues on the whiteboard according to priority, difficulty, and whether we want to just do it to get the momentum boost of being successful, even on something easy.\n3.) After choosing an issue, write out what the issue requires to be fulfilled in natural language on the whiteboard.\n4.) Write out the first glimmers of what code will be necessary to achieve each step on the whiteboard.\n5.) Create a new git branch, write said code, and check to see if it works.\n6.) In the incredibly rare situation that it works on the first go, celebrate wildly, or, stoically proclaim what we need to get done next.\n7.) If the screen comes up blank, lint that php.\n8.) If it returns an error with a line number, be thankful for such a magnificent gift. Fix it.\n9.) If there is no error returned, but the feature doesn't work, write some code to break something so that it gives you some good information. Fix it.\n10.) When that doesn't work, try to break it better. Fix it.\n11.) When that doesn't work, go find Wayne, Eric, or Jeremy. With their help: more better breaking. Fix it.\n12.) Commit and push to Github (which you should have been doing throughout anyway), and close the issue, carefully referencing the commit number.\n13.) Celebrate appropriately. This might involve [a unicorn and a pegasus.](http://www.scholarslab.org/grad-student-research/all-together-now/)\n"},{"id":"2014-04-18-podcast-micki-kaufman-on-quantifying-kissinger","title":"Podcast: Micki Kaufman on Quantifying Kissinger","author":"laura-miller","date":"2014-04-18 09:28:33 -0400","categories":["Announcements","Podcasts","Visualization and Data Mining"],"url":"podcast-micki-kaufman-on-quantifying-kissinger","content":"**Digital Humanities Speaker Micki Kaufman**\n**\"Everything on Paper Will Be Used Against Me\": [Quantifying Kissinger](http://www.mickikaufman.com/qk/)**\n\nScarcity  of  information  is  a  common  frustration  for  many  historians.  However,  for  researchers  of  twentieth-­ and  twenty-­first  century  history  the  opposite  problem  is  also  increasingly  common.  In  contrast  to  scholars  of ancient  history,  who  base  much  of  their  analyses  on  rare  and  unique  relics  of  antiquity,  historians  studying the  ‘Age  of  Information’  (and  the  even  more  recent  period  of  ‘Big  Data’)  increasingly  confront  a  deluge  of information,  a  vast  field  of  haystacks  within  which  they  must  locate  the  needles  -­  and  presumably,  use them  to  knit  together  a  valid  historical  interpretation.\n\nWhile  simply  having  such  a  large  volume  of  information  online  in digital  form  for  researchers  is  valuable,  the  usual  restriction  to  a  web-­based  ‘search’  form  interface  often renders  it  of  limited  use  and  approachability.  As  detailed  on  [the  project’s  web  site](http://www.mickikaufman.com/qk/), Ms. Kaufman's  work  involves  the  application  of  a  host  of  quantitative  text  analysis methods  like  word  frequency/correlation,  topic  modeling  and  sentiment  analysis  (as  well  as  a  variety  of  data visualization  deisgns  and  methods)  to  historical  research  on  the  DNSA’s  Kissinger  Collection,  comprising approximately  17600  meeting  memoranda  (‘memcons’)  and  teleconference  transcripts  (‘telcons’)  detailing the  former  US  National  Security  Advisor  and  Secretary  of  State’s  correspondence  during  the  period  1969-­ 1977.  This  application  of  computational  techniques  to  the  study  of  twentieth-­century  diplomatic  history  has generated  useful  finding  aids  for  researchers,  provided  essential  testing  grounds  for  new  historical methodologies,  and  prompted  new  interpretations  and  questions  about  the  Nixon/Kissinger  era.\n\n[Micki  Kaufman](http://www.mickikaufman.com/)  is a doctoral  student  in  US  history  at  the  Graduate  Center  of the  City  University  of  New  York, a  GC-­CUNY  Digital  Fellow,  and a recipient  of  GC-­CUNY’s  Provost’s  Digital  Innovation  Grant  in 2012–2014.\n\nClick below to stream the podcast, and you can view the [accompanying slides](http://www.mickikaufman.com/qk/preso/) on the [Quantifying Kissinger website](http://www.mickikaufman.com/qk/). As always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](https://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619?mt=10).\n\n[podloveaudio src=\"http://a1322.phobos.apple.com/us/r30/CobaltPublic/v4/76/c2/ee/76c2ee48-51c6-ac29-fda4-9c60724ec1eb/336-283879797946496247-2014.04.04_kaufman2.mp3\"]\n\n\n"},{"id":"2014-04-21-dhf_panel","title":"Criminal Women, Misdirection, and Learning to Listen: A Conversation about the Digital Humanities","author":"purdom-lindblad","date":"2014-04-21 03:50:09 -0400","categories":null,"url":"dhf_panel","content":"Please join us **Tuesday, April 22, at 10 AM** for the Digital Humanities Graduate Fellows Brunch.\n\nAlderman Room 421\n\nFellows Erik DeLuca, Gwen Nally, and Tamika Richeson share their projects as well as engage in a larger conversation about collaborating around digital projects. Erik investigates the listening networks with “Community Listening in Isle Royale National Park.” His work this year has focused on digital tools to allow listeners to interact with his ethnographic composition. Gwen seeks new approaches to understanding philosophical texts through the use of language processing, topic modeling, and Naive Bayesian Analysis. Her work has dived into markers within the texts that indicate hedging or misdirection with “Processing the Dialogues.” Tamika explores black women's lawbreaking in Civil War era Washington, D.C. to understand the racial and gendered context in which American criminal law took shape with her project, “Black Women in Civil War D.C.” Tamika seeks to use spatial narrative tools, such as [Neatline](http://neatline.org), to visualize black women’s lives within the city.\n\nJoin us for brunch.\n\n<!-- more -->\n\n[gallery ids=\"10169,10170,10172\"]\n\n**Erik DeLuca**: Making music and sound art of all sorts, that entangle algorithmic and intuitive modes of composing, excites me. A major element of my dissertation, \"Field(art)works: Paths to Composing,\" is an ethnographic composition that explores a deep listening network between a biologist and community of wolf-listening park visitors. I volunteer for both Sensate: A Journal for Experiments in Critical Media Practice and for The Bridge PAI in Charlottesville.\n\n**Gwen Nally** is a graduate student in the philosophy department. She studies Plato and teaches Bioethics. She was a Praxis Fellow in 2013, loves design, and does calligraphy.\n\n** Tamika Richeson** is a doctoral candidate in History at the University of Virginia. Her work explores black women's lawbreaking in Civil War era Washington, D.C. to understand the racial and gendered context in which American criminal law took shape. Her interests include nineteenth century social and cultural history, African American Studies, women and gender studies, digital humanities, criminal law, and public history. She currently serves on the Prince William County Historic Preservation Foundation executive board, and the President's Commission on Slavery and the University. Tamika Richeson has recently been awarded the Woodrow Wilson Dissertation Fellowship in Women's Studies.\n"},{"id":"2014-04-28-plane-table-mapping-aka-instant-gratification-mapping","title":"Plane Table Mapping aka Instant Gratification Mapping","author":"kelly-johnston","date":"2014-04-28 06:06:53 -0400","categories":["Geospatial and Temporal","Visualization and Data Mining"],"url":"plane-table-mapping-aka-instant-gratification-mapping","content":"<blockquote>_\"Plane table mapping is the most interesting of all to do.  One can hardly browse through an account of its various operations without wishing to go directly into the field and do them.\" - Down To Earth : Mapping for Everybody, 1944_</blockquote>\n\n\nHumans love maps.  Every day in the Scholars' Lab we help aspiring cartographers navigate the complexities of geographic information systems software.  But software is expensive and requires electricity. What if we could make a cartographic masterpiece appear on an ordinary sheet of paper without installing any software or downloading any data or loading any batteries in our GPS?\n\nThat's instant gratification mapping via plane table methods.  So let's \"_go directly into the field and do them_\"!\n\n[![UVA plane table mapping 1](http://www.scholarslab.org/wp-content/uploads/2014/04/20140416-UVA-plane-table-mapping-1-Kelly-Charles-Kromkowski-1024x768.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/04/20140416-UVA-plane-table-mapping-1-Kelly-Charles-Kromkowski.jpg)\n\nWe're talking about old-school cartography.  As early as the 16th-century, writers [described the plane table method](http://en.wikipedia.org/wiki/Plane_table#cite_note-kiely-2) in detail.  Long before computers and software, early 2oth-century school boys in knickers were taught to [map distant church steeples](http://books.google.com/books?id=Uf0BAAAAYAAJ&dq=the%20school%20world%20plane%20table%20mapping%201905&pg=PA25#v=onepage&q=the%20school%20world%20plane%20table%20mapping%201905&f=false) by \"screwing your drawing board to a camera tripod\".\n\n[![Plane Table School World](http://www.scholarslab.org/wp-content/uploads/2014/04/PlaneTableSchoolWorld.png)](http://www.scholarslab.org/wp-content/uploads/2014/04/PlaneTableSchoolWorld.png)\n\n[caption id=\"\" align=\"alignnone\" width=\"650\"]![](http://celebrating200years.noaa.gov/foundations/mapping/image2_650.jpg) Plane table mapping from [NOAA](http://celebrating200years.noaa.gov/foundations/mapping/image2.html)[/caption]\n\nHistoric resources like Low's [_Plane Table Mapping_](https://archive.org/details/planetablemappin031356mbp) inform our methodology.  Modern interpretations of [plane table mapping as art](http://mindstre.am/photos/@4918?limit=20&start=0) expand the boundaries beyond the two dimensional plane.  A new crop of 21st-century plane table mappers are [in the field](http://canoepost.blogspot.com/2011/02/where-is-keg-island.html), among them [geography students at Texas A & M University](http://geography.tamu.edu/class/aklein/geog332/labs/lab03/lab_instructions.pdf).\n\nWe developed our [Scholars' Lab plane table mapping workshop](http://teaching.scholarslab.org/courses/2014_Spring_GIS_Workshops/Plane_Table_Mapping_Workshop_Presentation.pdf) with a low-cost do-it-yourself mindset.  For our work surface (or plane table) my Scholars' Lab colleague Chris Gist attached a plywood table top to a camera tripod.  Rather than invest in an expensive [alidade](http://www.ebay.com/bhp/alidade) for sighting distant objects, we used the edge of a triangular architect's scale.  A second-hand compass pointed to magnetic north.  A smartphone app helped level the table, as will a marble.  A set of keys on a string served as a plumb line.  A pencil and eraser completed our mapping toolkit.\n\n[![Plane table mapping 5](http://www.scholarslab.org/wp-content/uploads/2014/04/20140416-UVA-plane-table-mapping-5-1024x768.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/04/20140416-UVA-plane-table-mapping-5.jpg)\n\nIn our [workshop introduction](http://teaching.scholarslab.org/courses/2014_Spring_GIS_Workshops/Plane_Table_Mapping_Workshop_Presentation.pdf) we covered the basics of establishing a baseline, then sighting from multiple stations toward each visible landscape feature to create intersecting lines.   The map began to emerge on the paper without taking a single distance measurement.  Lines and angles were drawn using only visual methods.  If we could see it, we could map it from afar even if the feature was behind a locked gate, across a busy street, or otherwise inaccessible.\n\nWe're beginners, so in our one-hour workshop we had our hands full making a simple map on level ground.  We didn't expand into the complexities of plane table mapping on uneven terrain. Even on flat ground we made mistakes.  We struggled with leveling, sighting, slipping scales, and tilting tables.  We learned that when something is wrong, it's quickly apparent while still on-site and can be corrected.  Mistakes are discovered and corrected in real-time, not later back in the office.\n\n[![Plane Table Map](http://www.scholarslab.org/wp-content/uploads/2014/04/Plane-Table-Map-1024x768.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/04/Plane-Table-Map.jpg)\n\nThis new cohort of plane tablers commented on their experience:\n\n\n<blockquote>_This was fun.  I had no idea how maps were made before GPS._\n\n_I'll use this to illustrate to my students how errors can easily creep into any data collection exercise._\n\n_I'm trying this with my kids this weekend.  It will get us outside and they'll love it. _\n\n_I'm seeing the world with new eyes._</blockquote>\n\n\nHappy mapping!\n\n[![plane table mapping 10](http://www.scholarslab.org/wp-content/uploads/2014/04/20140416-UVA-plane-table-mapping-10-Chris-Gist-Charles-Kromkowski-Kelly-1024x768.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/04/20140416-UVA-plane-table-mapping-10-Chris-Gist-Charles-Kromkowski-Kelly.jpg)\n\nThanks to Schaeffer Somers, University of Virginia School of Architecture, for the equipment loan.\n\n\n"},{"id":"2014-04-29-connect-create-inspire-the-ivanhoe-game-returns-2","title":"Connect, Create, Inspire: the Ivanhoe Game returns!","author":"stephanie-kingsley","date":"2014-04-29 08:18:53 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"connect-create-inspire-the-ivanhoe-game-returns-2","content":"The Scholars' Lab Praxis Fellows are thrilled to announce the beta release of the [Ivanhoe Game](http://ivanhoe.scholarslab.org/)!  Ivanhoe is a collaborative role-playing game in which players make critical interventions in a text, cultural object, or topic to help them learn.  Ivanhoe is about connecting ideas, crafting new interpretations, and inspiring creative scholarship.\n\n![ivanhoe logo](http://ivanhoe.scholarslab.org/ivanhoelogo.png)\n\nThe Ivanhoe Game is now available as a WordPress Theme!  It is the practicum project of this year's [Praxis Fellowship](http://praxis.scholarslab.org/), an intensive year-long program in the UVa [Scholars' Lab](http://www.scholarslab.org/) that trains humanities and social-sciences graduate students in digital-project management, design, development, troubleshooting, and distribution.  Fellows work together to compose a [project charter](http://praxis.scholarslab.org/charter.html), [wire-frame](http://praxis.scholarslab.org/topics/wireframes/) ideas, give dynamic [conceptual pitches](http://www.scholarslab.org/grad-student-research/two-ivanhoes-one-direction/), and write code to build a digital humanities tool, all the while being mentored by the brilliant and ever-patient Scholars' Lab faculty.  Fellows [blog](http://praxis.scholarslab.org/) throughout the year, reflecting on their triumphs and learning experiences and sharing them with the wider DH community.  See [posts from Veronica](http://www.scholarslab.org/grad-student-research/foreign-languages-and-ivanhoe-progress/) and [Scott](http://www.scholarslab.org/grad-student-research/more-better-breaking/) about the perplexities of coding with PHP, alternative [design ideas](http://www.scholarslab.org/digital-humanities/an-ivanhoe-design-idea/) from Zach, reactions from Eliza at first learning [version control in GitHub](http://www.scholarslab.org/grad-student-research/praxis-holidays/), thoughts from Francesca about the nature of [game play](http://www.scholarslab.org/grad-student-research/are-we-gaming-or-just-simulating/) and [collaboration](http://www.scholarslab.org/grad-student-research/forming-norming-storming-performing/), and my own [project-management musings](http://www.scholarslab.org/uncategorized/digest-4-on-managing-projects-not-people-reflections-after-a-project-management-crisis/) as I discovered my role within the group.  Praxis is an innovative approach to graduate training which emphasizes collaboration and iterative learning: Fellows work together in a trial-and-error process to discover how to take a project from being a mere idea to a useable product ready to be introduced to the world.\n\nOur project, the Ivanhoe Game WordPress Theme, is now ready to take its first steps into the world of digital scholarship and pedagogy.  This tool is a vibrant reimagining of a game originally developed in the UVa [SpecLab](http://books.google.com/books/about/SpecLab.html?id=VPXCk396uPYC).  (For [more on Ivanhoe’s history](http://www.ivanhoegame.org/?page_id=21), see \"[Designing Ivanhoe](http://texttechnology.mcmaster.ca/pdf/vol12_2_03.pdf),\" by Johanna Drucker; \"[IVANHOE: Education in a New Key](http://www.rc.umd.edu/pedagogies/commons/innovations/IVANHOE.html),\" by Jerome McGann; and \"[Subjectivity in the Ivanhoe Game: Visual and Computational Strategies](http://texttechnology.mcmaster.ca/pdf/vol12_2_05.pdf),\" by Bethany Nowviskie. ) The Ivanhoe Game can be played on any type of cultural object or topic.  In Ivanhoe, players assume roles and generate criticism by pretending to be characters or voices relevant to their topic and making moves from those perspectives.  We think of these moves as interventions—a text or work is not stable but, rather, dynamic and ever subject to interpretation by its readers.  Furthermore, these interventions are reflective and deliberate: they are \"self-conscious acts of interpretation,\" as Scott so concisely and perfectly puts it.  Ivanhoe thus provides a way of delving into a subject while also maintaining a firm focus on the players themselves.\n\nA few features of our Ivanhoe distinguish it from the original.  Whereas the SpecLab Ivanhoe Game was largely text based, we have designed Ivanhoe to accommodate subjects from any discipline and moves with all manner of media.  Another element of Ivanhoe that the Praxis Fellows have found particularly provocative and have given special attention is the network of moves that these games generate.  From one move can spring forth a multitude of other moves.  We have programmed ways of linking moves together as “Source” posts and “Responses” to emphasize that network.  Lastly, we have always seen Ivanhoe as something which should above all be accessible, so that the concept of Ivanhoe may be freely adapted by as wide a user base as possible.  We have built it as a WordPress Theme, which anyone with a WordPress page may easily apply and use for whatever purpose they like.\n\nNow that you have heard just enough about Ivanhoe to be a little baffled but exhilarated nonetheless, we invite you to try it out and explore its possibilities.   To download the Theme itself, visit our [informational site](http://ivanhoe.scholarslab.org/).  There, you will find out more about us and Ivanhoe, as well as documentation to assist you in using Ivanhoe.  If you wish to try Ivanhoe but do not have a WordPress page, visit our [testing app](http://ivanhoe-testing.herokuapp.com/), hosted on [Heroku ](https://www.heroku.com/)and available for a limited time for Ivanhoe trial.  As Ivanhoe is a work in progress, we would love your feedback, so feel free to post any issues you find to our [GitHub page](https://github.com/scholarslab/ivanhoe/issues?state=open).\n\nThanks, and we hope you enjoy connecting, creating, and inspiring with Ivanhoe.\n\n_Stephanie Kingsley, for the 2013-2014 Praxis Fellows_\n"},{"id":"2014-04-30-a-few-uses-for-ivanhoe","title":"An Ivanhoe example and guidelines for getting started","author":"stephanie-kingsley","date":"2014-04-30 10:00:30 -0400","categories":["Grad Student Research"],"url":"a-few-uses-for-ivanhoe","content":"As [I mentioned previously](http://www.scholarslab.org/announcements/connect-create-inspire-the-ivanhoe-game-returns-2/), any Ivanhoe game can be played on any topic.  Being interdisciplinary ourselves (Classical Archaeology, English, Religious Studies, and Sociology), our group has naturally tended toward interdisciplinary games: the [suffragette journalism game](http://www.scholarslab.org/grad-student-research/a-review-of-the-suffragette-game/), the Elgin Marbles debate game, a sci-fi game, etc.  We all had a great time on these games, and I think it was because we could all find something to relate to in them within our respective fields.  However, this intense interdisciplinarity often made finding topics for games difficult, and we all agreed that a shared knowledge or experience base would help players in conceiving and playing an Ivanhoe game.  This seems to have been borne out in [Ivanhoe testing](http://www.scholarslab.org/announcements/announcing-the-ivanhoe-information-website-and-beginning-of-testing/); we had many testers volunteer, but few ended up playing extended games.  I think the reason for this is that our volunteers were individuals from different disciplines who signed up over the internet and had no face-to-face time.  Given this, I suspect that testers had a difficult time connecting with one another and having shared interests.\n\nIf you're interested in playing Ivanhoe, I would suggest gathering either a group of colleagues (shared knowledge base) or friends (shared experience) and selecting a topic which you all find exciting.  Personally, my primary field of interest is book history and textual studies.  A game which I recently came up with was [a game on Poe's \"The Raven.\"](http://ivanhoe-testing.herokuapp.com/?ivanhoe_game=the-publication-of-poes-the-raven)  In this game, one person would be Poe, one person his editor, another his compositor (type-setter), etc.  Each week, \"Poe\" would make a move which would contain only the text of one stanza from \"The Raven.\"  Then, the editor would copy the text out of Poe's move, change it as he saw fit, and that would be his move.  This would continue, until all roles had messed with Poe's text, and the final versions is what the published result would be.\n\nIn order to play this game, players would need to learn about Poe's publishing practices, the publication history of \"The Raven\" specifically, the publisher's house style, etc.  Or, players might want to play such a game from a more theoretical standpoint: the editor might decide to take a Freudian stance and edit Poe's text from that perspective.\n\nThe multimedia possibilities for this game are marvelous.  Players could post videos demonstrating typesetting, images of later editions and illustrations, a sound clip of an aria the editor might have heard at the opera the evening before and which may have impacted his work on Poe's poem, etc.  The sky's the limit with Ivanhoe!\n\nThis is just one example, but keep an eye on [our blog](http://praxis.scholarslab.org/) this week and hear from my fellow Praxers about their ideas!\n"},{"id":"2014-04-30-all-systems-go-we-think","title":"All systems go! (we think...) ","author":"francesca-tripodi","date":"2014-04-30 06:00:32 -0400","categories":["Announcements","Grad Student Research"],"url":"all-systems-go-we-think","content":"Yesterday at 1PM we launched Ivanhoe 1.0. We are excited about our progress and hope you can take some time to [download ](http://ivanhoe.scholarslab.org/)the theme to your own WordPress accounts. We have made [documentation](http://ivanhoe.scholarslab.org/documentation.html) readily available for those new to Ivanhoe and we hope users will add to this space ([via GitHub](https://github.com/scholarslab/ivanhoe/issues?state=open)) so that we can continue to create a better product. As I reflect back on the last year I wanted to share a few things....\n\n**My favorite moment:** playing Apples-to-Apples during a holiday get together. I feel like the experience bonded our team in a cool way and renewed my enthusiasm for integrating games at work. One can never predict the unexpected outcomes of play.\n\n**What I learned about myself: **[I can't do it all](http://www.scholarslab.org/grad-student-research/when-expectations-meet-reality/), but that's ok... especially when you have a team of amazing people ready to step in and help you out.\n\n**How I would integrate Ivanhoe into the classroom:** A theoretical concept in Media Studies (among other disciplines) is the idea of \"[intertextuality](http://en.wikipedia.org/wiki/Intertextuality)\" - or how we make meaning of one type of media (a text, movie, television show, etc) based on our already existing knowledge of another form of media. I created a game in Ivanhoe using this concept. I uploaded a film from YouTube and inserted the following directions:\n\n_1. Watch one of the videos (original or move) available on the game._\n\n_2. Create a role based on the video you watched._\n\n_3. Make a move (or respond to a move) by inserting in another video from YouTube (preferably under 3 minutes) whose meaning is tied to your role AND the video you watched. Use the rationale section to justify the connection. _\n\n_4. The game ends when no more connections can be made._\n\nI look forward to seeing how others incorporate the tool and the games users end up concocting along the way....\n"},{"id":"2014-05-01-dialogical-code-and-the-adventure-of-pair-programming","title":"Dialogical Code and the Adventure of Pair Programming","author":"scott-bailey","date":"2014-05-01 05:25:28 -0400","categories":["Grad Student Research"],"url":"dialogical-code-and-the-adventure-of-pair-programming","content":"Near the end of last semester, as the developers of SLAB taught us Praxers to write code (PHP in this case), pushing us to learn different conditional loops and such through repeated problem solving exercises, they also encouraged us to work in pairs or in even larger groups. Coding is not something you do alone, they said. I'd say, rather, that really good, efficient, and fun coding is not something you do alone.\n\nThroughout this spring semester, as Veronica and I have written much of the PHP that drives the [Ivanhoe](http://ivanhoe.scholarslab.org/) WordPress Theme, we have pair programmed. We have sat together in the grad lounge of the Scholars' Lab and we've scrawled our ideas across three whiteboards (front and back), planning our next steps, prioritizing development GitHub issues, sketching out the logic of the next piece of code we're going to write. And then we've sat together, or sometimes stood or paced, and written the code, one of us typing, the other pouring through documentation, checking syntax, or suggesting a different approach. It's been a heavily dialogical process. A substantial amount of the back-end code of Ivanhoe is the surface hiding hours and hours of conversations.\n\nIt's been brilliant and fun and one of the best parts of my year.\n\nAs we're getting close to the end of Praxis (and we're still working, still coding, still trying to get at least the foundation of some of the more advanced features in), I'm thinking a lot about how much of our personalities or styles of thinking have been embedded in the code itself. The code is a literary object, produced iteratively by a group of people. Could you tell which of us is which just looking at it? Could you read the style of the comments, the organization of the conditionals, or the placement of the variable definitions and know something about us? About me?\n\nI suspect you can, but I also think it probably gets harder as the code develops and gets closer to release, as it is iteratively picked over, cleaned up, made to fit standards and requirements. All of which are good - clean, efficient code is beautiful. But so is the [archive of our work](https://github.com/scholarslab/ivanhoe/) on GitHub, which is messy and bears the impressions of our thinking (including all the commented-out code I refused to delete until the end, just in case we ever needed it again - silliness, given what GitHub does).\n\nOur commit history there is the record of our conversations, between all of us in the Scholars' Lab and Praxis who have put time and effort and caring into this work, not just those of us coding.\n"},{"id":"2014-05-02-fluffy-tree-the-future-of-ivanhoe","title":"Fluffy Tree: The Future of Ivanhoe","author":"veronica-ikeshoji-orlati","date":"2014-05-01 22:30:30 -0400","categories":["Grad Student Research"],"url":"fluffy-tree-the-future-of-ivanhoe","content":"On Tuesday, we launched [Ivanhoe 1.0](http://ivanhoe.scholarslab.org/). As [Scott](https://www.scholarslab.org/grad-student-research/dialogical-code-and-the-adventure-of-pair-programming/) noted yesterday, much of the PHP which drives our Ivanhoe Wordpress theme is the product of our many hours spent in the Scholars' Lab pair-programming.\n\n\n\n\nNow that Ivanhoe 1.0 is out in the world, Scott and I have settled down to tackle the next feature: fluffy tree.\n\n\n\n\n[![IMAG1333](http://www.scholarslab.org/wp-content/uploads/2014/05/IMAG1333-300x169.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/05/IMAG1333.jpg)\n\n\n\n\n(Minotaur Scott & Ammit Veronica tackle Fluffy Tree Bunny)\n\n\n\n\nFluffy tree will enable users to respond to multiple moves at once, thereby creating a broader network of connections within each game. The feature name arose from a conversation we and the Scholars' Lab developers had early in the semester about how to conceptualize the Ivanhoe game within the blog-based structure of WordPress - we didn't want a 'vine,' with one move linking to the next in a linear/hierarchical fashion, but a 'fluffy tree,' in which moves intertwined with one another in an organic fashion.\n\n\n\n\nTo write the code for fluffy tree, we have had to take a jumbled mess of potential connections and turn it into a clear, linear progression of events. This has required a lot of white-board time and adding JavaScript to our programming repertoire. While we do not yet have fluffy tree working, we have definitely figured out how to work on it - and really, knowing how to work collaboratively is a goal in itself.\n"},{"id":"2014-05-02-go-litel-boke","title":"\"Go litel boke!\"","author":"zachary-stone","date":"2014-05-02 07:05:02 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"go-litel-boke","content":"“Go, litel boke, go, litel myn tragedye,” cries Chaucer at the close of his _Troilus and Criseyde_. As he hands his book off to the vicissitudes of manuscript production—memorably evoked in his poem to Adam Scriveyn[1]—the persistent fear that scribes may “miswrite the” or “mysmetre” the work troubles the poet’s mind.\n\nIteration is a problem. As is influence. Chaucer exhorts his book to be humble, not to envy Virgil, Homer, Ovid, Lucan, or Statius; rather, this little book is to kiss their steps. And yet, Chaucer, we know, surely did not “kis the steppes” of Boccaccio, his putative source. Rather, he audaciously re-writes the Trojan War as a romantic tragedy of Boethian proportions. Whereas his erstwhile follower John Lydgate loves the politics of Troy in his monumental _Troy Book_, Chaucer seems enamored with the politics of love. Questions of who “miswrite” what abound and increase. Writing in response to Chaucer’s seemingly unflattering portrait of Criseyde, Robert Hennryson’s _Testement of Cresseid_ claims to pick up where Chaucer leaves off. Troy endlessly enchants English writers, and in the Trojan tradition we “so gret diuersite / In Englissh and in writyng in oure tonge.” Troynovant—whether London or not—rises endlessly in English.\n\n“But ȝet to purpos of my rather speche:” what has this to do with Ivanhoe? Well. As we’ve worked on Ivanhoe, I’ve commented about the ways in which DH development reminds me of manuscript production.[2]  What strikes me about Ivanhoe is the way in which it embraces the most unruly, anxiety-inducing aspects of medieval textuality—the possibility of endless revision, perpetually unbound books.\n\nIvanhoe is a broad concept, and we have discussed many definitions of it over the past year.  For me, Ivanhoe has become an alternate way of making meaning, a mode of communication that exploits ambiguities and varieties of meaning to create an open landscape in which “sentence” and “solas”—to reach back to the Chaucerian well—exist in dynamic tension and continually generate unexpected readings.\n\nPerhaps Chaucer isn’t that worried at the end of Troilus. Perhaps he’s actually in on the joke. He exhorts his book to “subgit be to alle Poyesye,” a game Chaucer clearly understands to consist of adaptation and expansion. His prayer is simply “That thow be vnderstonde” but he leaves that understanding open. The book, once sent out into the world, takes on a life of its own.\n\nTuesday we launched Ivanhoe, our own little “go litel program, go, oure tragedye” moment. We did so with the understanding that were giving up absolute control over Ivanhoe, and I think we share Chaucer’s prayer—to be understood.\n\nSo. How should you use this little thing we launched into the world? Honestly, part of Ivanhoe is figuring that out yourself. There are “so gret diuersite” of ways to do Ivanhoe—as many as there are to do Troy—that really it's up to each user. But maybe, it might be fun to return to Troynovant, or rather, to rebuild it.\n\nGo, litel boke, go, litel myn tragedye,\n\nTher god thi makere ȝet, er that he dye,\n\nSo sende myght to make in some comedye;\n\nBut litel book, no makyng thow nenvie,\n\nBut subgit be to alle Poyesye,\n\nAnd kis the steppes where as thow seest space\n\nUirgile, Ouide, Omer, Lucan and Stace.\n\nAnd for ther is so gret diuersite\n\nIn Englissh and in writyng of oure tonge,\n\nSo prey I god that non myswrite the,\n\nNe the mysmetre for defaute of tonge.\n\nAnd red wher-so thow [MS ȝow] or elles songe,\n\nThat thow be vnderstonde, god I biseche.\n\nBut ȝet to purpos of my rather speche --[3]\n\n\n\n[1] Geoffrey Chaucer, “Chaucers Wordes unto Adam,” <[http://www.bartleby.com/258/61.html](http://www.bartleby.com/258/61.html)>\n\n[2] [http://www.scholarslab.org/digital-humanities/on-stemmatics/](http://www.scholarslab.org/digital-humanities/on-stemmatics/)\n\n[3] Geoffrey Chaucer, _Troilus and Criseyde_, ed. Barry Windeatt (London: Longman, 1984),  < [http://quod.lib.umich.edu/c/cme/Troilus/1:5.28?rgn=div2;view=toc](http://quod.lib.umich.edu/c/cme/Troilus/1:5.28?rgn=div2;view=toc)>, V.1786-99.\n"},{"id":"2014-05-05-ivanhoe-and-imaginative-analysis","title":"Ivanhoe and Imaginative Analysis","author":"elizabeth-fox","date":"2014-05-05 06:00:14 -0400","categories":["Grad Student Research"],"url":"ivanhoe-and-imaginative-analysis","content":"When I wasn’t working as a Praxis fellow this year, I taught first-year writing.  In the class, we tackled the course subject—ghost stories—through a variety of topical lenses, looking at horror stories and films, web comics, and ghost hunting TV shows.  Although the course focused on argumentative writing, at the students’ request, I ended up adding a creative component: students could, for extra credit, write and submit their own ghost story.  I assumed the assignment would be simple and fun, allowing students to let off some creative energy at the end of the semester.  What I didn’t realize at first is how helpful these stories would be to students’ engagement with the material.  Most students did not just write ghost stories; they wrote works that responded to a semester’s-worth of texts, drawing on the styles and formats that we had considered during the past few months.  They incorporated the common tropes of ghost stories, nodding at—and poking fun at—the clichés of the genre.  They creatively explored the difference between a ghost story (acknowledged fiction) and a haunted encounter (believed to be true), allowing the conventions of each type of story to dictate the structure.\n\nAlthough not framed by digital media or the rules of gameplay, the experience couldn’t help but remind me of [Ivanhoe](http://ivanhoe.scholarslab.org/).  Here, once again, I saw the importance of _creative intervention_, allowing students to engage with texts in ways that enable imaginative analysis.  Such an approach, although seemingly outside the realm of argumentative writing, enabled students to synthesize topics and to showcase their understanding of course materials in ways that felt fun and dynamic.  Projects like Ivanhoe—which allow for a sustained creative endeavor and for interactions among multiple students—pointedly foster this type of creative dialogue.  In doing so, Ivanhoe allows for imaginative analysis on a grand scale, inviting students to join in a conversation within, rather than about, a text.  The result, as I discovered, is not just an increased understanding of the material, but also a greater sense of involvement with it. It’s one thing to assess a collection of texts; it’s quite another to feel yourself a part of them.\n\nThe experience also reminded me of the close connection between my efforts on the [Praxis](http://praxis.scholarslab.org/) team and my work as a teacher and a student at the university.  At times during the year, it’s been a challenge for me to see my various roles functioning symbiotically, especially when work on Ivanhoe came into competition with lesson planning or preparing for my oral exams.  It’s fortunate that this end-of-the-semester experience has reminded me of the bond among all of these scholarly pursuits.  My teaching, my research, and my work on Ivanhoe all inform each other, suggesting new ideas and approaches that (like any good Ivanhoe project) make key interventions in my thinking.  Just as my research and teaching framed my ideas for Ivanhoe, so my understanding of Ivanhoe’s values ultimately informed my teaching.  So now, as the semester draws to a close, I can consider with pleasure all the things that I’ve learned as a Praxis fellow, and all the ways that my time here will continue to influence by work in the years ahead.\n"},{"id":"2014-05-05-praxis-is-about-people-reflections-after-the-launch","title":"Praxis is about people: reflections after the launch","author":"stephanie-kingsley","date":"2014-05-05 10:00:21 -0400","categories":["Grad Student Research"],"url":"praxis-is-about-people-reflections-after-the-launch","content":"When I initially drafted the Ivanhoe launch announcement, my goal was to make it communicate in as concise a fashion as possible what Ivanhoe was and where people could download it and learn more.  I completely forgot that Ivanhoe has been as much about us, the Praxis Fellows, and our learning, as about software development.  It wasn't until Bethany pointed out that everyone might not know who we were and I might want to include a paragraph describing the program.  It then occurred to me: just as Ivanhoe is ultimately about the players--encouraging self awareness through role playing--Praxis, too, is about the players.\n\nI am dazzled when I think about how our group has evolved over the course of the year.  Those of you who have been following our blog will have seen countless posts about teamwork.  Francesca's post \"[Forming, Norming, Storming, and Performing](http://www.scholarslab.org/grad-student-research/forming-norming-storming-performing/),\" a reflection upon Bruce Tuckman's theory of group development, was in many ways a response to our experience drafting our charter.  That was a turbulent time for our team, as it required making decisions about the somewhat nebulous question, what did we want to get out of Praxis?  Then, the next difficult question: what did we want our Ivanhoe to be?  A series of heated debates ensued, until we began drawing out our ideas, which as I discussed in \"[Stephen Covey intervenes in wire-framing Ivanhoe](http://www.scholarslab.org/grad-student-research/stephen-covey-intervenes-in-wire-framing-ivanhoe/),\" greatly assisted our communications.  The second we began putting ideas on paper, conversation flowed better, and we started being able to make decisions more quickly.  Shortly after our concept pitches in November, Veronica mused on our group dynamics, highlight several problems in her post \"[Sticky Situations: Lessons in Group Cohesion](http://www.scholarslab.org/grad-student-research/sticky-situations-lessons-group-cohesion/).\"  She targeted not having defined roles within the team and not knowing how to articulate our ideas to one another as major problems.\n\nOnce we all had our roles, teamwork went a lot more smoothly.  Everyone had an individual purpose, and beginning in December our posts became much more skills oriented.  Several posts by Veronica and Scott, our developers, focused on coding and breaking things (clearly, this self-reflection was helpful, as the breakages were fixed and we now have a working game), and posts by our designers soon focused on learning CSS.\n\nAs project manager, I too had more specialized tasks, although it would take a bit longer for me to get a clear picture of what exactly my job entailed.  This process entailed learning important lessons, like how [not to create more work](http://www.scholarslab.org/grad-student-research/happy-new-year-and-a-few-thoughts-to-begin-it-with/) for the team than was necessary, and how [not to micromanage](http://www.scholarslab.org/uncategorized/digest-4-on-managing-projects-not-people-reflections-after-a-project-management-crisis/).  I was confused about my job: I thought I was supposed to be managing the team, when really, I needed to be managing the project, which at that point required me to be chief publicity person, keeper of the timeline, and stewardess of the Ivanhoe vision.\n\nOur division of labor and assumption of roles certainly increased productivity, but it was most important for how it helped us get along as a group.  Being busy graduate students, of course we were still stressed, but personal group stress was much lower once we all knew what we were doing.  We even got to the point where we rearranged tasks to fit better with individual members' personal lives, and workflow and communication continued to improve.  We knew we had to continue getting things done, but we would move forward only in the best way possible for our members.\n\nMy conclusion from all this is three-fold: 1) that human beings always function better when they have defined tasks before them, 2) that the development and happiness of those human beings are more important than any project itself, and 3) that the story of our Ivanhoe is also the story of our Praxis cohort.  In Ivanhoe, players keep role journals to reflect upon the evolution of their roles.  This blog is our Praxis role journal, and our learning experience this year has been our Ivanhoe game.  It thus seems appropriate to closely follow the Ivanhoe launch announcement with a photo gallery representing the great times we've had together this year:\n\n\n\n[gallery ids=\"9422,9592,9573,9586,9581,9588,9593,9560,10253,10252,10255,10254,10256,10261,10258\"]\n\n[![image_11](http://www.scholarslab.org/wp-content/uploads/2014/04/image_11-300x225.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/04/image_11.jpg)\n\nThanks to the entire Praxis team, Fellows and SLab mentors, for a wonderful year!\n"},{"id":"2014-05-08-welcoming-ammon-and-scott","title":"Welcoming Ammon Shepherd and Scott Bailey","author":"wayne-graham","date":"2014-05-08 04:39:34 -0400","categories":["Announcements"],"url":"welcoming-ammon-and-scott","content":"We are thrilled to announce two new additions to the Scholars' Lab team. Scott Bailey will join our R&D; group later this month and Ammon Shepherd will join us in July, both in the roles of Digital Humanities Software Developer.\n\n[Ammon Shepherd](http://mossiso.com/) joins us from the fabulous [Roy Rosenzweig Center for History and New Media](http://chnm.gmu.edu/) where he is currently  Associate Director of Technology, responsible for overseeing CHNM's vast server infrastructure. He is  finishing his PhD in history at George Mason University and has taught courses in Western Civ and Public History at Mason and Arizona State. Ammon's dissertation is entitled \"[Nazi Tunnels: German Factory Dispersal Projects of World War II](http://nazitunnels.org/).\"\n\nIn his new role at the Scholars' Lab, Ammon will be working at the nexus of software development and server infrastructure. He will be contributing not only to the [numerous open source projects](https://github.com/scholarslab/) the R&D; group maintains, but also helping with our ongoing work on [GIS data discovery](http://gis.lib.virginia.edu), the development of workflows for a geospatial \"head\" for the [Hydra Project](http://projecthydra.org/), in collaboration with Stanford University, and helping to support the server infrastructure that houses these projects and our collaborations with UVa faculty.\n\nWe're equally excited that [Scott Bailey](http://csbailey.org/) will be joining us shortly. Scott was one of our [2013-2014 Praxis Program Fellows](http://praxis.scholarslab.org/people.html), who pushed a lot of excellent code to the [source](https://github.com/scholarslab/ivanhoe) of the program's recently released [Ivanhoe](http://ivanhoe.scholarslab.org/) WordPress theme, meant to enable collaborative criticism through role play. Scott is finishing his Ph.D. in Philosophical Theology at UVa with a dissertation that examines \"vulnerability as a locus of dogmatic reflection\" through lenses of continental philosophy and neuroscience. Scott has taught in the Engineering School and been heavily involved in instructional technology support at UVa.\n\nIn Scott's new role as a DH Developer, he will be helping maintain and develop our numerous plugins for [Omeka](http://omeka.org) (many [Neatline](http://neatline.org)-related) as well as collaborating with scholars and Library staff on maintaining and updating numerous UVa faculty projects.\n\nBoth Ammon and Scott will be contributing to the other missions of the Scholars' Lab, including graduate student mentoring in our [DH Fellows](http://www.scholarslab.org/graduate-fellowship-in-digital-humanities/) and [Praxis](http://praxis.scholarslab.org) programs, as well as participating in other experimental humanities initiatives and pursuing independent research.\n\nWe are really excited to welcome Ammon Shepherd, his wife Jessica, and their entire family, and Scott Bailey and his partner Karen to the Scholars' Lab family.   \n"},{"id":"2014-05-09-washington-and-lee-trip","title":"Washington and Lee Trip","author":"brandon-walsh","date":"2014-05-09 05:52:22 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"washington-and-lee-trip","content":"_Cross-posted at [my personal website](http://bmw9t.github.io/blog/2014/05/09/washingtonlee/)._\n\nLast week [Sarah](http://www.scholarslab.org/people/sarah-storti/) and I drove to Washington and Lee University as part of [a new collaboration](http://news.blogs.wlu.edu/2014/01/30/wl-announces-digital-humanities-partnership-with-uva/) enabled by a grant from the Associated Colleges of the South. As part of the endeavor, Scholars' Lab fellows are guest teaching pieces of an Introduction to Digital Humanities course. Our task, in particular, was to co-teach for a day on the topics of project management and software development. While we each took part and taught in both conversations, Sarah took the lead on the former topic and I took the latter.\n\nI can't rave enough about the experience enough, so I've organized my thoughts into three sections below.\n\n**Undergraduates + Digital Humanities = Dynamite**\n\nI am endlessly delighted by the reactions of undergraduates when they get introduced to the digital humanities. In virtually every case, I have encountered students hungry to learn the material. The W&L students were no exceptions. We found students ready to learn, eager to participate, and wiling to ask hard questions about the affordances and limitations of the field. You can find reflections by the students on their [course blog](http://dhintro.academic.wlu.edu/). What's more, the Washington and Lee students stand poised to make real contributions to digital scholarship. They have worked up some really interesting projects on [the history of coeducation at W&L](http://beyondbowties.academic.wlu.edu/) and on [the changing vision and reality for Robert E. Lee's Chapel on the university grounds](http://leechapel.academic.wlu.edu/).\n\n**Co-Teaching**\n\nSarah and I work well together, and we have presented together in the past. But we had not taught together before the Washington and Lee trip. Full disclosure: I adore everything about co-teaching. It immediately disrupts the one-way transmission of information from the instructors to the students and forces the conversation to be more collaborative; co-teaching allows you to occupy simultaneously and more obviously the dual roles of student and teacher. It takes the pressure off any one person to keep the ship sailing smoothly, which empowers and enlivens the conversation. Co-teaching seems especially well-suited to the digital humanities, which value collaboration and play. Seminar discussions and workshops are different from working on teams to build projects, but co-instructors can make the experience a bit more lab-like, a bit more collaborative.\n\n**Teaching DH!**\n\nIt is one thing to learn and practice digital humanities. It is another thing entirely to turn around and help others do the same. I have only really been hacking away for two years now, so I felt a bit unqualified to talk down software development as an invited speaker. I tend to assume that the Scholars' Lab has a better sense of my own abilities than I do in most cases, though, and the invitation to W&L was no exception. The practice of putting together presentations on project management and software development was incredibly empowering. It helped me to have more confidence in myself as a digital humanist. No longer does the prospect of teaching an introduction to digital humanities course appear to be a vague and nebulous question mark. I now know that I could do it, because I have already done so in part. I also have a better sense of my own developing pedagogy of digital humanities. Opportunities to teach digital humanities like this, to perform with no net, are rare.\n\nYou teach to learn, and this is as true in the digital humanities as it is anywhere else. I learned a great deal from the bright undergraduates at W&L.\n\n\n"},{"id":"2014-05-13-on-co-teaching-and-gratitude","title":"On co-teaching and gratitude","author":"sarah-storti","date":"2014-05-13 08:06:32 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"on-co-teaching-and-gratitude","content":"This post developed out of a response to Claire Maiers’s comment on [Brandon’s post](http://www.scholarslab.org/digital-humanities/washington-and-lee-trip/) from last week about our co-teaching venture at Washington and Lee University. She asked us to go into a little more detail about how co-teaching actually worked for us: how we planned class time and decided who would lead what and when. I do try to answer that below, but I invite Brandon and any other practitioners of co-teaching to weigh in via comments. As I wrote this, I also found myself reflecting on how lucky and privileged we were to be afforded this opportunity in the first place. And so I’d like to take this opportunity to publicly thank Purdom Lindblad, all the fantastic people in the Scholars’ Lab, Sara Sprenkle and Paul Youngman at W&L, and Brandon, too, for taking a chance on me. Thanks also to the amazing students at W&L for their responsiveness and enthusiasm. This co-teaching venture was one of the highlights of my year.\n\nBefore we did anything else, Brandon and I met with Purdom to discuss the kinds of things we wanted the students to get out of discussions about project management and software development. Because Brandon and I are products of the [Praxis Program](http://praxis.scholarslab.org/), I think we both (more or less consciously) modeled these desired results on the things we had created during similar Praxis lessons: we wanted the students to _make_ things, not just to _talk_ about things. After that initial conversation, we decided to split primary responsibility for the topics: Brandon worked up a PowerPoint slideshow that conveyed his basic lesson outline for software development (including prompts for activities and discussions) and I did the same for project management. I should note that for me at least the conversation we had beforehand was very helpful here: I always like to backwards plan lessons, beginning with where I want the class to end up, so discussing “deliverables” for a lesson on project management with Brandon and Purdom before I started planning helped me shape my lesson outline. We emailed the slideshows to each other the day before our class so that we could internalize the basic narratives each of us wanted to develop over the course of the day. Finally, in all honesty, we just encouraged each other to interrupt, to add on, to redirect discussion, and to otherwise productively contribute to the lesson that was not “ours.” Sometimes that meant one of us wrote student responses on the whiteboard while the other talked, and sometimes that meant one of us contributed an example or a question to the other’s talking points, or pushed back against student comments. We both spent time in working groups with the students, helping them draw wireframes and design charter drafts. We kind of played it by ear.\n\nWe had four hours of class time to fill, which did seem kind of terrifying to both of us at first, and partly as a result of this we planned more activities than we actually had time for. But I think the most important element of our plan was flexibility: we knew what our goals were, and we each had a map of sorts to get us there, but we were both ready and willing to throw things out the window on the way—and we did. Teaching has always seemed much like improv comedy to me: you have to run with what’s working in the moment. It’s possible that not every teacher feels this way, but I think the fact that Brandon and I were both prepared to diverge from The Plan was critically important to our success.\n\nI’ll close this by quoting the last sentence of Claire’s comment: “I imagine [planning for co-teaching] is easier when the teachers already have a rapport.” This seems exactly, precisely right to me. As Brandon mentioned last week, while we had never taught together before this trip, we had recently planned and delivered a two-person presentation. But additionally, and importantly, I have observed Brandon’s teaching, and he has observed mine. We also regularly talk about teaching and about how simultaneously challenging and awesome it is. We knew what the other person was likely to feel okay with throughout the day, which made everything nearly stress-free. This is all to say that I think rule #1 of co-teaching is: know thy partner(s) in crime. Rule #2 is: come to an agreement about what, as a team, you want to deliver to your students at the end of the day. Brandon and I had excellent co-teachers for models in this regard (here’s looking at you, Bethany, Wayne, Jeremy, Eric R., Eric J., David) and I hope they and others feel welcome to add to the conversation. Thanks for the question, Claire!\n\n\n"},{"id":"2014-05-15-one-teach-one-drift","title":"One Teach, One Drift","author":"ed-triplett","date":"2014-05-15 11:53:02 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"one-teach-one-drift","content":"Like Sarah, Brandon and Gwen, I also drove over Afton Mountain to teach at Washington & Lee a couple of weeks ago. As I played peek-a-boo with the trucks on I-64 on that rainy, foggy morning I must admit I gained a bit more respect for Wayne Graham’s daily commute – but that is a story for another time. In this post, I want to hop on to Brandon and Sarah’s discussion about team teaching and add some reflections on my experience at W&L.\n\nI flew solo on this teaching mission, but as I expected would happen, toward the end of the day-long lesson I envied Brandon and Sarah’s ability to support each other as co-teachers. The tools & processes I was introducing to the students were things I have taught before – Photogrammetry & 3D modeling in Google SketchUp – but in the past it has been a team-teaching experience. Before I talk about teaching at W&L, I want to describe two team-teaching experiences that I had fluttering in the back of my mind before and (especially) after I trekked over Afton Mountain.\n\nLast year I worked with Wayne Graham from the Scholars’ Lab and Will Rourk from the Digital Media Lab to guide Prof. Louis Nelson’s graduate and undergraduate students through a semester-long photogrammetry [project](https://www.youtube.com/watch?v=56wfXn-Mn6U). Wayne and I essentially “discovered” photogrammetry together a couple of years ago, and we worked our way through several different software packages and capture methods by running experiments and – more importantly – talking regularly about the results. Consequently, when it was time to instruct others, we both knew the pitfalls, and we always had backup if a student needed individual attention during a class-wide demonstration. I have heard this method described as “[One Teach, One Drift](https://www.cdli.ca/resources/sdm/RelatedLearningActivities/5.4%20Template%20for%20One%20Teach,%20One%20Drift.pdf)” and I believe it is the optimal method for teaching software.\n\nLast Summer, as part of a 3-week long NEH Summer Institute on 3D Visualization of Humanities Heritage I was able to “drift” for a good friend and former IATH colleague Chad Keller in his “Intro to Trimble SketchUp” and his “Intro to 3D Studio Max” courses. My role as the “drifter” was completely spontaneous. The SketchUp course in particular was a daunting task for Chad, because his students were all college instructors, and at this early stage of the institute, most of them had never navigated in a 3D space before, let alone attempted to model a scale building. For the record, SketchUp is probably my least favorite 3D modeling software. I am far from an expert in it, but as I watched Chad’s demonstration slow down as different students missed a step in the complex process I decided to just stand up and “catch up” the stragglers so that Chad could continue without stopping every 30 seconds. By itself, Chad’s step-by-step tutorial was so good, that I unabashedly adopted it for my second demonstration at W&L. Still, I knew that unlike Chad’s experience at the NEH institute, I would not have a drifter, so I decided to model a “finished” version of the tutorial the day before.\n\nThere is no substitute for experience when it comes to staying calm through a potentially chaotic demonstration. Knowing how people will react when they feel they are getting left behind, and being aware that it is impossible to assemble a room full of people with the same visual & technological acuity becomes a well of patience when hands shoot up, inarticulate groans break out, or students restlessly move ahead while you are helping individual strugglers. After witnessing some of these completely normal and somewhat unavoidable issues in the past, I expected to see it again at W&L – especially given that I was “alone” on this mission. As it turned out, the W&L students had been so well prepared by their professors before I arrived that they displayed a truly rare level of patience, maturity and humor. It sounds simple, but assembling a class full of students who “get” that they are being introduced to something new – that they will not emerge from the end of a demonstration with expertise that can only come from practice – is exceptional. Mad props to Paul Youngman and Sarah Sprenkle for nurturing an experimental frame of mind in their students, and kudos to the students themselves for rolling with me as we checked out 3D modeling & photogrammetry.\n\nIn retrospect, I can’t believe that things went as smoothly as they did at W&L. Without a “drifter” to help keep the tutorial moving, I had no delusions that all the students would end the day with a completed model of a church – but I was pleasantly surprised by our progress. I’m also not going to say I was as serene as a yoga instructor when we noticed that the toolkit interface for SketchUp on Mac – which all of the students had – was completely different than the one I have always used on PC. Fortunately, the IQ Center at W&L was quick on the draw. In a swift and decisive move, they switched out the students’ personal Mac laptops with a bank of PCs and we forged ahead confidently. The only thing they could not fix was the fact I woke up with a cold and had to croak my way through the last 30 minutes of class.\n\nI am proud to say that there was an audible cheer when both sides of the room successfully cut out windows for their scaled 3D church at the end of the day. It bears repeating that there is no substitution for experience, and I learned a lot from my third time teaching Trimble SketchUp. As I walked to the garage and spoke to Sarah Sprenkle about how the day had gone, I was a little worried that I had set one of the groups of students on a very difficult path. The week before I had worked with this group at the Scholars’ Lab on refining their idea for a DH project, and they seemed excited by the idea of building a 3D model of Lee Chapel. After that meeting, I decided to cut the photogrammetry lesson I planned in half and run with Chad’s SketchUp tutorial. Yesterday I saw the model of Lee Chapel the students created on their project page [here](http://leechapel.academic.wlu.edu/blueprints/3d-model/), and I was floored by their execution in such a short time. 3D modeling software has a notoriously steep learning curve, and I have seen professional educators thrash through it with what can best be described as “Bug Rage” – but the students at W&L were never intimidated by it.\n\nMy final thought may seem ambiguous given my praise of the Lee Chapel group’s SketchUp model, but I think my future courses will not use Trimble SketchUp as the applied software for 3D modeling of heritage sites. As I told the students at the time, SketchUp is “simple” in comparison to more powerful software packages like 3D Studio Max, but in its attempt to anticipate a user’s workflow, it fails to introduce new users to the basic units of 3d modeling – polygons. The result is a process that is not particularly intuitive, and creates a system where the only sure way to correct a mistake is by reversing via the \"undo\" button. My advisor here at UVa, Prof. Lisa Reilly and I are constructing a [Fall 2014 course](https://pages.shanti.virginia.edu/Digital_Humanities/) on Digital Humanities methods for art and architectural historians that will include 3D modeling, so the lessons I have picked up from teaching and observing the students at W&L will have immediate impact on our syllabus. With additional class time, I may use SketchUp as a brief intro to the process of “rough-sketching” a scaled, 3D building, but after witnessing the success of the Lee Chapel group at W&L, I am less hesitant to offer students a more powerful/complex software solution. The optimal arrangement would include a 3D Studio Max drifter, but with a bit of preparation and students with the right attitude, I think I can handle the lessons solo again.\n\nThanks for reading, and thanks to W&L for a great experience.\n"},{"id":"2014-05-19-welcoming-our-new-scholars-lab-fellows","title":"Welcoming our new Scholars’ Lab Fellows!","author":"purdom-lindblad","date":"2014-05-19 09:20:32 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"welcoming-our-new-scholars-lab-fellows","content":"We are thrilled to announce our partnership with 9 new graduate fellows for the 2014-2015 academic year! Joining an illustrious [community of past recipients](http://www.scholarslab.org/graduate-fellowships/) of Scholars’ Lab Fellowships, the new cohort hail from 5 disciplines in the [humanities and social sciences at the University of Virginia.](http://artsandsciences.virginia.edu/home/index.html)\n\n**James Ambuske**, **Jennifer Foy** , and **Emily Senefeld **join us as our three [Digital Humanities Graduate Fellows](http://www.scholarslab.org/graduate-fellowship-in-digital-humanities/). Throughout the year, they will have the opportunity to collaborate closely with Scholars’ Lab staff to integrate digital tools and methods to their dissertation research.\n\nJames Ambuske's dissertation is titled \"Scotland's American Revolution: Emigration and Imperial Crisis, 1763-1803.\"\n\nJennifer Foy's dissertation is titled \"Mapping Sympathy: Sensibility, Stigma, and Space in the Long Eighteenth Century\"\n\nEmily Senefeld project is titled \"The Cultural Programs of the Highlander Folk School, 1932-1964.\"\n\nJames, Jennifer, and Emily's projects converge around questions of social networks and mapping. We are looking forward to seeing their projects develop.\n\nThis year marks the 4th year of our [Praxis Program!](http://praxis.scholarslab.org) Past Praxis teams developed [Prism](http://prism.scholarslab.org), a web application for crowd-based interpretations of texts, and re-imagined [Ivanhoe](http://ivanhoe.scholarslab.org), a platform for playfully (and collaboratively) interpreting texts and artifacts. This year’s cohort will refine Ivanhoe as well as explore our new Maker Space.\n\n_** 2014-2015 Praxis Fellows**_ are:\n\nAmy Boyd (English)\nSwati Chawla (History)\nAndrew Ferguson (English)\nJoris Gjata (Sociology)\nJennifer Grayburn (Art and Architecture)\nand Steven Lewis (Music)\n\nLook for more information about our fellows in the early fall on the Scholars’ Lab blog.\n"},{"id":"2014-06-05-come-work-with-us-makerspace-student-consultant","title":"Come Work With Us in Our New Makerspace","author":"laura-miller","date":"2014-06-05 10:44:40 -0400","categories":["Announcements","Experimental Humanities"],"url":"come-work-with-us-makerspace-student-consultant","content":"_Are you a UVA graduate student or upper-level undergraduate in the humanities?  Interested working in our new Makerspace?_\n\n[caption id=\"attachment_10367\" align=\"alignleft\" width=\"303\"][![133799283_8e9ab4cd6e_b](http://www.scholarslab.org/wp-content/uploads/2014/06/133799283_8e9ab4cd6e_b-300x232.jpg)](http://www.scholarslab.org/wp-content/uploads/2014/06/133799283_8e9ab4cd6e_b.jpg) photo courtesy of flicker user marymactavish[/caption]\n\nThe Scholars’ Lab in Alderman Library is opening a new Makerspace in Fall 2014.  It will allow user experimentation with 3D modeling and printing, physical computing (e.g. Arduino, wearables) and more.  We are seeking part-time student assistants to help maintain the public space, field users’ basic maker and general computing (both Mac and PC) questions, and connect researchers to Scholars’ Lab staff when necessary.  When not actively engaged with users, assistants will be asked to pursue their own research, use the equipment, and publish their processes and observations on the Scholars’ Lab blog.\n\nExperience with 3D modeling and printing, electronics, GIS, and/or programming preferred, but can be learned on the job.  The successful candidate will be able to work up to 10 hours per week.\n\nAn important aspect of the maker culture is apprenticeship and supporting makers in their pursuit of professional experience. We are looking for motivated individuals who are capable of working independently and value the opportunity to engage with and support a growing community.  Benefits of the job may include: access to expertise and mentoring in your field of interest, use of equipment and tools, and ability to shape Scholars’ Lab workshops and programming.\n\nCandidates should include a cover letter discussing their interest in working in the Scholars’ Lab, detailing any experience or interest in participating in a maker space, and outlining any previous experience with public service or assisting others in using technology.\n\nIf you would like to apply, please fill out an application in [CavLink](http://www.career.virginia.edu/students/cavlink/).\n"},{"id":"2014-06-09-check-out-copyipsum","title":"Check Out Copyipsum","author":"jeremy-boggs","date":"2014-06-09 06:00:37 -0400","categories":["Research and Development"],"url":"check-out-copyipsum","content":"Of the bazillion lorem ipsum generators out there, the one I use most often for day to day work is [Loripsum](http://loripsum.net). It has a few great features, like choosing length of paragraph, adding other HTML elements, and only getting plain text. You can also get all this stuff through a simple API.\n\nFriday morning, while bouncing between working on some redesign ideas for this site and messing with the [Marvel API](http://developer.marvel.com/), I decided that I really wanted to be able to get some lorem ipsum text from the Loripsum.net API using a command line tool. Thankfully, [Eric](/people/eric-rochester) was around to hold my hand as we put together [Copyipsum](http://github.com/clioweb/copyipsum).\n\nTo get going with Copyipsum, you can install it using `pip`:\n\n[code gutter=\"false\"]\npip install copyipsum\n[/code]\n\nYou can also clone the public repository:\n\n[code gutter=\"false\"]\ngit clone git://github.com/clioweb/copyipsum.git\n[/code]\n\nOr download the tarball:\n\n[code gutter=\"false\"]\ncurl -OL https://github.com/clioweb/copyipsum/tarball/master\n[/code]\n\nOnce you have a copy, you can run the setup yourself. Move into the copyipsum directory, then do:\n\n[code gutter=\"false\"]\npython setup.py install\n[/code]\n\nOnce you have it installed, you can now use the `copyipsum` command to save some lorem ipsum to your clipboard and use wherever you want. All the options available on the Loremipsum.net API are available as arguments to the `copyipsum` command, and are documented in the project's [README](https://github.com/clioweb/copyipsum/blob/master/README.md), but here are a few examples to illstrate some of the things you can do:\n\n**Get 10 Paragraphs**\n\n[code gutter=\"false\"]\ncopyipsum -p 10\n[/code]\n\n\n**Get 10 Paragraphs with Headings**\n\n[code gutter=\"false\"]\ncopyipsum -H -p 10\n[/code]\n\n**Get 10 Paragraphs with Headings and decorators like bold, italic, and mark**\n\n[code gutter=\"false\"]\ncopyipsum -d -H -p 10\n[/code]\n\n**Get 10 Long Paragraphs**\n\n[code gutter=\"false\"]\ncopyipsum -p 10 -s long\n[/code]\n\nAs usual, bug reports and feature requests are welcome! Feel free to add though through the project's [issue tracker](http://github.com/clioweb/copyipsum/issues).\n"},{"id":"2014-06-20-learning-ruby-opening-moves","title":"Learning Ruby: Opening Moves","author":"purdom-lindblad","date":"2014-06-20 05:48:53 -0400","categories":["Digital Humanities","Research and Development"],"url":"learning-ruby-opening-moves","content":"As the [Praxis](http://praxis.scholarslab.org) Fellows wrapped up [Ivanhoe](http://ivanhoe.scholarslab.org), I turned my attention to the [Praxis Network](http://www.praxis-network.org). The Praxis Network, which showcases eight like-minded, but differently enacted programs all exploring new ways of teaching humanities students, began with the goals of creating an easy way to compare a variety of programs and to provide a model for others. The success of the website, prompted new goals of better networking students within the Network as well as allied programs.\n\nAfter a few conversations with Bethany, Wayne, and Jeremy, we had a basic outline for an open directory of like-minded programs and another directory of Praxis Network students. I realized pretty quickly it is easy to talk through an idea, but much more difficult for me to break the larger idea into a series of small steps. Often breaking down the big into the small makes a great deal of sense; my challenge was to realize small, in this case, means exceptionally tiny and extremely specific.\n\nCreating the open directory of 'fellow traveler' programs began with a series of drawings defining (and redefining) directory. How would people contribute information? How would I collect the data and publish to the web? A sketch:\n\n\n\n\n  * create Google Form\n\n\n  * pull data from form and write Markdown Files\n\n\n  * edit if needed\n\n\n  * add to git\n\n\n  * push to gh-pages and voila!, published\n\n\nSimple, right? Now, the reality--I am new to Ruby and to using git. After creating the Google Form, I installed Ruby and the gems `google_drive`, `dotenv`, and `rake`. We, then, created a git repository and a `.gitignore` file. Next, Wayne and I created a `.env` file to keep passwords and form keys private. After the initial set up, I was ready to tackle the `Rakefile`.\n\n[gallery ids=\"10398,10399,10400\"]\n\nI am still trying to wrap my head around the `Rakefile`. At the moment, I am working with metaphors, which is not best practice when thinking about the highly specific process of laying out a series of steps for the computer to follow. Even so, I think of the `Rakefile` as a scratchpad to experiment with the series of instructions. First, I defined the requirements (the gems I previously installed--`ruby-gems`, `dotenv`, and `google_drive`) and defined the necessary tasks (import data from Google Drive) and methods (loop over this data and make a Markdown file for each row passed to it). I am still working on defining the content to display online. The next step is to investigate using Jekyll categories as a way to filter programs.\n\nMy biggest challenge, so far, has been that I have not devoted regularly scheduled time to working. Chunking an hour or so when it was convient means I have not internalized basic commands. Worse, each time I return to the script, I need to re-familiarize myself with each element within it. To address this, I have scheduled daily time on my calendar (with reminders) and have begun [Ruby the Hard Way](http://ruby.learncodethehardway.org) and [Ruby Koans](http://rubykoans.com).\n"},{"id":"2014-07-02-a-digital-declaration-of-independence-with-text-painting-and-map","title":"A (Digital) Declaration of Independence","author":"david-mcclure","date":"2014-07-02 05:56:04 -0400","categories":["Geospatial and Temporal"],"url":"a-digital-declaration-of-independence-with-text-painting-and-map","content":"_[Cross-posted from [dclure.org](http://dclure.org/essays/a-digital-declaration-of-independence-with-text-painting-and-map/)]_\n\n\n\n## [Launch the Exhibit](http://neatline.dclure.org/neatline/show/declaration-of-independence)\n\n\n\n[![declaration-of-independence](http://dclure.org/wp-content/uploads/2014/07/declaration-of-independence-1024x610.jpg)](http://neatline.dclure.org/neatline/show/declaration-of-independence)\n\nWay back in the spring of 2012, a couple months before we released the first version of Neatline, I drove up to Washington to give a little demo of the project to the folks at the Library of Congress. I had put together a couple of example exhibits for the presentation, but, the night before, I was bored and found myself brainstorming about Washington-themed projects. On a lark, I downloaded a [scan of the 1823 facsimile of the Declaration of Independence](http://www.archives.gov/exhibits/charters/declaration_transcript.html) from the National Archives website, and spent a couple hours tracing polygons around each one of the signatures at the bottom of the document. I showed the exhibit the next day, and had big plans to flesh it out and turn it into a real, showable project. But then I got swept up in the race to get the first release of Neatline out the door before DH2012 in Hamburg, and then sucked into the craziness of the summer conference season, and the project slipped down into the towering stack of things that I could never quite find time to work on.\n\nFor some reason, though, the idea popped back into my head a couple months ago - maybe because Menlo Park is submerged in a kind of permanent summer, and it pretty much always feels like a good time to eat ice cream and shoot off fireworks. After mulling it over for a couple weeks, I decided to resurrect it from the dead, spruce it up, and post it in time for the 4th of July. So, with two days to spare, here we go - an interactive edition of the Declaration of Independence, tightly coupled with three other \"views\" in an effort to add dimension to the original document:\n\n\n\n\n\n\n  1. A full-text, two-way-linked transcription of the manuscript and the signatures at the bottom. Click on sentences in the transcription to focus on the corresponding region of the scanned image, or click on annotated blocks on the image to scroll the text.\n\n[![transcript](http://dclure.org/wp-content/uploads/2014/07/transcript-1024x620.jpg)](http://dclure.org/wp-content/uploads/2014/07/transcript.jpg)\n\n\n\n\n  2. An interactive edition of [Trumbull's \"Declaration of Independence\" painting](http://en.wikipedia.org/wiki/Trumbull%27s_Declaration_of_Independence), with each of the faces outlined and interactively linked with the corresponding signature on the document.\n\n[![painting](http://dclure.org/wp-content/uploads/2014/07/painting1-1024x616.jpg)](http://dclure.org/wp-content/uploads/2014/07/painting1.jpg)\n\n\n\n\n  3. All of which is plastered on top of a map that plots out each of the signers' hometowns on a custom [Mapbox](https://www.mapbox.com) layer, which makes it easy to see how the layout of  the signatures maps on to the geographic layout of the colonies. Which, by extension, tracks the future division between Union and Confederate states in the Civil War - Georgia and the Carolinas look awful lonely over on the far left side of the document.\n\n[![map](http://dclure.org/wp-content/uploads/2014/07/map1-1024x615.jpg)](http://dclure.org/wp-content/uploads/2014/07/map1.jpg)\n\n\n\n\n\nOnce I positioned the layers, annotated the signatures and faces, and plotted out the hometowns, I realized that I had painted myself into an interesting little corner from an information design standpoint - it was difficult to quickly move back and forth between the three main sections of the exhibit. In a sense, this is an inherent characteristic of deeply-zoomed interfaces. The ability to focus really closely on any one of the three visual grids - which is what makes it possible to mix them all together into a single environment - has the side effect of making the other two increasingly distant and inaccessible, more and more so the further down you go. For example, once you've focused in on Thomas Jefferson's face in the Trumbull painting, it's quite a chore to manually navigate to the corresponding signature on the document - you have to zoom back, pan the map up towards the scanned image, find the signature (often no easy task), and then zoom back down.\n\nThis is especially annoying in this case, since this potential for _comparison_ is a big part of what's interesting about the content. What I really wanted, I realized, was to be able to switch back and forth in a really simple, fluid way among the different instantiations of any individual person on the document, painting, and map - I wanted to be able to flip through them like a slideshow, to round up all the little partial representations of the person and hold them side-by-side in my head. So, as an experiment, I whipped up a little batch of custom UI components (built with the excellent [React](http://facebook.github.io/react/) library, which fits in like a dream with Neatline's Javascript API) that provide a \"toggling\" interface for each individual signer, and the exhibit as a whole.\n\nBy default, when you hit the page, three top-level buttons in the right corner of the window link to the the three main sections of the exhibit - the hometowns plotted along the eastern seaboard, the declaration over the midwest, and the painting over the southeast. In addition to the three individual buttons, there's also a little \"rotate\" button that automatically cycles through the three regions, which makes it easy to toggle around without looking away from the map to move the cursor:\n\n[![exhibit-buttons](http://dclure.org/wp-content/uploads/2014/07/exhibit-buttons.jpg)](http://dclure.org/wp-content/uploads/2014/07/exhibit-buttons.jpg)\n\nMore useful, though, it's possible to bind any of the individual signers to the widget by clicking on the annotations. For example, if I click on Thomas Jefferson's face in the painting, the name locks into place next to the buttons, which now point to the representations of that specific person in the exhibit - \"Text\" links to Jefferson's signature, \"Painting\" to his face, and \"Map\" to Monticello:\n\n[![signer-toggle](http://dclure.org/wp-content/uploads/2014/07/signer-toggle-1024x840.jpg)](http://dclure.org/wp-content/uploads/2014/07/signer-toggle.jpg)\n\nOnce you've activated one of the signers, click on the name to show an overlay with a picture and biography, pulled from a public domain book published by the National Park Service called Signers of the Declaration:\n\n[![bio-overlay](http://dclure.org/wp-content/uploads/2014/07/bio-overlay-1024x624.jpg)](http://dclure.org/wp-content/uploads/2014/07/bio-overlay.jpg)\n\nThis is pretty straightforward on the map and document, where there's always a one-to-one correspondence between an annotation and one of the signers. Things get more complicated on the map, though, where it's possible for a single location to be associted with more than one signer. Philadelphia, for example, was home to Robert Morris, Benjamin Rush, Benjamin Franklin, John Morton, and George Clymer, so I had to write a little widget to make it possible to hone in on just one of the five after clicking the dot:\n\n[![philadelphia](http://dclure.org/wp-content/uploads/2014/07/philadelphia-1024x377.jpg)](http://dclure.org/wp-content/uploads/2014/07/philadelphia.jpg)\n\nLast but not least, each sentence in the document itself is annotated and wired up with the corresponding text transcription on the left - click on the image to scroll the text, or click on the text to focus the image:\n\n[![text](http://dclure.org/wp-content/uploads/2014/07/text-1024x617.jpg)](http://dclure.org/wp-content/uploads/2014/07/text.jpg)\n\nHappy fourth!\n"},{"id":"2014-07-24-codespeak-kit","title":"Announcing the #Codespeak Kit!","author":"bethany-nowviskie","date":"2014-07-24 05:20:14 -0400","categories":["Announcements","Research and Development"],"url":"codespeak-kit","content":"[![screenshot from the speaking in code website](http://www.scholarslab.org/wp-content/uploads/2014/07/Screenshot-2014-07-24-09.00.36-225x300.png)](http://codespeak.scholarslab.org)Today, the [Scholars' Lab](http://scholarslab.org/) is pleased to make a few modest contributions toward the broadening of a conversation we opened last fall, in a summit for digital humanities software developers called _[Speaking in Code](http://codespeak.scholarslab.org)_. The summit, generously funded by the [National Endowment for the Humanities](http://neh.gov/odh) and [UVa Library](http://lib.virginia.edu/scholarslab), brought together 32 advanced developers working on humanities data, tools, and systems, to discuss ways to create inclusive, welcoming developer communities and to address the social and intellectual implications of tacit knowledge exchange in their craft.\n\nOn the [codespeak site](http://codespeak.scholarslab.org) today, you'll find links to [a customizable kit](https://github.com/scholarslab/codespeakkit) that anyone can use to host their own _Speaking in Code_ gathering. It includes our [starter bibliography](https://github.com/scholarslab/codespeakkit/blob/master/bibliography.md), advice [for welcoming](https://github.com/scholarslab/codespeakkit/blob/master/planning.md) a diverse group of participants and [planning](https://github.com/scholarslab/codespeakkit/blob/master/logistics.md) an [event](https://github.com/scholarslab/codespeakkit/blob/master/schedule.md), and the framework of a Jekyll website (complete with instructions), [ready for you](https://github.com/scholarslab/codespeakkit/blob/master/README.md) to modify and publish easily, using GitHub Pages. You'll also find the beginnings of a set of DH developer advice posts and \"origin stories,\" to which [you are invited](http://codespeak.scholarslab.org/starting) to contribute, and an invitation to continue the conversation on Twitter (hashtag [#codespeak](https://twitter.com/search?q=%23codespeak)) and IRC (Freenode channel [#speakingincode](http://webchat.freenode.net/?channels=%23speakingincode&uio=d4)).  A white paper on the outcomes of the inaugural summit will also be available soon, and summit particpants may have other documents to share.\n\nRead a little more about the purpose and outcomes of _Speaking in Code_ at [CLIR](http://clir.org), in the following posts:\n\n\n\n\n    \n  * [ How We Learned to Start/Stop \"Speaking in Code\"](http://connect.clir.org/BlogsMain/BlogViewer/?BlogKey=2c505b4b-0f6f-4d2b-ad8a-83b24b48705c)\n\n    \n  * and [A Kit for Hosting \"Speaking in Code\"](http://connect.clir.org/blogs/bethany-nowviskie/2014/07/24/a-kit-for-hosting-speaking-in-code)\n\n\n"},{"id":"2014-07-28-neatline-2-3","title":"Neatline 2.3","author":"david-mcclure","date":"2014-07-28 07:01:55 -0400","categories":["Announcements"],"url":"neatline-2-3","content":"Today we're happy to announce [Neatline 2.3](http://omeka.org/add-ons/plugins/neatline/)! This release includes a couple of nifty new features and, under the hood, a pretty big stack of bug fixes, performance tweaks, and improvements to the development workflow. The coolest new feature in 2.3 is a simple little addition that we've gotten a number of requests for in the last few months - the ability to \"hard link\" to individual records inside of an exhibit. In the new version, when you select a record in an exhibit, a little fragment gets tacked on to the end of the URL that points back to that record. For example, if the record has an ID of `16`, the URL will change to something like:\n\n`www.omeka-site.org/neatline/show/exhibit**#records/16**`\n\nThen, if someone goes directly to this URL, the exhibit will automatically select that record when the page loads, just as if the reader had manually clicked on it - the map will focus and zoom around the record, the popup bubble will appear, the timeline will scroll, and any other custom event bindings added by the exhibit's theme will fire. This is nice because it makes it easier to use Neatline as a kind of geospatial \"footnoting\" system that can be referred to from external resources - sort of like the [Neatline Text](http://www.scholarslab.org/announcements/neatline-text/) extension, except the text doesn't have to be housed inside of Omeka. Imagine you're working on an article that makes reference to some geographic locations, and you want to plot them out in Neatline. This way you could put the text of the article anywhere on the web (a Wordpress blog, an online journal, etc.) and just link to the relevant parts of the Neatline exhibit using plain old anchor tags.\n\nFor example, check out this [simple little Neatline exhibit](http://neatline.dclure.org/neatline/show/record-links-demo), which just plots out the locations of eight US cities. Then, click on these links to open up the same exhibit, this time focused on the individual cities: [New York](http://neatline.dclure.org/neatline/show/record-links-demo#records/1526), [San Francisco](http://neatline.dclure.org/neatline/show/record-links-demo#records/1525), [Chicago](http://neatline.dclure.org/neatline/show/record-links-demo#records/1527), [Los Angeles](http://neatline.dclure.org/neatline/show/record-links-demo#records/1528), [Seattle](http://neatline.dclure.org/neatline/show/record-links-demo#records/1531), [Denver](http://neatline.dclure.org/neatline/show/record-links-demo#records/1530), [Atlanta](http://neatline.dclure.org/neatline/show/record-links-demo#records/1532), and (but of course) [Charlottesville](http://neatline.dclure.org/neatline/show/record-links-demo#records/1529).\n\n[![hard-link](http://dclure.org/wp-content/uploads/2014/07/hard-link1-1024x591.jpg)](http://neatline.dclure.org/neatline/show/record-links-demo#records/1525)\n\nCheck out the [change log](https://github.com/scholarslab/Neatline/releases/tag/2.3.0) for the full list of updates in 2.3, and grab the new production package from the [Omeka addons repository](http://omeka.org/add-ons/plugins/neatline/). Thanks Jenifer Bartle, Jacki Musacchio, Rachel King, Lincoln Mullen, and Miriam Posner for helping us find bugs and brainstorm about features! As always, drop a note on the [GitHub issue tracker](https://github.com/scholarslab/Neatline/issues?direction=desc&sort=created&state=closed) if you run into problems or have ideas for new features.\n"},{"id":"2014-08-18-omeka-neatline-mac-development-oh-my","title":"Omeka, Neatline, Mac, development, oh my!","author":"eric-rochester","date":"2014-08-18 05:41:08 -0400","categories":["Research and Development"],"url":"omeka-neatline-mac-development-oh-my","content":"At the Scholars' Lab, we're big big advocates of Open Source. All of our projects are available freely and openly on [Github](https://github.com/), and we're always more than happy to accept pull requests. We'd like to be able to empower everyone to contribute to our projects as much as they're able to and comfortable with.\n\nUnfortunately, one of our flagship projects, [Neatline](http://neatline.org/), isn't easy to contribute to. There are a number of reasons for this, but one is that the development environment is not trivial to get set up. In order to address this and make it easier for others to contribute, we've developed an [Ansible](http://www.ansible.com/) playbook that takes a not-quite-stock Mac and sets up an instance of Omeka with the Neatline plugin available, as well as all the tools necessary for working on Neatline.\n\n[Ansible](http://www.ansible.com/) is a system for setting up and configuring systems. It's often used to set up multiple servers—for instance, a database server and a static web server, both working with a dynamic web applications deployed on several computers. If you're familiar with [Chef](http://www.getchef.com/) or [Puppet](http://puppetlabs.com/), Ansible solves the same problems. In this case, we'll use it to configure our local development workstation.\n\nWe've published these playbooks on [Github](https://github.com/) in the [`neatline.dev` repository, on the `mac-ansible` branch](https://github.com/erochest/neatline.dev/tree/mac-ansible). You can get this by cloning it to your local machine. (Since this is for getting started developing Neatline, I assume that you're already comfortable with [git](http://git-scm.com/). If not, [there](http://rogerdudler.github.io/git-guide/) [are](https://try.github.io/) [lots](http://www.git-tower.com/learn/) [of](http://gitimmersion.com/) [great](http://www.vogella.com/tutorials/Git/article.html) [tutorials](http://git-scm.com/book).)\n\n[code lang=bash]\n$ git clone --branch mac-ansible https://github.com/erochest/neatline.dev.git\n[/code]\n\n\n\n## Requirements\n\n\n\nIn creating this, I've aimed for starting from a stock Mac. And I missed pretty badly. However, the necessary prerequisites are minimal. You'll just need to have these things installed.\n\n\n\n\n\n  * [XCode](https://itunes.apple.com/us/app/xcode/id497799835)\n\n\n  * [Homebrew](http://brew.sh/)\n\n\n\nOnce those two are on your machine, you can install the other two dependencies. These are available through [Homebrew](http://brew.sh/). So open Terminal and type these lines:\n\n[code lang=bash]\n$ brew install python\n$ brew install ansible\n[/code]\n\nThat's all. You should be ready to go.\n\n\n\n## Settings\n\n\n\nThis project includes a number settings that you can change to customize your installation. Those are found in the file [`playbook.yaml`](https://github.com/erochest/neatline.dev/blob/mac-ansible/playbook.yaml). The relevant section is labelled `vars`, and it allows you to set information about the Omeka database (`omeka_db_user`, `omeka_db_password`, and `omeka_db_name`), which version of Omeka you wish to use (`omeka_version`), where you wish to install it (`omeka_dir`), and where you want to point your browser to (`dev_hostname`) as you're working on the site. The defaults are:\n\n[code lang=text]\nvars:\n  db_user: root\n  db_password:\n  omeka_db_user: omeka\n  omeka_db_password: omeka\n  omeka_db_name: omeka\n  dev_hostname: omeka-neatline.dev\n  omeka_dir: \"{{ ansible_env.HOME }}/omeka/neatlinedev\"\n  omeka_version: stable-2.1\n  debug: true\n  neatline_repo: git@github.com:scholarslab/Neatline.git\n  php_version: 55\n[/code]\n\nChange these to reflect what you'd like your personal Omeka/Neatline installation to look like.\n\nOne option that I'll call out in particular is `neatline_repo`. This is the git repository that you'll be working with. If you're using github to host your project, you can [fork](https://help.github.com/articles/fork-a-repo) the primary Neatline repository (from the URL given above). And when you've completed your work, if you'd like to contribute back, you can send us a [pull request](https://help.github.com/articles/using-pull-requests) through the Github site.\n\n\n\n## Setting Up\n\n\n\nFinally, we're ready to actually create the system. This is quite easy. In the Terminal, from the `neatline.dev` directory, run the `neatline-dev` script.\n\n[code lang=bash]\n$ cd neatline.dev\n$ ./neatline-dev\n[/code]\n\nNow wait.\n\nAfter your computer whirs away for a while, you'll get your prompt back. When that happens, you should be able to point your browser to http://omeka-neatline.dev (in the example above). There you'll see the Omeka installation form.\n\n\n\n## What Just Happened?\n\n\n\nThe Ansible playbook does a number of tasks.\n\n\n\n\n\n  1. It installs all the dependencies that you'll need, including [PHP](http://php.net/), [NodeJS](http://nodejs.org/), and [MySQL](http://www.mysql.com/).\n\n\n  2. It sets MySQL to start automatically when you log in, and it creates the Omeka MySQL user and database.\n\n\n  3. It configures [Apache](http://httpd.apache.org/) to work with PHP and to find your Omeka directory.\n\n\n  4. It downloads and configures [Omeka](http://omeka.org/) and turns on debugging.\n\n\n  5. It clones [Neatline](http://neatline.org/) into Omeka's `plugin` directory.\n\n\n  6. It initializes [git flow](https://github.com/nvie/gitflow) for working in Neatline and leaves you on the `develop` branch.\n\n\n  7. And it installs the necessary JavaScript and PHP tools, including [Grunt](http://gruntjs.com/), [Bower](http://bower.io/), [Composer](https://getcomposer.org/), and [PHPUnit](http://phpunit.de/).\n\n\n\nAfter all that, it really needs a break.\n\nYou probably do too.\n\n\n\n## Future\n\n\n\nUnfortunately, that's only the first step that we need to take to make the Neatline code-base approachable. Some more things that we have planned include:\n\n\n\n\n\n  * Documentation on all the moving parts.\n\n\n  * Documentation on the overall architecture of Neatline.\n\n\n  * Documentation on the code. What's where? If you wish to change something, where would you find it?\n\n\n\nAs we get those parts in place, we'll keep you posted.\n"},{"id":"2014-08-20-prism-news-heroku-and-llc","title":"Prism News - Heroku and LLC","author":"brandon-walsh","date":"2014-08-20 04:57:54 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"prism-news-heroku-and-llc","content":"_Cross-posted on_[_ my personal site_](http://bmw9t.github.io)\n\nThis past year the Scholars' Lab has implemented many performance upgrades and bug fixes for [Prism](http://prism.scholarslab.org). The most recent upgrade is particularly exciting: users can now deploy their own personal Prism installations to Heroku with the click of a button. Well - it will take the click of a button and a few other commands. I've added a section detailing just how to do so under the \"[Deploy to Heroku](https://github.com/scholarslab/prism#deploy-to-heroku)\" section of the Prism Github's readme.\n\nIt was already possible to implement private user communities by marking uploaded prism games as \"unlisted\" and then distributing the links to your group of participants. The Heroku deploy function makes this process a bit easier by allowing to users to host all of their games in one place. The process also sets you up well to tinker with the Prism codebase using a live app, as Heroku provides instructions for cloning the app to your desktop.\n\nAll of this on the heels of another exciting announcement: the Praxis Program has a short article on Prism appearing in the [Digital Humanities 2013 special conference issue of Literary and Linguistic Computing](http://goo.gl/pJ9SbC). In the piece, we summarize Prism's and interventions into conversations on crowdsourcing with special reference to its user interface.\n\nIt's a good day to e-highlight!\n"},{"id":"2014-08-21-taking-the-alt-ac-route","title":"Taking the Alt-Ac Route","author":"scott-bailey","date":"2014-08-21 05:05:01 -0400","categories":["Digital Humanities","Research and Development"],"url":"taking-the-alt-ac-route","content":"I am not where I thought I’d be. A year ago I’d have told you that right about now I’d be frantically writing my dissertation, trying to get an article out, and desperately looking for professorial jobs in contemporary theology and/or continental philosophy. Instead, I’m still working on my dissertation (not quite as frantically as I perhaps should), but I’m mostly being a digital humanities developer in the [Scholars’ Lab](https://www.scholarslab.org/) at U.Va. Instead of reading book after book of theology on a daily basis, I read a lot of documentation for programming languages. Instead of hours each day writing theology deeply conditioned by years of reading Heidegger, Barth, and Jüngel, I’m writing, at the moment, rather a few lines of Javascript. Within the next few weeks I’m hoping to make that [Coffeescript](http://coffeescript.org/) instead. Why write var repeatedly when you don’t need to? Instead of days and nights spent moving between coffeeshops, library tables, and my own apartment, I sit (or stand) at my desk in the Scholars’ Lab offices for about eight hours a day, taking an occasional break from the space to work somewhere else in the library or go to a meeting in Clemons.\n\nIt’s a different pattern of life and work, one I had not expected. And it has been quite a transition since I started working in May. Those first few weeks settling in and working eight hours days, with few breaks, were exhausting. But it’s also been rather a lot of fun, learning new languages, working collaboratively on projects, and simply being around brilliant and engaging people every day.\n\nHow or why am I here though? Why not continue on the path I spent seven years on, aimed at the traditional professoriate? The quick and easy answer is the job market and the changing character and organization of universities, both of which are frequent topics in the news these days. The more difficult answer is about a changing sense I have of my own broad interests and what type of work can be satisfying. As a [Teaching + Technology Support Partner](http://tti.virginia.edu/ttsp/) at U.Va., then more significantly as a [Praxis Fellow](http://praxis.scholarslab.org/) here in the Scholars’ Lab, I began working with technology first within the pedagogical sphere and then more broadly within the academic world. I began to see the possibilities of use of different technologies within academic research, teaching, and scholarly communication. As a Praxis Fellow, I then realized that I really enjoy working with the technology itself, specifically, coding. The type of systematic and problem oriented thinking that you engage in when coding is remarkably similar to the type of thinking one engages in with dogmatic or systematic theology. Then, in the process of building [Ivanhoe](http://ivanhoe.scholarslab.org/), writing the code began to be the thing I most wanted to do out of all the things I was doing (dissertating, TAing, and so forth), and it was fulfilling each day to do narrow, concrete work and see the results.\n\nNow, a little bit of success and enjoyment with coding is not necessarily enough to spur a whole-sale career change. It is enough, though, to raise questions and possibilities about shifting the path just a bit. In the Scholars’ Lab, I am a digital humanities developer. That means I code, but it also means I think about what it means to do research in the humanities and will, over time, work with faculty and graduate students to help them with their own research. Because I am lucky enough to be in the Scholars’ Lab, it also means I do research of my own and I speak with the excellent people here in the lab, all with their own interests and specialties. Together, we compose a cadre of scholarly practitioners deeply interested in our own fields, in the larger state of humanistic inquiry, in the development of innovative research methods and tools, and in graduate education. From that standpoint, while my daily routine might look quite different from expected, the place where I am is not so far off the path of the traditional academic. In that case, while I am not where I thought I’d be, I am also not not where I thought I’d be.\n"},{"id":"2014-08-24-announcing-the-fall-2014-scholars-lab-gis-workshop-series","title":"Announcing the Fall 2014 Scholars’ Lab GIS Workshop Series","author":"laura-miller","date":"2014-08-24 13:15:51 -0400","categories":["Announcements","Geospatial and Temporal"],"url":"announcing-the-fall-2014-scholars-lab-gis-workshop-series","content":"_All sessions are one hour and assume attendees have no previous experience using GIS. Sessions will be hands-on with step-by-step tutorials with expert assistance. **All sessions will be taught by our GIS Specialist, Chris Gist, on Thursdays from 2:00-3:00 PM in the Alderman Electronic Classroom, ALD 421** (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community._\n\n**September 4th\nMaking your First Map**\nGetting started with new software can be intimidating. This workshop introduces the skills you need to work with spatial goodness. Along the way you’ll get a taste of Earth’s most popular geographic software and a gentle introduction to map making. You’ll leave with your own cartographic masterpiece and tips for learning more in your pursuit of mappiness at UVa.\n\n**September 11th\nGetting Your Data on a Map**\nDo you have GPS points or a list of latitude and longitude you would like to show as points on a map? This session will show you how to turn your data into map layers and how to connect them to make lines and polygons as well.\n\n**September 18th\nPoints on Your Map: Street Addresses and More Spatial Things**\nDo you have a list of street addresses crying out to be mapped? Have a list of zip codes or census tracts you wish to associate with other data? We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis.\n\n**September 25th\nGeoreferencing – Putting Old maps and Aerial Photos on Your Map**\nHave an old map or an aerial photograph that you would like to use as a spatial layer? This session will teach you techniques to properly place your data and make it useable in GIS software. We will also demo similar techniques for Google Earth.\n\n**October 2nd\nTaking Control of Your Spatial Data: Editing in ArcGIS**\nUntil we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done. If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better. This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software. We’ll explore tools to create new points, lines, and polygons and to edit existing datasets. At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on.\n\n**October 9th\nCollecting Your Own Spatial Data**\nResearch projects often rely on fieldwork to build new datasets. In this workshop we’ll focus on tools for spatial data collection. First we’ll take a quick look behind the curtain to see how GPS really works and how to use that knowledge to our advantage. Then we’ll evaluate free or low-cost options to gather locations and associated attributes using handheld GPS devices, smartphones, and apps. This workshop will introduce you to a range of devices and methods for mobile spatial data collection.\n\n_ _\n\n\n"},{"id":"2014-08-27-fall-2014-scholars-lab-gis-workshop-series","title":"Fall 2014 Scholars’ Lab GIS Workshop Series","author":"chris-gist","date":"2014-08-27 04:49:42 -0400","categories":["Announcements","Geospatial and Temporal","Research and Development"],"url":"fall-2014-scholars-lab-gis-workshop-series","content":"All sessions are one hour and assume attendees have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials with expert assistance.  All sessions will be taught on Thursdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community.\n\nSeptember 4th\n\n**Making your First Map**\n\nGetting started with new software can be intimidating.  This workshop introduces the skills you need to work with spatial goodness.  Along the way you’ll get a taste of Earth’s most popular geographic software and a gentle introduction to map making.  You’ll leave with your own cartographic masterpiece and tips for learning more in your pursuit of mappiness at UVa.\n\nSeptember 11th\n\n**Getting Your Data on a Map**\n\nDo you have GPS points or a list of latitude and longitude you would like to show as points on a map?  This session will show you how to turn your data into map layers and how to connect them to make lines and polygons as well.\n\nSeptember 18th\n\n**Points on Your Map: Street Addresses and More Spatial Things**\n\nDo you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis.\n\nSeptember 25th\n\n**Georeferencing – Putting Old maps and Aerial Photos on Your Map**\n\nHave an old map or an aerial photograph that you would like to use as a spatial layer?  This session will teach you techniques to properly place your data and make it useable in GIS software.  We will also demo similar techniques for Google Earth.\n\nOctober 2nd\n\n**Taking Control of Your Spatial Data:  Editing in ArcGIS**\n\nUntil we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on.\n\nOctober 9th\n\n**Collecting Your Own Spatial Data **\n\nResearch projects often rely on fieldwork to build new datasets.  In this workshop we’ll focus on tools for spatial data collection. First we’ll take a quick look behind the curtain to see how GPS really works and how to use that knowledge to our advantage.  Then we’ll evaluate free or low-cost options to gather locations and associated attributes using handheld GPS devices, smartphones, and apps.  This workshop will introduce you to a range of devices and methods for mobile spatial data collection.\n"},{"id":"2014-09-08-fellows","title":"The Fellows are Coming! The Fellows are Coming!","author":"purdom-lindblad","date":"2014-09-08 06:36:45 -0400","categories":["Announcements","Events","Grad Student Research"],"url":"fellows","content":"As the fall term begins, we are delighted to welcome our 9 (!) SLab Fellows. Ranging across 5 disciplines from the [arts, humanities, and social sciences](http://as.virginia.edu/). Our [Graduate Fellows in the Digital Humanities](/student-opportunities) and [Praxis Program Fellows](http://praxis.scholarslab.org) join a distinguished community of [past fellows](/people/).\n\n\n## Graduate Fellows in the Digital Humanities\n\n\nWe are excited about the opportunity to work closely with our [Graduate Fellows in the Digital Humanities](/student-opportunities), James, Jenny, and Emily as they develop their projects. **Please join us at their [welcome lunch and panel September 12th, at 12 PM ](http://scholarslab.org/events/2014-15-scholars-lab-graduate-fellows-panel/)in the Scholars' Lab Commons.**\n\nAs a [previous post](/2014/05/19/welcoming-our-new-scholars-lab-fellows/) noted, James, Jennifer, and Emily’s projects converge around questions of social networks and mapping.\n\n[James Ambuske’s](/people/james-ambuske/) (History) dissertation is titled “Scotland’s American Revolution: Emigration and Imperial Crisis, 1763-1803.”\n\n[Jennifer Foy’s](/people/jennifer-foy/)  (English) dissertation is titled “Mapping Sympathy: Sensibility, Stigma, and Space in the Long Eighteenth Century”\n\n[Emily Senefeld's](/people/emily-senefeld/) (History) project is titled “The Cultural Programs of the Highlander Folk School, 1932-1964.”\n\n\n## Praxis Program Fellows\n\n\nMarking our 4th year of the [Praxis Program](/student-opportunities), we warmly welcome:\n\n\n\n\n  * [Amy Boyd](/people/amy-boyd/) (English)\n\n\n  * [Swati Chawla](/people/swati-chawla/) (History)\n\n\n  * [Andrew Ferguson](/people/andrew-ferguson/) (English)\n\n\n  * [Joris Gjata](/people/joris-gjata/) (Sociology)\n\n\n  * [Jennifer Grayburn](/people/jennifer-grayburn/) (Art and Architecture)\n\n\n  * and [Steven Lewis](/people/steven-lewis/) (Music)\n\n\nPast Praxis teams developed [Prism](http://prism.scholarslab.org/), a web application for crowd-based interpretations of texts, and re-imagined [Ivanhoe](http://ivanhoe.scholarslab.org), a platform for playfully (and collaboratively) interpreting texts and artifacts.\n\nThis year’s cohort will refine Ivanhoe as well as explore our new Makerspace. With a talented and disciplinarily diverse team, we are looking forward to their dive into Ivanhoe. Keep track of their experiments on our [blog](/blog/)!\n"},{"id":"2014-09-08-realignment","title":"Realignment","author":"jeremy-boggs","date":"2014-09-08 12:05:01 -0400","categories":["Announcements"],"url":"realignment","content":"I'm happy (and relieved...and nervous) to share a redesigned and realigned web site for the [Scholars' Lab](http://scholarslab.org)!  Realignment is an apt word for these changes, I think, because they were done in an effort to better showcase the work the Scholars' Lab has been doing and the manner in which we do that work. The realignment has been a few months in the making, and there are lots of great updates to the site, so this will be a modest summary of those updates, with more posts to come detailing some of the motivations behind new features and content.\n\nWe've taken care to highlight the work and focus areas of the Scholars' Lab throughout the site. Additionally, we've added some content features for better communication and sharing of our work and events. In particular:\n\n\n\n\t\n  * We clearly delineate the SLab's areas of focus—Project Incubation, Graduate Training, Experimental Humanities, and Geospatial Scholarship—on the home page.\n\n\t\n  * We have new section of the site for our [Makerspace](http://scholarslab.org/makerspace/)!\n\n\t\n  * We're featuring splendid photographs of all our current staff and graduate fellows. (Many thanks to [Shane Lin](http://scholarslab.org/people/shane-lin/) for the photographs.) a complete list of all the people who are or have been associated with the Scholars' Lab on the [People](/people/) page.\n\n\t\n  * We maintain an archive of events on our [Events](/events/) page, with permanent pages for each event, collecting related posts, comments, and social media conversations about particular events.\n\n\t\n  * We provide a persistent but not-intrusive way to sign up for our newsletter.\n\n\nThe part I'm most proud of: We decided this summer to do what we ask our Praxis Program fellows to do each year, and devise a [charter](http://scholarslab.org/about/charter/) that summarizes the things we care about as scholars, teachers, and colleagues and spells out the ways in which we expect to conduct ourselves and our collaborations.\n\nThe design itself leans toward a more modern aesthetic than the previous design. You can read more about the technical details of the site on our [Colophon](http://scholarslab.org/about/colophon/) page, but here's a rundown of some technical and design changes:\n\n\n\n\t\n  * We spent some much needed, ongoing attention to [accessibility](http://scholarslab.org/about/accessibility/) in the site's design and markup.\n\n\t\n  * We include space for a decorative header image, one that authors can change on individual posts and pages.\n\n\t\n  * We use bigger, more readable typography throughout. (As with the previous design, I wanted to make sure that the content we share on the site, is the primary focus, with other elements serving a support role.)\n\n\t\n  * We have a main navigation that works much better across devices and screen widths.\n\n\nThere are still a few wrinkles to iron out, and if I know me half as well as I think I do, I'll be ironing those out for a while. If you see a problem with the site, feel free to leave a comment here or add a ticket to our [Github repo for our WordPress theme](http://github.com/scholarslab/labnotes/issues).\n\nComplaints and issues should be sent to me. Candy and compliments should all go to my incredible colleagues at the Scholars' Lab for content, feedback on design, terrific feature ideas, and support.\n"},{"id":"2014-09-11-ivanhoe-considerations-for-the-next-cohort","title":"Ivanhoe considerations for the next cohort","author":"stephanie-kingsley","date":"2014-09-11 11:17:41 -0400","categories":["Grad Student Research"],"url":"ivanhoe-considerations-for-the-next-cohort","content":"Welcome to the next Praxis cohort! As Purdom mentioned yesterday [in her post](http://scholarslab.org/announcements/fellows/), the 2014-2015 Praxis Fellows will continue working on the [Ivanhoe Game](http://ivanhoe.scholarslab.org/).  As last year's Praxis project manager, I frequently found myself in the position of spokesperson for Ivanhoe. People would ask me, \"What is Ivanhoe?\" and I would deliver an answer like \"Ivanhoe is a platform for making collaborative interventions in a text via role play.\" While a pretty good description of the game, this response nonetheless invites many questions—questions which my dazzled but bewildered auditors would inevitably ask:\n\n\"How do you play?\"\n\n\"Can you win?\"\n\n\"What do you mean by a 'role'?\"\n\n\"What do you mean by 'intervention'?\"\n\n\"Do you have to have read _Ivanhoe _to play?\"\n\nWell, the last question is straightforward enough (you do not have to have read the novel of that name by Sir Walter Scott to play), but the others are a bit more complex. Ivanhoe originators Johanna Drucker and Jerome McGann and SpecLab pioneers Bethany Nowviskie, Geoffrey Rockwell, and Chad Sansing [debated these very questions](http://www.ivanhoegame.org/?page_id=21) in a special issue of _TextTechnology _(12:2) in 2003, when Ivanhoe development was in full swing. Eleven years later, our Praxis cohort took up these questions while building Ivanhoe as a [WordPress](http://wordpress.org/) Theme. We designed our Ivanhoe to be as open-ended and non-prescriptive as possible, not wanting to predetermine game play in any way. But what would be wrong with exploring the possibilities of Ivanhoe, showing users what we found, and inviting users to join that conversation? Perhaps Praxers' own experimentation and transparency regarding their experiences might better guide users toward getting into Ivanhoe and trying it out themselves.\n\nI'm excited to see where the next cohort takes Ivanhoe, how they interpret it, and what kinds of games they play. But at this crucial moment before the games begin, I feel it necessary to impart a few thoughts on what our own cohort considered to be the _essential_ elements of Ivanhoe. I will begin by listing them:\n\n\n\n\t\n  1. The Text\n\n\t\n  2. Moves\n\n\t\n  3. Role Play\n\n\t\n  4. Collaboration\n\n\n**The Text**\n\n[![The player, confronted with a variety of texts](http://scholarslab.org/wp-content/uploads/2014/09/image_5-300x225.jpg)](http://scholarslab.org/wp-content/uploads/2014/09/image_5.jpg)\n\nAny Ivanhoe game begins with a text. But what constitutes a 'text'? On [the original Ivanhoe informational website](http://www.ivanhoegame.org/?page_id=21), the SpecLab researchers termed it a \"'discourse field,' the documentary manifestation of a set of ideas that people want to investigate collaboratively.\" Our cohort considered an Ivanhoe text to be merely the focus of the interpretation. This could be a literary text, a work of visual art, a piece of music, a film, or even a concept—such as a historical time period. One example of the last would be the[ Suffragette Journalism Game](http://www.scholarslab.org/grad-student-research/a-review-of-the-suffragette-game/), which took as its 'text' the 19th-century women's suffrage protests in England. To some extent, the Ivanhoe game we played on the Elgin Marbles was also a concept game, since we didn't have the marbles themselves on hand to study and interpret but rather found ourselves interpreting the whole historical legacy of the marbles. So a 'concept text' can work well for Ivanhoe, but is a concept--which may come without \"documentary manifestation\"--a legitimate text? Are there any constraints on what can be a 'concept text'? Is there a point in an Ivanhoe game at which a concept, loosely interpreted, can break down and the game drift into meaninglessness? Do you need a sense of textual integrity for the game to have coherence and meaning?\n\n**The Move**\n\n[![image_2](http://scholarslab.org/wp-content/uploads/2014/09/image_21-300x225.jpg)](http://scholarslab.org/wp-content/uploads/2014/09/image_21.jpg)\n\nThe second integral feature of an Ivanhoe game is the move, which our developer Scott Bailey succinctly defined as a \"self-conscious act of interpretation.\" These acts constitute what we like to refer to as \"textual interventions.\" The first Ivanhoe games played by Drucker and McGann, as well as the SpecLab researchers, involved actual changes made directly into the texts: Players added text, changed it, or deleted it from works such as Sir Walter Scott's _Ivanhoe _and Henry James' _The Turn of the Screw. _Our cohort began with a game on Edgar Allen Poe's \"The Tell-tale Heart,\" which, following its predecessors, involved changes to the text itself. As we found ourselves becoming more and more interdisciplinary in our aims, however, the typical move became more a commentary on the text than changes to it. Does mere _interpretation _actually _intervene _in a text?  Which term is more characteristic of an Ivanhoe move?  What are we actually doing when we make an Ivanhoe move, and how do moves make the game itself progress?\n\n**Role Play**\n\n_[Actors refused to participate]_\n\nRole play has always been the cornerstone of the Ivanhoe Game: each player assumes a role—a particular voice or critical stance from which to make moves. In the SpecLab's _A Turn of the Screw _game, players even chose aliases so that other players couldn't guess their roles; arguably, these aliases add another level of role play. But when we interpret a text, don't we automatically assume a role of some sort? When I read, I approach a text from the role of \"Stephanie reading for fun\" or \"Stephanie reading to have brilliant ideas,\" for instance—and my reading styles in these roles, alas, are very different. Thus, it is important to keep in mind the _self-conscious _in the definition of a role. These are \"_self-conscious _acts of interpretation\": your role must be deliberate and specific, and you must constantly be reconsidering your role and striving to make moves which develop that role. It is a more directed manner of analysis than simply trying to come up with points of interest for class discussion, as the Stephanie-bent-on-brilliance attempts to do. The element of self consciousness, therefore, is absolutely integral to the Ivanhoe Game.\n\nDirectly related to self consciousness, what about the idea of the role journal? The SpecLab _Turn of the Screw _game included role journals in which players commented on why they made certain moves; these journals were kept secret from other players. Additionally, players also could comment on other players' moves; these comments were public. This game had two ways players could be self-conscious of game play. I particularly want to pose this question to the next cohort: is the role journal a core feature of Ivanhoe? Our own cohort spent hours debating this question (see [Scott's excellent post](http://www.scholarslab.org/grad-student-research/role-journals-texts-pedagogy-and-pragmatism/) on the issue), but after we decided to keep the role journal, it quickly was pushed to the back-burner during development and became a secondary feature in terms of importance. Must an Ivanhoe game have an actual role journal, or is the role journal merely an emblem of the self-conscious attitude of the players?  Is its function simply to keep players adhering to their roles, or does it serve other purposes?  This is a question which I would love to see this year's cohort test.\n\n**Collaboration**\n\n[![Competition or collaboration? A constant tug-of-war.](http://scholarslab.org/wp-content/uploads/2014/09/image-300x225.jpg)](http://scholarslab.org/wp-content/uploads/2014/09/image.jpg)\n\nLastly, Ivanhoe must have players; for me, this means it is collaborative. Even if players are competing, the result is still collaboration because one player's ideas necessarily inform and impact those of the other players. This collaboration is one feature which we considered to be very important to Ivanhoe; it's what makes it a game—or as Francesca wrote [in her post ](http://www.scholarslab.org/grad-student-research/look-its-a-game-its-a-simulation-no-its-gamification/)last year, _gamification_, an idea and debate worth consideration for the next cohort but one which I will not attempt to discuss here.  Can you have a one-person Ivanhoe game?  I do not think so, but then that is really an opinion guided by my firm belief in the final--and perhaps most essential--Ivanhoe feature: FUN!  Grab some friends and have fun with collaborative criticism in an Ivanhoe game of your own!\n\n**Conclusion...?**\n\nWhat I realize as I go down the list of core features is 1) what I assumed were core features are perhaps not so self evident as I had thought, and 2) in attempting to create a list I have hovered dangerously—and somewhat tantalizingly—close to deconstructing Ivanhoe altogether. Why? What is so elusive about Ivanhoe? My neat-and-tidy _list_ ended up melting into a series of questions and the important but nonetheless vague mantra of \"fun.\"\n\nPerhaps, then, my contribution to the next cohort is not a list of Ivanhoe essentials, but a few questions which they can consider as they start their play-testing. I will be fascinated to watch their progress on the [Praxis blog](http://praxis.scholarslab.org/), and I encourage readers to follow it as well. Watch for their reflections on Ivanhoe, and even join in the discussion by leaving comments. Play an Ivanhoe game, and let us know what you think are the core features. Ivanhoe, after all, is about getting conversation started. Good luck, Praxers, and let's get gaming!\n"},{"id":"2014-09-15-about-whom-i-have-become","title":"About Whom I Have Become... ","author":"joris-gjata","date":"2014-09-15 11:33:18 -0400","categories":["Grad Student Research"],"url":"about-whom-i-have-become","content":"Hi everybody! It is not easy to talk about oneself, especially when you are not sure who your audience is. Still, I will not let the amorphous ambiguous unidentifiable audience scare and silence me. For this post, I want the main audience to be the 2014-2015 Praxis Fellows.\n\nWe are on the way to becoming a team and for this crucial achievement to become reality, time is not the only one that has to do the work. I consider introductions like this an important step towards team building. So, let me tell you what I think you need to know about me.\n\nI was born in Korca, Albania but moved to Tirana, the capital, for my high school years at the ‘Mehmet Akif’ Turkish college. I completed my undergraduate studies in International Relations and International Economics at the Middle East Technical University (METU) in Ankara, Turkey. Afterwards, I pursued a Masters degree in International Political Economy at the London School of Economics and Political Science (LSE) in London, UK. Another important experience for me was being a Visiting Scholar for nine months at the Central Asia-Caucasus Institute of SAIS Johns Hopkins University in Washington, DC. I got interested in transition economies and the role state-society relationships played in shaping or influencing the outcomes of economic policies and reforms, plus acquired skills on organizing events that involved both academics and policy makers. Now, I am in the fifth year of the Sociology PhD program, preparing a dissertation proposal on the emergence of a new form of regulation in the United States - ratings, in two fields - healthcare and finance. This project is part of my broader interest and fascination with innovation as a process of knowledge formation and manipulation.\n\nHaving worked for some time with the literature on the diffusion and implementation of innovations among organizations, I have developed the conviction that we need to better conceptualize innovation, recognizing its multidimensionality and its transformation through time. I view the Praxis Program as an opportunity to engage more actively with my ideas on innovation in an environment that embodies and ‘lives’ some of the most essential practices for creative production: collaboration, interdisciplinarity, vision and passion. I am curious and eager to learn about project-based teams in action; especially the processes through which collaboration becomes possible when anchored around an idea that is developed collectively - though in the case of Ivanhoe the idea is given to us from another collectivity and other collaborative efforts.\n\nI am looking forward to getting involved and reflecting systematically on the processes through which innovation emerges and is transformed by bringing into a community of practice people of different backgrounds like me and you. I hope this experience will help solidify my identity as a researcher that is ‘in’ not ‘out of the world’. In the Praxis Program, I want to learn about the tools that can help researchers make their ideas grow and matter. I expect to broaden my knowledge and enhance my skills on project management, team work coordination, data visualization, and effective communication of ideas to multiple audiences.\n\nThe most important thing I would bring to a collaborative digital humanities project - and a team like ours - is my rich experience with managing differences and change. As I mentioned above, I have passed through four different systems of education (Albania, Turkey, UK, US), wandered intellectually among three different fields of research (politics, economics, and sociology), and acquired valuable insights comparing different cultural configurations. My education has been in English most of the time but I have good knowledge of several other foreign languages: Italian, Turkish, French and basic German, Greek, and Russian. I hope who I have become till now can be a modest contribution to what we want us to become in the future - an effective team with a collaborative digital humanities project that addresses a diverse audience with global not only local concerns. Furthermore, having read some literature on collaboration and knowledge-sharing/transfer among teams and organizations, I believe I can bring to the project and the team more awareness of the processes that enable collaborations to succeed and be productive.\n\nWho I have become till now is due to several communities and beings that supported me - my families and friends, my husband Ali, our cats Sheqeri and Piperi, and our birds Dielli dhe Qielli, among others. I am grateful that I have the chance to continue becoming and grow in conversations with you dear 2014-1015 Praxis fellows supported by a supportive community like the Scholars' Lab.\n"},{"id":"2014-09-15-hello-world-3","title":"Hello world","author":"amy-boyd","date":"2014-09-15 06:15:58 -0400","categories":["Grad Student Research"],"url":"hello-world-3","content":"I'm Amy R. Boyd, a third-year Ph.D. student in English. I am interested in the British nineteenth century, especially the intersections between literature, science, and gender, as well as theories of the novel. I took enough computer science classes as an undergraduate to complete a minor in CS, so I'm excited to have the opportunity to put my long-dormant skills to use once again. Other than my [half-finished digital edition](http://www.amyrboyd.com/GOL/) of a portion of Lewis Carroll's _Game of Logic_, completed for Professor David Vander Meulen's course on editing and textual criticism, I have not had much of a chance to marry my work in literary studies with my more tech-y interests.\n\n\n\n\nI'm happy to be a part of this, even though I'm still not entirely certain what the Ivanhoe Game is or does or is could do in the future. I look forward to honing my coding skills on a humanties-centric project and finding my way as a digital humanist. And, of course, I'm happy to be a part of anything that gives me a chance to work with the other brilliant Praxis fellows.\n\n\n\n"},{"id":"2014-09-16-lets-play","title":"Let’s Play","author":"andrew-ferguson","date":"2014-09-16 10:10:59 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"lets-play","content":"Hi. I’m Andrew Ferguson. Three years ago, while fighting my way through English Lit exam lists full of doorstop novels and deep-end theory, I decompressed with a hobby that seemed as far from literary scholarship as could be: watching other people play videogames online, in archived or livestreamed forms called Let’s Plays, or LPs. And, somewhere along the eighth time I saw _Final Fantasy VI_ utterly broken by players with skill levels I found previously unimaginable, I realized my two preoccupations weren’t all that dissimilar, with each having much to learn from the others—and, what’s more, that it was the literary that had much more to learn from the videogames than the other way around.\n\nAll that has now fed into my dissertation, “Let’s Play: Narrative Strategies and the Playerly Text,” attempts to read a variety of 20th/21st century novels through modes of videogame play. The authors covered range from Kathy Acker to Flann O’Brien to Colson Whitehead; the games from _Portal_ to _Pac-Man_ to _The Binding of Isaac_. There’s a few chunks out in the wild: on [Vladimir Nabokov’s _Pale Fire_ and _Super Mario Bros._](http://scholarworks.iu.edu/journals/index.php/textual/article/view/5052)_; _and on [James Joyce’s _Finnegans Wake_ and _Metroid_](http://hjs.ff.cuni.cz/main/essays.php?essay=ferguson); the rest (knock wood) will be finished by the end of the year.\n\nUpshot being, I spend a lot of time watching and thinking about different ways to play things; hence, I am inordinately excited to be part of this particular year’s Praxis cohort as we think through all the different ways we can build on last year’s superb [_Ivanhoe _WordPress plug-in](http://ivanhoe.scholarslab.org/). But it’s not so much the case of leaving my own impression on a game with a distinguished lineage; rather it’s the chance to take part in something that mirrors the best of what I’ve seen in the LP community: a collective practice of building, critiquing, rebuilding, and gradually broadening the range of responses within texts and projects, as well as the practice itself and the composition of the community. It’s a mode of criticism that, even prior to joining this particular cohort, I have come to think of as “cohortative”—at once insistent, desirous, encouraging, and passionate.\n\nVery much looking forward to getting to know everyone here, especially my colleagues—and discovering along with them new modes of play.\n\n\n"},{"id":"2014-09-16-prism-in-the-classroom-questions-to-frame-discussion","title":"Prism in the Classroom: Questions to Frame Discussion","author":"brandon-walsh","date":"2014-09-16 11:53:31 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"prism-in-the-classroom-questions-to-frame-discussion","content":"Cross-posted on [my personal site](http://bmw9t.github.io/blog/2014/09/16/prism-pedagogy/).\n\nI have been touting the use of [Prism](http://prism.scholarslab.org) in the university classroom for some time now, but a recent exchange with [Annie Swafford](http://annieswafford.wordpress.com/) suggested to me that it might be worth explicitly outlining how I would go about doing so. With that in mind, I’ve composed the following set of questions for how I might frame discussion of Prism in the classroom. I’ve admittedly only had very brief chances to implement the tool in the classroom myself, so the thoughts come largely out of speculation and conversation. It should be noted as well that I assume below that you have already chosen a text and categories along which it should be marked (I may write on ways to approach such choices at a later date). In what follows, I move from general questions that I think would be helpful in framing any discussion of the tool to a particular use-case in [James Joyce’s A Portrait of the Artist as a Young Man](http://prism.scholarslab.org/prisms/4213c156-aea5-11e2-80bf-c82a14fffe99/visualize?locale=en). The former questions inform and engage my latter use-case.\n\nI prepare for class discussion by assembling a list of questions to be explored, and I would organize a Prism discussion around two lines of inquiry: tool-specific and visualization-specific. Some of these questions can be helpful for framing your own thoughts. Others could usefully be posed to the class as a whole as a means of framing discussion.\n\n**Tool-Specific Questions**\n\nHow do the tool and our framing of it affect how we read the text? How is Prism’s mode of reading different from what we normally do? Is it the same that we’ve always been doing – close reading in a different form? What are the problems with the form? Can we really boil interpretation down to a series of numbers, visualize it, and move forward? Or is there more to interpretation than that? How do individual interpretations join in with the group reading? How much is the interpretive process encapsulated in the marking of a text? The visualization? The conversation that follows? How do the terms you choose for highlighting (the facets) guide the experience of reading the text? How do the explanations you provide for those terms affect the marking experience? When do the terms break down? If the terms propose a binary, what happens to that opposition over the course of the experience?\n\n**Visualization-Specific Questions**\n\nWhich passages were marked the least for a particular category? The most? Why in either case? Which passages were particularly contentious, marked in many different ways? Where do particular categories cluster? How does the visualization show a relationship between the categories? How does your own interpretation link up to the collected visualization produced by the tool? Do the two visualizations tell us anything meaningful? Would we be able to find these meanings on our own? How does the visualization reflect the interpretive process? Why might we care more about a particular visualization for a particular reading? How is the quantified version of interpretation that Prism generates distinct from what we might learn from a discussion on our own? Can we imagine limits to this approach?\n\nThe primary job of an instructor using Prism is to help the students connect the results of the tool to the larger discussions encapsulated by the marking categories. Look at the results with a skeptical eye and ask how they can be meaningfully related to the ideas and provocations of the marking categories. My favorite early use of Prism asked users to mark [James Joyce’s A Portrait of the Artist as a Young Man](http://prism.scholarslab.org/prisms/4213c156-aea5-11e2-80bf-c82a14fffe99/highlight?locale=en) along the categories of “modernism” and “realism.” In a class, I would intersperse observations based on the visualizations with a discussion of the passage and the two marking categories. What do we mean by modernism? By realism? How is each expressed at the level of the text? What do we mean by literary experiment? By fragment? By realist details? What different genres does the text move through? Does the text construct a coherent narrative?\n\nPutting realism and modernism alongside one another in Prism forces students to reconsider the binary, which quickly breaks down in practice. We can talk about whole novels or poems as belonging to one or another category, but can we do the same for individual sentences? For words? 80% of users at the time of this writing believe that the first word of the excerpt, “once,” is modernist. But why? If you look at [the winning facet visualization](http://prism.scholarslab.org/prisms/4213c156-aea5-11e2-80bf-c82a14fffe99/visualize?locale=en), people seem primarily to be marking whole passages as one category – they are interpreting realism and modernism in chunks, not in terms of individual words. Readers tend to mark as modernist those generic changes where the excerpt suddenly adopts the form of nursery rhyme or of a fairy tale, suggesting that it is not any one genre but the shift between several in rapid succession that readers find to be modernist. The font size visualization suggests that those passages referencing physical actions by people are more likely to be associated with realist: “His father told him that story” and “When you wet the bed first it is warm then it gets cold” are marked as being especially realist. With this observation in hand, why these details? Why are the body and the bodily detail markers of a realism? Why might an association with the family suggest realism? How do they come under pressure in the face of aesthetic experiment?\n\nObviously these suggestions are just beginnings for how to approach Prism in the classroom. Many other fascinating examples have already surfaced, particularly those that use the tool to teach basic reading and foreign language skills. Get in touch if you have used the tool in your classroom! I would love to hear how you did so.\n"},{"id":"2014-09-16-the-digital-sea-of-exchange","title":"The Digital Sea of Exchange ","author":"jennifer-grayburn","date":"2014-09-16 05:59:03 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"the-digital-sea-of-exchange","content":"Greetings! My name is Jennifer Grayburn and I’m a sixth-year PhD Candidate in the History of Art and Architecture Department at the University of Virginia. My research focuses on medieval Northern architecture and its intersection with Old Icelandic texts. Using cultural memory theory and sea-basin frameworks, my dissertation explores the spread of architectural ideas, especially across the North Sea and North Atlantic. Teaching is my main passion and, over the past six years, I have worked as a library supervisor, teaching assistant, and instructor at the University of Virginia for courses like Architectural Survey I and Viking Art and Archaeology.\n\nIn the Middle Ages, the sea was the main venue for organic and fluid exchange. For us, however, this process of exchange has been accelerated by the internet and is no longer limited to physical travel. Praxis is my opportunity both to satisfy my own personal curiosity about the technology we use on a daily basis and to consider more critically how technology affects opportunities to disseminate information. Exchanging information is the foundation of the learning process and digital tools allow us to do so in increasingly interactive and decentralized ways. Ivanhoe is the ideal project to consider how digital tools and creative play can complement existing educational strategies. While Ivanhoe is text-based at this point, I'm looking forward to jumping in and seeing how we might incorporate images and other creative expressions to expand its relevance to multiple disciplines.\n\n[![112.NEF](http://scholarslab.org/wp-content/uploads/2014/09/112.NEF_-1024x319.jpg)](http://scholarslab.org/wp-content/uploads/2014/09/112.NEF_.jpg)\n\n\n"},{"id":"2014-09-17-individuality-and-collective-effort","title":"Individuality and Collective Effort","author":"steven-lewis","date":"2014-09-17 07:46:39 -0400","categories":["Grad Student Research"],"url":"individuality-and-collective-effort","content":"Hey everyone! My name is Steven Lewis. I’m a second-year Ph.D. candidate in Music. My research interests include late 20th century jazz neoclassicism, early jazz, and 19th century African-American secular music. In my most recent project, I explored jazz performance as a means of constructing counternarratives of black American music history. I earned a BA in Jazz Saxophone, and I see my practice as an improviser as an important extension of my research.\n\nJazz is a music built on a balance between individuality and collective effort. The musicians in a good jazz band bring their personal styles into conversation with each other to create something new. I see Praxis as an opportunity to engage in this sort of improvisatory dialogue with my colleagues, each of whom is coming into the program with a unique set of skills and interests. The Ivanhoe project, with its emphasis on creativity and collective play, strikes me as an exciting way to engage my interests as a jazz musician while learning about digital technologies and their implications for my own work as a humanist.\n"},{"id":"2014-09-22-a-fox-among-others","title":"A Fox... Among Others","author":"swati-chawla","date":"2014-09-22 04:19:36 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"a-fox-among-others","content":"_The fox knows many things, but the hedgehog knows one big thing._\n\nGreetings! I am Swati Chawla, a second year PhD student at the department of history. The question, “Can the masters’ tools dismantle the masters’ house?” posed at a [dhpoco](http://dhpoco.org) talk last year resonated with my work on Tibetan exile in India, and was the reason I applied for the Praxis fellowship. I am interested in better understanding technologies that help a homeless and stateless population create virtual homes in physical and digital spaces. As someone who worked at state museums and archives in Delhi, I am aware of the limitations of government-regulated brick-and-mortar institutions, and optimistic about the potential of digital spaces as collaborative, democratic and voluntary.\n\nBut what was that about foxes and hedgehogs, you might ask... The fragment quoted above is attributed to the Greek poet Archilochus. Isaiah Berlin uses it to distinguish between hedgehog-y thinkers who relate everything to a single organizing principle, and fox-y ones who pursue random, unrelated, and often contradictory ends, or seek a variety of experiences solely for what they are in themselves.  In an academic environment that organizes scholars around narrowly defined specializations (by geographical, temporal, and thematic focus in my discipline), I believe the Scholars’ Lab is one of those rare spaces that values fox-y skills. I am excited about working in the multi-disciplinary Praxis cohort, even as I am finding my way in the discipline of history after seven years of training in literary studies.\n\nAnd finally, to quote the inimitable [Andrew Ferguson](http://scholarslab.org/author/af3pj/)-- [Let's Play](http://scholarslab.org/digital-humanities/lets-play/)!\n"},{"id":"2014-09-22-visualizing-early-america-through-mapscholar-and-beyond","title":"Visualizing Early America through MapScholar and Beyond","author":"jim-ambuske","date":"2014-09-22 07:25:40 -0400","categories":["Geospatial and Temporal","Grad Student Research","Research and Development","Visualization and Data Mining"],"url":"visualizing-early-america-through-mapscholar-and-beyond","content":"Hello, DH World! As this is my first official post as a DH Grad Fellow in the Scholars’ Lab, I’d like to start it by thanking the folks in the Lab for the opportunity to join the team for this academic year. I feel really fortunate that I have the chance to hang out with bright and fun people for the next several months.\n\nNow on to the topic at hand. I'd like to talk briefly about a project developed independently of the Scholars' Lab in which I played a role, before moving on to muse how that experience will bear on my work as a new DH Grad Fellow in the SLab.\n\nIn the introduction to his remarkable work, [In the Eye of All Trade: Bermuda, Bermudians, and the Maritime Atlantic World, 1680-1783](http://uncpress.unc.edu/books/T-7164.html), the historian Michael J. Jarvis asks, “what did early America look like from the deck of a ship, and how might this perspective change the ways we understand it?”\n\nThis provocative question challenges scholars of early America to rethink how historical actors in a variety of contexts interpreted the world around them in spatial and geographical terms. A sailor traversing trade routes connecting London, Bermuda, and mainland colonial ports like Philadelphia or New York had a very different sense of the world in comparison to Thomas Jefferson atop Monticello or the [Catawba](http://memory.loc.gov/cgi-bin/query/h?ammem/gmd:@field(NUMBER+@band(g3860+ct000734))) in colonial South Carolina. What role then can the digital humanities play in our efforts to reconstruct these historical perspectives?\n\nOne solution is a new tool called “[MapScholar](http://mapscholar.org/).” MapScholar is a simple, yet dynamic interactive visualization platform that enables anyone to tell stories through the creation of digital map galleries. The program works with the Google Earth online plugin in web browsers. It gives users the ability to georeference multiple historical maps on the Google Earth globe. Archives and libraries have made a prolific number of maps available online in the last few years. This has created new opportunities for users of programs like MapScholar or [Neatline](http://www.neatline.org) to bring together different kinds of sources in new and innovative ways. In MapScholar, a number of tools permit users to annotate maps with text, shapes, images, data, and even video. Different modes allow curators to display maps as an “Atlas” or as a “Book” depending on the particular goals of the project.\n\nThis ongoing initiative was conceived and developed by professors [S. Max Edelson](http://history.virginia.edu/user/297) and [Bill Ferster](http://shanti.virginia.edu/people/40) with support from the National Endowment for the Humanities, the American Council of Learned Societies, and UVA. I’ve been extremely fortunate to work with Max, Bill, and the rest of the team over the last two years to build this program and test the limits of its possibilities.\n\nWe’ve created a site, _[Visualizing Early America: Three Maps that Reveal the New World](http://www.viseyes.org/mapscholar/?https://docs.google.com/spreadsheet/ccc?key=0AhTK6MB0cCLQdGNUSFQ1OENETUN1WHJMUWRzc0ZNMnc#gid=0)_, to demonstrate MapScholar’s capabilities. It tells the story of three key moments in early American history. The featured maps reflect European and American perceptions of colonial North America. The site also highlights some of the tools that one can use in creating and interpreting these digital galleries. I encourage you to take a look!\n\nMy work on MapScholar has informed the project I’d like to pursue during my time as a DH Fellow. My dissertation centers on the massive emigration of Scots to North America in the era of the American Revolution. Im' interested in how that migration informed Scots’ perception of the British Empire. Between 1763 and 1775 roughly 40,000 Scots left home for the colonies, and as farmers and tradesmen from both the Highlands and Lowlands removed to places like New York or North Carolina, leading figures in Scotland debated what the loss of those people meant for Scotland and the stability of the British Empire at a time when American colonists increasingly questioned their own attachment to Great Britain.\n\nI want to visualize part of this emigration phenomenon using [Neatline](http://neatline.org) in an attempt to understand how the local origin of the emigrants, their professions, and their stated reasons for leaving Scotland influenced the kind of discussions politicians and commentators had in trying to assess the potential consequences of this migration. In other words, I want to recreate their collective mental map and show how the changes in that map altered the arguments for or against emigration over time.\n\nDeveloping this project will help me to write one of the key chapters of my dissertation. The story I am telling is transatlantic in scale, and using a digital tool like the [Scholars' Lab](http://scholarslab.org/)'s Neatline to organize the geography of Scottish emigration more effectively will enable me to clarify my dissertation’s argument. And that, I think, points to the larger potential of the digital humanities. Tools like MapScholar and Neatline can inform the direction of our scholarship by bringing to “life” historical sources in new and compelling ways.\n\nI’m excited to begin my time in the Scholars’ Lab and look forward to pushing the digital envelope, especially with my Fellows in (digital) crime, [Jennifer Foy](http://scholarslab.org/people/jennifer-foy/) and [Emily Senefeld](http://scholarslab.org/people/emily-senefeld/).\n"},{"id":"2014-09-26-upgrading-neatline-and-omeka","title":"Upgrading Neatline and Omeka","author":"ammon-shepherd","date":"2014-09-26 07:53:08 -0400","categories":["Research and Development"],"url":"upgrading-neatline-and-omeka","content":"One of my first projects here at the Scholars' Lab was to help update some Omeka/Neatline sites. These are sites we keep around as examples of our [Neatline](http://neatline.org) plugin for [Omeka](http://omeka.org), and they were a few versions behind. While a pretty easy process to do by hand, having a script to take care of it makes it even easier for future upgrades, too. I call the script ONUS (Omeka Neatline Upgrade Script). While the script was written specifically for our purposes, it may be useful if you have an Omeka install using the Neatline plugin. It can be found here on github.com, feel free to use at your own risk.\n\n[https://github.com/scholarslab/onus](https://github.com/scholarslab/onus)\n\nI perhaps went a little overboard and made the script pretty robust. I was going to take the opportunity to learn some Ruby, but ended up writing it in Bash. What is Bash (or ksh or zsh or tcsh)? Bash is a shell. When you open a terminal, or command line, what you type in is called a shell. It takes your commands and does stuff with them. They are all basically the same, but have various things they do better than others, hence the great number of them. A shell script is basically a document that has a list of commands that the shell can run. Most shells allow for some logic, like if/while/case/for statements, variables, functions, etc. Usually very basic programming. (A good tutorial for learning the command line in general is found here: [http://cli.learncodethehardway.org/book/](http://cli.learncodethehardway.org/book/))\n\nI thought I could write the script very quickly in Bash since I know that language relatively well (much better than Ruby), but one thing I learned is that Bash does not handle comparing floating point numbers. Floating point numbers is a fancy way of saying numbers with decimal points, like version numbers, (ex. 2.2.1 and 1.3.1). Bash does not have an easy, default way to compare these numbers like most programming languages (like Ruby, Python, and Perl), and even other shells (like ksh and zsh).\n\nIn retrospect it probably would have been better to write this in Ruby or some \"real\" programming language from the get go, but alas I didn't have the need for comparing floating point numbers until most of the logic was already figured out and coded in Bash.\n\nI'll run through how to use the script as well as go through some of the logic for working with floating point numbers in Bash.\n\n\n## Running the Script\n\n\nThe script runs the commands needed to upgrade Omeka from 1.5.x to the latest version, and Neatline from 1.x.x to the latest version.\n\nYou would run this script on the server/computer where the Omeka installation is found. You can pass the path to the Omeka install to the script, or it will prompt you for it.\n\nThe script can take four flags/switches/options:\n\n-L : Upgrade Omeka and Neatline to the latest and greatest versions. (Note: \"Pre-2.0 versions of Neatline can't be upgraded directly to version 2.2. Upgrade to version 2.0 first!\")\n-n [number]  :  Where [number] is a valid Neatline tag from https://github.com/scholarslab/Neatline. This will upgrade Neatline to the specified version number. Note, all Neatline version have three digits x.x.x.\n-o [number]  :  Where [number] is a valid Omeka tag from https://github.com/omeka/Omeka. This will upgrade Omeka to the specified version number.\n-s  :  Do not upgrade Omeka\n-t  :  Do not upgrade Neatline\n\n\n### Basic Usage:\n\n\n\n\n\n\t\n  * Download the onus.sh file from the github repo. It doesn't really matter where you put this script, but your home directory is a good location.\n\n\t\n  * On the command line, enter the directory where the file is located and type the following command. This will allow you to execute the script.\n\n\t\n    * \n\n    \n    chmod u+x onus.sh\n\n\n\n\n\n\n\n\t\n  * If needed, change any default variables at the top of the file, ex. paths to MySQL, PHP, and git. The default is to use your account's system default. This may be different based on your system, if for example you are testing with [MAMP](http://www.mamp.info/de/) on your Mac computer.\n\n    \n    MYSQL=\"/path/to/bin/mysql\"\n    MDUMP=\"/path/to/bin/mysqldump\"\n    MADMIN=\"/path/to/bin/mysqladmin\"\n    PHP=\"/path/to/bin/php\"\n\n\n\n\n\t\n  * Change one 'sed' line if needed, to work with GNU/Linux. Remove the empty double quotes after -i\n\n\t\n    * \n\n    \n    sed -i \"\" \"80s/.*/${migrate}/\" ${path}/plugins/Neatline/migrations/2.0.0/Neatline_Migration_200.php\n\n\n\n\n\n\n\n\t\n  * Run the script by typing the following at the command promp; make sure to type the period ( . ) before the forward slash ( / ) and then the script name.\n\n\t\n    * \n\n    \n    ./onus.sh /path/to/omeka/install\n\n\n\n\n\n\n\n\nThis will upgrade Omeka and Neatline to the next available major release. Run the script as many times as needed to get to the latest version.\n\nFor more examples, please visit the GitHub page at [https://github.com/scholarslab/onus](https://github.com/scholarslab/onus)\n\nNOTE: One important thing to be aware of is when you  upgrade from 1.5.x versions of Omeka and 1.x.x versions of Neatline the Neatline functionality will be broken until Omeka and Neatline are upgraded to the very latest stable versions. So if you plan on using this script, then plan on upgrading to the very latest versions!\n\n\n## \n\n\n\n\n## Getting Back Neatline Exhibits After Upgrading to 2.0.0\n\n\nWe noticed one big problem when upgrading sites with versions of Omeka and Neatline previous to 2.0.0 by hand. This section will detail the steps to fix it by hand for demonstration purposes, but they are included in the script.\n\nOmeka and Neatline both go through some significant database (and code) changes from 1.5.x to 2.x.x. The biggest seemed to be that the upgrade script for Neatline didn't \"take\" and needed to be done manually. For these sites, the Neatline Exhibits did not get transferred from the old database table to the new table.\n\nThe first step is always to make a backup copy of the database and files. That way if anything goes awry, you can easily put things back together.\n\n\n\n\t\n  1. To back up the database, simply take a MySQL dump.\n\n    \n    mysqldump -uUserName -pPassword databasename > databasename.sql\n\n\nDo this in the main directory of Omeka. Then make a zip file of the entire Omeka directory.\n\n    \n    zip -r omeka-backup.zip /path/to/omeka/\n\n\n\n\n\t\n  2. Next, deactivate any plugins, including Neatline and NeatlineMaps. One of the big changes with the 2.x.x version is that NeatlineMaps is rolled into Neatline.\n\n\t\n  3. Grab a 2.0.x version of Omeka. Either do this with github\n\n    \n    git clone https://github.com/omeka/Omeka NewOmeka\n\n\nor with a zip file.\n\n    \n    wget http://omeka.org/files/omeka-2.0.4.zip\n    unzip omeka-2.0.4.zip\n\n\n\n\n\t\n  4. Add the 2.0.0 version of Neatline plugin into the NewOmeka/plugins directory, along with any other upgraded plugins you may need. NeatlineText, NeatlineSimilie and NeatlineWaypoints may be needed if you used that functionality in the previous version.\n\n\t\n  5. Copy the db.ini file from the old installation to the NewOmeka/ directory.\n\n\t\n  6. Now load the admin page for NewOmeka/ in the browser: http://domain/NewOMeka/admin/. Upgrade the database and login to upgrade and reactivate the Neatline plugin and other plugins as needed.\n\n\t\n  7. You may notice things go smoothly, except the existing Neatline exhibits may not transfer. To get them into the new database tables, add the following two lines at line 80 in the NewOmeka/plugins/Neatline/migrations/2.0.0/Neatline_Migration_200.php file:\n\n    \n    $fc = Zend_Registry::get(\"bootstrap\")->getPluginResource(\"FrontController\")->getFrontController();\n    $fc->getRouter()->addDefaultRoutes();\n\n\n\n\n\t\n  8. Run the following database command to allow the background process to run:\n\n    \n    mysql -uuser -ppassword database --execute=\"UPDATE prefix_processes SET status='starting' WHERE id=1;\"\n\n\n\n\n\t\n  9. Run the following php command to get the processes started.\n\n    \n    /path/to/bin/php /path/to/NewOmeka/application/scripts/background.php -p 1\n\n\n\n\n\t\n  10. Finally, if everything in the new version looks good, you can remove the old and replace it with the new.\n\n    \n    mv /path/to/omeka/ /path/to/old-omeka/\n    mv /path/to/NewOmeka /path/to/omeka\n\n\n\n\n\n\n\n## Some Script Logic\n\n\nInitially, I used the script to upgrade both Omeka and Neatline to the next higher version, going through every single minor version incrementally. When upgrading from Omeka 1.5.1 and Neatline 1.0.0 to the latest versions (2.2.2 for Omeka and 2.3.0 for Neatline), I had to run the script over 20 times!\n\nThat was way too labor intensive and time consuming, so next I added some logic to just skip to the next major release. That dropped the times needed to run the script down to four. This is how the script behaves if run without any options.\n\nBut I could do better than that! I added in some command line options/flags that allow you to upgrade to any Omeka or Neatline version you specify. Now you can upgrade from Omeka 1.5.x and Neatline 1.x.x directly to Omeka 2.0.4 and Neatline 2.0.0, then right to Omeka 2.2.2 and Neatline 2.3.0. Two steps!\n\n\n## Bash and floating points\n\n\nAs mentioned above, Bash does not work with floating points, so I had to create a function to deal with that. Dealing with version numbers, especially with minor version numbers kind of requires the need to compare floating point numbers...\n\nIn the script I use two different functions:\n\n    \n    # Compare two floating point numbers.\n    # Usage:\n    # if $( compare_floats number1 number 2 ); then\n    #     echo 'number1 is less'\n    # else\n    #     echo 'number2 is less'\n    # fi\n    #\n    # result  : the string 'true' or 'false'\n    # number1 : the first number to compare\n    # number2 : the second number to compare\n    # Read it as: is number1 less than number2? It returns 'true' if number1 is\n    # less, and 'false' if number1 is greater.\n    function compare_floats() {\n        echo | awk -v n1=$1 -v n2=$2 '{if (n1<n2) printf (\"true\"); else printf (\"false\");}'\n    }\n    \n\n\nThis function basically compares two numbers. It outputs true if the first number is less than the second number, and false if the first number is greater than the second number. Another way to think about it would be to ask the question, is the first number less than the second number? It returns 'true' if that is true, and 'false' if that is false.\n\nThe function is basically echoing the result of the awk command, so let's look at what it does a bit more closely.\n\n    \n    awk -v n1=$1 -v n2=$2 '{if (n1<n2) printf (\"false\"); else printf (\"true\");}'\n\n\nFirst we call the awk command with two 'variable' flags. The -v says that the next expression sets a value to a variable, so n1 is the variable and $1 is the value. The $1 and $2 are actually variables themselves. When you call this function later in the script, you pass it two numbers. These numbers are automatically assigned to variables, the first number to $1 and the second to $2.\n\nThe next part of the awk command processes an if/else statement; the part within the single quotes and curly braces. The single quotes are required by Bash, and the curly braces tell awk to process the action. The part within the braces is the basic if/else statement. If the comparison is true, then do the first step; else/otherwise do the next step. So, in our case, if the first number (n1) is less than the second number (n2), then print \"true\", otherwise print \"false\".\n\nSee here if you are interested in learning more about the ways of awk [http://www.grymoire.com/Unix/Awk.html#uh-1](http://www.grymoire.com/Unix/Awk.html#uh-1).\n\nThis function is used in two ways in the script. First, it just does a basic check in an if statement. Check if this number is less than that number:\n\n    \n    if $( compare_floats $n_upgrade 2.0.0 ); then\n\n\nThis checks if the next Neatline version is less than 2.0.0. If that is true, it runs some commands.\n\nSecond, we can use the function as part of a multi conditional check:\n\n    \n    if [[ -d $path/archive/ && $( compare_floats $o_upgrade 2.0.0 ) == \"true\" ]]; then\n\n\nHere we check if the /archive/ directory exists (used with Omeka versions less than 2.0) and we also check if the next version to upgrade Omeka to is less than 2.0.0. If both of those conditions are met, then we can run some code. Otherwise we do some more conditional checking and different code running.\n\nAnd, the next function...\n\n    \n    # Pass the current version first, then the array\n    # the function echoes the version just greater than the current version,\n    # i.e., the next version to upgrade to.\n    #\n    # Usage:\n    # variable=$( get_next_version $num array[@] )\n    #\n    # variable : the next version greater than $num\n    # $num : the current version\n    # array[@] : an array of all possible versions\n    function get_next_version() {\n        num=$1\n        declare -a ARRAY=(\"${!2}\")\n        for i in ${ARRAY[@]}\n        do\n            if awk -v n1=$num -v n2=$i 'BEGIN{ if (n1<n2) exit 0; exit 1}'; then\n                echo $i\n                break\n            fi\n        done\n    }\n    \n\n\nWith this function we are doing a similar thing, comparing two numbers, but we are comparing one number against a list of numbers. To run this function you pass the current version and a list of possible version numbers. The function will compare the number you pass it with the list, and echo the next highest number from the list.\n\nLet's break down this function as well.\n\n    \n    num=$1\n    declare -a ARRAY=(\"${!2}\")\n    \n\n\nThese first two lines are simply getting the input from calling the function and turning them into an internal variable and an internal array. You may remember the '$1' is the first number passed to the function. But where is the '$2'? It's expanded and changed a bit because it is actually an array, or a list of numbers, rather than a single number. In Bash, you can write a variable with a dollar sign (ex. $myNumber) or with curly braces (ex. ${myNumber}). The second option allows you to string multiple variables together and do basic string or array manipulation. For example we could put two variables together: ${myNumber}${anotherNumber}\n\nIn this case we are declaring a new array named 'ARRAY' and pre-populating it with the values of the passed array. We do some indirect expansion to get the values of the passed array, that's the \"${!2}\" part. Basically, this part says set the values of the array passed as the values of the new array (in this case we call it 'ARRAY'), rather than setting the name of the array as the value of the new array. See here for more explanation on Bash arrays and indirection [http://wiki.bash-hackers.org/syntax/arrays#indirection](http://wiki.bash-hackers.org/syntax/arrays#indirection)\n\nNext we do a standard for loop to go through every element or value of the array.\n\n    \n    for i in ${ARRAY[@]}\n\n\nNext we check the current version number against each number in the list of version numbers, using the same awk command as before.\n\n    \n    if awk -v n1=$num -v n2=$i 'BEGIN{ if (n1<n2) exit 0; exit 1}'; then\n\n\nIn this case, though, instead of printing 'true' or 'false' the command exits without errors (exit 0) or exits with an error (exit 1). The if statement that includes the awk command will execute the code within the if statement only if the statement returns true, that is, if the awk command exits without errors. So, if the first number is less than the second number, then exit without errors (true) and echo the second number. This becomes the next version number.\n\nBasically you can think of this as getting the current version number, then comparing this to a list of all possible version numbers, starting with the lowest number. When the current version is greater than the possible version number, do nothing. When the current version is not less than the possible version number, then the possible version number is greater, and therefore should be the next version to upgrade to. The break statement within the for loop tells the for loop to stop looping when it has found the next version number.\n\n\n\n\n## Conclusion\n\n\nFew, that was a long winded explanation of the script. The two above functions, and accompanying explanation, could have been avoided by using a programming language (like Ruby, Python, or Perl) instead of a shell script because they handle floating point comparisons naturally.\n\nSo, just to summarize, if you have Omeka and Neatline installed already, and would like to upgrade to the latest version, then you can run this script on the server where Omeka is installed. It requires that you have git installed. All of the other programs it depends on are pretty standard.\n"},{"id":"2014-09-26-what-the-junk","title":"What. The. Junk.","author":"jeremy-boggs","date":"2014-09-26 10:07:28 -0400","categories":["Experimental Humanities"],"url":"what-the-junk","content":"So I had to take a sick day yesterday. Stuffy nose, scratchy throat, headache, grouchiness. Attempting to brighten my day, Wayne sent me a nice text message:\n\n[caption id=\"attachment_11169\" align=\"aligncenter\" width=\"640\"]![Extra parts](http://scholarslab.org/wp-content/uploads/2014/09/extra-parts.jpg) Extra parts[/caption]\n\nTurns out he was joking. He actually said \"totally\" when I asked, but that turned out to be a lie. Today, here's what I came back to:\n\n![Not joking sticky note attached to the printer, with message that it is out of order](http://scholarslab.org/wp-content/uploads/2014/09/out-of-order-1024x768.jpg)\n\nSo OK, maybe it's not as bad as it seems. Most of the time when we've clogged up the extruder, it's because we've leveled the build platform too close and caused it to scrape against the extruder, essentially mashing up the filament on the nozzle. Unloading and reloading the filament usually solves this, followed by running the leveling utility again.\n\nI tried all that this time, but to no avail. [What. The. Junk.](http://undergroundcliche.blogspot.com/2014/07/lumberjanes-what-junk.html)\n\nSo as is typical of my kin, I feared the worse. I expected to spend a ton of time taking the extruder apart, soaking the nozzle in acetone or some other crazy substance to dissolve the filament that had built up in the nozzle. (This instructable has a good breakdown of the [steps to unclog a 3D printer nozzle](http://www.instructables.com/id/Clogged-MakerBot-Nozzle/).) With a heavy sigh that is also typical of my kin, I proceeded to disassemble the extruder.\n\nFirst thing you need to do, of course, is turn off and unplug the printer. You'll touch wires and things, and its better to be safe than sorry. Then you have to take off the fan assembly in front of the extruder, and pull out the drive assembly:\n\n[caption id=\"attachment_11164\" align=\"aligncenter\" width=\"1024\"]![Printer carriage with fan and stepper motor removed.](http://scholarslab.org/wp-content/uploads/2014/09/disassembled-1024x768.jpg) Printer carriage with fan and stepper motor removed.[/caption]\n\nAt this point, I took a look at the drive assembly, which in our case includes the fabulous [spring-loaded arm replacement](http://scholarslab.org/experimental-humanities/reprinting-printed-parts/) for the drive. I noticed something a bit odd:\n\n[caption id=\"attachment_11168\" align=\"aligncenter\" width=\"1024\"]![Stepper motor with spring-loaded arm](http://scholarslab.org/wp-content/uploads/2014/09/stepper-motor-assembly-1024x768.jpg) Stepper motor with spring-loaded arm[/caption]\n\n[caption id=\"attachment_11167\" align=\"aligncenter\" width=\"1024\"]![Close up of drive block.](http://scholarslab.org/wp-content/uploads/2014/09/stepper-motor-assembly-detail-1024x768.jpg) Close up of drive block showing obstruction[/caption]\n\nTurns out there was junk in it. What the junk indeed. I pushed the arm down to take some tension off the drive, and used a knife to wiggle the bit of plastic that had gotten lodged in there.\n\n![plastic](http://scholarslab.org/wp-content/uploads/2014/09/plastic-1024x768.jpg)\n\nNow it's working like new. Or at least like it did before it got clogged. Not sure how this little piece of plastic got stuck in here, but my guess is that it broke off the end of the filament when unloading it. The nozzle wasn't clogged at all, but this was preventing new filament from going completely through the drive assembly into the nozzle.\n"},{"id":"2014-10-03-bonjour-je-mappelle-julia","title":"Bonjour! Je m'appelle Julia.","author":"julia-schrank","date":"2014-10-03 09:20:39 -0400","categories":["Experimental Humanities"],"url":"bonjour-je-mappelle-julia","content":"Bonjour, Laboratoire des Savants!\n\nHello! I'm Julia, one of the new Makerspace student consultants. When I'm not being a smiling face at the SLab desk, I am a first year M.A.-Ph.D. student in French. I am brand-new to Charlottesville and to UVa, and so far I'm loving everything, particularly this miniature Tour Eiffel, which was my first ever 3D print!\n\n[caption id=\"attachment_11118\" align=\"aligncenter\" width=\"225\"][![Julia holding a tiny, black 3D-printed Eiffel Tower.](http://scholarslab.org/wp-content/uploads/2014/09/photo-e1411154953963-225x300.jpg)](http://scholarslab.org/wp-content/uploads/2014/09/photo-e1411154953963.jpg) Just look at it. Seriously.[/caption]\n\n3D printing is definitely what brought me to the Makerspace, since I'm a self-professed Material Girl. This moniker describes me in my everyday life but also me as a researcher: I am obsessed with objects. The period I hope to study is centered around 1900 in Paris, which as you may know was the year the city of light hosted the World's Fair. French Studies scholars call this period the Belle Époque, which translates literally to \"the Beautiful Period.\"\n\nThe Belle Époque was given its name for many reasons, but my own personal favorite has to do with the glut of extremely beautiful, refined objects created in this era. However, since I am not yet considered cool enough by Paris museums to touch and examine the extant objects from this period, I have to find another way to gain the tactile information I feel that I need to appropriately assess the value of these items. This is where 3D printing can meet my research needs in a spectacular, unprecedented way.\n\nIn my time at the Makerspace, I hope to make the untouchable touchable, and invite friends, colleagues, students, and Scholars' Lab visitors to hold pieces of Paris's past in the format of the future. I also hope to bring to life design ideas from my favorite artists of the period and recreate fantastical objects that were lost to history. It will be a long journey for me, as I am starting at square one with this technology, but I hope it is one I can share with all of you to help you come closer to solving your own research questions with the help of our Makerbots.\n\nTo start this mutual journey, I'd like to leave you all with a stanza from a song by Regina Spektor that addresses the consciousness of objects in a museum:\n\n\n<blockquote>\"First there’s lights out, then there’s lock up\nMasterpieces serving maximum sentences\nIt’s their own fault for being timeless\nThere’s a price to pay and a consequence\nAll the galleries, the museums\nHere’s your ticket, welcome to the tombs\nThey’re just public mausoleums\nThe living dead fill every room\nBut the most special are the most lonely\nGod, I pity the violins\nIn glass coffins they keep coughing\nThey’ve forgotten, forgotten how to sing\"</blockquote>\n\n\nLet's free the objects from their cages and let them sing again in glorious 3D! Allons-y!\n"},{"id":"2014-10-11-minard-napoleon-neatline","title":"Minard + Napoleon + Neatline","author":"david-mcclure","date":"2014-10-11 05:15:56 -0400","categories":["Geospatial and Temporal"],"url":"minard-napoleon-neatline","content":"_[Cross-posted from [dclure.org](http://dclure.org/logs/minard-napoleon-neatline/)]_\n\n\n\n## [**Open the Exhibit**](http://dmmh.dclure.org/neatline/show/minard)\n\n\n\n[![minard-overview](http://scholarslab.org/wp-content/uploads/2014/10/minard-overview-1024x619.jpg)](http://dmmh.dclure.org/neatline/show/minard)\n\nYesterday I made the hop across the country to Boston for the [NEH Workshop on Digital Methods for Military History](http://www.northeastern.edu/nulab/dmmh/) at  Northeastern University, where I'll be giving a couple of workshops about [**Neatline**](http://neatline.org/) and soaking up lots of interesting new projects from old friends and new friends alike. Beautiful fall foliage aside, I'm very excited about this! Ever since we worked on the Hotchkiss projects back in 2012, Neatline and military history have been a great pairing. Which is no accident - military history is often about how things change over time on maps, which is basically the tag line for Neatline. In fact, I've always assumed that military history played a pretty big role in inventing the whole vocabulary of visual concepts and techniques that have been picked up by digital tools like Neatline - the flowcharts, the arrows, the diagrammatic ways of representing how things move from one place to another. Hotchkiss was using colored pencils to scratch out annotations onto his battle maps back in the 1870s, which is more or less exactly what Neatline is, minus the computer screen.\n\nAnyway, when I started putting together the workshop, I decided to use this as an excuse to build out a little Neatline exhibit that I've been rolling around in my head about for about three years - an interactive version of [Charles Minard's classic flow diagram of Napoleon's 1812 invasion of Russia](http://en.wikipedia.org/wiki/Charles_Joseph_Minard#Work). This is not an original idea. Minard's map is sort of like the \"Stairway to Heaven\" of digital mapping and information visualization, and it's been remade digitally dozens of times. But, I decided to give it try, and see if I could find some kind of interesting riff. I started out by georeferencing a scan of Minard's diagram, and then traced out vector annotations on top of each of the individual segments that represent the deteriorating size of the Grande Armée over the course of the invasion:\n\n[![segment-annotations](http://scholarslab.org/wp-content/uploads/2014/10/segment-annotations.jpg)](http://scholarslab.org/wp-content/uploads/2014/10/segment-annotations.jpg)\n\nOnce the basic structure in place, I realized that I didn't really have an intuitive sense of the _scale_ of the whole thing - how far was it from the Neman River to Moscow? How long did it take? It turns out that the march was about 540 miles in each direction. I fired up Illustrator, blocked in a little ruler-like shape, and dragged out the annotation along the top of the map:\n\n[![540-miles](http://scholarslab.org/wp-content/uploads/2014/10/540-miles-1024x178.jpg)](http://scholarslab.org/wp-content/uploads/2014/10/540-miles.jpg)\n\nThen, the question of time. This seemed like a good excuse for some d3. I made a little chart on the right side of the screen that plots out the size of the French army over the course of the ~5-month interval of time between when Napoleon crossed the Neman on June 24 and when that last little bit of the army stumbled back out of Russia in December 14. Then, in the exhibit theme, I wrote a little bit of Javascript that wires up the graph with the vector annotations on the map - hover on the graph to highlight the corresponding blocks in Minard's diagram, and click to zoom to that location:\n\n[![graph](http://scholarslab.org/wp-content/uploads/2014/10/graph-1024x979.jpg)](http://scholarslab.org/wp-content/uploads/2014/10/graph.jpg)\n\nAnyway - simple but fun. I'm looking forward to spending the next two days learning from people who actually know something about military history!\n"},{"id":"2014-10-21-come-explore-the-makerspace","title":"Come explore the Makerspace!","author":"ethan-reed","date":"2014-10-21 13:01:11 -0400","categories":["Experimental Humanities"],"url":"come-explore-the-makerspace","content":"Hello! My name is Ethan Reed. I’m a second-year PhD candidate in UVa’s Department of English, and one of the Student Assistants in the Makerspace at the Scholars’ Lab.\n\nFor me, maker technology represents a powerful opportunity to change the ways we teach and the ways think. This may seem obvious, but for me the Makerspace is also a learning space as well as a thinking space. I think that even as people perform research there and investigate problems, that process is itself a kind of critical work. Fortunately there is a word for this lovely concept: critical making.\n\nIn[ “Critical Making: Conceptual and Material Studies in Technology and Social Life,”](http://www.tandfonline.com/doi/pdf/10.1080/01972243.2011.583819) an article by Matt Ratto (that I found through [this thoughtful and informative post](http://maker.uvic.ca/scholarship/) by Jon Johnson at UVic’s Maker Lab), Ratto takes a minute to look at how something like critical making might address the “disconnect between deterministic, conceptual understandings of the role of technology in social life, and the more material and nuanced understanding of how one relates to them.” He then puts the goal of it as follows:\n\n“Our goal is therefore to use material forms of engagement with technologies to supplement and extend critical reflection and, in doing so, to reconnect our lived experiences with technologies to social and conceptual critique.”\n\nThis is awesome! I couldn’t agree more, and find Ratto’s idea to be one of the main ways I think about our Makerspace at the Scholars’ Lab. Having an end product is great, but that making process can be just as, if not more important.\n\nAs I connect this to my own research on things like global literary networks, systems of cultural value, and how they connect to the individual authors and texts that interact through them, knowing how real examples of networks and systems operate seems crucial. More generally, I think that leaving technology “black-boxed” and not knowing how it works under the hood has material consequences in how authors write about technology and how we read, think, and talk about what they’ve written.\n\nI also think it would be in bad faith not to mention simply how much fun it is to tinker with these technologies. There is a specific kind of joy in hearing and smelling a [Makerbot ](http://www.makerbot.com/)hard at work printing something found on [Thingiverse](http://www.thingiverse.com/). Watching it offers that familiar kind of meditative hypnosis inspired by campfires burning wood or washing machines spinning clothes. Of course, the best part is then learning how and why it all works the way it does, getting your hands on the thing to tinker with it for a while and think about the results.\n"},{"id":"2014-11-06-rules-and-flexibility-learning-from-games-for-ivanhoe","title":"Rules and Flexibility: Learning from Games for Ivanhoe","author":"joris-gjata","date":"2014-11-06 09:02:21 -0500","categories":["Grad Student Research"],"url":"rules-and-flexibility-learning-from-games-for-ivanhoe","content":"Do we want well-defined rules and roles? Do we want them to be fluid? Can rules and roles provide creative fluidity and playful flexibility? These questions have been a recurring theme in our conversations with the Praxis team as well as in our meetings with some of the Scholars Lab members.\n\nFor me, questions about the importance of rules and roles emerged after our first introduction with [the Ivanhoe project](http://ivanhoe-staging.herokuapp.com) - when we learned about its history and vision. My immediate reaction to Ivanhoe was that it aims to do too much as it is meant to be anything you want. Thus, I thought the freedom it offered to users was theoretically attractive - as it allowed them to choose having rules or not, and put no limits to their expression of creativity - but not useful in helping them act and keep the game going. This 'freedom through design' unfortunately seemed to unintentionally limit players as it did not offer a clear understanding of what the purpose of the game was. Players risked isolating themselves and others with their moves: less interaction with others also made the game feel less of a game and more of another-thing-on-my-to-do-list.\n\nToo few rules, too much fluidity and ambiguity! This was my first thought on why I was confused and suspicious about Ivanhoe as a game. I was given too many options and were left with so many decisions to make. As a user/player I was not even asked the questions - e.g. do you want to play a game with rules or without rules? - I had to think of them myself. The challenges in playing Ivanhoe as a game for me seemed to come out of too much complexity, a lot of fluidity, unlimited flexibility and not clearly defined purposes of use. These feelings and thoughts could be just specific to my experience or because of my culture. As some researchers have noticed, the ability to perceive choices and the understanding of freedom changes from culture to culture (watch this fascinating TedTalk on [The Art of Choosing ](http://www.ted.com/talks/sheena_iyengar_on_the_art_of_choosing?language=en))\n\nAs the 2014-15 Praxis team, we discussed this issue briefly during the process of drafting our charter. Previous charters had referred to the fluidity of roles and rules. While we understood the purpose of this statement - to allow for a processual definition of roles and rules and not assume that you know from the start who you are and what you do best - i.e. acknowledging individuals' becoming rather than being - we were skeptical about the assumption it implied, that clearly defined rules and roles are more detrimental to creative work. This week we have to decide in what direction we want to go with the Ivanhoe project and what roles we are interested in taking to realize our goals within the Praxis program. The question of defining rules and roles has reemerged recently in our discussion about what aspects of design and development we want to concentrate on in order to make Ivanhoe simpler and more accessible.\n\nIvanhoe seems to require some kind of shared interest that makes individual players feel part of a group, a team or a greater collectivity brought together by the game. In its actual development and conceptualization, this game is different from [Kari Kraus and her collaborator's game DUST](http://argdust.weebly.com/uploads/2/1/8/1/21813550/dust_creative_brief_nasa_compressed.pdf), as it lacks a clearly defined purpose, a particular audience and a narrative. It resembles much more a game like [Minecraft](http://en.wikipedia.org/wiki/Minecraft) in the flexibility of use it gives players. Nevertheless, even when compared to Minecraft, it lacks some design features that could make it an engaging game, one that keeps you in for a considerable time: visualization of the flow of the game i.e. a story line and extensive documentation that offers different scenarios for play. In this context, if I would have to choose between enhancing the playfulness and game-ness of Ivanhoe through new design features _within_ the game - e.g. through turn-taking and visualization of the path/progression of a game through a map - and motivating audience through the provision of extensive _documentation_ of different games and options available in this platform, I would prefer the former. I think it is important to acknowledge the power of well-designed rules and their ability to enable creativity and learning. Rules can be seen as channeling flexibility and fluidity into useful streams and productive avenues. We cannot assume the playfulness will break in and make Ivanhoe a game that keeps you involved and excited. Definitely, rules cannot be perfect but they can be a good starting point for action.\n\nAs [Joseph Schumpeter](http://en.wikipedia.org/wiki/Joseph_Schumpeter) and [Karl Polanyi](http://en.wikipedia.org/wiki/Karl_Polanyi) note in their works - _Capitalism, Socialism, and Democracy_ and _The Great Transformation_ respectively -in advanced capitalist societies like the United States, regulation tends to be seen with suspicion because it is viewed as in opposition to freedom and democracy - much valued principles in American culture. They pointed out to the problems of propagating this myth that tended to silence any debate on the design of rules and different ways of organizing life. I see their point also highlighted by [Abbott and Snidal (2000)](http://www.jstor.org/stable/2601340) in their article theorizing the use of hard law and soft law in the international context. Among others, they argue that less formalized rules i.e. soft law are a temporary mechanism of gaining time needed to negotiate large differences among parties and that in international negotiations, developing countries preferred hard law i.e. more formalized rules because soft law tended to benefit much more those already in power.\n\nIn short, don't fear the rules! Let's talk more about the KIND of rules and regulation we want and need to best realize our objectives. Regulation is a necessary enabling mechanism. Maybe to further develop Ivanhoe as a game, we need to reflect on our own game experiences: what kept you engaged in the game? Was there a particular element of that game that made you stay active as a player? Were there rules that enabled a sense of community or shared purpose to drive the game? What kind of rules make a game good or bad for you?\n"},{"id":"2014-11-07-on-not-knowing-what-im-doing","title":"On Not Knowing What I'm Doing","author":"steven-lewis","date":"2014-11-07 06:50:00 -0500","categories":null,"url":"on-not-knowing-what-im-doing","content":"A couple of weeks ago, Jeremy and the rest of the Scholars’ Lab staff helped us learn the basics of HTML, CSS, and Git. Then I promptly forgot them. Then, with wonderfully patient help from the staff, I remembered them, only to forget them again. After putting my limited HTML and CSS knowledge to good use working on my web page, I’m cautiously optimistic that I won’t take all semester to figure out what I’m doing.\nLike many people, I spend most of my time doing things that I already know how to do. Doing things that I’m already good at doing makes me feel good about myself. The culture of graduate school only reinforces this habit. I’m sure that I’m not alone in admitting that, when a professor or colleague in one of my seminars mentions a scholar whose work I don’t know, I often mumble that I’ve “read a little bit of” their work or respond with a sage nod of affirmation, or a knowing grunt. There’s an overwhelming pressure to know everything about everything, or to fake it until you do.\nAll of this has made the past few touch-and-go weeks of basic HTML, CSS, and Git alternately terrifying and liberating. It’s been a very long time since I’ve learned a completely new skill and been totally unsure of what I’m doing. I can’t remember the last time I approached someone after a lesson, looked them in the eye, smiled sheepishly, and asked them to re-explain everything that they just covered, but more slowly, please. At the same time, being in an environment where I’m encouraged to ask questions, make mistakes, and play around with concepts makes me feel like a child in the best way possible.\n\nI just hope that my completed web page looks like it was made by a grown person and not an eleven year old who audited a computer science class.\n"},{"id":"2014-11-11-steps-taken","title":"Steps Taken","author":"andrew-ferguson","date":"2014-11-11 11:02:47 -0500","categories":["Grad Student Research"],"url":"steps-taken","content":"It’s a busy time around the Praxis Lab. At the moment our attention is divided between conceptual thought on the future of Ivanhoe, and practical education in the basic tools we’re going to need to carry out any of our concepts—plus, of course, the little external distractions of classes, comprehensive exams, job applications, etc.\n\nAs a result, everything feels very baby-steppy at this point, such that I think it’s worth considering and celebrating the steps we have taken and realizing that they may not be so small as they seem.\n\nFirst, we have now delivered [our charter](http://praxis.scholarslab.org/charter/charter-2014-2015/). This document showcases the collaboration and camaraderie we have developed in these opening months, between movie viewings, foodie chat, and coffee machine queues. While the charter is important of itself, it’s also something we know that we can revisit as needed; it’s the interpersonal bonds we will be relying on to carry us through the project development.\n\nSecond, we have all established a web presence, including rudimentary personal websites (my own of which I’ll link once I’m a _little_ more confident about it…). This shows off an initial development of skills in HTML, CSS, and Git that will strengthen with use over the Praxis year and beyond.\n\nThird, we are already making plans for presentations about aspects of Ivanhoe, especially for next summer—while many deadlines are still several months off, [DH2015](http://dh2015.org/)’s just hit a few days back, and we put in a poster-presentation abstract in hopes both of having it approved, and also of figuring out over the next 6 months what we’ll actually have to show in Sidney.\n\nFinally, we’re all coming to grips with our thoughts about what Ivanhoe is, and what it could be—between lots of talks about the nature of play, the differences between games and platforms, and the pitfalls of [gamification](http://bogost.com/writing/blog/gamification_is_bullshit/) (that may just be me, more in a future post).\n"},{"id":"2014-11-14-podcast-kari-kraus","title":"Podcast: Kari Kraus on Humanistic Design","author":"laura-miller","date":"2014-11-14 07:36:51 -0500","categories":["Events","Podcasts"],"url":"podcast-kari-kraus","content":"**F****inding Faultlines: An Approach to Humanistic Design**\n\nHistorically we know that many new technologies have inherited parts from prior technologies. The skateboard remediated the surfboard; the camera pilfered from the gun; the telephone harvested batteries and wires from the telegraph; and early models of the phonograph borrowed the treadle mechanism of the sewing machine. In each of these instances, the logic of part-whole relationships governs the design. “Many of a technology’s parts are shared by other technologies,” notes Brian Arthur in The Nature of Technology, “so a great deal of development happens automatically as components improve in other uses ‘outside’ the host technology.” [1]\n\nDrawing inspiration from this assemblage model of design, I'll report on research I recently conducted at the University of Maryland investigating how individuals locate the fault lines in objects and analyze them into their component parts. I’ll discuss several potential application domains, including the design of fragmented or non-finito products: intentionally unfinished things [2], such as a sketch, the torso of a statue, or (in the case of my own work) an alternate reality game that incorporates mobile and web apps.\n\n_Kari Kraus is an associate professor in the College of Information Studies and the Department of English at the University of Maryland. Her research and teaching interests focus on new media and the digital humanities, digital preservation, game studies and transmedia storytelling, and speculative design. She is writing [a book](http://www.karikraus.com/?p=141) about how artists, designers, and humanities researchers think about, model, and design possible futures.  In addition, she is collaborating on [DUST](https://fallingdust.com/), a multiplayer collaborative online adventure for students in middle and high school, created through a partnership between Brigham Young University, University of Maryland, College Park, and NASA, with funding from the National Science Foundation._\n\n[1] Qtd. in Kevin Kelly, What Technology Wants (New York: Viking, 2010) 45. [2] Jin-min Seok, et al., \"Non-Finito Products: A New Design Space of User Creativity for Personal User Experience,\" CHI 2014, Toronto Canada.\n\n[audio mp3=\"http://scholarslab.org/wp-content/uploads/2014/11/kari-kraus1.mp3\"][/audio]\n\nThis talk was recorded in Alderman Library, Rm 421 on October 20, 2014.  If you encounter problems with the audio, please email [scholarslab@virginia.edu](mailto:scholarslab@virginia.edu).\n"},{"id":"2014-11-18-playing-with-toast-our-first-ivanhoe-game","title":"Playing with toast: Our first Ivanhoe game","author":"jennifer-grayburn","date":"2014-11-18 07:00:44 -0500","categories":null,"url":"playing-with-toast-our-first-ivanhoe-game","content":"[\n](http://scholarslab.org/wp-content/uploads/2014/11/toaster-72747_640.jpg)We were pleased—and perhaps a bit surprised—that we completed our [Praxis Charter](http://praxis.scholarslab.org/charter/charter-2014-2015/) so painlessly. It was both a test run to see how effectively we could work together as a new team and an opportunity to synthesize our many divergent ideas and goals for this experience. In terms of our work dynamic, we were quickly introduced to what will likely be a major reoccurring obstacle: our schedules simply do not overlap. We eventually did find one precious hour during which all six of us are available and we tentatively reserved it each week in case we need it. Generally, though, we developed a system that seems to work well for us: balancing critical, individual contributions with short, effective meetings. By mauling over drafts and comments independently _before _our meetings, we were able to make the most of our meeting time and plowed through our major revisions and concerns as a team.\n\nThe ease of writing our charter was likely assisted by our shared goals for the [Ivanhoe](http://ivanhoe.scholarslab.org/) platform. While acknowledging that we all want to gain something different from the Praxis Program, we generally agreed that our ultimate product —the redesign of Ivanhoe—should be open, engaging, accessible, and flexible in order to appeal to as many players and communities as possible. We do not want Ivanhoe to be a neglected platform or a mere requirement for a classroom assignment; our goal is to capture the original sense of play so clearly embodied in the early [Ivanhoe games](http://www.rc.umd.edu/pedagogies/commons/innovations/IVANHOE.html#session) played by Jerome McGann, Johanna Drucker, and Bethany Nowviskie.\n\nIn our charter, we agreed that, “while committed to accessibility, the team will be wary of diffusing our attention among too many goals, platforms, or disciplines. We will do our best to focus on a smaller set of outcomes to be developed to completion.” Yet, this is easier said than done! After playing a few Ivanhoe games as a cohort, we began to consider _how_ exactly we could redesign Ivanhoe to inspire the sense of play we desire. In our first Ivanhoe game,“[Toast Sandwich](http://ivanhoe-staging.herokuapp.com/?ivanhoe_game=toast-sandwich),” which was selected for our shared love of all things food and cooking, we responded as various characters to Mrs. Beeton’s infamous toast sandwich recipe. We agreed to make this an open game with no rules or restrictions regarding roles and moves. While fun, the game itself petered out fairly quickly and we recognized a few obstacles that inhibited creative, interactive, and _extended_ play. First, when making a move, we could only respond to one other previous move. While this did not alter the game directly (we could still write our move in a way that incorporated previous moves), it did limit how those moves were tracked and ordered by the game. Second, we were not able to see the previous moves while making our own move without opening another tab. This made it difficult to incorporate or refer directly to past moves as the game commenced. Finally, with no notification function, it was just far too easy to forget about the game. If no one makes a move and no one remembers to check for other moves, the game ends with little opportunity to resume or follow-up.\n\n![toaster-72747_640](http://scholarslab.org/wp-content/uploads/2014/11/toaster-72747_640.jpg)\n\nWhile not the best game, our play was nevertheless instructive! Already, we felt that multiple functions needed to be altered: multiple responds, a split interface of some kind, and a notification system are necessary to help facilitate intuitive and fun play. Yet, on the other hand, we also discussed our encounter with the formal design of the website and Ivanhoe interface, which seemed clunky and dated. Could we also incorporate a formal redesign to help foster excitement and desire to play the game? Could we design or display the games in a way that would make them easily accessible to those not actively playing? On a related note, there seems to be a need to build up a collection of documentation or example games showing _how _to play. One obstacle that we encountered was perhaps _too much _flexibility. Recently, Praxis Fellow Jordis Gjata grappled with this issue in her blog \"[Rules and Flexibility](http://scholarslab.org/grad-student-research/rules-and-flexibility-learning-from-games-for-ivanhoe/),\" asking if total flexibility or more stringent game-like rules were needed to enhance players' experiences.\n\nSo much for our focus on smaller outcomes! We quickly amassed a workload impossible to complete in our short time frame. To me, the question about which aspects we need to focus on is tied directly to the needs of our players. But, who _are_ our potential players?! We talked a lot about the use of the Ivanhoe platform within pedagogical frameworks, targeted for classroom use and skill development. Yet, fellow Praxis Fellow Andrew Ferguson, in his blog \"[Steps Taken](http://scholarslab.org/grad-student-research/steps-taken/),\" brought up a significant concern about the arbitrary ‘gamification’ of required assignments and asked if this _forced _play would ultimately diminish the playful atmosphere that we desired. As a result, we considered how a platform like Ivanhoe could also be useful for people _already_ engaging in such role play and collaborative writing—not for a grade, but rather for fun. There is no clear path forward, as our next step seems to be enmeshed in the question of our players’ motivation: will we be creating a platform that helps to excite and motivate students required to participate for a course grade or that tries to entice writers and players already motivated to play to use this particular platform?\n\nIn an attempt to better understand the implications of this question, I spent the last few weeks researching what we hoped would the focus of the Ivanhoe experience: that is, creative play. Specifically, what is it? Can play be used for education purposes? Is there a way that we can inspire it with the structure or design of Ivanhoe? I plan to answer these questions in my next blog post.\n"},{"id":"2014-11-19-No_Content_Found","title":"Old Wine in New Wineskins","author":"zachary-stone","date":"2014-11-19 06:55:22 -0500","categories":null,"url":"No Content Found","content":"[Last week](http://scholarslab.org/events/uva-medieval-colloquium-kathleen-kennedy/) the Scholar’s Lab and The Medieval Colloquium @ UVA had the pleasure of hosting [Professor Kathleen Kennedy](http://sites.psu.edu/kek16/). Kathleen came to us from Penn State-Brandywine where she teaches all things Medieval. Recently, though, Kathleen’s interest in medieval materiality—with particular regard to the book—has lead her in some new directions.\n\nOver the course of a workshop and a talk, and a lot of food, Kathleen encouraged us to think about resonances of between modern and medieval media technology and presented two related ideas. First, drawing on her experience with the few medieval texts surviving in large (200+) numbers—such as the Wycliffite Bible, Books of Hours, and late medieval statutes—she articulated the need for a “prosopography of the book.” As she walked a group of graduate students through a recent publication on the topic she demonstrated how large data sets might influence our understanding of individual codices. In her model Book History and Codicology—on discipline focused on social milieu the other on individual objects—are brought together in composite readings made possible, in part, by technological advances.\n\nIn her talk she expanded on these thoughts by situating “Old Media Studies” in relation to both New Media Studies and DH. In a provocative talk that brought together a diverse array of scholars and students she made the case for thinking about medieval texts and books in Media studies terms. Her talk explored both the intellectual opportunities such an approach provides as well as the pragmatic work the label might accomplish. Moreover, she argued that the intellectual and pragmatic work of “Old Media Studies” might actually be the same thing. In this, the allusion of her title is apt. Unlike the new wine shoved unto old wine skins that ruins both, the new wine skin preserves aged wine and protects it for the future.\n"},{"id":"2014-11-19-troubleshooting-acceptance-testing-in-rspec-and-capybara","title":"Troubleshooting Acceptance Testing in RSpec and Capybara","author":"scott-bailey","date":"2014-11-19 06:06:30 -0500","categories":["Research and Development"],"url":"troubleshooting-acceptance-testing-in-rspec-and-capybara","content":"Over the past two weeks, I’ve been pair programming with [Eric](http://scholarslab.org/people/eric-rochester/). Together, we’ve been building out a suite of acceptance tests for [Ivanhoe](http://ivanhoe.scholarslab.org) to provide a basic check on critical, user-facing functionality as we refactor parts of the [codebase](http://github.com/scholarslab/ivanhoe). While learning to write basic tests has been relatively straightfoward for me, due to the natural language character of acceptance tests with [RSpec](https://relishapp.com/rspec) and [Capybara](http://jnicklas.github.io/capybara/), we have run into a couple of hang-ups in getting the test suite functional. I want to briefly describe one of these problems and how we resolved it.\n\n\n\n\n\n### Database Transactions and WordPress Problems\n\n\n\n\n\nAfter getting a number of tests working, Eric and I decided to straighten out the database transactions occuring before running the suite and between each test. Before the suite runs, we set up a testing database from fixtures, and between each test we want to reset the database. This reset ensures that any given test is unaffected by database events from previous tests. Initially, this was to be handled with two Ruby gems: [database_cleaner](https://github.com/DatabaseCleaner/database_cleaner) and [sequel](https://github.com/jeremyevans/sequel). The database reset wasn’t working when we attempted to run it however. Eric realized that, due to the testing running in a different process than the application, we would not be able to use the default transaction strategy. Given that WordPress uses a MySQL database, the deletion strategy was not available, leaving us only with truncation. Truncation, however, without some fine tooling, would have cut off more of the database than necessary, requiring a more extensive rebuild from the fixtures. We decided that it would make more sense to write more restrained database transactions ourselves in the [`spec_helper.rb`](https://github.com/scholarslab/ivanhoe/blob/feature/rspec_scott/spec/spec_helper.rb) file. \n\n\n\n\n\nWhile this provided an intial solution, it led to lengthy test times, and lengthy test times lead to a distinct lack of excitement for running tests regularly, substantially hindering the effectiveness of testing at all. We set out to refactor the database transactions to reduce testing time, but in doing so created a perhaps more significant problem. Running our tests led to two significant problems in WordPress:\n\n\n\n\n\n\n\t\n  * TinyMCE, the WYSIWYG editor used by default in the WordPress editor, was no longer working, with relevant failing tests indicating that TinyMCE was not defined on pages with the WordPress editor\n\n\t\n  * an admin user, with full privileges, was able to login, but not able to access the back-end Dashboard at all, being given instead a message that the user did not have the necessary permissions to access the page. \n\n\n\n\n\n### Tracing Errors\n\n\n\n\n\nThe first issue we discovered due to the failure message returned by RSpec and the screenshots at point of failure provided by [capybara-screenshot](https://github.com/mattheworiordan/capybara-screenshot). The second we only discovered in the course of attempting to access the backend of Wordpress to create a post to test whether TinyMCE was being loaded correctly (Ivanhoe uses front-end forms for creating posts of various custom post-types). Given that the first issue was clear and explicit immediately, I began investigating why TinyMCE wasn’t defined. Using the developer tools in Chrome, I discovered that rather a number of links to Javascript files were in fact not being generated in the footer, among those TinyMCE and its component pieces. Thus began a day of tracing out how default Javascript files are queued in the WordPress core. While illuminating as to how WordPress generates and gathers such links, this ultimately led nowhere due to a lack of PHP errors, failure to generate new and helpful PHP errors, and inability to locate any point where the testing script might interfere with the generation of these default links. \n\n\n\n\n\nAs part of this work, though, I tried to create a post through the WordPress Dashboard to check whether TinyMCE was being loaded along with the editor when called through WordPress core (vs. when called through the front-end forms in Ivanhoe). Doing so revealed the second issue, that even as an admin user I was not able to access the Dashboard due to permissions. Checking the database tables against those of a working installation of WordPress and Ivanhoe, I discovered that the `wp_usermeta` table was being overwritten with key-value pairs from a different table, thereby preventing WordPress from recognizing the user’s permissions. Without knowing precisely why, Eric and I thought that fixing this might fix the Javascript loading problems. \n\n\n\n\n\nDuring our weekly pair programming time, we set out to figure out how and why the testing script was causing this mis-copy of the database table. Through a repeated cycle of starting with a clean database, running the script with different system actions commented out, and checking the database, we eventually discovered that the database fixtures themselves, loaded during the test script, were creating the erroneous key-value pairs in the `wp_usermeta` table. After generating a clean set of fixtures, our tests worked, passing as expected. Both errors in WordPress were resolved. To ensure that we don’t run into this problem again, we used a [rake task](https://github.com/scholarslab/ivanhoe/blob/feature/rspec_scott/Rakefile#L17) to create backup copies of the clean database tables and then used those in [`spec_helper.rb`](https://github.com/scholarslab/ivanhoe/blob/feature/rspec_scott/spec/spec_helper.rb) to ensure clean data between tests.\n\n\n\n\n\nWe still don’t know when the error in the fixtures was introduced, and I’ll be spending at least a bit of time trying to figure that out, but, for now the testing suite for Ivanhoe is working. We’ll continue building that out for better coverage and begin refactoring the Ivanhoe codebase. When I mentioned to [Wayne](http://scholarslab.org/people/wayne-graham/) and [Ammon](http://scholarslab.org/people/ammon-shepherd/) that we had figured out the error, and it turned out not to be the script at all, neither was surprised. They were, I suspect, not suprised by two things: that it took several days of work to debug a problem in WordPress and that it wasn’t the script at all, just bad fixtures.   \n\n\n\n\n"},{"id":"2014-12-05-podcast-jentery-sayers-on-remaking-the-past","title":"Podcast: Jentery Sayers on Remaking the Past","author":"laura-miller","date":"2014-12-05 08:11:14 -0500","categories":["Events","Experimental Humanities","Podcasts"],"url":"podcast-jentery-sayers-on-remaking-the-past","content":"### _Remaking Victorian Miniatures: The Speculative Stitches between 2D and 3D_\n\n\nIn both digital humanities and popular culture, there is a rapidly growing interest in big data. How not to read a million books? How to wrangle petabytes of data? How to discover and express patterns across thousands of images? Frequently, this research is framed as a response to an historically unprecedented abundance of information. For instance, over 40 million photos are posted to Instagram each day, and roughly 6,000 tweets appear on Twitter each second. But in this talk I shift the focus away from amounts of data and toward ways of seeing with computers. In this sense, contemporary computing is more about mediation than media, or more about vision than the visible. As a case in point, I draw material from the [Kits for Cultural History](http://maker.uvic.ca/kitsposter/) project at the University of Victoria's [Maker Lab in the Humanities](http://maker.uvic.ca/) in order to outline how digital humanities methods also help scholars think small. More specifically, I explain how the Kits use computer vision, photogrammetry, and fabrication techniques to remake Victorian miniatures (such as Gustave Trouvé's electric jewelry) in 3D, based on extant 2D drawings, diagrams, photographs, and research notes from the second half of the nineteenth century. These miniatures pose a number of curious problems for humanities scholars: How to historicize the minutiae of their manufacture? How to understand their undocumented uses and reception? And how to reconstruct them when they no longer exist, or parts of them are simply not accessible? Comparable to big data projects, these acts of remaking involve some serious speculation where archival gaps are concerned. They also rely quite heavily on computation to stitch together evidence in the presence of absences. In short, they are matters not of \"how many\" but of \"how this becomes that.\"\n\nJentery Sayers is Assistant Professor of English and Director of the [Maker Lab in the Humanities](http://maker.uvic.ca/) at the University of Victoria. His research interests include comparative media studies, critical theories of technology, and cultural histories of dead devices. He is currently working on a book about the \"digging condition\" of digital humanities, or how new media, data, and computing were embedded in materialist metaphors and methods during the 2000s. With William J. Turkel (Western University), he is the PI of the SSHRC-supported Kits for Cultural History project, which is reconstructing pre-digital technologies using physical computing and fabrication techniques.\n\n[audio mp3=\"http://scholarslab.org/wp-content/uploads/2014/12/Sayers1.mp3\"][/audio]\n\nThis talk was recorded in Alderman Library, Rm 421 on November 20, 2014.  You can follow Jentery's [accompanying slides](http://uvicmakerlab.github.io/stitches/#/title) and read his further thoughts on the [relevance of remaking](http://maker.uvic.ca/remaking/). If you encounter problems with the playback, please email [scholarslab@virginia.edu](mailto:scholarslab@virginia.edu).\n"},{"id":"2015-01-08-spring-2015-scholars-lab-gis-workshop-series","title":"Spring 2015 Scholars’ Lab GIS Workshop Series","author":"chris-gist","date":"2015-01-08 09:29:41 -0500","categories":["Announcements","Geospatial and Temporal"],"url":"spring-2015-scholars-lab-gis-workshop-series","content":"All sessions are one hour and assume attendees have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials with expert assistance.  All sessions, except where noted, will be taught on **Wednesdays from 1PM to 2PM in the Alderman Electronic Classroom, ALD 421** (adjacent to the Scholars’ Lab) and are free to attend and are open to the UVa and larger Charlottesville community.\n\nJanuary 21st\n**Making Your First Map with ArcGIS\n**Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa.\n\nJanuary 28th\n**Getting Your Data on a Map\n**Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map.\n\nFebruary 4th\n**Georeferencing a Map\n**Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS.\n\nFebruary 18th\n**Easy Demographics\n**Need to make a quick demographic map or religious adherence?  This workshop will show you how easily navigate Social Explorer.  This powerful online application makes it easy to create maps with contemporary and historic census data and religious information.\n\nFebruary 25th\n**Historic Census Data\n**Would you like to map the poverty in Philadelphia around the turn of the 20th Century?  How about a racial breakdown by state in the 1860s?  This workshop will focus on how to download historic census boundary and tabular data to make historic demographic maps.\n\nMarch 4th\n**Learning Old-School Mapping Techniques\n**How did folks make maps before GPS and satellite imagery?  In this workshop we’ll focus on plane table mapping.  Using just a flat surface, a sheet of paper, a straight edge, and a pencil we’ll learn techniques to create accurate maps for large geographic areas.   With plane table mapping, if you can see it, you can map it.  This technique can be particularly helpful to researchers such a biologists working in small areas under heavy tree canopy where GPS fails.\n\nMarch 18th\n**DIY Aerial Photography\n**Want to do your own aerial photography?  We will discuss techniques, equipment and issues of capturing your own data.\n\nMarch 25th\n**DIY Aerial Photography\n**In this special session, we will spend a couple hours outside collecting aerial data from various platforms.\n"},{"id":"2015-01-19-getting-under-the-hood-with-arduino","title":"Getting “under the hood” with Arduino","author":"ethan-reed","date":"2015-01-19 03:23:00 -0500","categories":["Digital Humanities","Experimental Humanities"],"url":"getting-under-the-hood-with-arduino","content":"Circuit boards, breadboards, digital inputs/output pins, analog outputs – “physical computing” can be an intimidating prospect for people with no experience.  As a person with almost no experience, I know these apprehensions first-hand.  Learning a new vocabulary, basic electronics, even basic programming? Ah!\n\nI’m writing from my own experience with Arduino to say this: getting started with physical computing and even basic programming is easier than it sounds.  Taking a page from the pedagogical playbook of [Learn Code The Hard Way](http://learncodethehardway.org/), it worked best for me when I tried to _do _something even if I didn’t understand everything when I got started.  I followed a set of instructions through to a given goal, and then learned about what I’d done and how I’d done it after the fact.  Arduino kits like the ones we have at the Makerspace are the perfect way to go about doing this.\n\nSo what is an Arduino?  As [their website](http://arduino.cc/en/Guide/Introduction) explains, these are miniature computers that “can sense and control more of the physical world than your desktop computer.”  They can take inputs from all sorts of sensors in the physical world and produce all sorts of outputs back into the physical world.  This ranges from making lights blink, a buzzer ring, taking temperatures, or [all sorts of crazy stuff](http://playground.arduino.cc/Projects/Ideas) that is much, much more complicated. And people are finding [new things to make every day](http://www.instructables.com/id/Arduino-Projects/).\n\nBut when I first got an Arduino it was just a box. Inside the box was an Arduino board, a breadboard, various wires and other things to plug in, and a USB to connect to my computer. Most importantly, there was an instruction manual.  Inside the manual I found a series of snappy pictures and diagrams with user-friendly (and often funny) explanations of what was going on.  I was using the manual from [SparkFun Inventor's Kit - V3](https://www.sparkfun.com/products/retired/11576). It suggested I get started trying to make an LED plugged into my breadboard blink at regular intervals.\n\nThe surprise was this: going through this thing was not only not-scary, it was incredibly fun. It was like building from a Lego manual, but for a circuit board – the bright and strangely shaped objects fitting together, the clearly diagrammed instructions, and the tangible end-product that gives not only a satisfied feeling of having built something, but also the slight amazement at what has been built (I did that? – really?). My light was blinking in a matter of minutes. I disassembled and moved onto the next chapter, building a potentiometer (a dial instead of a switch) to change the rate at which an LED blinked.\n\nBut where’s _the programming_? Something has to be making all these electronics do what you want. Indeed, there is some programming in programs called _sketches_.  But the programming was as simple as downloading the sketch files from the SparkFun tutorial into the Arduino program. Tada! Programming accomplished. And there are comments throughout the files explaining how the code works, and where you might try playing around with it.\n\nOkay, so Arduinos are easy-to-use, inspire child-like joy a la Legos, and teach us some basics about physical computing and programming.  What about the bigger picture – why would I want to learn how to use them?\n\nFirst, I consider it a form of what Matt Ratto calls “[critical making](http://www.tandfonline.com/doi/pdf/10.1080/01972243.2011.583819),” something I talk about in [my first post](http://scholarslab.org/experimental-humanities/come-explore-the-makerspace/) here.  Second, it’s a portal into understanding some of the other amazing work digital scholars are doing with physical computing. I’m thinking of a recent article on “circuit bending” by Nina Belojevic on “[Circuit Bending Videogame Consoles as a Form of Applied Media Studies](http://www.nanocrit.com/issues/5/circuit-bending-videogame-consoles-form-applied-media-studies),” in which she takes apart an NES console, rebuilds it purposefully as a “glitch console,” and reflects on the social implications of her material engagement with the physical hardware.  “By actively taking apart, breaking, remaking, and tinkering with these material devices,” she says, “hobbyists, hackers, artists, and scholars can engage with, study, highlight, and challenge social justice issues” – encouraging people to ask not just about developers, but about “who soldered the VRAM onto their console’s circuit board.”  Jon Jonson at UVic’s Maker Lab has posted about a related project on “[Building an SNES ‘Glitch Controller’](http://maker.uvic.ca/snes/)” – work that, he writes, is “contingent upon haptic exploration and depends highly on trial and error,” a kind of creative play to which I think Arduinos can be a great introduction.  This is not to mention projects that use actual Arduino’s as an interface with the physical world, [of which there are many](http://www.instructables.com/id/20-Unbelievable-Arduino-Projects/). So come in and try them out!\n"},{"id":"2015-01-21-spring-2015-makerspace-workshops","title":"Spring 2015 Makerspace Workshops","author":"laura-miller","date":"2015-01-21 09:20:41 -0500","categories":["Experimental Humanities"],"url":"spring-2015-makerspace-workshops","content":"**Introduction to Omeka\nWednesday, January 28**\n_10:00 am–11:30 am · Alderman Library, Room 421_\n[ ![omekalogo_sm](http://scholarslab.org/wp-content/uploads/2015/01/omekalogo_sm-110x110.jpg)Omeka](http://omeka.org) is a simple, free, web publishing system developed at the [Roy Rosensweig Center for History and New Media](http://chnm.gmu.edu/) at George Mason University. It was specifically built to enable scholars, archives, libraries, museums, and independent researchers to create online exhibits of their work without having to know HTML or CSS. If you have a collections of digital resources that you want to show in a scholarly way, Omeka could be a great tool to have in your toolkit.\n_Instructor: [Ronda Grizzle](http://scholarslab.org/people/ronda-grizzle/)_\n\n**Introduction to 3D Printing\nThursday, January 29**\n_2:00 pm–3:30 pm · Alderman Library, Room 421_\n[![orginial Replicator close-up](https://gallery.mailchimp.com/3ac105f4d87dddbd34542ab41/images/bbd892f7-5885-4086-a1f6-fa738bdfe104.jpg)](http://scholarslab.org/makerspace/)This workshop will introduce participants to the exciting world of desktop fabrication. We'll provide a brief overview of current trends and tools for 3D modeling and printing. We'll also go over the basics of model creation with photogrammetry, and discuss how 3D printing works, including a live demonstration with one of our Makerbots.\n_Instructor: [Jeremy Boggs](http://scholarslab.org/people/jeremy-boggs/)_\n\n**Working with Arduino I\nThursday, February 5**\n_2:00 pm–3:30 pm · Alderman Library, Room 421_\n[![An open Sparkfun Inventor's Kit](http://scholarslab.org/wp-content/uploads/2014/05/makerspace11-110x110.jpg)](http://scholarslab.org/wp-content/uploads/2014/05/makerspace11.jpg)Do you want to hack your personal items with switches or sensors? [Arduino](http://arduino.cc) is a tool for making microcomputers that can sense and control the physical world. This workshop will introduce participants to the basics of physical computing programming through a series of hands-on exercises using our Arduino kits. No electronics experience required!\n_Instructor: [Jeremy Boggs](http://scholarslab.org/people/jeremy-boggs/)_\n\n**Introduction to Neatline\nWednesday, February 11**\n_10:00 am - 11:30 am · Alderman Library, Room 421_\n[![neatline](http://scholarslab.org/wp-content/uploads/2012/08/neatline1-110x110.jpg)](http://scholarslab.org/wp-content/uploads/2012/08/neatline1.jpg)Using [Neatline](http://neatline.org/), anyone can create beautiful, interactive maps, timelines, and narrative sequences from collections of archives and artifacts, telling scholarly stories in a whole new way. Join us for this hands-on introduction. See [http://neatline.org/](http://neatline.org/) for more information.\n_Instructor: [Ronda Grizzle](http://scholarslab.org/people/ronda-grizzle/)_\n\n**Working with Arduino II\nThursday, February 12**\n_2:00 pm–3:30 pm · Alderman Library, Room 421_\n[![An Arduino Uno board](http://scholarslab.org/wp-content/uploads/2014/05/makerspace21-110x110.jpg)](http://scholarslab.org/wp-content/uploads/2014/05/makerspace21.jpg)New to microcontrollers? Or used an Arduino before and want more time to play in a supportive environment? Come on by! [Arduino](http://arduino.cc) is a tool for making microcomputers that can sense and control the physical world. This workshop will introduce participants to the basics of physical computing and programming through a series of hands-on exercises using our Arduino kits. This workshop builds on the Working with Arduino I workshop, but it's not required to attend this one.\n_Instructor: [Jeremy Boggs](http://scholarslab.org/people/jeremy-boggs/)_\n\n**HTML for Beginners\nThursday, February 19**\n_2:00-3:30 pm · Alderman Library, Room 421_\nWonder how websites work? Want to get started creating web content of your own, but have no idea how to do that? This is the class for you. We'll cover everything from how URLs work to basic HTML coding skills to general netiquette. This workshop is intended for absolute beginners with no knowledge of HTML.\n_Instructor: [Ronda Grizzle](http://scholarslab.org/people/ronda-grizzle/)_\n**\nIntro to Wearables and Soft Circuits\n**Wednesday, February 25  [THIS EVENT HAS BEEN RESCHEDULED FOR **MARCH 18 at 10:00 AM**]\n_ 10:00-11:30 am · Alderman Library, Room 421_\n![](https://gallery.mailchimp.com/3ac105f4d87dddbd34542ab41/images/9a6e3ac8-1bc5-4919-af59-1d7d655e0e26.jpg)Have ideas to make your life simpler with hacks for your outerwear or accessories?  This beginner workshop will introduce the basics of circuity and give an overview of current trends in wearable computing. Participants will make their own circuit using LED’s and conductive thread. Materials will be provided and no experience with sewing or electronics is necessary.\nInstructors: [Jeremy Boggs](http://scholarslab.org/people/jeremy-boggs/) and [Purdom Lindblad](http://scholarslab.org/people/purdom-lindblad/)\n\n**Working with Arduino III\nThursday, February 26**\n_2:00 pm–3:30 pm · Alderman Library, Room 421_\n[![Partially assembled Sparkfun Inventor's Kit, with an Arduino board, breadboard, and soldering iron](http://scholarslab.org/wp-content/uploads/2014/09/makerspace7-110x110.jpg)](http://scholarslab.org/wp-content/uploads/2014/09/makerspace7.jpg)New to microcontrollers? Or used an Arduino before and want more time to play in a supportive environment? Come on by! [Arduino](http://arduino.cc) is a tool for making microcomputers that can sense and control the physical world. This workshop will introduce participants to the basics of physical computing and programming through a series of hands-on exercises using our Arduino kits. This workshop builds on the Working with Arduino I and II workshops, but they're not required to attend this one.\n_Instructor: [Jeremy Boggs](http://scholarslab.org/people/jeremy-boggs/)_\n\n**Introduction to 3D Printing\nThursday, March 5**\n_2:00 pm–3:30 pm · Alderman Library, Room 421_\n[![The Makerspace printer desk](http://scholarslab.org/wp-content/uploads/2014/05/makerspace4-110x110.jpg)](http://scholarslab.org/wp-content/uploads/2014/05/makerspace4.jpg)This workshop will introduce participants to the exciting world of desktop fabrication. We'll provide a brief overview of current trends and tools for 3D modeling and printing. We'll also go over the basics of model creation with photogrammetry, and discuss how 3D printing works, including a live demonstration with one of our Makerbots. This course is a repeat of the Jan. 29 session.\n_Instructor: [Shane Lin](http://scholarslab.org/people/shane-lin/)_\n\n\n\n_Scholars' Lab workshops assume attendees have no previous experience. They will be hands-on with with expert assistance.  All are free to attend, and they are open to the UVa and larger Charlottesville community._\n\n**If you can't make a session but would like to learn more about any of the above topics, please visit the student consultants in our [Makerspace](http://scholarslab.org/makerspace/) or email [scholarslab@virginia.edu](mailto:scholarslab@virginia.edu) to set up an individual appointment.**\n"},{"id":"2015-01-23-can-ivanhoe-facilitate-playful-learning-both-in-and-out-of-the-classroom","title":"Can Ivanhoe facilitate playful learning both in and out of the classroom?","author":"jennifer-grayburn","date":"2015-01-23 06:52:52 -0500","categories":["Grad Student Research"],"url":"can-ivanhoe-facilitate-playful-learning-both-in-and-out-of-the-classroom","content":"_You can discover more about a person in an hour of play than in a year of conversation –Plato_\n\n\nFrom the beginning of the year, the Praxis cohort recognized 'play' as one of the key aspects of the Ivanhoe experience. Yet, _how _does play shape Ivanhoe? What are the effects of this play? In previous years, the focus has been on the role of play in the classroom, but we considered from the beginning, as expressed in our [Praxis Charter](http://praxis.scholarslab.org/charter/charter-2014-2015/), that Ivanhoe might be useful, fun, and even educational for other communities as well. Is Ivanhoe relevant--educationally or otherwise--for established, self-motivated textual play, like fan fiction and collaborative creative writing? Is the original educational conception and identity of Ivanhoe lost if such communities do use it? Or does 'education' expand to include informal opportunities to learn and grow outside of the classroom?  As we still struggle with the identity of our players, it is important to consider what exactly Ivanhoe does, how it does it, and for whom it is relevant. In this blog, I will start at the beginning: What is play? Can it be educational? How is it educational? Is it something that we can foster or produce within a formal setting? And finally, how does Ivanhoe provide a unique experience that incorporates aspects of both play and education?\n\n\"Play\" is a term that seems relatively straightforward; it is something we have all done and something that we all believe that we can recognize. But, what does it really mean? In its most casual definition, play is defined by [Webster Dictionary](http://www.merriam-webster.com/dictionary/play) as a \"recreational activity.\" This definition implies that these are activities done for fun, for enjoyment--activities that are_ not work. _This dichotomy between work and play, however, is ambiguous at best and misleading at worst. First, this opposition no longer reflects the realities of many people's livelihoods, as the formal line between work and play is blurring  increasingly in our age of ubiquitous connectivity, flex time schedules, and home-based employment. Second, by setting work and play in opposition, we imply that play is the antithesis of work and potentially classifiable as superficial, unproductive, and without purpose.\n\nThe concept of play, however, has been the subject of much scholarly discussion, especially within studies on childhood development. While scholars admit that play is a difficult concept to pin down in a concise definition, most acknowledge that it is often easily recognizable by those experiencing it. Yet, play should not be conflated with 'fun' and 'enjoyment.' Researchers have argued that play can, in fact, be a way for people to explore not only happy things, but also things that cause insecurity, fear, and anxiety. Rather, it seems that play--whether scary or fun--is something that needs to be self-motivated, something that provides a sense of control through the choice to participate. In _Play, Creativity and Digital Cultures, _Jackie Marsh cites S. Millar in order to argue that play is not actually a _thing_, not a_ _noun. Rather, \"play is best used as an adverb; not as a name of a class of activities, nor as distinguished by the accompanying mood, but to describe how and under what considerations an action is performed\"(211). Sometimes the choice to engage in play is determined by social protocol or relationship expectations, but it is, regardless, something that lets the player select his or her own level of engagement and participation.\n\nScholars who study children's play focus largely on self-motivation as a way for children to explore places, activities, relationships, and physical phenomena within a safe, low-risk environment. In other words, play is a self-initiated, low-stakes opportunity to work through new information, recognize patterns, establish networks, and otherwise _learn_ about issues and processes that concern the player._ _While it can manifest in many forms, play not only_ can _be educational, it _is_ educational;  this form of education, however, is typically subconscious and based on experience, practice, adjustment, and even failure rather than memorization and formal instruction. Since players (rather than instructors) are in control of these experiences, it allows them to be spontaneous, flexible, and adaptable in order to account for new players or their own shifting interests and questions. That is, one advantage of play over formal instruction is that students can intuitively adjust the form of their play to account for what they have already learned and to explore new areas of inquiry.\n\nWith this in mind, it is easy to see how established, self-motivated textual play--such as fan fiction and other forms of creative writing--can offer important educational opportunities. Reading and writing encourage skill practice and familiarity with different ways to communicate and create meaning. Collaborative writing, moreover, requires writers to negotiate relationships, provide and implement feedback, and integrate critically new information into contexts provided by other contributors. Similarly, fan fiction requires critical analysis of a text and the self-conscious creativity to work within or push the boundaries of a provided context. Players can experience a broad range of awareness during these activities and many might not recognize that they are performing and practising such skills. Yet, the flexible time commitment, pursuit of a topic of interest, opportunity to experiment and create without potentially lasting consequences (like grades), freedom to change directions when needed, and choice to include (or exclude) other participants means that players maintain control of their own engagement throughout the process.\n\nWhen Jerome McGann theorized the educational value of Ivanhoe with Johanna Drucker and Bethany Nowviskie in \"[IVANHOE: Education in a New Key](http://www.rc.umd.edu/pedagogies/commons/innovations/IVANHOE.html#session),\" he argued that Ivanhoe works \"to promote rigorous as well as imaginative thinking; to develop habits of thoroughness and flexibility when we investigate our cultural inheritance and try to exploit its sources and resources; and to expose and promote the collaborative dynamics of all humane studies.” These are similarly the skills already being honed by those engaged in self-motivated textual play. The necessary component of Ivanhoe, however, is the active and self conscious _role-_play, for it not only requires students to interpret or reinterpret a cultural text, it requires them to embody it. This must be done self-consciously as players both internalize and synthesize (rather than summarize or describe) aspects of a text and as they negotiate and respond to the moves of their collaborators. While this performance can complement and even enhance established, social, and self-motivated textual play, our inclusion of a private 'role journal' in Ivanhoe to record motivations and justifications for moves challenges even causal writers to be more self-conscious and self-reflexive. As Sandra Wills, et. al., notes in her discussion of classroom role-play, such reflection is a key component to foster a deeper learning opportunity, not only to consider and recognize why and how a player does something, but also to practice communicating it.\n\nIt is clear, then, that play _does _provide advantages for learner-controlled education. Similar skills can, of course, be taught in a formal setting in a more straightforward manner so that the participants are fully aware of their status as 'student' or 'learner.' The question now is if such playful activity (and its self-motivating benefits) can be replicated in a classroom. Over the past few decades, a large corpus of scholarship and guides have been produced to tap into the educational opportunities of play for more formal educational experiences. Yet, if students are _forced_ to participate in play or lack the control to alter the play to meet their own interests, does play loose its very essence and efficacy? In other words, does a playful game-like activity transform back into work when the student looses control? When it is required, formal, and potentially high-stakes in terms of grading? In a previous blog post, Andrew expressed this very [skepticism of 'gamification'](http://scholarslab.org/grad-student-research/steps-taken/) in the classroom.\n\nAccording to Alexandra Ludewig and Iris Ludewig-Rohwer at the University of Western Australia, required and graded game play might, in fact, be limited in its efficacy. In \"[Does web-based role-play establish a high quality learning environment? Design versus evaluation](http://www.iier.org.au/iier23/ludewig.pdf),\" Ludewig and Ludewig-Rohwer test the educational results of an established role-playing experience designed for language students. Using Bloom's Taxonomy of Educational Objectives, the role-play experience was designed to \"encourage students to engage in self-directed and peer-assisted learning, involve experiential and real-world learning, incorporate resource-based and problem-based learning, include reflective practice and critical self-awareness, utilize open learning and alternative delivery mechanisms and also allow for freedom of choice and individual learning style preferences” (165). Yet, when the students were surveyed and tested following their experience, Ludewig and Ludewig-Rohwer discovered a great disjunction between what the role-play _should _ have done and what it actually did (or at least what it did according to students' expectations). While some students enjoyed the experience, many reported anxiety about the unfamiliar assignment format, uncertainty about grading, and no increase in language abilities.\n\nWhile such an example seems to indicate that Ivanhoe might not be the playful pedagogical tool that we assumed, all is not lost!  I suspect that the high-stake consequences of grading in the example above increased the students' anxiety, while the discrepancies between the flexible application of skill and the more formal testing format made evaluation difficult. The sense of play was not absent in the experience itself, but rather in the uncertainty as to if and how it would be evaluated, especially for students so accustomed to an established format of graded assignments. We, as the designers of Ivanhoe, cannot control how players and instructors experience and use the game; however, we continue to emphasize performance in order to encourage playful, experimental encounters and reflection in order to facilitate self-aware educational opportunities.\n\nIn itself, the activity of role-play _does_ allow students to control their level of participation and interaction in a low-stakes setting (that is, students cannot 'fail' at the game; moves build upon each other to form the burgeoning interpretation). Even if students are required to participate for a grade, they do control their role, analysis, engagement, and response to their fellow collaborators. Again, a student's experience is dependent upon their own expectations and how an instructor introduces, structures, and evaluates a classroom game. The role journal will encourage students to make moves critically, justify their choices, and reflect upon their development. It, moreover, can help to initiate classroom discussion and to encourage evaluation on students' insights, connections, and applications, rather than innate creativity. If this is done successfully, the students may not have 'fun' with an assignment, but will still be able to apply playfully the same skills as self-motivated players and better communicate the process of their own learning.\n\nFor more detailed examples of on-line role-play assignments and tips for evaluation, see Sandra Wills, et. al., _The Power of Role-Based E-Learning._\n\n\n\nConsulted Sources:\n\nMcGann, Jerome, with Johanna Drucker and Bethany Nowviskie. “[IVANHOE: Education in a New Key](http://www.rc.umd.edu/pedagogies/commons/innovations/IVANHOE.html#session).” _Romantic Circles_ (Dec. 2014).\n\nLudewig, Alexandra, and Iris Ludewig-Rohwer. \"[Does web-based role-play establish a high quality learning environment? Design versus evaluation](http://www.iier.org.au/iier23/ludewig.pdf).\" _Issues in Educational Research_ 23, no. 2 (2013): 164-179.\n\nSmidt, Sandra. _Playing to Learn: The Role of Play in the Early Years._ New York: Routledge, 2011.\n\nWillett, Rebekah, Muriel Robinson, and Jackie March, eds. _Play, Creativity and Digital Cultures._ New York: Routledge, 2009.\n\nWills, Sandra, Elyssebeth Leigh, and Albert Ip. _The Power of Role-Based E-Learning._ New York: Routledge, 2011. (includes detailed example of online role-playing assignments and ideas for evaluations)\n"},{"id":"2015-01-31-what-could-make-ivanhoe-special","title":"What Could Make Ivanhoe 'Special'? ","author":"joris-gjata","date":"2015-01-31 10:06:30 -0500","categories":["Grad Student Research"],"url":"what-could-make-ivanhoe-special","content":"Last semester was not bad for our Praxis team. Our _achievements_ were modest but considerable: a [charter](http://praxis.scholarslab.org/charter/charter-2014-2015/), a team with a [common interest](http://scholarslab.org/uncategorized/playing-with-toast-our-first-ivanhoe-game/) in any activity concerning food, plus a vibrant debate on redesigning the website for our common project - Ivanhoe. While we failed in meeting certain deadlines and charter objectives, i.e. having tangible visible results, I want to emphasize how productive _the process_ has been for us in my view.\n\nWe had an exemplary meeting as a team to decide on a tagline for Ivanhoe. It was a successful meeting because we had a limited amount of time, a very specific well-defined task and an agreed process of making decisions. The most significant element of this meeting for me was free brainstorming and discussion of the logic behind our preferences and choice of words. The task in front of us seemed small -what is a tagline after all: put five-six words together and you're done. But we did not underestimate the importance of this task. We considered carefully each and every word of the tagline, making sure that it reflected the ideas that gave rise to Ivanhoe at the first place as well as our vision as a team about its future.\n\nLook at our notepad!\n[![photo-7](http://scholarslab.org/wp-content/uploads/2015/01/photo-7-224x300.jpg)](http://scholarslab.org/wp-content/uploads/2015/01/photo-7.jpg)\nWithin an hour of conversation, we decided to call Ivanhoe: _'A Gateway to Collaborative Textual Play'_. The key concepts we built this phrase on were: fluidity, interpretation, reflection, performativity, and collaboration. Our tagline draws a vision of Ivanhoe not just as a space with pre-established boundaries - a platform or a place of some kind - but a space that enables the emergence of boundaries, actors and places through play. That is why we chose the term 'gateway' and followed it with the preposition 'to': to indicate its initial purpose as an enabling environment for learning. As [Jeniffer's last post](http://scholarslab.org/grad-student-research/can-ivanhoe-facilitate-playful-learning-both-in-and-out-of-the-classroom/) seems to conclude, Ivanhoe _can_ enable learning through play. In the current version, Ivanhoe enables learning through the 'role' and 'role journal' features. These features require players to reflect on their moves/interpretations - before and after they complete them.\n\nWhile I agree that Ivanhoe is a _learning_ environment, I want to argue that this is not enough to define it and that we need to specify further _the kind_ of learning this environment enables that other environments do not.\n\nThe existing version of Ivanhoe seems to emphasize one kind of learning - learning that results from reflection on one's _individual_ actions and thoughts. One might argue that this kind of learning through such self-reflection can happen everywhere and does not need a facilitating enabling environment like Ivanhoe. For example, commenting to a Facebook status or a WordPress blogpost also requires self-reflection - you have to think about yourself, your words and interpretation of a particular text - a photo, or music video, or quote from somebody's book, etc. This is similar to what you can do in Ivanhoe.\n\nI want to suggest that one way to make Ivanhoe 'special' i.e. to define its use as a learning environment, could be to create a feature that makes it enable another kind of learning - learning from others through reflection on the trajectory of play and the relationships among players' interpretations. This feature could be incorporated within the existing 'role' and 'role journal' features. It would require players not only to reflect on their own _individual_ moves before and during play, but also to reflect on the moves of others and the way in which the relationships between players' moves shape the process of play and its end-result - an 'Ivanhoe game'. The aim of such feature in Ivanhoe would be to enable a kind of learning that is difficult to achieve in other environments - WordPress, Facebook, a museum, or even a classroom: learning about others and how one's interactions with others shape cultural objects or texts of common interest. This kind of learning could be achieved by requiring reflection in a journal - before, during and/or after play - about the set of relationships that contribute to the emergence of the text of common interest and define its form. I argue that Ivanhoe can become special if it is able to promote reflection about other players' interpretations of a text and the interaction between interpretations that give a particular form to that text. I am looking forward to our team's discussion of this option and the implementation of such idea into a well-designed Ivanhoe feature.\n\nWe have a tagline and we agreed that for us it is not just a statement about what Ivanhoe does in its current state, but more of an expression of our _aspirations_ about what Ivanhoe can do. The conversation that led to the tagline creation helped us as a team consolidate our ideas about Ivanhoe and what we wanted it to be. However, we need to continue the debate on what Ivanhoe does and what can make it special as it will shape our future work this spring semester. I hope that the time we take learning new skills- html, css, php, etc.- will not be seen separate from the time we give to talking and reaching an agreement about the use and purpose of Ivanhoe - or how to make it special. I am looking foreword to more successful meetings this new year.\n"},{"id":"2015-02-02-moving-peoplelinking-lives-dh-symposium","title":"Moving People/Linking Lives DH Symposium","author":"brandon-walsh","date":"2015-02-02 09:16:35 -0500","categories":["Digital Humanities","Events","Geospatial and Temporal","Grad Student Research"],"url":"moving-peoplelinking-lives-dh-symposium","content":"I am pleased to announce that \"[Moving People, Linking Lives: An Interdisciplinary Symposium](http://movingpeoplelinkinglives.org)\" will take place March 20-21, 2015 at the University of Virginia. Friday, March 20 events will take place in the Kaleidoscope Room. Saturday, March 21 events will take place in Alderman 421 except for an evening reception, location to be determined.\n\nPresentations and workshops will open dialogue across different fields, periods, and methods, from textual interpretation to digital research. Invited participants include specialists on narrative theory and life writing, prosopography or comparative studies of life narratives in groups, and the diverse field of digital humanities or computer-assisted research on cultural materials, from ancient texts to Colonial archives, from printed books to social media.\n\nInvited participants include: Elton Barker, Jason Boyd, James Phelan, Susan Brown, Margaret Cormack, Courtney Evans, Will Hanley, Ben Jasnow, Ruth Page, Sue Perdue, Sidonie Smith. We hope to have lots of locals involved with digital work participate as well, and we particularly encourage graduate students to join in for the weekend!\n\nOur symposium will bridge the gaps among our fields; share the innovations of several digital projects; and welcome the skeptical or the uninitiated, whether in our historical fields or in the applications of technology in the humanities. Booth, Clay, and Ogden have each led digital projects with some common themes and aims: locating, identifying, and interpreting the narratives—or very often, the lack of discursive records—about individuals in groups or documents, in Homer or other ancient text, Medieval French hagiography, and nineteenth-century printed collections of biographies in English. We want to open discussion of many potential methods including our own—data mining and digital editions of texts; relational databases and historical timelines and maps—for research on groups of interlinked persons, narratives or data about their lives, and documents or other records, and synthesizing and visualizing this research in accessible ways that reach students and the public. Digital innovation, however, should be informed by traditions of scholarly interpretation and advanced theoretical insights and commitments. Narrative theory and Theory generally, ideological critique including studies of gender and race, textual and book history studies, transnational and social historiography, philology and language studies, archeology, cultural geography and critical cartography, are all gaining influence on digital projects.\n\nInvited participants will be posting about their research to [our blog](http://movingpeoplelinkinglives.org) in the weeks leading up to the symposium, anyone is free to comment on the posts. In addition, our participants will be building a [Zotero-powered bibliography](http://movingpeoplelinkinglives.org/bibliography/) in the weeks leading up to the symposium full of rich materials related to the event’s discussion.\n\nOrganized and hosted by Alison Booth, Jenny Strauss Clay, and Amy Odgen and sponsored by the Page Barbour Committee, the departments of English, French, and Art, the Institute for Humanities and Global Cultures, the Scholars’ Lab and Institute for Advanced Technology in the Humanities, and other entities at UVa, **all events are free and open to the public**. More information can be found on the blog as planning progresses, and you can follow us on twitter at @livesdh.\n\nJoin in the conversation on the blog at [movingpeoplelinkinglives.org](http://movingpeoplelinkinglives.org), and we hope to see many come out for fruitful interchange in March!\n"},{"id":"2015-02-06-refactoring-ivanhoe","title":"Refactoring Ivanhoe","author":"scott-bailey","date":"2015-02-06 06:12:29 -0500","categories":["Research and Development"],"url":"refactoring-ivanhoe","content":"As one of last year’s [Praxis Fellows](http://praxis.scholarslab.org/), I helped build [Ivanhoe](http://ivanhoe.scholarslab.org), a “WordPress Theme enabling collaborative criticism through roleplay - for scholars, students, and cultural enthusiasts.” While Ivanhoe was perfectly functional when released, one could not say that it exemplified orderly, well-formed code. It was, and is, after all, code written by novice developers. That it was still released as such is, I think, a testament to the Praxis Program, which legitimately treats Fellows, graduate students typically learning about digital humanities, project management, and coding for the first time, as fellow workers and collaborators of the Scholars’ Lab staff, not children whose mess must be cleaned up after the fact. Mentorship entails developing skills and capacities over time through partnership, not paternalistic oversight. \n\n\n\n\n\nAnd yet, this year we have a new cohort of Praxis Fellows, most of them learning to code for the first time, tasked with improving Ivanhoe in terms of features and design. In order to facilitate their ability to understand the [Ivanhoe code base](https://github.com/scholarslab/ivanhoe) to a sufficient degree to add to it without too much time lost overcoming the wall of unorganized code, we decided to refactor Ivanhoe, part of that effort being the introduction of a set of acceptance tests, about which I’ve already [written](http://scholarslab.org/uncategorized/troubleshooting-acceptance-testing-in-rspec-and-capybara/). \n\n\n\n\n\nThat’s the official line at least, and a true one, but not the whole story for me personally. As part of the development team in last year’s Praxis cohort, I contributed substantially to Ivanhoe. Throughout my time in Praxis, I advanced significantly in my ability to code, and have learned far more about best practices, efficient code, and application organization since becoming a full developer in the Scholars’ Lab. However, there is still code from the very earliest commits in Ivanhoe still in the codebase, from before I (or my fellows) had really clear ideas about how to structure files, comment code clearly, or craft elegant functions. Doing a refactor of Ivanhoe was a chance for me to look back at a lot of code I had written early on, and implement improvements based on everything I’ve learned since. I want to share a few highlights from that refactor, now current in the develop branch of the Ivanhoe repo, which I carried out in frequent consultation with [Eric Rochester](http://scholarslab.org/people/eric-rochester/), our senior developer. \n\n\n\n\n\n### Tests Are Awesome, Type Errors Are Not\n\n\n\n\n\nThe refactor process began with implementing the test suite about which I’ve already written, which would provide a way to make sure that the refactored code generally worked as expected. Once the test suite was written, I created a new branch containing the tests, merged in the develop branch of the repo, and ran the tests. And found a serious fail immediately. The develop branch of Ivanhoe implements a feature that allows the user to respond to more than one source move at a time. Since this feature was merged into develop back in late April, early May, it was, apparently and unbeknownst to us, broken due to a simple error: at times an integer was passed to the function generating the response form, while at other times, with the multiple sources, an array was passed. This simple type error [was easily fixed](https://github.com/scholarslab/ivanhoe/commit/1beeebe081beba1f34b2486047173acd46a50bc1) (and fixed again [differently](https://github.com/scholarslab/ivanhoe/commit/12c96d36e55396efeff42e460f2033903dab9d00) after using PHP classes), and I became immediately enamored of testing. \n\n\n\n\n\nRest assured, every step of the way through the refactor, I ran my tests, always quietly elated when line after line of green passes appeared in my terminal. \n\n\n\n\n\n### Function Reorganization\n\n\n\n\n\nOne of the biggest problems with the code base as we left it at the end of the Spring semester was the [functions.php](https://github.com/scholarslab/ivanhoe/blob/master/functions.php) file, a 1000+ lines of unorganized, sometimes commented functions addressing numerous aspects of the theme. In order to make these functions more accessible to those just jumping into Ivanhoe development, I sorted through these functions, identifying major types of functions and then breaking those sets of functions out into separate files in the includes directory. These files are then pulled into the [functions.php](https://github.com/scholarslab/ivanhoe/blob/develop/functions.php) file, necessary in WordPress, with require_once statements. Broken up and reorganized, functions are immediately more easily accessible for development. For the initial breakup of functions, see [here](https://github.com/scholarslab/ivanhoe/commit/fc87bcf3290903833441ea2ce1c95fcdcf01c310).\n\n\n\n\n\n### Cleaning Up Messy Templates\n\n\n\n\n\nSome of the view templates in Ivanhoe are fairly straightforward, but others in the released version were quite messy, with multiple custom WordPress queries sometimes nested within each other, variable definitions scattered throughout the page, inconsistent formatting, and often very little commenting to indicate what’s happening throughout the template. While the fixes for these issues were relatively minor, they make a difference to the legibility of the code. When possible, I moved variable definitions and logic to the top of each template file. I added comments noting the beginning and end of presentation sections for each custom query, especially when nested. Some of the templates are still complex, but now more accessible for someone coming fresh to the project. For some of these changes, see [here.](https://github.com/scholarslab/ivanhoe/compare/61a36dde...cf1407d1)\n\n\n\n\n\n### Presentation vs Data-related Logic\n\n\n\n\n\nLacking a true templating language and with a substantial API, WordPress can make a hash of the distinction between presentation logic and data-related logic. That’s a problem, but probably not an excuse for our own failure to differentiate the two in Ivanhoe. For instance, in the released version of Ivanhoe, we have a function called [ivanhoe_get_move_responses](https://github.com/scholarslab/ivanhoe/blob/master/functions.php#L623), which uses a `get_posts` query to get any responses to a move, iterates over the array of responses to build an unordered list with the post titles of the responses as list items, and then echoes that chunk of HTML. In the template using this function, we simply call the function. This one function does more than it needs to or should. It builds a usable chunk of data in the array of responses, which has nothing necessarily presentational about it, builds a presentational element (the unordered list), and then actually implements that presentation. \n\n\n\n\n\nThe [refactor](https://github.com/scholarslab/ivanhoe/commit/6269404d2aed7ebcc7a9c402412b14e12d72c734) breaks this up into [two functions](https://github.com/scholarslab/ivanhoe/blob/develop/includes/post_meta.php#L300), one data-related and the other presentational. It makes a further change for best practice and versatility, replacing the immediate echo at the end of the presentational function with a return. The `echo` is moved to the view template, keeping the actual presentation where it belongs. \n\n\n\n\n\nBoth of these changes - breaking up functions based on type, and replacing echo with return in presentational functions - were implemented multiple times throughout the refactor. \n\n\n\n\n\n### PHP Classes - Unncessary But Fun(?)\n\n\n\n\n\nIn the released version, a [single template](https://github.com/scholarslab/ivanhoe/blob/master/ivanhoe-post-form.php), with quite a bit of procedural logic, generated the various new move, game, and role forms. I mentioned earlier in this post that I worked with Eric on this refactor; it was the primary subject of our pair programming time for the last while. Our conversation on this particular file went roughly like this:\n\n\n\n\n\nEric: When I look at this file, I think about PHP classes. \n\n\n\n\n\nScott: Will PHP classes make this file easier to deal with? \n\n\n\n\n\nEric: Maybe, maybe not. \n\n\n\n\n\nScott: Alright, let’s do it. \n\n\n\n\n\nNow, this refactor was largely about making the Ivanhoe theme more accessible for novice developers to engage. It was secondarily an opportunity for me to practice and implement some of what I have learned on one of my own previous projects. It was also, though, thirdly, an opportunity for me to learn more about refactoring and coding patterns with Eric’s help. Re-working the post form template into a single base class with three sub-classes is an example of this. It wasn’t necessary, and it didn’t shorten the code or boost application performance in a truly substantial manner. It did, however, provide me a clearer idea of how recognizing patterns within code written according to one paradigm can lead you to achieve the same results through an entirely different paradigm. \n\n\n\n\n\nBriefly, we had one file that was generating effectively the same form three times, with only minor changes in labels and data depending on whether the form needed to generate a new game, role, or move. These changes were controlled by a series of variable definitions and `if` statements following a top-down order in the file. Given that all three instances of the form shared a basic composition, it made sense to create a [single base class](https://github.com/scholarslab/ivanhoe/blob/develop/includes/post_form/BasePostForm.php) for the post form with all of the shared functionality as methods on the class. The differences between forms were handled by sub-classes for each different post type (game, role, move), either adding sub-class specific methods or, if necessary, overwriting a method from the base class. The original file generating the forms then became [much simpler](https://github.com/scholarslab/ivanhoe/blob/develop/ivanhoe-post-form.php) - it basically just checks for post type, then concretely instantiates the appropriate sub-class. To see these changes, check [here](https://github.com/scholarslab/ivanhoe/compare/74f3e7d...b575e5f92a94cf012d8951a5f993ec1c1136fc59).\n\n\n\n\n\n### (Un)Finished\n\n\n\n\n\nThe major aspects of the refactor are now done, and it’s been merged down into the develop branch of the repository, available to be hacked on by this year’s Praxis cohort. I look forward to seeing what they do with it, and to then, somewhere down the road, coming back to refactor again with all that I will have learned by that time. \n\n\n\n"},{"id":"2015-02-09-call-for-applications-praxis-fellowship","title":"Call for Applications, Praxis Fellowship","author":"purdom-lindblad","date":"2015-02-09 09:02:32 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"call-for-applications-praxis-fellowship","content":"UVa grad students! Apply by **March 2** for a unique, funded opportunity to work with an interdisciplinary group of your peers, learning digital humanities skills from the experts, and collaboratively design and execute an innovative digital project. The 2015-2016 Praxis cohort will get underway in September, thanks to generous support by the [UVa Library](http://www.library.virginia.edu/) and the endorsement of [GSAS](http://gsas.virginia.edu/).\n\nEach year, the [Scholars’ Lab](http://scholarslab.org/) Praxis Program provides six UVa graduate students with hands-on experience in digital humanities collaboration and project creation. Team members work to take a shared project from conception to design and development, and finally to publication, community engagement, and analysis. [Praxis](http://praxis.scholarslab.org/) is a unique and well-known training program in the international digital humanities community. Our fellows [blog](http://scholarslab.org/archives/) about their experiences and develop increased facility with project management, collaboration, and the public humanities, even as they tackle (most for the first time, and with the mentorship of Scholars’ Lab faculty and staff) new programming languages, tools, and digital methods. The program prepares fellows with digital skills they can apply to the Praxis fellowship project as well as to their future research.\n\nIn 2012, the Scholars’ Lab inspired like-minded institutions to create the [Praxis Network](http://praxis-network.org/), made up of allied but differently-inflected humanities education initiatives from around the world, mainly focused on graduate training, and all engaged in rethinking pedagogy and campus partnerships in relation to the digital humanities. The [Praxis Network Student Directory](http://praxis-network.org/students) showcases how Praxis Program alumni have traveled diverse career paths, including tenure-track teaching and distinguished digital humanities positions within academic libraries and research centers.\n\nWe will welcome six new, competitively-selected students to the UVa Praxis Program in late August 2015. _**This fellowship replaces recipients' teaching responsibilities for the academic year. Fellows are expected to devote approximately 10 hours per week in the fall and spring semesters to learning together and building a collaborative digital humanities project in the Scholars’ Lab.**_ Fellows join our [vibrant community](http://scholarslab.org/people/), have a voice in intellectual programming for the Scholars’ Lab, and can make use of a dedicated grad lounge.\n\nAll University of Virginia graduate students working within or committed to the humanities are eligible to apply to join the 2015-16 cohort. We particularly encourage applications from women, LGBT students, and people of color, and will be working to put together an interdisciplinary and intellectually diverse team.\n\nApply by emailing an expression of interest to [Purdom Lindblad](mailto:jpl8e@virginia.edu), indicating the applicant’s\n\n\n\n\t\n  * research interests,\n\n\t\n  * summary of plan for use of digital technologies in the applicant’s research,\n\n\t\n  * summary of skills, interests, and methods the applicant will bring to the Praxis Program,\n\n\t\n  * and a statement of what the applicant hopes to gain as a Praxis Fellow.\n\n\nApplicants must be available for in-person interviews on Grounds between March 16th and 27th.\n\nFor questions about the [Praxis Program](http://praxis.scholarslab.org) or the [application process](http://scholarslab.org/graduate-fellowships/), please email [Purdom Lindblad](mailto:jpl8e@virginia.edu).\n\nDeadline: March 2, 2015\nNotifications: April 10th, 2015\n"},{"id":"2015-02-11-new-developments-for-the-praxis-network","title":"New Developments for the Praxis Network","author":"purdom-lindblad","date":"2015-02-11 10:36:38 -0500","categories":["Announcements"],"url":"new-developments-for-the-praxis-network","content":"The [Praxis Network](http://praxis-network.org) was developed as part of a 2012-2013 [Scholarly Communication Institute](http://uvasci.org/) focused on graduate education. Its goal is to share [model programs](http://praxis-network.org/compare/) that are engaged in methodological training and collaborative research in the humanities. Deeply invested in demystifying collaborative, iterative, and public work, Praxis Network programs prep students to have a broader view of the humanities.\n\nThe first iteration of the Praxis Network was simply about sharing the model of these aligned, but differently inflected programs. This next phase is about deepening the connections among programs, exploring ways to facilitate networking our students, and to create a space for other interesting, similarly oriented programs to share their missions. Two new directories have been added as a first step towards identifying and strengthening networks among students and programs. The [Student Directory](http://praxis-network.org/students/) highlights the research interests of Praxis Network students.\n\nThe [Directory of Related Programs](http://praxis-network.org/institutions/) showcases like-minded programs invested in rethinking humanities methodological training and fellowships in the digital humanities. Do you run a similar effort? [Add your program!](https://docs.google.com/forms/d/12psqdYT1sZCOGjKgD9dyNX0kK3QC1NG7FyQjJwjNEH8/viewform)\n"},{"id":"2015-02-16-podcast-thorny-staples-on-managing-smithsonian-research-data","title":"Podcast: Thorny Staples on Managing Smithsonian Research Data","author":"laura-miller","date":"2015-02-16 08:38:37 -0500","categories":["Events","Podcasts"],"url":"podcast-thorny-staples-on-managing-smithsonian-research-data","content":"**_Managing the Record of Research at the Smithsonian_**\n\nCan institutions effectively manage cross-team digital research data in real time? Can it preserve that data so that it can be seamlessly presented in conjunction with publications?\n\nTo answer those questions, the Smithsonian Institution has built a first pilot system, called SIdora, designed to be used by Smithsonian researchers to capture and organize digital \"evidence\" as they create it in their research process, and use it directly in their analysis and dissemination activities. The goal is to actively support the research process as it unfolds, leaving behind a coherent expression of the digital content for a complete research project that can permanently stand alongside related publications. Sidora, a general information architecture and software environment based on Islandora and Fedora, is designed to manage research output as if it were part of a network of information. Staples will present the architecture and demo the software, using research data from a complete excavation of an archaeological site in Panama, and an international study of mammal populations.\n\n[![thorny_staples](http://scholarslab.org/wp-content/uploads/2015/01/thorny_staples-110x110.jpg)](http://scholarslab.org/wp-content/uploads/2015/01/thorny_staples.jpg)Thorny Staples is currently the Director of the Office of Research Information Services at the Smithsonian Institution. He has previously been Director of the Fedora Project; Director of Community Strategy and Alliances for DuraSpace; CIO of the National Museum of American Art at the Smithsonian Institute; Director of Digital Library Research and Development at the University of Virginia; and Project Director at the Institute for Advanced Technology in the Humanities at the University of Virginia.\n\n\n\nThis talk was recorded in Alderman Library, Rm 421 on February 9, 2015.  Click below to stream the podcast.  If you encounter problems with the playback, please email [scholarslab@virginia.edu](mailto:scholarslab@virginia.edu). As always, you can listen to our podcasts on the Scholars' Lab blog, or [subscribe](http://www.scholarslab.org/category/podcasts/) [on iTunesU](https://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619?mt=10).\n\n[podloveaudio src=\"http://a1611.phobos.apple.com/us/r30/CobaltPublic1/v4/cc/b6/a9/ccb6a98c-8360-32d4-0022-4a0647686fc1/335-5035744038474421641-staples.mp3\"]\n\n\n"},{"id":"2015-02-24-call-for-applications-graduate-fellowship-in-the-digital-humanities","title":"Call for Applications, Graduate Fellowship in the Digital Humanities","author":"purdom-lindblad","date":"2015-02-24 04:28:21 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"call-for-applications-graduate-fellowship-in-the-digital-humanities","content":"Applications for the [Scholars’ Lab](http://scholarslab.org)'s prestigious [Graduate Fellowships in the Digital Humanities](http://scholarslab.org/graduate-fellowships/) are now being accepted for the 2015-2016 academic year. **Applications are due March 23, 2015.**\n\nThe fellowship supports ABD graduate students doing innovative, dissertation-related work in the digital humanities at the University of Virginia. The Scholars’ Lab offers Grad Fellows advice and assistance with the creation and analysis of digital content, as well as consultation on intellectual property issues and best practices in digital scholarship and DH software development.\n\nFellows join our [vibrant DH community](http://scholarslab.org/people/), have a voice in intellectual programming for the Scholars’ Lab, can make use of a dedicated grad lounge, and participate in one formal colloquium at the Library per semester.\n\nSupported by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew & Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities, the Graduate Fellowship in Digital Humanities is designed to advance humanities research and provide emerging digital scholars with an opportunity for growth.\n\nEligibility\n\n\n\n\t\n  * Applicants must be ABD, having completed all course requirements and been admitted to candidacy for the doctorate in the humanities, social sciences or the arts at the University of Virginia.\n\n\t\n  * Applicants must be enrolled full time in the year for which they are applying.\n\n\t\n  * A faculty advisor must review and approve the scholarly content of the proposal.\n\n\t\n  * Applicants are strongly encouraged to demonstrate prior experience in digital scholarship. Experience can include work on a collaborative project, comfort with programing, design, and code management, experience with public scholarship, and critical engagement with digital tools. Applicants with [Praxis Program](http://praxis.scholarslab.org) or equivalent experience will have a competitive edge, but all are welcome to apply.\n\n\nA complete application package will include the following materials:\n\n\t\n  * a cover letter, addressed to the selection committee;\n\n\t\n  * a [Graduate Fellowship Application Form;](http://www.scholarslab.org/wp-content/uploads/2012/10/fellowsapp2013.pdf)\n\n\t\n  * a dissertation abstract;\n\n\t\n  * a summary of the applicant’s plan for use of digital technologies in his or her dissertation research;\n\n\t\n  * a summary of the applicant’s experience with digital research and/or public humanities projects;\n\n\t\n  * a description of UVa library digital resources (content or expertise) that are relevant to the proposed project;\n\n\t\n  * and 2-3 letters of nomination and support, at least one being from the applicant’s dissertation director.\n\n\nQuestions about Grad Fellowships and the application process should be directed to: [Purdom Lindblad](mailto:jpl8e@virginia.edu)\n\nDeadline: March 23, 2015\nNotifications: April 17, 2015\n\n\n"},{"id":"2015-02-24-on-the-shelf","title":"On the Shelf","author":"andrew-ferguson","date":"2015-02-24 09:04:50 -0500","categories":["Grad Student Research"],"url":"on-the-shelf","content":"The past couple weeks, we have been crash-coursing PHP via Wayne Graham’s surely famous slide decks. We’ve been doing this despite not (yet) delivering our task for the fall semester—the redesign of the Ivanhoe webpage.\n\nIt’s a necessary shift. While we’re fortunate to be assisted by the Scholars’ Lab dev team—including Scott Bailey, a holdover from last year’s Praxis cohort—if we’re going to improve on the game’s workings, we have to be able to get in and make those changes, and we also need lots of practice in making those changes. Start any later, and there just won’t be enough time to develop skills or features.\n\nBut, as is often the case in academic study (as also, I suspect, in DH), this problem isn’t so much disappearing or even receding as it is mutating: taking on a new and more urgent form. In this case, that’s the design of the Ivanhoe game itself. Our hope is to take the Ivanhoe that presently exists and implement some of the features that almost-but-didn’t-quite make it in last year, while also streamlining the frontend with the aim of making players return to the game at more regular intervals.\n\nIn all of this, we’re holding onto the principles we had established for the Ivanhoe site, but couldn’t quite yet put into practice there: leaving as much as possible open to the players themselves to customize, so they can experience it in the ways most amenable to their own textual plays. We hoped to symbolize this on the site by incorporating an array of styles that would allow users to approach the website in, if not the infinite ways that we hope Ivanhoe will afford them, then at least enough that they can choose their own stripe of Ivanhoe. Of course, this means implementing these styles across multiple pages, rather than the landing page alone—this was the hurdle we were at when it became imperative to switch tasks.\n\nWe are at the point now of signing off on our prototype game design, which has been heavily informed by the conversations we had about the site itself. And perhaps it was necessary, if roundabout, to approach things this way. After all, the experience of a text is rarely smooth and never instantaneous. Why should a project devoted to furthering our textual experiences prove any different?\n"},{"id":"2015-02-26-monkey-mind","title":"Monkey Mind","author":"purdom-lindblad","date":"2015-02-26 10:20:35 -0500","categories":["Digital Humanities","Research and Development"],"url":"monkey-mind","content":"A monkey is easily flummoxed by a coconut. A hole is cut into the coconut and filled with sweet food (or something shiny). The monkey slips her hand into the coconut, grasps the treat, and is trapped. She can readily unhand the treat (or shiny object), but is unwilling to let go. Letting go, for monkeys as well as humans, is harder than it sounds.\n\nIn my case, letting go means releasing the need to share what I've been thinking about lately in a well-formed thesis. What I have to say here is all a bit tangled still.\n\nHumanities requires attention to the world and should be applicable to carving out the ways and means of peace. Peace means shelter, access to resources, work that does not harm people or the planet as much as meaning safety, non-violence, and calm. I often return to a conversation with [Veronica](http://scholarslab.org/people/veronica-ikeshoji-orlati/) in which she said academic study carves out a space to reflect on both the object of study and our present moment.\n\nI also keep returning to Bethany's [DH2014 keynote](http://nowviskie.org/2014/anthropocene/). What do we work on, what are the priorities, at a moment of mass extinction? Mixed in are bits of floating inspiration, such [as designing for care rather than against ourselves](http://www.bustle.com/articles/30292-vancouvers-thoughtful-raincity-homeless-benches-puts-londons-anti-homeless-spikes-to-shame). (Why aren't Vancouver's bench shelters in every city?)\n\nFurther, I have been thinking a great deal about connections and modes of sharing. The new [Praxis Network ](http://praxis-network.org)directories are laying the groundwork for more meaningful connections among the students, alumni, and programs of our partnering groups. I think virtual conferences, workshops, and casual meet-ups can go a long way to personalizing the Praxis Network and extending it beyond one’s home program. My colleague, [Jeremy](http://scholarslab.org/people/jeremy-boggs/), reminds me that much more than the platform for sharing, it is our attention to others, our empathy and willingness to share that forge meaningful connections.\n\nAdding to this mental stew, I recently attended two powerful trainings on bystander intervention at UVa, offered in the wake of our difficult autumn. Jeremy and I, along with 2 other UVa librarians Melinda Baumann and Matthew Vest, attended a 4-day [Green Dot ](https://www.livethegreendot.com/gd_overview.html)training. The premise of Green Dot is that all of us (bystanders) can reduce inter-personal violence (harassment, stalking, sexual violence) by becoming more aware of what is happening around us and checking in on a situation that feels like it could lead to violence.\n\nThis idea of checking in was also strongly echoed in a recent [Suicide Awareness training](http://www.virginia.edu/studenthealth/caps/FacultyStaffSuicideAwareness.html) offered by [CAPS, UVa's Counseling and Psychological Services](http://www.virginia.edu/studenthealth/caps.html). The goals of the Suicide Awareness training were to help people recognize others who are in distress and to provide examples of how to help. Much like Green Dot, the advice was simple--ask. One of the counselors said simply noticing and asking about another’s emotional state could be enough for that person to seek help, even if that person brushes you off. Both Green Dot and the Suicide Awareness training ask for a cultivation of awareness and empathy for the people around us.\n\nI keep mapping the notion of being aware of what's happening, not ignoring it even when it is awkward or hard, and stepping in to redirect choices that lead to violence back to the bench-shelter, and in less-able-to-articulate it way to Bethany's keynote, the Praxis Network, and my own research plan for use of Scholars' Lab R&D time. Bethany asked what are the things we, as a community, attend to.\n\nI am trying to better connect my work in the digital humanities to an active cultivation of empathy and care of people and our planet. I am not advocating for a radical humanities, though that'd be awesome. I am attempting to know the landscape of my own monkey-mind, to better understand what to release as well as what to nurture; when to hold on, and when to let go.\n\n\n"},{"id":"2015-03-02-adventures-in-converting-subversion-to-git","title":"Adventures in Converting Subversion to Git","author":"wayne-graham","date":"2015-03-02 04:37:03 -0500","categories":["Research and Development"],"url":"adventures-in-converting-subversion-to-git","content":"While the Scholars' Lab was founded in 2006, we manage a lot of projects that had their roots in the eText Center in the late 1990s. These projects have lived through the numerous \"best practices\" of the various eras, many still bearing the marks of those bygone eras (you see a lot of projects that used FTP clients to manage the project as evidenced by numerous `WS_FTP.log` files for those who remember that). Most of the legacy projects we work with were migrated to Subversion by the the 2000s, but if you're one of the cool kids, you'll know that everyone uses [git](http://git-scm.com/) these days (thanks in large part to [GitHub](https://github.com)). Recently we've been working on one of these projects ([Salem Witch Trials](http://salem.lib.virginia.edu)) to help get it ready for a forthcoming book and we found that Subvesion kept getting in the way of actually doing work. Little and big issues kept nagging collaborators like flaky user permissions and issues with adding numerous files had made the Subversion repository to nearly 4Gb. Ben Ray came by my office one day and asked if there was a \"better\" way to do this. I suggested an experiment with `git` and GitHub and seeing if that would help ease some of the pain points he was having with Subversion.\n\nAt first, I naively thought this was be pretty straight forward. We've migrated other Subversion repositories to `git` and they've been relatively painless. However, in dealing with projects that started in the 1990s, you always expect a little (ok, a lot) of weirdness. I started out this process using the [`git-svn utility`](https://www.kernel.org/pub/software/scm/git/docs/git-svn.html) which converts repos from an SVN-style (directories `branches`, `tags`, and `trunk`) to git-style repo (`trunk` becomes master branch, and converts `branches` and `tags`). Part of this step is to map the authors in the repository to how git addresses its authors. I ran a bit of bash off the `svn log` to create a list of the authors:\n\n[code lang=\"bash\"]\n$ cd path/to/svn_repo\n$ svn log -q | awk -F '|' '/^r/ {sub(\"^ \", \"\", $2); sub(\" $\", \"\", $2); print $2\" = \"$2\" <\"$2\">\"}' | sort -u > authors.txt\n[/code]\n\nThis just generates a text file (`authors.txt`) with unique authors and I had to expand the mappings to read like so:\n\n[code lang=\"text\"]\nwsg4w = Wayne Graham <wsg4w@uva.edu>\n[/code]\n\nAfter getting the authors, I made a clone of the repository and ran in to the first issue. The default convention in SVN is to create directories for your stuff in branches, tags, and trunk. The `trunk` directory is where your stuff typically is, but this is more of a convention than an enforced policy. In my case the repo was structured like `trunk/branch/stuff`. This meant I needed to pass another flag to get the actual source files out.\n\n[code lang=\"bash\"]\n$ git svn clone https://subversion.lib.virginia.edu/repos/salem -T trunk/branch -A authors.txt --no-metadata\n[/code]\n\nThis would start and get some way through and start throwing 500 errors. Ok, no big deal as I think to myself, \"I'll just mirror the SVN repo locally and then I can run this again on my own machine without any crazy network stuff in the way.\"\n\n[code lang=\"bash\"]\n$ cd mkdir -p /tmp/salem\n$ svnadmin create /tmp/salem\n$ echo \"exit 0;\" > /tmp/salem/hooks/pre-revprop-change\n$ chmod +x /tmp/salem/hooks/pre-revprop-change\n$ svnsync init file:///tmp/salem https://subversion.lib.virginia.edu/repos/salem\n$ svnsync sync file:///tmp/salem\n[/code]\n\nThis too got through some of the download process and started throwing [500 errors](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes#5xx_Server_Error). After submitting a trouble ticket, it turned out the Subversion server was running out of memory trying to check out the nearly 4Gb repository. After the server admins increased the amount of memory on the virtual server running Subversion, I ran the `svnsync` again. After a good long while, I had a local copy of the entire repo and could on it without any network latency.\n\nSo I reran the `git svn` utility, but instead of an `https` connection, I change that to the `file` URI:\n\n[code lang=\"bash\"]\n$ git svn clone file:///tmp/salem -T trunk/branch -A authors.txt --no-metadata\n[/code]\n\nAfter a bit of churning, this process finished and I had a newly minted `git` repository with all the history from the SVN repo. I then start looking at things and notice that the repo is really big, even with all of the compression that occurs with `git`. I start looking around and notice that at some point in the project's history, all of the images that were being used were added to the project history. Not only that, there were also copies of all of those images in a tarball that was being tracked. Someone had realized this wasn't good and had removed it, but because of the way in which SCM systems work, we would continue to track these files.\n\nTo figure this out, I first counted the objects `git` was tracking.\n\n[code lang=\"bash\"]\n$ git count-objects -v\ncount: 5414\nsize: 41548\nin-pack: 40222\npacks: 1\nsize-pack: 2164015\nprune-packable: 0\ngarbage: 0\nsize-garbage: 0\n[/code]\n\nThat `size-pack` told me there was over 2Gb of data that it knew about. What's in there? I took a look at the `git index` and pulled the largest blobs out.\n\n[code lang=\"bash\"]\n$ git verify-pack -v .git/objects/pack/pack-*.idx | sort -k 3 -n | tail -5\n5b7e8c63a0bacd3dc2ab92db2d1d1cbc2359e69c blob   4715942 4715522 2077929726\nf3e135fd90caa6a05a1da13a2afc60c8a0af1063 blob   4743461 1703778 6751124\n6f9cbe6fa3fd702a70d666160329ef1176dd4a07 blob   8042973 7227900 1043663492\nd18b98c09c0dcbf9edc2f6ccf91672a399c8a79d blob   9662999 2747934 8477062\n17ccd45824bb4cb1e1c8b03e5780fa31175c18ab blob   48199680 47913744 93724263\n[/code]\n\nThis gave me references for the blobs, but I also needed to figure out what file was taking up so much space. I used the hash of the really big file as a good candidate for removal (`17ccd45824bb4cb1e1c8b03e5780fa31175c18ab` or `17ccd45` with its shorthand).\n\n[code lang=\"bash\"]\n$ git rev-list --objects --all | grep 17ccd45\n17ccd45824bb4cb1e1c8b03e5780fa31175c18ab trunk/branch/cocoon/html/Essex/vol2/gifs/gifs.tar\n[/code]\n\nSo now I know the path, let's see were this was introduced.\n\n[code lang=\"bash\"]\n$ git log --oneline --branches -- trunk/branch/cocoon/html/Essex/vol2/gifs/gifs.tar\n...\nddb3b1e Second commit\n[/code]\n\nI then rewrote the `git` history to yank references to this blob out since revision `ddb3b1e`.\n\n[code lang=\"bash\"]\n$ git filter-branch --index-filter 'git rm --ignore-unmatch --cached trunk/branch/cocoon/html/Essex/vol2/gifs/gifs.tar' -- ddb3b1e^..\n[/code]\n\nNow I need to delete the objects and prune and reindex the `git` database.\n\n[code lang=\"bash\"]\n$ git clone --no-hardlinks file:///Users/yourUser/your/full/repo/salem salem-smaller\n[/code]\n\nThis took a really long time, and after checking the repo size, it was still the same size. Time for some more drastic measures.\n\nAfter some poking around on StackOverflow (where you go when you need to figure out something like this), I came across this question [Which commmit has this blob?](https://stackoverflow.com/questions/223678/which-commit-has-this-blob#223890) which had some promising information about finding large files (read the entire thread; lots of really good advice). However, none of it seemed to be helping decrease the size of the repository `packfile`. In fact, when I would run the command to see what blobs were taking up the most room, I always found the same files, even if I yanked them out and rewrote the history.\n\nAfter several more hours of research I came across Ted Naleid's approach at [Finding and Purging Big Files From Git History](http://naleid.com/blog/2012/01/17/finding-and-purging-big-files-from-git-history). One of the parts I was missing was actually cloning the local directory. So, I cloned the local directory and removed the hard links:\n\n[code lang=\"bash\"]\n$ git clone --no-hardlinks file:///Users/yourUser/your/full/repo/salem salem-smaller\n[/code]\n\nAfter checking again the objects were much smaller:\n\n[code lang=\"bash\"]\n$ git count-objects -v\ncount: 0\nsize: 0\nin-pack: 32347\npacks: 1\nsize-pack: 1978880\nprune-packable: 0\ngarbage: 0\nsize-garbage: 0\n[/code]\n\nSmaller, but there's a lot more I can yank out that should have never been there. Using Ted's approach, I wrote a bash script to generate file paths for all the blobs.\n\n[code lang=\"bash\"]\n#! /usb/bin/env bash\n\nclear\n\necho \"Finding all objects in the repo...\"\ngit rev-list --objects --all | sort -k 2 > allfileshas.txt\n\n#git rev-list --objects --all | sort -k 2 | cut -f 2 -d\\ | uniq\necho \"Generating the SHA hashes and sorting them biggest to smallest...\"\ngit gc && git verify-pack -v .git/objects/pack/pack-*.idx | egrep \"^\\w+ blob\\W+[0-9]+ [0-9]+ [0-9]+$\" | sort -k 3 -n -r > bigobjects.txt\n\necho \"Generate object SHAs\"\nfor SHA in `cut -f 1 -d\\  < bigobjects.txt`; do\n  echo \"Looking up $SHA...\"\n  echo $(grep $SHA bigobjects.txt) $(grep $SHA allfileshas.txt) | awk '{print $1,$3,$7}' >> bigtosmall.txt\ndone;\n\necho \"Done.\"\necho \"Look at the bigtosmall.txt file for large files.\\n\"\necho \"You can remove any large files from your repo history with:\\n\"\necho \"\\t git filter-branch --prune-empty --index-filter 'git rm -rf --cached --ignore-unmatch MY-BIG-DIRECTORY-OR-FILE' --tag-name-filter cat -- --all\"\necho \"\\nYou can then compress it by cloning the repo without hard links:\"\necho \"\\t git clone --no-hardlinks file:///Users/yourUser/your/full/repo/path repo-clone-name\"\n[/code]\n\n[gist id=ed6d074267e60d7fef07 file=gistfile12.sh]\n\nThis generates several text files, but I was concerned with the `bigtosmall.txt`. This has the file paths of the large files in the repo.\n\n[code lang=\"text\"]\n17ccd45824bb4cb1e1c8b03e5780fa31175c18ab 48199680 trunk/branch/cocoon/html/Essex/vol2/gifs/gifs.tar\nd726f0a0cab047838e3405ad59d3c5399f42db87 12300550 trunk/branch/cocoon/html/maps/DHS/danvers_hist_soc/put_hse2.tif\n06a4076cac85350be52261a8f11df0ecb42d6696 10610964 trunk/branch/cocoon/html/maps/images/musick_nurse.tif\n6f9cbe6fa3fd702a70d666160329ef1176dd4a07 8042973 trunk/branch/cocoon/images/small/casey.tif\n5b7e8c63a0bacd3dc2ab92db2d1d1cbc2359e69c 4715942 trunk/branch/cocoon/html/archives/essex/eia/large/eia22r.jpg\n20ea6bb7b466cd4ba4716834bae7507989ff88b7 3861655 trunk/branch/cocoon/html/archives/essex/eia/large/eia06r.jpg\n8a37fabb82418c6e6b07abf08821a053b2dc4b11 3770686 trunk/branch/cocoon/html/archives/essex/eia/large/eia13r.jpg\n5fecd828115d3909cbe70de0be3936f96fb61868 3708386 trunk/branch/cocoon/html/maps/DHS/danvers_hist_soc/Summerhouse\n[/code]\n\nLooking through this, what I found was that most of these large files were in an `archives` directory, or `images` directory. What I did was move these more static files to a shared location on the server (most of which are a child of the `html` directory), so I ran the following to remove the `html` directory from the history:\n\n[code lang=\"bash\"]\n$ git filter-branch --prune-empty --index-filter 'git rm -rf --cached --ignore-unmatch trunk/branch/cocoon/html' --tag-name-filter cat -- --all\n[/code]\n\nThis forces `git` to go through all of the commits, removing references to these blobs (whose history can be managed as a separate entity). This took a while, and after the history rewrite was finished, I recloned the repo and the pack size was quite a bit smaller. After a lot of this, it got a lot smaller:\n\n[code lang=\"bash\"]\n$ git count-objects -v\ncount: 0\nsize: 0\nin-pack: 24334\npacks: 1\nsize-pack: 966726\nprune-packable: 0\ngarbage: 0\nsize-garbage: 0\n[/code]\n\nNow that it's under a single Gigabyte, I'm happy. So is GitHub with it's [\"suggestion\" that repos should be under 1Gb in size](https://help.github.com/articles/what-is-my-disk-quota/). There's probably more that could be cleaned up, but this is more finding odd files here and there. Hopefully this saves someone else some digging (including my future self for the next migration from Subversion to git).\n\nDealing with these legacy projects, particularly on performance issues related to decisions made 10 - 15 years ago in the workflow for source management can be difficult to track down and figure out; not to mention obscure and and somewhat impenetrable. However, there does come a point where people working on a long-running project will start to feel real pain in working on the system and being able to improve this performance helps ensure people continue to _want_ to work on a project rather than abandoning it due to frustration with the tooling.\n\n\n<blockquote>_After a lot of really high-CPU spiking git rewrites, I discovered [BFG](https://rtyley.github.io/bfg-repo-cleaner/) which is an alternative to the `git-filter-branch` strategy. It claims it's up to 720x faster. While I didn't do a timed comparison (I really wanted to stop messing with this), I can say that it was orders of magnitude faster. So much so that if I need to do this again, I'll probably use that tool first._</blockquote>\n"},{"id":"2015-03-03-something-about-php","title":"Something about PHP","author":"steven-lewis","date":"2015-03-03 06:28:29 -0500","categories":null,"url":"something-about-php","content":"We’ve spent the past month and a half learning PHP. It’s an arduous task only complicated by our own busy schedules. Trying to learn a new language becomes much more difficult when also trying to wrangle sixty undergraduates every week, or finish a dissertation, or find a job.\n\nAnd yet progress continues steadily. The decision to sideline the redesign of the Ivanhoe information page was a good one in that it’s given us more time to work on the fundamentals of programming.   Our development team has produced multiple promising wireframes of the redesigned Ivanhoe game, and the group is close to approving a prototype design of the game. The rest of us are in the process of researching Wordpress plugins, hoping to find one that will provide email and social media notifications to players when someone makes a new move in a game that they’re playing. The goal of all of this is to make the Ivanhoe game responsive and appealing, something that people _want _to play.\n\nOf course, all of this leads inevitably back to grasping the fundamentals of PHP coding. The PHP homework has been many of our single greatest struggle, lurking unfinished in some corner of our minds even as we made progress on other areas of our larger project. PHP is difficult to internalize not because it’s radically unfamiliar but because it’s similar enough to written English to cause repeated problems. For example, to express “and” in PHP, one would write “&&” rather than “&.” A typical line of PHP code is _almost _intuitive enough to write unaided, and is simultaneously just complicated enough to make an aspiring programmer throw up their hands in frustration. Mistakes will be—and are currently being—made, and it’s at times like this that we’re most lucky to have such a patient and generous Scholar’s Lab staff.\n\nP.S.: Does anyone have ideas for great uses of Wordpress plugins as teaching tools? If so, send them our way!\n"},{"id":"2015-03-09-neatline-2-4-0","title":"Neatline 2.4.0","author":"wayne-graham","date":"2015-03-09 10:52:52 -0400","categories":["Announcements","Experimental Humanities","Research and Development"],"url":"neatline-2-4-0","content":"We're happy to announce a new version of Neatline which adds a couple new features along with resolving a few small issues.\n\nThe two main features in this release  were implemented based on community feedback. First, it's now possible to set the opacity of a WMS layer when its selected using the \"selected\" opacity setting. Previously this setting only pertained to drawn geometries on a Neatline record. Second is the ability for custom themes to provide containers for Neatline widgets. This gives theme developers more control over where elements of a Neatline exhibit are displayed on the page.\n\nThere were also some issues that are resolved. An optimization was introduced in Neatline 2.3 which caused Neatline to not render WMS maps created using MapWarper. This has been corrected and maps created using the [NYPL MapWarper ](http://maps.nypl.org/warper/)and [Harvard WorldMap WARP](http://warp.worldmap.harvard.edu/) tools properly render in Neatline. By the way, these two resources provide a great number of maps for you to use in Neatline without needing to run your own instance of [GeoServer](http://geoserver.org/) or other service. We also fixed an issue where styles with an underscore (\"_\") in them would not render properly, and we fixed an issue that would move SVG layers at specific zoom levels.\n\nYou can check out the [Changelog](https://github.com/scholarslab/Neatline/blob/master/CHANGELOG.md) for more detail on these changes. As always, you can download the latest release from the [Omeka Add-Ons  Repository](http://omeka.org/add-ons/plugins/neatline/). If you run into any issues, you can always ask a question on the [Omeka Forums](http://omeka.org/forums/) or submit an issue or feature request on our [issue tracker](https://github.com/scholarslab/Neatline/issues).\n"},{"id":"2015-03-10-watermarking-and-ocr-ing-your-images","title":"Watermarking and OCR-ing Your Images","author":"ammon-shepherd","date":"2015-03-10 06:34:00 -0400","categories":["Research and Development"],"url":"watermarking-and-ocr-ing-your-images","content":"In the process of my dissertation research I have accumulated over 2,000 images, nearly all scans of documents. One goal of my dissertation is to make these documents open and available (where appropriate) in an Omeka repository. In order to more correctly attribute these documents to the archives where I got them, I need to place a watermark on each image.\n\nI also need the content of the documents in a format to make it easy to search the text.\n\nThe tools to do each of those steps are readily available, and easy to use, but I needed a script to put them together so I can run them on a handful of images at a time, or even hundreds at a time.\n\nI'll walk through the problem and show the steps I used to solve it.\n\nWhen at the [Neuengamme Concentration Camp Memorial Archive](http://nazitunnels.org/2013/09/neuengamme-second-week-part-2.html) near Hamburg in the summer of 2013, I found about 25 testimonials of former inmates. In most cases I took a picture of the written testimonial (the next day I realized I could use their copier/scanner and make nicer copies). So I ended up with quite a number of folders, each containing a number of images.\n\n[![Lots of images in folders](http://scholarslab.org/wp-content/uploads/2014/11/Screen-Shot-2014-11-18-at-10.52.38-AM.png)](http://scholarslab.org/wp-content/uploads/2014/11/Screen-Shot-2014-11-18-at-10.52.38-AM.png)\n\n\n\nSo the goal became to watermark each image, and then run an OCR program on each image to get the contents into plain text.\n\n\n## Watermark\n\n\nThere are many options for water marking images. I chose to use the incredibly powerful ImageMagick tool. The ImageMagick website has a [pretty good tutorial](http://www.imagemagick.org/Usage/annotating/) on adding watermarks to single images. I chose to add a smoky gray rectangle to the bottom of the image with the copyright text in white.\n\nThe image watermark command by itself goes like this:\n\n    \n    width=$(identify -format %w \"/path/to/copies/filename.png\"); \\\n    s=$((width/2)); \\\n    convert -background '#00000080' -fill white -size \"$s\" \\\n    label:\"Copyright ©2014 Ammon\" miff:- | \\\n    composite -gravity south -geometry +0+3 - \\\n    \"/path/to/copies/filename.png\" \"/path/to/marked/filename.png\"\n    \n\n\n\n\nThis command can actually be run on the command line as is (replacing the copyright text and paths to files of course). The command is actually three commands and should be written on one line, but for ease of reading, the backslash (\\) denotes where I split the commands onto the next line. I'll explain the command below.\n\nThe first line gets the width of the image to be watermarked and sets it to the variable \"width\".\n\n    \n    width=$(identify -format %w \"/path/to/copies/filename.png\"); \\\n    \n\n\n\n\nThe second line gets half the value of the width, and sets it to the variable \"s\".\n\n    \n    s=$((width/2)); \\\n    \n\n\n\n\nThe third line starts the ImageMagick command (and is broken onto several lines using the \\ to denote that the command continues on the next line). The code from `convert` to the pipe `|` creates the watermark, a dark grey rectangle with white text at the bottom of the image.\n\n    \n    convert -background '#00000080' -fill white -size \"$s\" label:\"Copyright ©2014 Ammon\" miff:- | \\\n\n\n\n\nThe rest of the command tells ImageMagick where to put the watermark, the original image to use, and where to put the image with a watermark and what to call it.\n\n    \n    composite -gravity south -geometry +0+3 - \\\n    \"/path/to/copies/filename.png\" \"/path/to/marked/filename.png\"\n    \n    \n\n\nThe results can be seen on the following image.\n\n\n\n[![Watermark applied!](http://scholarslab.org/wp-content/uploads/2014/11/Screen-Shot-2014-11-18-at-1.40.12-PM-1024x260.png)](http://scholarslab.org/wp-content/uploads/2014/11/Screen-Shot-2014-11-18-at-1.40.12-PM-1024x260.png)\n\n\n\n\n\n\n\n\n## OCR\n\n\nMost of the images I have are pictures of typed up documents so they are good candidates for OCR (Optical Character Recognition), or grabbing the text out of the image.\n\nOCR can be done using a program called [tesseract](https://code.google.com/p/tesseract-ocr/).\n\nThe tesseract command is relatively simple. Give it an input file name, an output file name, and an optional language.\n\n    \n    tesseract \"/path/to/input/file.png\" \"/path/to/output/file\" -l deu\n\n\nThis will OCR file.png and create a file named file.txt. The `-l` (lowercase letter L) option sets the language to German (deu[tsch]).\n\nBelow are two examples. Note that the translation is not letter-for-letter perfect, but the software does a good job.\n\n\n\n[![ocr-example2](http://scholarslab.org/wp-content/uploads/2015/02/ocr-example2.png)](http://scholarslab.org/wp-content/uploads/2015/02/ocr-example2.png) [![ocr-example1](http://scholarslab.org/wp-content/uploads/2015/02/ocr-example1.png)](http://scholarslab.org/wp-content/uploads/2015/02/ocr-example1.png)\n\nThe benefits of OCR'ing documents is apparent when needing to search for specific details.\n\n\nNow that I have each page OCR'ed, I can do searches on these files, where otherwise I had to read through the entire PDF page by page or look at every single image. For example, today I’m looking through a 50+ page PDF transcript of a survivor interview to find the parts where she talks about her experiences at the Porta Westfalica camp. While I will read through each page individually, I can get a quick sense of where I should be looking by doing a search on the OCR'ed pages to find out where the term ‘Porta’ is found.\n\n\n\n\n[![Screen Shot 2015-01-30 at 1.17.16 PM](http://nazitunnels.org/wp-content/uploads/2015/01/Screen-Shot-2015-01-30-at-1.17.16-PM-1024x180.png)](http://nazitunnels.org/wp-content/uploads/2015/01/Screen-Shot-2015-01-30-at-1.17.16-PM.png)\n\n\n\n\nNow I know that at least on pages 47 and 48 is where I’ll find some description of her time in Porta Westfalica.\n\n\n\n\n\n## The Script\n\n\n[![img009](http://scholarslab.org/wp-content/uploads/2015/03/img009-300x183.jpg)](http://scholarslab.org/wp-content/uploads/2015/03/img009.jpg)\n\nImagine typing in those commands for every single image that I want to OCR and watermark. That would take way too long, and computers are really good at doing repetitive tasks, so I'll let the computer take care of that. That's where writing a script comes in very handy. A script is basically a file that tells the computer a bunch of commands to execute. In this case the commands are the ImageMagick and tesseract commands; with some logic thrown in to find the right files and put the results in the right place.\n\nThe mascot is a crop from this image of monks (found in a 1911 book about characters of the Middle Ages, _SCENES & CHARACTERS OF THE MIDDLE AGES_, By the Rev. EDWARD L. CUTTS, b.a. LATE HON. SEC. OF THE ESSEX ARCHÆOLOCICAL SOCIETY, [http://www.gutenberg.org/files/42824/42824-h/42824-h.htm#Page_39](http://www.gutenberg.org/files/42824/42824-h/42824-h.htm#Page_39)). The name 'cowl' (Copy, OCR, Watermark, Language) is inspired by monks, whose work it was to copy and transcribe documents (similar to what this script does).\n\nOriginally, I wrote a script using BASH, a shell scripting language specific to Unix based computers. That script is available at my GitHub repo: [https://github.com/mossiso/ocr-watermark](https://github.com/mossiso/ocr-watermark)\n\nA nice write up on how to use this script in its original format (and the basis for the content of this post) is found on my dissertation blog: [http://nazitunnels.org/2014/11/watermarking-and-ocring-your-images.html](http://nazitunnels.org/2014/11/watermarking-and-ocring-your-images.html) The most up to date steps will be at the GitHub repo linked above. I plan to update the BASH script to behave the same way as the below-detailed script, so things will definitely change in the future.\n\nI was able to rewrite the script in a more universally available language; Ruby. That script is available here: [https://github.com/mossiso/cowl](https://github.com/mossiso/cowl)\n\nHere is how to use the Ruby script. The most up to date version of these steps is at the GitHub repo linked above.\n\n\n## Set up\n\n\nThis assumes you already have ruby, git and bundler installed.\n\n\n\n\t\n  * Installing Ruby: [https://www.ruby-lang.org/en/documentation/installation/](https://www.ruby-lang.org/en/documentation/installation/)\n\n\t\n  * Installing Git: [http://git-scm.com/book/en/v2/Getting-Started-Installing-Git](http://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\n\n\t\n  * Installing bundler: [http://bundler.io/#getting-started](http://bundler.io/#getting-started)\n\n\t\n  * NOTE: At the moment, you should also have tesseract and GhostScript installed. There are ruby gems to handle these, but they are not playing nicely yet, so these commands are called from the command line for now.\n\n\t\n    * Instructions for installing tesseract: [https://code.google.com/p/tesseract-ocr/wiki/ReadMe](https://code.google.com/p/tesseract-ocr/wiki/ReadMe)\n\n\t\n    * Instructions for installing GhostScript: [http://www.ghostscript.com/doc/9.15/Install.htm](http://www.ghostscript.com/doc/9.15/Install.htm)\n\n\t\n      * If you're on a Mac, it is highly recommended that you use homebrew ([http://brew.sh/](http://brew.sh/)) or some such thing for installing programs\n\n\n\n\n\n\n\n\nDownload the repo in your home directory:\n\n    \n    <code>git clone https://github.com/mossiso/cowl\n    </code>\n\n\nThis creates a folder called cowl and puts four files into it. Now change directories into the cowl directory.\n\n    \n    <code>cd cowl\n    </code>\n\n\nGet the required gems by running bundler.\n\n    \n    <code>bundle\n    </code>\n\n\nEdit the cowl file to change the default copyright text. Change the line (line 21 in the image below) that looks like this:\n\n    \n    <code>options.mark_text = \"Copyright ©2014 The Marvellous and awesome Me\"\n    <a href=\"http://scholarslab.org/wp-content/uploads/2015/02/edit-copyright1.png\"><img src=\"http://scholarslab.org/wp-content/uploads/2015/02/edit-copyright1.png\" alt=\"edit-copyright\" height=\"418\" class=\"aligncenter size-full wp-image-11562\" width=\"886\"></img></a>\n    </code>\n\n\n\n\n## [](https://github.com/mossiso/cowl#examples)Examples\n\n\nIn your terminal program, enter into the directory where you have images.\n\n    \n    <code>cd /path/to/images/\n    </code>\n\n\nRun the cowl command (if you ran the git command in your home directory, and your home directory is /home/billy)\n\n    \n    <code>ruby /home/billy/cowl/cowl\n    </code>\n\n\nRunning without any options will create a copy of the images, OCR them, put a watermark on the copies, and combine them all into one PDF.\n\nThe default text for the watermark is hard coded in the cowl script. You can change it there, or use the -t option.\n\n    \n    <code>ruby /home/billy/cowl/cowl -t \"Copyright ©2015 Billy Jorgenson Photography\"\n    </code>\n\n\nTo change the language used in the OCR to English (since most of my documents are in German, I hard coded it to use German :) ).\n\n    \n    <code>ruby /home/billy/cowl/cowl -l eng\n    </code>\n\n\nIf you have a PDF file to start with, run the command with the -g option. This will break the PDF into PNG images, then make copies, run the OCR, put on the watermarks, and finally create a new PDF with the watermark on each page.\n\n    \n    <code>ruby /home/billy/cowl/cowl -g\n    </code>\n\n\nYou can also use the tool as a way to make copies of your images (with or without a watermark).\n\n    \n    <code>ruby /home/billy/cowl/cowl -nop # no watermark, ocr, or PDF\n    \n    ruby /home/billy/cowl/cowl -op # no ocr or PDF\n    </code>\n\n\nIf you run into any issues or have an idea for an upgrade, feel free to add an issue to the GitHub repo, or even send me an email.\n\nHappy cowl'ing!\n"},{"id":"2015-03-11-printing-things-that-print-a-miniature-hand-press-project","title":"Printing Things That Print: A Miniature Hand-Press Project","author":"ethan-reed","date":"2015-03-11 10:24:19 -0400","categories":["Experimental Humanities"],"url":"printing-things-that-print-a-miniature-hand-press-project","content":"For the past few months, fellow English PhD candidate [James Ascher](https://twitter.com/jpsa) and I have had a small side-project going on in the Makerspace: printing a small, desktop-sized hand press, and getting it to work consistently.  The model we found calls it a “[Pocket Gutenberg](http://www.thingiverse.com/thing:113044),” but the idea of a small, hand-powered, personal “hobby” printing press is an historic one going back to Ben Lieberman and the “Liberty Press” – a history James will hopefully be posting about soon.\n\nI want to talk about our experience making this thing and trying to get it to work.  By putting it up here, I’m hoping it will be an example of the kinds of small exploratory projects that often become gateways into the growing scholarly field of remaking old technologies. On my end, I was also inspired by [Jentery Sayers](http://www.jenterysayers.com/) in his recent visit to the Scholars’ Lab (all the way from the [Maker Lab at UVic](http://maker.uvic.ca/)) in which he discussed exactly this kind of project, though on a much larger and more rigorous scale (we have [his talk in podcast form - check it out](http://scholarslab.org/podcasts/podcast-jentery-sayers-on-remaking-the-past/)!).\n\nSo what did we make? As linked above, we started by printing a desktop hand press. James then printed a plate with the motto, “Collaborate -> Iterate -> Discuss (and Print).” Using some ink we had meant for rubber stamps, we inked the plastic plate and gave it a shot – as you can see, what came out is readable (sort of) but not very pretty.\n\n\n[![IMG_1118](http://scholarslab.org/wp-content/uploads/2015/02/IMG_1118-300x225.jpg)](http://scholarslab.org/wp-content/uploads/2015/02/IMG_1118.jpg)\n\n\nWe jerry-rigged a few modifications (i.e. taped-on cardboard to raise and even out the paper) and then I printed out a single set of type from [another model on Thingiverse](http://www.thingiverse.com/thing:296026), and sized it so that it would fit with our press. Printing with this type was a real pain as there was no way to keep them in place – a problem which, again, involved another series of jerry-rigged semi-solutions. Until we figured out a way to make this setting and pressing of type easier, our new press was going to be very limited in its applications.\n\n[gallery link=\"file\" columns=\"2\" ids=\"11586,11621\" size=\"medium\"]\n\nAs you can see, our goal in re-creation here wasn’t so much fidelity to a specific historical object as it was to explore the capabilities of an older technology (the hand-press) shrunk down in a new way to an individual desktop size that anyone could use on their own. Clearly, even this more modest goal led to a series of joyful problems.  It turns out stamp ink is, well, mainly meant for stamps, and that not all paper is created equal.  These may seem like obvious insights (and I’m sure they were to James, who knows much more about books as physical objects than I do) but for someone only recently beginning to learn about bibliography, textual studies, and the history of printing, these were practical concerns I had heard of but never had to really deal with before.  And they are all things we are still working on with making our press work.\n\nSo why does this matter? In an earlier post titled “[The Relevance of Remaking](http://maker.uvic.ca/remaking/),” Sayers offers some questions that I found to be helpful in making meaning out of experiences with old technology. Two in particular – “From what materials was it [the old technology] made?” and “Through what measures was it deemed a success?” – were particularly revealing in our case.  I was faced with problems like: what kind of paper works best when you are using plastic type? There’s a big difference between printer scrap paper we used originally and the softer paper, like from a cheap paperback, that we ultimately decided would work best.  For that matter, I wondered what kind of paper works best using _metal _type – and what about differences in kinds of metals? What about different kinds of ink? What’s in “gel-based” stamp ink, anyways (as it reads on the back of the box)?  When we tried it again with a small sample of the oil-based kind of ink used in relief printing (courtesy of [Josef Beery](http://www.josefbeery.com/)), and the prints came out much clearer, even after multiple presses (despite being limited by the granularity of the Replicator 2 model and print).\n\n\n[![mini-press relief ink](http://scholarslab.org/wp-content/uploads/2015/02/mini-press-relief-ink-e1425004210362-225x300.jpg)](http://scholarslab.org/wp-content/uploads/2015/02/mini-press-relief-ink.jpg)\n\n\nThese aren’t new problems – they’ve been around for as long as people have been trying to print things. And despite being surrounded by scholars interested in these fields here at UVa, it’s been very easy for me to take them for granted on a day-to-day basis – something I didn’t realize until trying to deal with them first-hand.\n\nTo summarize: What James and I are up to here is very small-scale. But while piecing together something from a 3D printer is just the tip of the iceberg in terms of remaking old technologies, it’s been a really good way for me to get my foot in the door – although the technology we’re building is not strictly historical, the problems associated with it are historic ones.  Used in conjunction with the kinds of resources we have at the [Rare Book School](http://www.rarebookschool.org/) or [Special Collections](http://small.library.virginia.edu/), this was a useful supplement to the study of books and book-making that I had the power to explore on my own. It certainly opened my eyes to a whole set of new questions in a physical, immediate way.\n\nRegarding the bigger picture: we haven’t thought through every potential use of this press, or what scholastic infrastructure it will fit into as a project.  In that sense it is surely subject to the “speculative condition” of purely exploratory research Bethany identifies and discusses in [this enlightening post about speculative computing](http://nowviskie.org/2014/speculative-computing/); but I would maybe liken our project even more to the kinds of projects she has [described in another post as \"too small to fail.\"](http://nowviskie.org/2012/too-small-to-fail/) In this sense, even if our press ended up being useless, this particular failure could serve as “a concrete experiment” our peers can talk to us about and learn from. But this smallness and speculative-ness have their payoffs – most of what I ended up learning about remaking old technologies I came across because we had already started working with the desktop hand press. It was only after diving in and having problems that I had the impetus to see what others in the Maker community had to say about similar projects.  What I found was really encouraging and left me with a whole bunch of new questions regarding our mini-project: when Sayers asks in another post, [“Why Fabricate?”](http://maker.uvic.ca/whyfab/), that we consider “for which methodologies and research areas does material depth especially matter,” and when Devon Elliott _et al. _ask in [“New Old Things”](http://www.cjc-online.ca/index.php/journal/article/view/2506) (an article cited in Sayers’ piece) for us to seek out “projects that allow us to imaginatively remake past technological artifacts and to experiment with past technological worlds,” the project acquired a new level of meaning for me.\n\nI’m not sure if “make first, ask questions later” is a policy I would always encourage, but in this small-scale situation, given the time, resources, and encouragement to explore, following my interests ended up leading to not only to real, living problems, but also real opportunities to combine my work at the Makerspace with my work in literary study. Success!  In this instance, exploration resulted in the surprise of discovering a new set of important questions already being explored by others.\n"},{"id":"2015-03-13-novice-struggles-and-expert-blindness-how-my-discomfort-with-php-will-make-me-a-better-instructor","title":"Novice struggles and expert blindness: How my discomfort with PHP will make me a better instructor","author":"jennifer-grayburn","date":"2015-03-13 09:38:45 -0400","categories":null,"url":"novice-struggles-and-expert-blindness-how-my-discomfort-with-php-will-make-me-a-better-instructor","content":"It was my third blank screen. I switched back over to my text editor and tried again. I looked at the format of my PHP, looked at my functions. I looked up the PHP documentation, deleted elements that did not seem right, and saved again. Back in my browser, the screen was still blank. Last week, my computer at least gave me an error warning to let me know I had incorrect code. Today, there was nothing—even the static html components failed to appear. At this point, I had no idea what to do. I had exhausted my knowledge of PHP, exhausted my understanding of what to look for or how to fix it. After two straight hours, I was frustrated, overwhelmed, and tired, yet had nothing to show for the time and effort I put into it. It is the first time I remember feeling like I would and could never build the competency I needed to complete our remodel of [Ivanhoe.](http://ivanhoe.scholarslab.org/) I felt inadequate. I felt stupid. I made excuses that maybe I just was not 'meant' to work with computers.\n\nLast week, Steven Lewis [blogged about his own difficulties with our PHP homework.](http://scholarslab.org/uncategorized/something-about-php/)  For him, the challenge is not that the PHP is completely unfamiliar, but rather than it incorporates elements of English that are familiar, though not used in a familiar or intuitive way. This is a key observation, for it shows how we try to make sense of the PHP based on our own experiences and cognitive framework. We see symbols that look familiar and try to use them and read them in a way that already makes sense to us. I have used this type of association in the past when learning new languages; in order to make sense of an unfamiliar language, I look for cognates or contexts and grammatical elements that look familiar based on my experiences with English. Of course, this process works better for some languages than others (for examples, there are less similarities between English and highly inflected languages or languages that use different scripts), but it nevertheless reveals how we learn by association and then application based on our established knowledge set.\n\nSo on one hand, I understand how this comparison of code to language can make PHP look deceptively intuitive, especially when symbols work in a way that is unfamiliar to our native language. For me, however, the challenge is less the code form as it is the logic behind it. The logic of PHP is more mathematical than grammatical, more literal and precise than our everyday language. This difference has been my biggest obstacle, for while I can memorize and regurgitate the system of code, I still struggle to grasp how all of the pieces intersect and how to solve problems when they arise.  I still cannot apply my knowledge to new contexts, cannot see how one set of tasks are similar to another, cannot anticipate how literally the computer will process my command. Even the resources available to consult are new and unfamiliar; traditional experts and peer reviewed publications are replaced by flexible online forums with an overwhelming number comments suggesting alternative approaches and opposing advice. Last week was a low point. I was frustrated and depressed; I did not know how to start or even which questions to ask when given the chance. It felt hopeless and I had the strongest urge to give up.\n\nI am familiar with the concept of “expert blindness,” but had always associated it with content, assumed that an expert knew so much information that they were unable to unpack it and introduce it in an order that made sense to someone new to the field. My PHP-exhaustion last week, however, expanded my understanding of expert blindness to incorporate not just information, but also skills. It has been years since I have been truly stumped, completely unsure how to even proceed, let alone succeed. Over the past 10 years as an undergraduate and graduate student, I have not only expanded my knowledge of my field, but also developed proficiency in the tools and skills necessary to analyze and present that knowledge. If I run into an issue with my research, I know who to consult, which publications to scan, how to read the plans and diagrams, what professional expectations to meet, and how to integrate that information with the vast compilation of knowledge I already have. In other words, I can apply my knowledge to new contexts and problem solve so efficiently that I cannot remember a time when I could not. My own expert blindness crept up on me, affecting not only how I learn, but also how I expect others to learn.\n\nAs an instructor, I have always tried to established clear learning goals and to execute them in a way that engages the students actively. I have constructed assignments to help them learn new vocabulary, develop their writing, and analyze the formal qualities of an unknown artifact. Looking back, however, I recognize moments when I took students' knowledge and skills for granted because they are things I do without thinking. Most recently, I remember asking my students to analyze a building plan in order to identify different housing forms and, therefore, different eras of construction. While I lectured on these structure forms in class, the students' papers revealed that some did not understand how to read the plans I introduced. When a plan shows building walls with varying thicknesses of lines, it is not at all obvious that other line variations showing stages of construction do not in fact display real striations or stipulations in the landscape. While I was able to revisit the topic in class after recognizing my lapse of instruction, it made me realize how much we gloss over--especially in lecture courses--and how easy it is for students to repeat that information correctly on an exam even if they do not fully grasp it. I wonder if my students ever felt as I did during my PHP exercises—confronted with a task or certain methods that frustrated them, throwing everything they had at it with no particular order or insight. I wonder if I then interpreted their regurgitation of my own words as deep and meaningful learning. How many students drop classes or majors because they were frustrated and felt they could never understand? Is there a better way to present course content, not to get all of the content in, but rather to ensure that the students come away enlightened and enriched by what they confronted, confident that they can continue to develop those skills not only as students, but also as life-long learners? How do we encourage them to ask questions, even if they seem too broad or simple? While I came to the Praxis Program hoping to gain concrete digital skills, I will come away with a renewed affinity with my students as learners confronted with something so new that it challenges them. As academics, it is often very easy to stay within the established bounds of our disciplines, very difficult to get out of our comfort zones in order to learn something (anything) that requires completely different skill sets than our own. I wonder if we should perhaps make this discomfort a bigger priority within the graduate curriculum in order to broaden our thinking as researchers and think critically about our methods as instructors.\n\nMy frustration with PHP is not gone, but my realization that this frustration is a natural part of the learning process--something I expect my own students to experience--helps me push through it. We are now working directly with the Ivanhoe code and the practical application of more abstract concepts allows me to see concrete results. By creating my own localized git branch, I have been able to play with the code on my own terms, to change something and see how it affects the site in a low stakes setting. But I never would have made it this far without the support and encouragement of the dedicated Scholars' Lab staff. They have a willingness to repeat concepts, ability to recontextualize and reword information in personalized ways, and openness to my never-ending stream of questions. Most importantly, they have encouraged us to jump in, to make mistakes, to learn from them. While they have their own expertise in the field, their own \"content knowledge\" of how to do these tasks themselves, they have also developed what Mitchell J. Nathan et. al in \"[Expert Blind Spot](http://www.colorado.edu/ics/sites/default/files/attached-files/00-05.pdf)\" describes as \"pedagogical content knowledge.\" The former refers to their own competence in the field, while the latter underscores their anticipation of the learning obstacles of novices and their ability to structure and teach in a way that meets the needs of learners rather than the expectations of experts (Nathan 7). They do not distinguish themselves as experts from us as students; rather, there is the acknowledgement that they, too, were once where we are (including their own humanities background), that they have merely had more time to learn and master these skills. This self-awareness allows them to recognize learning opportunities and (dare I say?) enjoy them. At our first Praxis meeting months ago, [Jeremy Boggs](http://scholarslab.org/people/jeremy-boggs/) showed us the xkcd comic, [xkcd: Ten Thousands](http://xkcd.com/1053/). While I thought it was a nice mindset then, I saw myself exclusively as the character with knowledge to share. I recall this comic now and it resonates more deeply with my recent experiences; I am both characters at once as long as I can embrace unrestrictedly the expertise others offer me and recognize that I can offer expertise in return. Perhaps it is impossible to do either successfully without grasping the other. Regardless, there is no doubt that I will embrace both as I move forward.\n"},{"id":"2015-03-16-podcast-rob-nelson-on-topic-modeling-in-the-humanities","title":"Podcast: Rob Nelson on Topic Modeling in the Humanities","author":"laura-miller","date":"2015-03-16 12:13:21 -0400","categories":["Events","Podcasts"],"url":"podcast-rob-nelson-on-topic-modeling-in-the-humanities","content":"**The Potential and Pitfalls of Topic Modeling for Humanities Research**\n\nThis talk will introduce the text-mining technique called topic modeling, briefly explaining what it is and how it's done. It will then turn to more substantial questions: what does this technique offer humanities researchers and what are its methodological limitations and problems? Both the potential and the pitfalls of topic modeling will be illustrated through research that uses topic models of newspapers to explore Civil War nationalism.\n\nDr. Robert K. Nelson is the director of the Digital Scholarship Lab at the University of Richmond. His [current research](http://dsl.richmond.edu/civilwar/index.html) uses a text-mining technique called topic modeling to uncover themes and reveal historical patterns in massive amounts of text from the Civil War era.  He is currently completing two projects from this research.  One is a digital project that will publish and analyze multiple topic models of Civil War-era archives including the [Richmond _Daily Dispatch_](http://dsl.richmond.edu/dispatch/) and the _New York Times_.  The other is an essay that analyzes these models to produce a comparative analysis of Union and Confederate nationalism and patriotism.\n\nThis event is co-sponsored by the [Institute for Advanced Technology in the Humanities](http://www.iath.virginia.edu/), the [Data Science Institute](http://dsi.virginia.edu/)'s Center for the Study of Data and Knowledge, and the Scholars' Lab.\n\nThis talk was recorded in Alderman Library, Rm 421 on February 25, 2015.  Click below to stream the podcast, and follow along with [Rob's slides](http://scholarslab.org/wp-content/uploads/2015/03/UVA_2015_talk.pdf).  If you'd like to hear more from the Scholars' Lab, subscribe to our podcast series [on iTunesU](https://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619?mt=10).\n\n[podloveaudio src=\"http://a643.phobos.apple.com/us/r30/CobaltPublic3/v4/02/76/7d/02767d82-95d8-2f30-6639-b9acaaa3f16a/304-7485002225613836972-nelson.mp3\"]\n\n\n"},{"id":"2015-03-25-hearing-silent-woolf","title":"Hearing Silent Woolf","author":"brandon-walsh","date":"2015-03-25 05:12:43 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"hearing-silent-woolf","content":"_[This week I presented at the [2015 Huskey Research Exhibition](http://gradcouncil.com/2015-sessions/) at UVA. The talk was delivered from very schematic notes, but below is a rough recreation of what I discussed. The talk I gave is a crash course in a new project I've started working on with the generous help of the [Scholars' Lab](http://scholarslab.org) that thinks about sound in Virginia Woolf's career using computational methods. [Eric Rochester](http://scholarslab.org/people/eric-rochester/), especially, has been endlessly giving of his time and expertise, helping me think through and prototype work on this material. The talk wound up receiving first prize for the digital humanities panel of which I was a part. The project is still very much inchoate, and I'd welcome thoughts on it. Cross-posted on [my own blog](http://bmw9t.github.io/blog/2015/03/23/woolf-huskey/).]_\n\nWhen I talk to you, you make certain assumptions about me as a person based on what you're hearing. You decide whether or not I might be worth paying attention to, and you develop a sense of our social relations based around the sound of my voice. The voice conveys and generates assumptions about the body and about power: am I making myself heard? Am I registering as a speaking voice? Am I worth listening to?\n\n[![occupy](http://scholarslab.org/wp-content/uploads/2015/03/occupy-300x199.jpg)](http://scholarslab.org/wp-content/uploads/2015/03/occupy.jpg)The human microphone, made famous by Occupy Wall Street, nicely encapsulates the social dimensions of sound that interest me: one person speaks, and the people around her repeat what she says more loudly, again and again, amplifying the human voice without technology. Sound literally moves through multiple bodies and structures the social relations between people, and the whole movement is an attempt to make a group of people heard by those who would rather not listen.\n\nAs a literary scholar, I am interested in how texts can speak in similar ways. The texts we read frequently contain large amounts of speech within them: conversations, monologues, poetic voice, etc. We talk about sound in texts all the time, and the same social and political dimensions of sound still remain even if a text appears silent on the page. If who can be heard and who gets to speak are both contested questions in the real world, they continue to structure our experiences of printed universes.\n\nAll of this brings me to the quotation mark. The humble piece of punctuation does a lot of work for us every day, and I want to think more closely about how it can help us understand how texts speak. The quotation mark is the most obvious point at which sound meets text. Computational methods tend to focus on the vocabulary of a text as the building blocks of meaning, but they can also help us turn quotation marks into objects of inquiry. Quotation marks can tell us a lot about how texts engage with the human voice, but there are _lots_ of them in texts. Digital methods can help us make sense of the scale.\n\n[![woolf](http://scholarslab.org/wp-content/uploads/2015/03/woolf-219x300.jpg)](http://scholarslab.org/wp-content/uploads/2015/03/woolf.jpg)I examine Virginia Woolf's quotation marks, in particular, for a number of reasons. Aesthetically, we can see her bridging the Victorian and modernist literary periods, though she tends to fall in with the latter of the two. Politically, she lived through periods of intense social and political upheaval at the beginning of the twentieth century. Very few recordings of Woolf remain, but she nonetheless thought deeply about sound recording. The worldwide market for gramophones exploded during her lifetime, and her texts frequently featured technologies of sound reproduction. Woolf's gramophones frequently malfunction in her novels, and I'm interested in seeing how her quotation marks might analogously be irregular or broken intentionally. Woolf is especially good for thinking about punctuation marks in this way: she owned a printing press, and she often set type herself.\n\nThe following series of histograms gives a rough estimation of how Woolf's use of quotation changes over the course of her career. [On GitHub](https://github.com/erochest/woolf/commits/master)  you can find the script I've been working on with Eric to generate these results. The number of quotations is plotted on the y-axis against their position in the novel on the x-axis, so each histogram represents more quoted speech with higher bars and more concentrated darknesses. If you have an especially good understanding of a particular novel, _Mrs. Dalloway_, say, you could pick out moments of intense conversation based on sudden spikes in the number of quotations. The histograms are organized in such a way that to read chronologically through Woolf's career you would read left to right line by line, as you would the text of a book. The top-left histogram is Woolf's earliest novel, the bottom-right corner her last.\n\n[![1915_the_voyage_out](http://scholarslab.org/wp-content/uploads/2015/03/1915_the_voyage_out-300x225.jpg)](http://scholarslab.org/wp-content/uploads/2015/03/1915_the_voyage_out.jpg) [![1919_night_and_day](http://scholarslab.org/wp-content/uploads/2015/03/1919_night_and_day-300x225.jpg)](http://scholarslab.org/wp-content/uploads/2015/03/1919_night_and_day.jpg) [![1922_jacobs_room](http://scholarslab.org/wp-content/uploads/2015/03/1922_jacobs_room-300x225.jpg)](http://scholarslab.org/wp-content/uploads/2015/03/1922_jacobs_room.jpg)[![1925_mrs.dalloway](http://scholarslab.org/wp-content/uploads/2015/03/1925_mrs.dalloway-300x225.jpg)](http://scholarslab.org/wp-content/uploads/2015/03/1925_mrs.dalloway.jpg) [![1927_to_the_lighthouse](http://scholarslab.org/wp-content/uploads/2015/03/1927_to_the_lighthouse-300x225.jpg)](http://scholarslab.org/wp-content/uploads/2015/03/1927_to_the_lighthouse.jpg) [![1928_orlando](http://scholarslab.org/wp-content/uploads/2015/03/1928_orlando-300x225.jpg)](http://scholarslab.org/wp-content/uploads/2015/03/1928_orlando.jpg)[![1931_the_waves](http://scholarslab.org/wp-content/uploads/2015/03/1931_the_waves-300x225.jpg)](http://scholarslab.org/wp-content/uploads/2015/03/1931_the_waves.jpg)[![1937_the_years](http://scholarslab.org/wp-content/uploads/2015/03/1937_the_years-300x225.jpg)](http://scholarslab.org/wp-content/uploads/2015/03/1937_the_years.jpg) [![1941_between_the_acts](http://scholarslab.org/wp-content/uploads/2015/03/1941_between_the_acts-300x225.jpg)](http://scholarslab.org/wp-content/uploads/2015/03/1941_between_the_acts.jpg)\n\nTo my eye, the output suggests high concentrations of conversation in the novels at the beginning and ending of Woolf's career. We can see that her middle period, especially, appears to have a significant decrease in the amount of quoted speech. In one sense, this might make sense to someone familiar with Woolf's career. Her first two novels feel more typically Victorian in their aesthetics, and she really gets into the thick of modernist experiment with her third novel. One way we often describe the shift from Victorian to the modernist period is as a shift inward, away from society and towards the psychology of the self. So it makes sense that we might see the amount of conversation between multiple speaking bodies significantly fall away over the course of those novels. The [seventh histogram](http://scholarslab.org/wp-content/uploads/2015/03/1931_the_waves.jpg) is especially interesting, because it suggests the least amount of speech of anything in her corpus. But if we visualize things a different way, we see that this novel, _The Waves_, actually shows a huge spike in punctuated speech. This graph represents the percentage of each text that is contained within quotation marks, the amount of text represented as punctuated speech.\n\n[![percentage-quoted](http://scholarslab.org/wp-content/uploads/2015/03/percentage-quoted.jpg)](http://scholarslab.org/wp-content/uploads/2015/03/percentage-quoted.jpg)\n\nThis might look like a problem with the data: how could the text with the fewest number of quotations also have the highest percentage of quoted speech? But the script is actually giving me exactly what I asked for: _The Waves_ is a series of monologues by six disembodied voices, and the amount of non-speech text is extremely small. More generally, charting the percentage of quoted speech in the corpus appears to support my general readings of the original nine histograms: roughly three times as much punctuated speech in the early novels as in the middle period, with a slight leveling off in the end of her career.\n\nWe could think of _The Waves_ as an anomaly, but I think it more clearly calls for a revision of such a reading of speech in Woolf's career. The spike in quoted speech is a hint that there is something else going on in Woolf's work. Perhaps we can use the example of _The Waves_ to propose that there might be a range of discourses, of types of speech in Woolf's corpus. Before I suggested that speech diminished in the middle of Woolf's career, but that's not exactly true. My suspicion is that it just enters a different mode. Consider these two passages, both quoted from _Mrs. Dalloway_:\n\n\n<blockquote>Mrs. Dalloway said she would buy the flowers herself.\n\nTimes without number Clarissa had visited Evelyn Whitbread in a nursing home. Was Evelyn ill again? Evelyn was a good deal out of sorts, said Hugh, intimating by a kind of pout or swell of his very well-covered, manly, extremely handsome, perfectly upholstered body (he was almost too well dressed always, but presumably had to be, with his little job at Court) that his wife had some internal ailment, nothing serious, which, as an old friend, Clarissa Dalloway would quite understand without requiring him to specify.</blockquote>\n\n\nIn each case, the text implies speech by Mrs. Dalloway and by Hugh without marking it as such with punctuation marks. Discourse becomes submerged in the texture of the narrative, but it doesn't disappear entirely. Moments like these suggest a range of discourses in Woolf's corpus: dialogue, monologue, conversation, punctuated, implied, etc. All of these speech types have different implications, but it's difficult to get a handle on them because of their scale. I began the project by simply trying to mark down moments of implied speech in _Mrs. Dalloway_ by hand. Once I got to about two hundred, it seemed like it was time to ask the computer for help.\n\nThe current plan moving forward is to build a corpus of test passages containing both quoted speech and implied speech, train a python script against this set of passages, and then use this same script to search for instances of implied speech throughout Woolf's corpus. Theoretically, at least, the script will search for a series of words that flag text as implied speech to a human reader - said, recalled, exclaimed, etc. Using this lexicon as a basis, the script would then pull out the context surrounding these words to produce a database of sentences meant to serve as speech. At Eric's suggestion, I'm currently exploring the [Natural Language Toolkit](http://www.nltk.org/) to take a stab at all of this. My own hypothesis is that there will be an inverse relationship between quoted speech and implied speech in her corpus, that the amount of speech left unflagged by quotation marks will increase in the middle of Woolf's career. Once I have all this material, I'll be able to subject the results to further analysis and to think more deeply about speech in Woolf's work. Who speaks? What about? What counts as a voice, and what is left in an ambiguous, unsounded state?\n\nThe project is very much in its beginning stages, but it's already opening up the way that I think about speech in Woolf's text. It tries to untangle the relationship between our print record and our sonic record, and further work will help show how discourse is unfolding over time in the modernist period.\n"},{"id":"2015-04-01-on-sharing-credit-and-courting-trolls","title":"On Sharing Credit and Courting Trolls","author":"swati-chawla","date":"2015-04-01 07:19:55 -0400","categories":null,"url":"on-sharing-credit-and-courting-trolls","content":"The Praxis team was invited to two presentations the last week. The first at the [Moving People/ Linking Lives Page-Barbour Symposium](http://movingpeoplelinkinglives.org), and then at[ UVa’s Huskey Research Exhibition](http://gradcouncil.com/2015-sessions/). With a lot of help from Praxis teammate [Jennifer Grayburn](http://scholarslab.org/people/jennifer-grayburn/), I prepared a slideshow showcasing a brief history of Ivanhoe and the work of [the two Praxis cohorts](http://praxis.scholarslab.org/people/).\n\nMy talk for the “Lighting Round” of [Moving People/ Linking Lives](http://movingpeoplelinkinglives.org) and the presentation (with [Steven Lewis](http://scholarslab.org/people/steven-lewis/)) for the [Huskey exhibition](http://gradcouncil.com/2015-sessions/) was modified only slightly from the following write-up (co-authored with [Steven Lewis](http://scholarslab.org/people/steven-lewis/) and [Jennifer](http://scholarslab.org/people/jennifer-grayburn/)[ Grayburn](http://scholarslab.org/people/jennifer-grayburn/)):\n\n[gallery columns=\"2\" link=\"none\" size=\"medium\" ids=\"11840,11841\"]\n\nThe work that Steven and I presenting today is not our work alone: Ivanhoe was first developed by Jerome McGann, Johanna Drucker, and Bethany Nowviskie in the 2000s, and in its most recent avatar, it was updated by the Praxis cohort of 2013-14, our immediate predecessor. We stand before you representing— hopefully faithfully— our teammates [Amy R Boyd](http://scholarslab.org/people/amy-boyd/), [Andrew Ferguson](http://scholarslab.org/people/andrew-ferguson/), [Joris Gjata](http://scholarslab.org/people/joris-gjata/), and [Jennifer Grayburn](http://scholarslab.org/people/jennifer-grayburn/). Our discussion today is focused on Ivanhoe’s use as a pedagogical aid, and we will be using examples from the [game](http://mellon-seminar.herokuapp.com/?ivanhoe_game=renovating-the-university-library-minutes-of-the-faculty-advisory-board) played by participants of the [Mellon Graduate Seminar on Composing the Humanities in a Digital Age](http://seminar.scholarslab.org).\n\nIvanhoe grew out of a dissatisfaction: it was a dissatisfaction with limitations inherent in existing forms of interpreting texts, where readers felt the need for \"a more imaginative form wanted to develop a more imaginative form of critical methodology.” We have chosen to describe Ivanhoe as a gateway for textual play. We define text in the broadest sense possible, and although the Ivanhoe game developed through the collaborative analysis of literature, we’ve expanded its scope to be inclusive of a variety of media. A text can be anything: a painting, a piece of music, a passage of fiction or nonfiction prose. We define “play” as the sort of playful collaborative interpretation of a text that occurs during the course of an Ivanhoe game.\n\n\n\n**![page-2](http://scholarslab.org/wp-content/uploads/2015/03/page-2-300x225.jpg)**\n\n**Collaboration, Reflection, Performativity:**\nBehind this layout is a commitment to three principles: Collaboration, Reflection, and Performativity.\n\n![page-3](http://scholarslab.org/wp-content/uploads/2015/03/page-3-300x225.jpg)![page-4](http://scholarslab.org/wp-content/uploads/2015/03/page-4-300x225.jpg)![page-5](http://scholarslab.org/wp-content/uploads/2015/03/page-51-300x225.jpg)Collaboration is fundamental to the mechanics of Ivanhoe gameplay since every game involves multiple players, and is composed of responses to the central textual object and responses to fellow players. Players work together to weave a discursive web around the central text of the game, and in doing so, they draw meaning out of the subject text in relation to each others’ responses. Like collaboration, reflection too is written into the mechanics of Ivanhoe gameplay, and it is this component that best distinguishes Ivanhoe from other forms of collaborative writing (fan fiction, for example). Not only are players making “moves,” they are required to reflect on these moves through the vantage point of their role vis-a-vis the text. The primary means through which we encourage reflection during games is the requirement that all players maintain a role journal. The journal is something that is separate from the gameplay itself and allows players to reflect on the choices they made in making their moves. It provides players a space dedicated to writing about the ways that their moves relate to the character they created, and think about the ways that their moves relate to the moves of the other players. Performativiy is what sets Ivanhoe apart from other modes of methods of solitary reflection, or forums such as “comments” sections on blogs. In short, Ivanhoe requires players to have a stake in the game as their respective “role”. When players create this role for themselves at the beginning of the game, they are expected to “perform” in that role for the duration of the game. This forces them to delve deeply into the implications of a given critical perspective, a particular location and point of view. It is also what makes the Ivanhoe game fun!\n\n**![page-6](http://scholarslab.org/wp-content/uploads/2015/03/page-6-300x225.jpg)**\n\n**Next Steps:**\n\nThe question that animates Ivanhoe is: how do we make a program that students want to engage with by choice, and not by force. Towards that end, we are working on three features this year:\n\n1. Email notifications\n\nEmail notifications are an important addition in this version of the Ivanhoe game because players know immediately when a new move is made. This allows them to respond to each other quickly and keeps the game moving at a relatively fast pace. In our own experience playing the game, pacing--specifically knowing when moves have been made--is extremely important to keeping players engaged with the game currently being played.\n\n2. Responding to multiple moves\n\nThis is another important functional addition to the first version of Ivanhoe, in which players could respond to only one move at a time responding to more than one move at once allows players to “stretch” the ways in which they perform their character. Responding to multiple moves will also open up different analytical possibilities and allow for drawing connections between larger groups of ideas.\n\n3. Intuitive and streamlined interface\n\nOur new interface is designed to allow more personalization by the game administrator and to increase functionality. We eliminated unnecessary visual clutter, reorganized information for more intuitive reading, and implemented options to change color schemes/game images.\n\nIn closing, we mentioned that Ivanhoe was being used in a class on John Milton’s Paradise Lost in Washington and Lee University, and that we were hoping to get some feedback from the players in a couple of weeks. In fact, I fortuitously met the instructor for this course at the Moving Peoples/ Linking Lives symposium.\n\n**The Questions:**\n\nHere are some of the questions that were posed to us after the two presentations. I hope to address some of them in future blogposts. Thoughts and comments welcome!\n\n\n\n\t\n  * Has/ how does Ivanhoe been folded into classroom pedagogy?\n\n\t\n  * Does/ how does Ivanhoe in its current for account for the difference between fiction and non-fiction?\n\n\t\n  * You say so much about reflection and collaboration. But these assume responsible playing. How would a game of Ivanhoe deal with trolls?\n\n\t\n  * Can you include your own performance as a response to a move?\n\n\nThe full powerpoint slides are here:\n\n[Ivanhoe at Moving People/ Linking Lives](http://scholarslab.org/wp-content/uploads/2015/03/2015-03-21-.-presentation.-praxis.-moving-people-linking-lives.pptx)\n[Ivanhoe at Huskey Research Exhibition](http://scholarslab.org/wp-content/uploads/2015/03/2015-03-22-.-presentation.-praxis.-huskey.pptx)\n\n**My Learnings:**\n\nPresenting our collective work was an honor for me, but also brought with up some questions of sharing credit and acknowledging the contributions of others. Luckily, all the thought (and love!) we had put into [our charter](http://praxis.scholarslab.org/charter/charter-2014-2015/) at the beginning of the year came in handy. We had said: \"All members of the Praxis Program team deserve equal credit for their contribution…. Our shared web site should always list general credit in alphabetical order to emphasize the non-hierarchical ethos of the program. Whenever a member of the Praxis Program references the work to the outside community, the project as a whole should be credited to all members.” Since one of these presentations involved a competitive event, we were told that we could not compete as a group of six. We decided as a team that it was okay for Steven and me to formally compete for the prize, as long as the work was credited to the whole team in the presentation and conference publicity materials. While working on a team project with a think tank in Delhi, I remember my mentor saying that a lot more gets done when everyone is generous with sharing credit. I am very glad we have lived up to the ethos of our charter and done exactly that!\n\n\n"},{"id":"2015-04-06-adventures-in-3d-printer-maintenance","title":"Adventures in 3D Printer Maintenance","author":"shane-lin","date":"2015-04-06 11:38:42 -0400","categories":["Experimental Humanities"],"url":"adventures-in-3d-printer-maintenance","content":"Recently, the fan on the bottom of our Makerbot Replicator Dual Extruder began to make some unhealthy noises. Per SLab desk protocol, the first attempt at a remedy was to smack the thing near where it was making the noise, which made it go away for a little while. Eventually, we got tired of hitting our printer every time we started it up, so I took off the bottom panel to oil the cheap 40mm case fan within. Here's what I found.\n\n[![2015-02-11 13.19.05](http://scholarslab.org/wp-content/uploads/2015/02/2015-02-11-13.19.05-222x300.jpg)](http://scholarslab.org/wp-content/uploads/2015/02/2015-02-11-13.19.05.jpg)\n\nAmazingly, like an Alaskan infrastructure project, this fan didn't actually lead anywhere. The only inlets were the small holes in the corner formed by the chamfer of the horizontal plates, so it mostly served to move hot air around the Mightyboard chamber. This lead me to wonder whether or not a fan was actually necessary, especially since the Replicator 2 has no bottom fan at all for its Rev. G Mightyboard.\n\nThis line of inquiry lead to a surprising discovery. Original Replicators with Rev. E boards use very inexpensive [linear voltage regulators](http://en.wikipedia.org/wiki/Linear_regulator) to bring the 24V power supply down to the 5V that the Mightyboard requires. The regulator is the thing right next to the plug in that picture. Linear regulators function by bleeding off excess energy as waste heat. Hence the fan.\n\nA 19V voltage drop at a ballpark 300mA for the board means about 5.7W that needs to be dissipated. That's actually a lot of power! It also seems that Mightyboards from the Rev. E era fail at a pretty alarming rate and the main culprit is the voltage regulator. The Internet is [a bit divided on the exact mechanism of this failure](https://groups.google.com/forum/#!msg/makerbot/5n4HwWyazlE/koxqMAG9xmsJ) and whether or not improved cooling will significantly alleviate the issue, but most people seem pretty certain that it's a nigh-inevitable and invariably fatal problem. When the regulator blows (with a sharp \"pop!\"), the whole board goes with it.\n\nSo, great.\n\nIn the past, Makerbot has reportedly been pretty good about replacing failed Mightyboards, but not about actually correcting the fatal flaw. With the original Replicator now several generations behind and the delays with our recent Replicator 2 replacement parts order, I am not at all confident that we will be able to source a spare in the future. Replicator 2s do not have this issue because Makerbot upgraded the Rev. G boards to switching regulators (costing upward of ten of dollars!) that produce far less heat and do not require any active cooling at all.\n\nShort of just waiting for the board to blow, there are two approaches to this problem. The first and best long-term approach is to replace the linear regulator with a switching one. But without the certainty that we can get the board replaced should we screw up the procedure, I was less keen on this plan to start. The second approach is to improve the cooling, which seemed much more straightforward.\n\nThere is a [Thingiverse compilation](http://www.thingiverse.com/thing:33779) of various parts useful to achieving this goal. The easiest first step for us was to simply drill holes in the side of the Replicator over where the fan is positioned, so that we can draw in cool air from outside to blow over the regulator. We also printed out a 40mm duct from the Thingiverse collection to use instead of the original offsets to minimize hot air re-circulation.[![2015-02-11 13.45.02](http://scholarslab.org/wp-content/uploads/2015/02/2015-02-11-13.45.02-300x222.jpg)](http://scholarslab.org/wp-content/uploads/2015/02/2015-02-11-13.45.02.jpg) [![2015-02-11 14.48.34](http://scholarslab.org/wp-content/uploads/2015/02/2015-02-11-14.48.34-222x300.jpg)](http://scholarslab.org/wp-content/uploads/2015/02/2015-02-11-14.48.34.jpg) [![2015-02-11 14.50.16](http://scholarslab.org/wp-content/uploads/2015/02/2015-02-11-14.50.16-222x300.jpg)](http://scholarslab.org/wp-content/uploads/2015/02/2015-02-11-14.50.16.jpg)\n\nHere's the result (the original screws were a few millimeters too short, so we had to pick up longer ones - they're M3, but 4/32'' works in a pinch since longer M3s are hard to find).\n\n[![2015-02-11 15.51.03](http://scholarslab.org/wp-content/uploads/2015/02/2015-02-11-15.51.03-222x300.jpg)![2015-02-11 16.08.44](http://scholarslab.org/wp-content/uploads/2015/02/2015-02-11-16.08.44-300x222.jpg)](http://scholarslab.org/wp-content/uploads/2015/02/2015-02-11-15.51.03.jpg)\n\nThe airflow is much improved now. The next step is to pick up better quality fans so we don't have to oil them every few weeks and to install a second, output fan on the other side of the board. Our Replicator has lasted close to three years in its original form, which seems much better than most people have reported (some of those complaining about failures were on their 3rd board) . Hopefully, it will function for some time yet, even if we decide not to do the regulator swap.\n"},{"id":"2015-04-08-everything-is-not-always-awesome-when-youre-part-of-a-team","title":"Everything Is (Not Always) AWESOME When You're Part of a Team!","author":"joris-gjata","date":"2015-04-08 05:35:47 -0400","categories":["Grad Student Research"],"url":"everything-is-not-always-awesome-when-youre-part-of-a-team","content":"You have watched _The Lego Movie_ I hope. It is a story about an individual who seems to be happy alone with his own routine as a 'normal' person, but one day finds out he is 'the chosen one' meant to save people. He does not believe in himself and his ability to be the hero. Nevertheless, he ends up being victorious against evil because of his great team. The movie ends with a song whose refrain summarized its conclusion: [\"everything is awesome, everything is cool when you're part of a team...\"](https://www.youtube.com/watch?v=vx5n21zHPm8) Yes, I really appreciate the idea that individuals are social beings and heroes are the result of collective action. I value the emphasis on team work in this movie, but I would like to draw attention also to the hard work that goes into working within a team and as a team.\n\nGetting involved in the Ivanhoe project as part of an interdisciplinary team has made me more aware of the challenges of team work. Working with my Praxis 2014-15 fellows and Scholars' Lab staff/members on Ivanhoe has taught me several_ important lessons about being part of a team_ that I will definitely keep in mind for future reference wherever I go, especially if working on collaborative projects.\n\nFirst, _I have understood that what makes being part of a team totally awesome is people that are genuinely interested in helping you learn and grow as a whole person._ My teammates and the Scholars' Lab staff have been very understanding and flexible during my pregnancy and also with the arrival of my child. They can recognize that researchers are humans and life has to it more than research. They have made me feel supported and also helped me overcome what I would call 'expert stubbornness' (not only ['expert blindness'](http://scholarslab.org/uncategorized/novice-struggles-and-expert-blindness-how-my-discomfort-with-php-will-make-me-a-better-instructor/) as my teammate Jennifer says in her recent blogpost). _Expert stubbornness_ is when you try to do your own deep thinking and hard work to find the answer to a question before asking help from others. I tend to have a lot of this stubbornness, but the rule 'if you cannot figure out in 5-10 minutes, ask somebody' has helped me a lot in overcoming this 'vice'. It made me realize the implicit assumption underlying my behavior: that experts know it all themselves and being an expert means addressing whatever issue without help, alone like a hero. I have learned that time is the most valuable resource and getting help is good for everybody. Thank you Scholars' Lab for being a safe place where there is no such thing as 'a stupid question'! You are always there to help us with questions about php, programming and the implementation of our ideas about Ivanhoe. And thank you dear teammates! I would agree with [Swati's recent blogpost](http://scholarslab.org/uncategorized/on-sharing-credit-and-courting-trolls/): it is awesome to be part of a team that shares responsibility generously for failures and successes.\n\nSecond, _I have learned how challenging it is to create a common vision for the project, given team members with diverse backgrounds and viewpoints_. Our discussions about [the Praxis 2014-15 charter](http://praxis.scholarslab.org/charter/charter-2014-2015/) and the conversations about the conceptualization of Ivanhoe as a gateway to collaborative textual play helped us in this direction. Nevertheless, we still have unresolved conceptual issues regarding [what could make Ivanhoe 'special'](http://scholarslab.org/grad-student-research/what-could-make-ivanhoe-special/). Having no clear resolution on several important conceptual issues, made being part of a team not always awesome, at least for me.\n\nMost importantly, _I have realized how challenging it is to make project management a shared responsibility and develop shared leadership within a team._ Last semester I supported the vision that management and organization was a common responsibility of the whole team and it would have to emerge from our interactions as team members and human beings. However, when it came to practice, I felt frustrated waiting for such coordination to emerge. It took time to establish good accountability measures for ourselves and each others like delineating simple tasks, specifying deadlines, sending checking-in emails, etc. Grasping the whole purpose of Github and how to use it to facilitate team communication also took time. As a result, last semester we were not able to turn words into deeds and reflect philosophies into practices with a new Ivanhoe info page. This taught me to be more patient and generous with others and myself as work is not always fast and not always without tension in a team of experts.\n\nWorking within the 2014-2015 Praxis team last semester, made me understand the importance of learning more about project management and organization in multicultural teams. It is hard, but I think it is one of those things that can make being part of a team and working on a collaborative project pretty awesome. Managing a multicultural team and coordinating members' actions towards the completion of a collaborative project involves special skills that also need to be taught. We think being a researcher by definition means that we are skilled in data management. However, we forget that our research generally as grad students and scholars is solitary work - the academy tends to evaluate individuals not teams. Furthermore, we may be good at managing data but maybe not so good at managing people.\n\nI started reading about _shared leadership_ in teams - where \"the source of leadership influence is distributed among team members rather than concentrated or focused in a single individual\" (Carson, Tesluk and Marrone 2007, 1220). [Research](http://www.ilo.bwl.uni-muenchen.de/download/unterlagen-ws12_13/leadership_and_learning/literature_hoegl1/carson_et_al_2007.pdf) shows that the development of shared leadership depends on two interrelated factors: \"internal team environment, including a shared purpose, social support, and voice, and level of external coaching support.\" (Carson et al. 2007, 1218). As I make sense of my team experience to this point, I think that the conditions for developing shared leadership were there. Yes, maybe we needed to develop voice more inside our team, and maybe a crash course in project management at the beginning of Praxis team work could have been useful. However, overall I believe we were able to develop high level of shared leadership. Our roles and responsibilities have changed over time, and it is in teams with high level of shared leadership that you tend to see such shifts and/or rotations in leadership, \"in such a way that different members provide leadership at different points in the team's life cycle and development\" (Carson et al. 2007, 1220). As [Andrew mentioned in his latest blogpost](http://scholarslab.org/grad-student-research/on-the-shelf/), our team has truly experienced the iterative nature of any collaborative digital humanities project. It is a challenge to know that it takes time to become a team, and still act as a team within a limited timeframe.\n\nTo conclude, _it is not always awesome to be part of a team, but it is definitely rewarding._ As sociologists Martin Ruef (2010) shows with his book _[The Entrepreneurial Group](http://press.princeton.edu/titles/9214.html)_, entrepreneurship and innovation are much more successful when they involve collective effort and collaboration in the form of teams. Teamwork is hard but worth committing too. Let's fight the ideology of individual heroes, geniuses, and lone researchers, but also let's prepare for the challenges of being part of team. Knowing and learning about how to face those challenges will help keep collaborative projects alive and make working with diverse teams truly awesome.\n\n**References:\n**\nCarson, Jay B., Paul E. Tesluk, and Jennifer A. Marrone. 2007. “Shared Leadership in Teams: An Investigation of Antecedent Conditions and Performance.” _Academy of management Journal_ 50(5):1217–34.\n\nRuef, Martin. 2010. _The Entrepreneurial Group: Social Identities, Relations, and Collective Action_. Princeton University Press.\n"},{"id":"2015-04-10-linking-out","title":"Linking Out","author":"jeremy-boggs","date":"2015-04-10 10:17:18 -0400","categories":["Experimental Humanities"],"url":"linking-out","content":"I'm in the [Accessible Future](http://accessiblefuture.org) workshop in Atlanta, and we'd had a interesting conversation about how, and how often, web content should \"link out\" to pages beyond ones own web site. I feel like we've talked about this topic every workshop, and there are lots of interesting issues that keep coming up that I never feel are really resolved or explored in depth. I've not fully resolved them myself, and since I haven't written a post here in ages I figured, in the spirit of [being flummoxed by a coconut](http://scholarslab.org/digital-humanities/monkey-mind/) I'd write something up here to see if I can't start working through this.\n\nApproaches to external links usually comes up when we get to talking about whether links to external pages should be opened in a new tab or new window. There seems to be some fear—and I've had this conversation at work during consultations and workshops all the time—that if you don't open external links in a new tab, people will never come back to your web page. And I've always wondered what this fear is founded in; it seems to me that if your content is compelling and useful, people will come back to it. If we're worried about people not returning to our pages, it makes me think the bigger problem is that the content on our pages isn't as useful as is could be. I'm sure this isn't universally true, and certainly don't mean to say that anyone who has this concern is writing crap content or over-worrying. But I just wonder if its speaking to a different issue with the confidence one has in the content they're sharing.\n\nIt also seems to say something about what expectations we have of people and their competency with browsers. There are lots of features in browsers people could use, if they knew those features existing and they knew how to use them. You can save your browsing history and review it; I do this _all the time._ You can actually choose to open links in new tabs; I also do this _all the time_. So I think that, when we start making choices for the reader, we're forcing a specific user experience they may not want, and we're also making it less possible for people to become more savvy users of their browser.\n\nMy own stances on external links, as a developer and an author (ha!) are:\n\n\n\n\t\n  1. **I do not open external links in new tabs or windows.** In fact, I have a browser plugin that automatically disables that. I personally think this is forcing a specific choice on my reader instead of giving them that choice. I open things in new tabs all the time, but I deliberately do it, using my right-click or keyboard shortcut to open a new tab. _I know how to do this_. This is an option in, as far as I know, every modern browser, though I'm sure its not a feature more. So the question here for me is, should we be thinking more about how to show people to take more control of their browser settings and use, or do we impose specific? Right now, I'm in the camp that favors reader choice, though I worry that's not the right way of looking at it.\n\n\t\n  2. **I link out to external pages rather liberally.** I understand arguments that too many links can be harmful to understanding. If you have whole paragraphs filled with linked text are really hard to read, especially depending on your link styles. (I've occasionally wondered whether it'd be worthwhile having some sort of setting to turn off link styles just to facilitate reading. Not sure what this would look like, though.) But I also like to link to specific things I think are useful to read, instead of just assuming that if a reader wants more information they can search for particular terms. They can still do that, too.\n\n\nI want to have a more nuanced approach to this. I'm not confident my stances are correct, or that I've considered enough. If you have any thoughts about this, please share!\n"},{"id":"2015-04-20-validating-data-with-types","title":"Validating Data with Types","author":"eric-rochester","date":"2015-04-20 09:47:06 -0400","categories":null,"url":"validating-data-with-types","content":"# Validating Data with Types\n\n\n\n\nRecently, I had to opportunity to help [J. Warren York](http://politics.virginia.edu/node/181), a graduate student in the Department of Politics here at UVa. He’s looking at how tax law affects political contributions and advocacy, so this was an interesting project that may tell us something useful about how the US government works [insert your favorite broken-government joke here].\n\n\n\n\nTo do this, he needed to download data from a number of different sources in different formats (JSON, YAML, and CSV), pull it all apart, and put some of it back together in a couple of new data files. One of those sources is the [Database on Ideology, Money in Politics, and Elections (DIME)](http://data.stanford.edu/dime). The data from them tells how much people and organizations have contributed to various candidates, PAC, and other groups.\n\n\n\n\nAnd while I’ve seen worse, it wasn’t the cleanest data file out there. (To get an idea of what the data looks like, you can see a sample of 100 rows from this data file in [this Google Sheet](https://docs.google.com/spreadsheets/d/1-m_8pm_s2gfpSAkMF0IYHNjKvWjqDe2d__CrbpjBxFA/edit?usp=sharing).)\n\n\n\n\nFor most projects that I’m reasonably sure that I’ll be the only developer on, I use [Haskell](https://www.haskell.org/). This is a [functional](http://en.wikipedia.org/wiki/Functional_programming), [statically typed](http://stackoverflow.com/questions/1517582/what-is-the-difference-between-statically-typed-and-dynamically-typed-languages) programming language with a (partially deserved) reputation for being difficult. However, I find that it gives me a nice balance of safety and flexibility, of power and expressiveness.\n\n\n\n\nGiven Haskell’s reputation, the previous sentence probably seems to border on insanity. Hopefully this post will prove this at least partially correct and will highlight some of the nicer aspects of working in Haskell. It leverages types to provide some assurances that the data is well-formed and consistent. This means I can perform data validation quickly and easily, and that helps everyone.\n\n\n\n\n<blockquote>\n\n> \n> This post is actually runnable Haskell. If you have the [GHC](https://www.haskell.org/ghc/) compiler installed you can copy and paste this post into a file, say `Validating.lhs`, and run it from the command line:\n> \n> \n\n>     \n>     <code class=\"sourceCode bash\">$ <span class=\"kw\">runhaskell</span> Validating.lhs contribDB_1982.csv</code>\n> \n> \n\n> \n> However, to follow this post, you don’t need to know Haskell. I’ll try to explain enough of the concepts and syntax that matter as they come up, so that anyone familiar with computer programming should be able to follow along without getting into the weeds of exactly what’s happening on each line.\n> \n> \n\n> \n> So first some pre-amble and boilerplate. This just makes available the libraries that we’ll use.\n> \n> \n</blockquote>\n\n\n\n    \n    <code class=\"sourceCode haskell\">\n    <span class=\"fu\">></span> <span class=\"ot\">{-# LANGUAGE OverloadedStrings #-}</span>\n    <span class=\"fu\">></span>\n    <span class=\"fu\">></span> <span class=\"co\">-- If you want more details about the code, including brief</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- explanations of the syntax, you've come to the right place.</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- Pay attention to the comments. This still isn't a tutorial</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- on Haskell, but hopefully you'll have a more detailed</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- understanding of what's happening.</span>\n    <span class=\"fu\">></span>\n    <span class=\"fu\">></span> <span class=\"co\">-- First, Haskell code is kept in modules. Executable files are</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- in the `Main` module.</span>\n    <span class=\"fu\">></span> <span class=\"kw\">module</span> <span class=\"dt\">Main</span> <span class=\"kw\">where</span>\n    <span class=\"fu\">></span>\n    <span class=\"fu\">></span> <span class=\"co\">-- Import statements make the code from these modules available</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- in this module. Qualified imports make the code available</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- under an alias (e.g., Data.ByteString.Lazy is aliased to B).</span>\n    <span class=\"fu\">></span> <span class=\"kw\">import qualified</span> <span class=\"dt\">Data.ByteString.Lazy</span> <span class=\"kw\">as</span> <span class=\"dt\">B</span>\n    <span class=\"fu\">></span> <span class=\"kw\">import           </span><span class=\"dt\">Data.Csv</span>\n    <span class=\"fu\">></span> <span class=\"kw\">import qualified</span> <span class=\"dt\">Data.Text</span>            <span class=\"kw\">as</span> <span class=\"dt\">T</span>\n    <span class=\"fu\">></span> <span class=\"kw\">import qualified</span> <span class=\"dt\">Data.Vector</span>          <span class=\"kw\">as</span> <span class=\"dt\">V</span>\n    <span class=\"fu\">></span> <span class=\"kw\">import           </span><span class=\"dt\">System.Environment</span></code>\n\n\n\n\nTo validate the data, we just need to follow the same steps that we would to load it. Those steps are:\n\n\n\n\n\n\n  1. Define the data that you want to use;\n\n\n  2. Define how to read it from a row of CSV data; and\n\n\n  3. Read the input.\n\n\n\n![Profit!](http://scholarslab.org/wp-content/uploads/2015/03/gnomes_plan.gif)Profit!\n\n\n\nThat’s it. In fact, the last item is so inconsequential that we’ll skip it. But let’s see how the rest of it works.\n\n\n\n\n## Defining the Data\n\n\n\n\nFirst we need to define the data. We do this using _types_. If you only know languages like Ruby, JavaScript, or Python, you may be unfamiliar with types. Basically, they specify what your data will look like. For example, they might specify that a `Person` data instance has a `name` string field and an `age` integer field.\n\n\n\n\nIf you come from Java or C#, you know what types are, but Haskell uses them very differently. In Haskell, types are used to express, encode, and enforce the requirements of your program as much as possible. The guideline is that invalid program states should not be expressible in the types you define. To help with that, some of the loopholes in Java’s type system have been closed (looking at you, `null`): this makes these specifications more meaningful. And because Haskell employs type inference, you also don’t need to actually declare the type of every little thing, so you get more benefit for less work.\n\n\n\n\nIn short, types are how we specify what data we’re interested in.\n\n\n\n\nAt this point in the process, programming in Haskell is a typical data modeling exercise. But it’s also the foundation for the rest of this post, so we’ll linger here.\n\n\n\n\nBefore we define the data types, we’ll first define some aliases. These aren’t really enforced, but they make the data types that use these more clear.\n\n\n\n    \n    <code class=\"sourceCode haskell\">\n    <span class=\"fu\">></span> <span class=\"kw\">type</span> <span class=\"dt\">OrgName</span> <span class=\"fu\">=</span> <span class=\"dt\">T.Text</span>\n    <span class=\"fu\">></span> <span class=\"kw\">type</span> <span class=\"dt\">Year</span>    <span class=\"fu\">=</span> <span class=\"dt\">Int</span>\n    <span class=\"fu\">></span> <span class=\"kw\">type</span> <span class=\"dt\">Amount</span>  <span class=\"fu\">=</span> <span class=\"dt\">Double</span></code>\n\n\n\n\nThe first data type that we’ll create is `Party`. This will be similar to enumerations in other languages, but in Haskell they’re just regular data types. A `Party` can be either a `Dem` (Democrat), `GOP` (Republican), `Independent`, or `Unknown`.\n\n\n\n    \n    <code class=\"sourceCode haskell\">\n    <span class=\"fu\">></span> <span class=\"co\">-- This statement says that you can make a value of type Party </span>\n    <span class=\"fu\">></span> <span class=\"co\">-- using any of the constructors listed (separated by pipes).</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- In this case, none of the constructors take extra data, so</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- the semantics comes soley from which constructor is chosen.</span>\n    <span class=\"fu\">></span> <span class=\"kw\">data</span> <span class=\"dt\">Party</span> <span class=\"fu\">=</span> <span class=\"dt\">Dem</span> <span class=\"fu\">|</span> <span class=\"dt\">GOP</span> <span class=\"fu\">|</span> <span class=\"dt\">Independent</span> <span class=\"fu\">|</span> <span class=\"dt\">Unknown</span></code>\n\n\n\n\nWe want to know what kind of entity is receiving the contribution. However, we don’t actually care about who the recipient was: we just want to distinguish between candidates, committees, and state-level elections. We’ll use the `ContribEntry` data type for this information.\n\n\n\n\nThe following declaration states that a `ContribEntry` can be either a `Candidate`, which must have year information and party information; a `Committee`, which must have only a year; or a `StateLevel`, which must have a year and a state code.\n\n\n\n    \n    <code class=\"sourceCode haskell\">\n    <span class=\"fu\">></span> <span class=\"co\">-- This shows how values are given types. `contribYear ::</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- !Year`, says that the `contribYear` field must contain</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- values of type `Year`. The exclamation mark tells the</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- Haskell compiler to execute this value immediately. Unlike</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- most other languages, Haskell will normally wait to</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- evaluate expressions until absolutely necessary.</span>\n    <span class=\"fu\">></span> <span class=\"kw\">data</span> <span class=\"dt\">ContribEntry</span>\n    <span class=\"fu\">></span>         <span class=\"fu\">=</span> <span class=\"dt\">Candidate</span>  {<span class=\"ot\"> contribYear ::</span> <span class=\"fu\">!</span><span class=\"dt\">Year</span>,<span class=\"ot\"> contribParty ::</span> <span class=\"fu\">!</span><span class=\"dt\">Party</span> }\n    <span class=\"fu\">></span>         <span class=\"fu\">|</span> <span class=\"dt\">Committee</span>  {<span class=\"ot\"> contribYear ::</span> <span class=\"fu\">!</span><span class=\"dt\">Year</span> }\n    <span class=\"fu\">></span>         <span class=\"fu\">|</span> <span class=\"dt\">StateLevel</span> {<span class=\"ot\"> contribYear ::</span> <span class=\"fu\">!</span><span class=\"dt\">Year</span>,<span class=\"ot\"> stateCode ::</span> <span class=\"fu\">!</span><span class=\"dt\">T.Text</span> }</code>\n\n\n\n\nEach row of the data file will have information about a single contribution made by an individual or organization. Because we’re primarily interested in the data from organizations, this will be collected in an `OrgContrib` data type. It will hold the organization’s name (`orgContribName`), its district (`orgDistrict10s`), the contribution information (`orgContribEntry`), and the amount of the contribution (`orgContribAmount`).\n\n\n\n    \n    <code class=\"sourceCode haskell\">\n    <span class=\"fu\">></span> <span class=\"kw\">data</span> <span class=\"dt\">OrgContrib</span>\n    <span class=\"fu\">></span>          <span class=\"fu\">=</span> <span class=\"dt\">OrgContrib</span>\n    <span class=\"fu\">></span>          {<span class=\"ot\"> orgContribName   ::</span> <span class=\"fu\">!</span><span class=\"dt\">OrgName</span>\n    <span class=\"fu\">></span>          ,<span class=\"ot\"> orgDistrict10s   ::</span> <span class=\"fu\">!</span><span class=\"dt\">T.Text</span>\n    <span class=\"fu\">></span>          ,<span class=\"ot\"> orgContribEntry  ::</span> <span class=\"fu\">!</span><span class=\"dt\">ContribEntry</span>\n    <span class=\"fu\">></span>          ,<span class=\"ot\"> orgContribAmount ::</span> <span class=\"fu\">!</span><span class=\"dt\">Amount</span>\n    <span class=\"fu\">></span>          }</code>\n\n\n\n\nThat’s it. We’ve now defined the data we’re interested in. On top of the guarantees that types allow the programming language to enforce, this exercise is also helpful because it clarifies what we want from the data and helps us better understand the domain that we’re working in.\n\n\n\n\n## Data from CSV\n\n\n\n\nHowever, we haven’t connected this data with the CSV file yet. Let’s do that now.\n\n\n\n\nTo make this happen, we’ll need to take the data types that we just defined and define instances of `FromField` for ones that are populated from a single field, like `Party`, and `FromNamedRecord` for others, which are built from an entire row.\n\n\n\n\n`FromField` and `FromNamedRecord` are [type classes](http://en.wikipedia.org/wiki/Type_class). In object-oriented terms, these are similar to small interfaces, some only declaring one or two methods. Data types can implement the type classes that make sense, but omit the ones that do not.\n\n\n\n\nIn this case these type classes define what data types can be read from a row of CSV and how that should happen.\n\n\n\n\n`Party` is the first data type we’ll tackle. It only reads a single field, so we’ll define `FromField`. In the CSV file, the data is encoded with numeric codes, which we’ll change into `Party` values.\n\n\n\n    \n    <code class=\"sourceCode haskell\">\n    <span class=\"fu\">></span> <span class=\"co\">-- This defines a instance of `FromField` for `Party`.</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- `parseField` is the only method. Multiple listings for this</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- function, combined with the string literals in place of the</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- parameter, means that the method acts as a big case</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- statement on its one parameter. When the function is passed</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- the string \"100\", the first definition will be used. The</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- last clause, with the underscore, is a catch-all, in which</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- the parameter's value will be ignored.</span>\n    <span class=\"fu\">></span> <span class=\"kw\">instance</span> <span class=\"dt\">FromField</span> <span class=\"dt\">Party</span> <span class=\"kw\">where</span>\n    <span class=\"fu\">></span>     parseField <span class=\"st\">\"100\"</span> <span class=\"fu\">=</span> return <span class=\"dt\">Dem</span>\n    <span class=\"fu\">></span>     parseField <span class=\"st\">\"200\"</span> <span class=\"fu\">=</span> return <span class=\"dt\">GOP</span>\n    <span class=\"fu\">></span>     parseField <span class=\"st\">\"328\"</span> <span class=\"fu\">=</span> return <span class=\"dt\">Independent</span>\n    <span class=\"fu\">></span>     <span class=\"co\">-- This catch-all is probably a bad idea....</span>\n    <span class=\"fu\">></span>     parseField _     <span class=\"fu\">=</span> return <span class=\"dt\">Unknown</span></code>\n\n\n\n\nNotice my comment on the next to last line. Having a catch-all field like this introduces some [code smell](http://en.wikipedia.org/wiki/Code_smell), and it weakens the type-safety of the field. A better practice would be to define a `Party` constructor for every numeric code and throw an error when we find something unexpected. Since we’re only interested here in two parties, that would be overkill, so in this case we’ll be more flexible.\n\n\n\n\nNow we can define how to read `ContribEntry` data. This is complicated because we have to look at the value of the `recipient_type` field in order to figure out which constructor to use.\n\n\n\n\nWe’ll also define a utility function, `defaulting`, that defaults empty strings to a given value.\n\n\n\n    \n    <code class=\"sourceCode haskell\">\n    <span class=\"fu\">></span> <span class=\"co\">-- This defines the function defaulting. The first line is the</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- type value. The definition of `defaulting` is a more</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- complicated case statement that first tests `T.null v`</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- (i.e., that it's empty), and `otherwise` is the \"else\" part</span>\n    <span class=\"fu\">></span> <span class=\"co\">-- of the statement.</span>\n    <span class=\"fu\">></span><span class=\"ot\"> defaulting ::</span> <span class=\"dt\">T.Text</span> <span class=\"ot\">-></span> <span class=\"dt\">T.Text</span> <span class=\"ot\">-></span> <span class=\"dt\">T.Text</span>\n    <span class=\"fu\">></span> defaulting d v <span class=\"fu\">|</span> T.null v  <span class=\"fu\">=</span> d\n    <span class=\"fu\">></span>                <span class=\"fu\">|</span> otherwise <span class=\"fu\">=</span> v\n    <span class=\"fu\">></span>\n    <span class=\"fu\">></span> <span class=\"kw\">instance</span> <span class=\"dt\">FromNamedRecord</span> <span class=\"dt\">ContribEntry</span> <span class=\"kw\">where</span>\n    <span class=\"fu\">></span>     parseNamedRecord m <span class=\"fu\">=</span> <span class=\"kw\">do</span>\n    <span class=\"fu\">></span>         <span class=\"co\">-- Read the recipient_type field. The `.:` operator</span>\n    <span class=\"fu\">></span>         <span class=\"co\">-- reads a specific field from the CSV row.</span>\n    <span class=\"fu\">></span>         rtype <span class=\"ot\"><-</span> m <span class=\"fu\">.:</span> <span class=\"st\">\"recipient_type\"</span>\n    <span class=\"fu\">></span>         <span class=\"co\">-- If recipient_type is empty, give it a default value</span>\n    <span class=\"fu\">></span>         <span class=\"co\">-- of \"CAND\", and then branch on that.</span>\n    <span class=\"fu\">></span>         <span class=\"kw\">case</span> defaulting <span class=\"st\">\"CAND\"</span> rtype <span class=\"kw\">of</span>\n    <span class=\"fu\">></span>             <span class=\"st\">\"CAND\"</span> <span class=\"ot\">-></span> <span class=\"kw\">do</span>\n    <span class=\"fu\">></span>                 <span class=\"co\">-- Read the cycle (year) and recipient_party fields</span>\n    <span class=\"fu\">></span>                 cycle <span class=\"ot\"><-</span> m <span class=\"fu\">.:</span> <span class=\"st\">\"cycle\"</span>\n    <span class=\"fu\">></span>                 party <span class=\"ot\"><-</span> m <span class=\"fu\">.:</span> <span class=\"st\">\"recipient_party\"</span>\n    <span class=\"fu\">></span>                 <span class=\"co\">-- Create a Candidate</span>\n    <span class=\"fu\">></span>                 return (<span class=\"dt\">Candidate</span> cycle party)\n    <span class=\"fu\">></span>             <span class=\"st\">\"COMM\"</span> <span class=\"ot\">-></span> <span class=\"kw\">do</span>\n    <span class=\"fu\">></span>                 <span class=\"co\">-- Read the cycle and return a Committe</span>\n    <span class=\"fu\">></span>                 cycle <span class=\"ot\"><-</span> m <span class=\"fu\">.:</span> <span class=\"st\">\"cycle\"</span>\n    <span class=\"fu\">></span>                 return (<span class=\"dt\">Committee</span> cycle)\n    <span class=\"fu\">></span>             r <span class=\"ot\">-></span> <span class=\"kw\">do</span>\n    <span class=\"fu\">></span>                 <span class=\"co\">-- Everything else is a state-level contribution.</span>\n    <span class=\"fu\">></span>                 <span class=\"co\">-- Get the cycle and return that.</span>\n    <span class=\"fu\">></span>                 cycle <span class=\"ot\"><-</span> m <span class=\"fu\">.:</span> <span class=\"st\">\"cycle\"</span>\n    <span class=\"fu\">></span>                 return (<span class=\"dt\">StateLevel</span> cycle r)</code>\n\n\n\n\n(You might be wondering why I haven’t needed to define a `FromField` for `Year` for the “cycle” fields. Remember that `Year` is just an alias for `Int`, and the CSV library already defines `FromField` for the `Int` type.)\n\n\n\n\nWe can finally define the instance for `OrgContrib`. After the complexity of `ContribEntry`, this one will be much simpler. We’ll extract the values for a few fields, parse the `ContribEntry`, and then create and return the `OrgContrib` value.\n\n\n\n    \n    <code class=\"sourceCode haskell\">\n    <span class=\"fu\">></span> <span class=\"kw\">instance</span> <span class=\"dt\">FromNamedRecord</span> <span class=\"dt\">OrgContrib</span> <span class=\"kw\">where</span>\n    <span class=\"fu\">></span>     parseNamedRecord m <span class=\"fu\">=</span> <span class=\"kw\">do</span>\n    <span class=\"fu\">></span>         name     <span class=\"ot\"><-</span> m <span class=\"fu\">.:</span> <span class=\"st\">\"contributor_name\"</span>\n    <span class=\"fu\">></span>         district <span class=\"ot\"><-</span> m <span class=\"fu\">.:</span> <span class=\"st\">\"contributor_district_10s\"</span>\n    <span class=\"fu\">></span>         contrib  <span class=\"ot\"><-</span> parseNamedRecord m\n    <span class=\"fu\">></span>         amount   <span class=\"ot\"><-</span> m <span class=\"fu\">.:</span> <span class=\"st\">\"amount\"</span>\n    <span class=\"fu\">></span>         return (<span class=\"dt\">OrgContrib</span> name district contrib amount)</code>\n\n\n\n\nWith these in place, we can read the data and have it verified at the same time. For example, if the file reads correctly, I know that the `Year` data are integers and that `Party` fields contain valid data.\n\n\n\n\nAnd that’s really all there is to it. Below the end of the article, I’ve included a function to read the CSV data from a file and the `main` function, which controls the whole process. However, reading and validating the data has already been taken care of.\n\n\n\n\nOf course, while these types provide reasonable validation, you could get much better, depending on how you define your types and how you parse the incoming data. (For example, you could only allow valid state codes for `StateLevel` or limit years to a given range.)\n\n\n\n\nIf you’re wondering about tests, the implementations of `FromField` and `FromNamedRecord` would be good to have tests for. However, the parts of the program’s requirements that are enforced in the types don’t really need testing; for example, I wouldn't test that party fields will always be parsed as a `Party`.\n\n\n\n\nTypes also come in handy in other circumstances: when you’ve left the code for a while and need to get back into it, they provide a minimum amount of guidance; and when you need to refactor, they act as a base-line set of regression tests, to tell you when you’ve broken something.\n\n\n\n\nOverall, I find that this small program shows how Haskell can provide a lot of power and expressivity for relatively little code.\n\n\n\n\nBut the immediate benefit in this case is that I was able to provide John more assurances about his data, and to provide them more quickly. It’s a nice example of leveraging types to write better programs that provide real-world benefits.\n\n\n\n\nThe full code for this project is in my [popvox-scrape](https://github.com/erochest/popvox-scrape) repository. Feel free to check it out.\n\n\n\n\n\n* * *\n\n\n\n    \n    <code class=\"sourceCode haskell\">\n    <span class=\"fu\">></span><span class=\"ot\"> readData ::</span> FilePath <span class=\"ot\">-></span> <span class=\"dt\">IO</span> (<span class=\"dt\">Either</span> <span class=\"dt\">String</span> (<span class=\"dt\">Header</span>, <span class=\"dt\">V.Vector</span> <span class=\"dt\">OrgContrib</span>))\n    <span class=\"fu\">></span> readData filename <span class=\"fu\">=</span> <span class=\"kw\">do</span>\n    <span class=\"fu\">></span>     rawData <span class=\"ot\"><-</span> B.readFile filename\n    <span class=\"fu\">></span>     return (decodeByName rawData)\n    <span class=\"fu\">></span>\n    <span class=\"fu\">></span><span class=\"ot\"> main ::</span> <span class=\"dt\">IO</span> ()\n    <span class=\"fu\">></span> main <span class=\"fu\">=</span> <span class=\"kw\">do</span>\n    <span class=\"fu\">></span>     args <span class=\"ot\"><-</span> getArgs\n    <span class=\"fu\">></span>     <span class=\"kw\">case</span> args <span class=\"kw\">of</span>\n    <span class=\"fu\">></span>         [filename] <span class=\"ot\">-></span> <span class=\"kw\">do</span>\n    <span class=\"fu\">></span>             dataRows <span class=\"ot\"><-</span> readData filename\n    <span class=\"fu\">></span>\n    <span class=\"fu\">></span>             <span class=\"kw\">case</span> dataRows <span class=\"kw\">of</span>\n    <span class=\"fu\">></span>                 <span class=\"dt\">Left</span> err <span class=\"ot\">-></span> putStrLn (<span class=\"st\">\"ERROR: \"</span> <span class=\"fu\">++</span> err)\n    <span class=\"fu\">></span>                 <span class=\"dt\">Right</span> (_, rows) <span class=\"ot\">-></span> putStrLn (  <span class=\"st\">\"SUCCESS: \"</span>\n    <span class=\"fu\">></span>                                             <span class=\"fu\">++</span> show (V.length rows)\n    <span class=\"fu\">></span>                                             <span class=\"fu\">++</span> <span class=\"st\">\" read.\"</span>)\n    <span class=\"fu\">></span>\n    <span class=\"fu\">></span>         _ <span class=\"ot\">-></span> putStrLn <span class=\"st\">\"usage: runhaskell Validate.lhs data-file.csv\"</span></code>\n\n\n\n"},{"id":"2015-04-23-podcast-ben-wright-and-joesph-locke-on-creating-american-yawp","title":"Podcast: Ben Wright and Joesph Locke on Creating American Yawp","author":"laura-miller","date":"2015-04-23 06:53:38 -0400","categories":["Events","Podcasts"],"url":"podcast-ben-wright-and-joesph-locke-on-creating-american-yawp","content":"**Democratizing the Digital Humanities: The American Yawp as Case Study**\n\nAfter a year-long collaboration, over 350 historians have produced a beta edition of [The American Yawp](http://www.americanyawp.com), a free and online, collaboratively built, open American history textbook designed for college-level history courses. This talk will explore the creation and dissemination of this project, the landscape of open educational projects in the humanities, the methods used to harness the energy of hundreds of academics, and the potential for large-scale collaboration and open resources to make practical the democratic promise of the digital humanities.\n\n[![yawp_wright](http://scholarslab.org/wp-content/uploads/2015/02/yawp_wright-110x110.jpg)](http://scholarslab.org/wp-content/uploads/2015/02/yawp_wright.jpg)Ben Wright is an assistant professor of history at Abraham Baldwin Agricultural College. His manuscript, Antislavery and American Salvation, is under advance contract with LSU Press. His digital projects include The American Yawp and [abolitionseminar.org](http://www.abolitionseminar.org/), a NEH-sponsored educational tool on the antislavery movement, designed for K-12 educators and their students. He also serves as managing editor of [Teaching United States History](http://www.teachingushistory.co/), a critical forum discussing pedagogy in college-level American history courses.\n\n[![yawp_locke](http://scholarslab.org/wp-content/uploads/2015/02/yawp_locke-110x110.jpg)](http://scholarslab.org/wp-content/uploads/2015/02/yawp_locke.jpg)Joseph Locke is an assistant professor history at the University of Houston-Victoria, where he teaches courses in American history and researches the historical interplay between religion and the American South. His first book, _Making the Bible Belt: Prohibition and the Politicization of Southern Religion_, is forthcoming from Oxford University Press.\n\nThis talk was recorded in Alderman Library on March 19, 2015.  Click below to stream the podcast, and follow along with the [presentation slides](http://scholarslab.org/wp-content/uploads/2015/04/Abbreviated-Yawp-presentation.pptx).  If you'd like to hear more from the Scholars' Lab, subscribe to our podcast series [on iTunesU](https://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619?mt=10).\n\n[podloveaudio src=\"http://a1874.phobos.apple.com/us/r30/CobaltPublic7/v4/33/f1/f7/33f1f772-e042-97be-e987-b297c01feae8/304-8494564249797709976-wright_locke_final.mp3\"]\n"},{"id":"2015-04-23-task-management-bullet-journal","title":"Task Management & Bullet Journal","author":"ronda-grizzle","date":"2015-04-23 09:15:06 -0400","categories":null,"url":"task-management-bullet-journal","content":"Confession: time management has always been a challenge for me. I'm easily distractable, and I'm exceptionally bad at attempting to do more than one thing at a time. When multiple \"fires\" get lit I lose most of my productivity on task switching and to overwhelm.\n\nI've tried several time management apps for computer and phone. Yet, I always end up spending too much time fiddling with the app to try to make it work well, or forget to look at my handy computerized to list after I've typed it. For a while, I thought EverNote was going to be a winner. But I end up, every time, back with good old pen and paper to-do lists, and they work better than any app I've tried. I also remember the items on my task list better when I write them out by hand than if I type them. The act of writing helps with the transfer of information to long term mental storage.\n\nI can't remember where I first learned about [Bullet Journal](http://www.bulletjournal.com/), but I suspect it was on Lifehacker or BoingBoing. I knew as soon as I watched the short video introduction by creator Ryder Carroll that this might be a system I could actually use. Bullet Journal works because it’s more than a task list. It provides a space to bring together tasks, meeting notes, reading notes, inspiration for future work and research, lists of various kinds, and questions that need answers. It's also a hackable system, as the [thousands of youtube videos](https://www.youtube.com/results?search_query=bullet+journal&page=&utm_source=opensearch) about how people use and adapt it attest.\n\nHere's the basic system, as outlined in [the introductory video](https://youtu.be/GfRf43JTqY4):\n\n1. Acquire a quad-ruled notebook, not too small and not too large. You want a notebook that's compact enough to be carried with you, but not so small that you don't have room to write. I use the 5\" x 8\" Moleskine, or a Moleskine knockoff when I can find one.\n2. Number the pages. Without numbered pages, you won't be able to find anything later.\n3. On the first facing page, start an Index page. Being able to find all those notes you took and pages where you noodled on about a project idea you had is what makes the journal usable.\n4. On the left side of the first two-page spread create a simple monthly calendar by listing the days of the month down the page, filling in the days of the week beside them. On the facing page, create a list of the tasks you know need to be completed in that month. Once this is done, note the starting page number for that month on your index page.\n5. On the next two page spread, begin creating your daily calendar by writing the month and date, then listing under it the tasks and appointments for that day.\n\nThe symbol set is what makes these daily calendar pages easy to scan and mentally parse: squares for tasks, circles for meetings/appointments, an exclamation point for inspiration/short notes, an eye for notes about topics you want to explore more deeply.\n\nThese basic shapes can be annotated with stars or asterisks to indicate priority items. I added a symbol to mark personal items, so that I can combine my personal and work to do lists. With the symbol, I am able to easily find the items I need to based on where I am. Filling in half of the square on the diagonal marks partially done tasks. Any item that becomes irrelevant is struck through with a line.\n\nAt the end of the month, per the instructions from bulletjournal.com, mark through any tasks that have become irrelevant, place an arrow in the boxes for tasks that are moving forward to the next month, and update your index page.\n\nEasy peasy.\n\nI've been using a bullet journal for about a year, and I really love it. I'm happy to report that [Eric](http://scholarslab.org/?p=5477) and [Scott](http://scholarslab.org/?p=8764) have adopted it, as well, and seem to like it. A few things  aren't working for me, though, not because of bullet journal, but because of how I'm using it.\n\nThe journal is great for keeping daily tasks on the radar, but I'm finding that I'm not tracking longer term tasks as well as I'd like. The long term tasks associated with larger goals just never seem get moved from that beginning of the month task list onto the daily pages. I've also noticed that tasks not completed on any given page sometimes get dropped when that page is no longer in view.\n\nThe remedies for these issues are pretty simple. First, I need to apply agile methodology to breaking down my large projects (work and personal) into the smallest incremental tasks possible, just like I would if it were a large project to which multiple people were contributing. I've decided that I need [a personal kanban board](http://www.personalkanban.com/pk/personal-kanban-101/) so that I have a clear picture of all the tasks associated with a project and the priority I've assigned to them, which task is in progress, which tasks are completed, and I've got the wall space in my cube picked out for it, since electronic, app-based kanban boards have proven ineffective for me, too.\n\nSecond, instead of waiting to transfer tasks at the end of the month, I've started transferring them when I turn a page. Tasks that are on my current two page spread, no matter what day of the month it happens to be, include any tasks that were left undone on the previous two page spread. The first time a task gets shifted, it gets marked with a star to make it a priority item. Tasks that are shifted a second time are examined for why they've ended up in \"the procrastination pile,\" which is a whole other blog post! The point is, if a task is moved twice, there's a reason why it's not getting done. I have to get to the bottom of it and either complete the task or delegate it or consciously decide that it's not something that needs to be done.\n\nThe last benefit of bullet journaling is it forces me to pause at the end of every single day to set up the next day's tasks and appointments. My mornings used to always start (OK, they sometimes still do!) in a rush of organizing my schedule for the day. It was stressful, ineffective, and created the feeling that I was always running to catch up with myself. By pausing at the end of each day to set the next day, I'm mentally prepared and ready to focus when I sit down at my desk, which is a much calmer and healthier way to approach my work.\n\nDo you use bullet journal? I'd love to know what you think and how you've hacked it.\n\n"},{"id":"2015-05-04-ninjaflex-on-the-makerbot","title":"NinjaFlex on the Makerbot","author":"shane-lin","date":"2015-05-04 11:53:55 -0400","categories":["Experimental Humanities"],"url":"ninjaflex-on-the-makerbot","content":"Announcing version 0.1 public beta of the [Scholars Lab Makerspace Ninjaflex profile for Makerbot Replicators](http://scholarslab.org/wp-content/uploads/2015/05/ninjaflex100.txt)!\n\n[![2015-05-04 15.09.58 HDR](http://scholarslab.org/wp-content/uploads/2015/05/2015-05-04-15.09.58-HDR-225x300.jpg)](http://scholarslab.org/wp-content/uploads/2015/05/2015-05-04-15.09.58-HDR.jpg)\n\nWe've had two spools of Ninjaflex flexible filament for about as long as we've had our Makerbot Replicator 2. We've tried to print with it from time to time, but seldom with very good result. With our Rep2 dialed in and printing PLA like a champ for the last few months, I decided that it was a pretty good time to finally figure out Ninjaflex.\n\nA few issues were immediately obvious when printing with the default Makerbot flexible filament profile. The extruded lines were too thin and there was significant \"ooze\", causing a lot of thin, dangling strings when the nozzle shifted without printing. A quick search turned up a[ Ninjaflex profile for Makerbots on Thingiverse,](http://www.thingiverse.com/thing:408757) with some helpful hints on parameters to change. Our profile is a slight modification of that, with a major bug fix and a handful of small adjustments.\n\nIn my experience, the two really important variables are \"retractDistance\" and \"feedrate\" (inconsistent camelcase as-is). RetractDistance controls the amount that the filament is retracted for moves; slightly increasing this to 1.3 (mm) dramatically reduces ooze. reedRate is the speed of extruder when extruding. Going slow is critical to success with Ninjaflex and so I've had good results with a consistent 20mm/s speed for all components.\n\nAdditional issues unrelated to software has been feeding and clogging. When using our Replicator 1 Dual Extruder, I found that the friction of the Ninjaflex against the feed tube was too high for the Rep1 extruder's motor to overcome, even when manually unspooled; I think that others have had some luck with printing a tubeless guide.  We just switched to the Replicator 2. But even there, with a full and heavy spool of Ninjaflex, I still relied on manual unspooling.\n\nEven so, our printer would still sometimes become clogged. Based on online commentary, this seems fairly common and, for us at least, is resolved by unloading, snipping off the nodule of material pooled at the end of the filament, and reloading. This seems to happen more frequently after the Ninjaflex has been heated and then left to cool without being extruded and when transitioning between Ninjaflex and PLA. Others have created custom-printed extruder parts to alleviate these issues, but we haven't yet tried them out.\n\nSetting up a custom profile in the Makerbot software isn't actually very intuitive. To create a profile, select Advanced Options\" in the print settings and click \"Create Profile\". Name the profile something descriptive and an appropriate base profile if you're starting from scratch (any will be fine if you're going to use our Ninjaflex Profile)\n\n![Screen Shot 2015-05-04 at 2.40.12 PM](http://scholarslab.org/wp-content/uploads/2015/05/Screen-Shot-2015-05-04-at-2.40.12-PM-300x231.png)\n\nAfter the profile is created, select it from the dropdown and click \"Edit Profile\" to open the JSON file which contains it.\n\n[![Screen Shot 2015-05-04 at 2.41.02 PM](http://scholarslab.org/wp-content/uploads/2015/05/Screen-Shot-2015-05-04-at-2.41.02-PM-300x231.png)](http://scholarslab.org/wp-content/uploads/2015/05/Screen-Shot-2015-05-04-at-2.41.02-PM.png)\n\nYou can test out our profile by simply copy-pasting it in.\n\nOur profile is 0.2mm infill, 100% infill, no rafts or supports. Based on experimentation, rafts actually seem to work okay while supports tend to be harder to remove (although it is quite easy to cut them off, as well as perform general resurfacing and re-edging with scissors so long as you can get them in there). Infill actually works pretty well too, although they tend to be better with thicker walls (we use 3 instead of the usual 2).\n\nWe've mostly printed well-supported models; I don't know how well Ninjaflex would print latices, very thin structures, and extreme overhangs (I suspect \"not well\").\n\nIf you try out our profile, let us know how well it works for you!\n"},{"id":"2015-05-05-expanding-our-makerspace-community","title":"Expanding Our Makerspace Community","author":"laura-miller","date":"2015-05-05 10:46:07 -0400","categories":null,"url":"expanding-our-makerspace-community","content":"_Are you a UVA graduate student or upper-level undergraduate in the humanities?  Interested working in our Makerspace?_\n\n[![Partially assembled Sparkfun Inventor's Kit, with an Arduino board, breadboard, and soldering iron](http://scholarslab.org/wp-content/uploads/2014/09/makerspace7-300x200.jpg)](http://scholarslab.org/wp-content/uploads/2014/09/makerspace7.jpg)Our Makerspace is designed to foster experimentation with 3D modeling and printing, physical computing (e.g. Arduino, wearables) and more.  We are seeking part-time student consultants to help maintain the public space, field users’ basic maker and general computing questions, and connect researchers to Scholars’ Lab staff when necessary.  When not actively engaged with users, students will be asked to experiment with the equipment, to pursue their own research, and to publish their processes and observations on the Scholars’ Lab blog. They will also be expected to conduct informal workshops to train new users.\n\nExperience with 3D modeling and printing, electronics, sewing, and/or programming preferred, but can be learned on the job.  The successful candidate will be able to work up to 10 hours per week.\n\nAn important aspect of the maker culture is apprenticeship and supporting makers in their pursuit of professional experience. We are looking for motivated individuals who are capable of working independently and value the opportunity to engage with and support a growing community.  Benefits of the job may include: access to expertise and mentoring in your field of interest, use of equipment and tools, and ability to shape Scholars’ Lab workshops and programming.\n\nCandidates should include a cover letter discussing their interest in working in the Scholars’ Lab, detailing any experience or interest in participating in a maker space, and outlining any previous experience with public service or assisting others in using technology.\n\nIf you would like to apply, please fill out an application in [CavLink](http://www.career.virginia.edu/students/cavlink/).\n"},{"id":"2015-05-14-announcing-2014-2015-fellows","title":"Announcing 2014-2015 Fellows!","author":"purdom-lindblad","date":"2015-05-14 07:55:21 -0400","categories":["Announcements","Grad Student Research"],"url":"announcing-2014-2015-fellows","content":"We are thrilled to announce the 2015-2016 Scholar's Lab fellows for both the [Praxis Program](http://praxis.scholarslab.org/) and the [Graduate Fellowship in the Digital Humanities](http://scholarslab.org/graduate-fellowships/). We are welcoming 8 fellows from 5 disciplines from the [arts, humanities, and social sciences](http://gsas.virginia.edu/). Our graduate fellows are joining a robust and vibrant community of [past fellows](http://scholarslab.org/people/)!\n\n\n\n\n\n\n\n\n\n\n## Graduate Fellows in the Digital Humanities\n\n\n\n\n\n\n\n\n\nWe are looking forward to working with Brandon Walsh and Veronica Ikeshoji-Orlati, our 2014-2015 Graduate Fellows in the Digital Humanities.\n\nVeronica Ikeshoji-Orlati's (Classical Art and Archaeology) dissertation is titled, _Music, Performance, and Identity in 4th century BCE South Italian Vase-Painting._\n\nBrandon Walsh's (English) dissertation is titled, _AudioTextual: Modernism, Sound Recording, and Networks of Reception._\n\n\n\n\n\n\n\n\n\n## Praxis Program\n\n\n\n\n\n\n\nWe are delighted to welcome 6 diverse disciplinary team members to the 5th year of the Praxis Program:\n\n\n\n\n\n\t\n  * James Ascher (English)\n\n\t\n  * Bremen Donovan (Anthropology)\n\n\t\n  * Ethan Reed (English)\n\n\t\n  * Gillet Rosenblith (History)\n\n\t\n  * Rachel Trapp (Music, Composition)\n\n\t\n  * Lydia Warren (Music, Critical and Comparative Studies)\n\n\n\n\nLook forward to more details around the Praxis Program’s new project in the fall!\n"},{"id":"2015-05-19-can-you-get-the-data-out-of-this-file","title":"Can you get the data out of this file?","author":"ammon-shepherd","date":"2015-05-19 10:05:36 -0400","categories":["Geospatial and Temporal"],"url":"can-you-get-the-data-out-of-this-file","content":"That was the question I was asked by a student who came into the Scholars' Lab this semester.\n\nMy answer is always, \"Yes. That can be done.\" Wether I know how at the time is a different matter, but that's the beauty of the Lab, the opportunity to learn new things.\n\n\n# The Challenge\n\n\nA student came in with a database file and the need to view the data in the file. At first I thought it would be easy to pull the data into a database and export it out as a spread sheet. Once I got a copy of the SQL file, I realized it would be a bit more work than firing up phpMyAdmin to import the SQL file and export as a CSV or Excel file.\n\nFirst of all, the SQL file was a database dump from PostgreSQL. Second, it was a PostGIS data file.\n\n\n# Solution\n\n\nTo be able to view the data in the file I needed to set up a PostgreSQL database with PostGIS, and then use QGIS (a free, cross-platform alternative to ArcMaps) to actually visualize the geographic data stored in the SQL file.\n\n\n## PostgreSQL\n\n\nInstalling PostgreSQL is the first step. There are many ways to do it.\n\n\n### For Windows, Linux, etc\n\n\n\n\n\n\t\n  * From PostgreSQL themselves: [http://www.postgresql.org/download/](http://www.postgresql.org/download/)\n\n\t\n  * Handy tutorial for different OS's: [https://www.codefellows.org/blog/three-battle-tested-ways-to-install-postgresql](https://www.codefellows.org/blog/three-battle-tested-ways-to-install-postgresql)\n\n\n\n\n### For the Mac:\n\n\nSince I'm using a Mac and had some issues, I'll detail that installation below.\n\n\n\n\t\n  * The easiest: [http://postgresapp.com/](http://postgresapp.com/)\n\n\t\n  * The next easiest: Use homebrew,`brew install postgresql`\n\n\t\n  * The hardest: Install from source, but that's not really needed. The PostgreSQL site has a list of install options, including the two above: [http://www.postgresql.org/download/macosx/](http://www.postgresql.org/download/macosx/)\n\n\nI went the homebrew way because of a required extension that doesn't get installed with the PostgreSQL app or other means.\n\n\n#### Installing\n\n\n\n    \n    <code class=\"hljs\">brew install postgres postgis\n    </code>\n\n\nFollow the instructions for starting and stoping the PostgreSQL server, with the addition of making life easier by installing and using `lunchy`.\n\n    \n    <code class=\"hljs\">brew install lunchy\n    </code>\n\n\nNow you can start and stop PostgreSQL with `lunchy start postgres` and `lunchy stop postgres`\n\nWe'll also need to install pgrouting. This looks like it can be installed with homebrew, but it didn't actually install for me. I ended up installing from source.\n\nUsing this StackExchange answer as a guide, install pgrouting: [http://gis.stackexchange.com/questions/26330/issues-with-installing-pgrouting-in-mac](http://gis.stackexchange.com/questions/26330/issues-with-installing-pgrouting-in-mac)\n\nThere is a bug in the main stable version of pgrouging, so you'll need to grab the development branch from their github repo.\n\n[https://github.com/pgRouting/pgrouting/archive/develop.zip](https://github.com/pgRouting/pgrouting/archive/develop.zip)\n\nDownload and unzip that file.\n\nEnter the directory created by unzipping, and do the following steps:\n\n    \n    <code class=\"hljs\">mkdir build\n    cd build\n    cmake -DPOSTGRESQL_INCLUDE_DIR='/usr/local/Cellar/postgresql/9.4.1/include/server' -DBoost_DIR='/usr/local/Cellar/boost/1.57.0' ..\n    make\n    sudo make install\n    </code>\n\n\n\n\n#### Enabling the Extensions\n\n\nAfter installing and starting PostgreSQL, you'll need to create/install/enable the extensions.\n\nLog into the PostgreSQL server:\n\n    \n    <code class=\"hljs\">psql postgres\n    </code>\n\n\nView which extensions are already installed by typing `\\dx` on the PostgreSQL command prompt.\n\nCreate an extension by typing `create extension <extension name>`, replacing `<extension name>` with the name of the extension, of course.\n\nYou'll need to have the following extensions installed:\n\n\n\n\t\n  * plpgsql\n\n\t\n  * hstore\n\n\t\n  * intarray\n\n\t\n  * pgcrypto\n\n\t\n  * postgis\n\n\t\n  * pgrouting\n\n\nInstalling the extensions this way lets you see if there are any problems before trying to import the SQL file.\n\n\n## Import the SQL file\n\n\nFirst set up a role/user and a database.\n\n    \n    <code class=\"hljs\">createdb dbname\n    createuser username\n    </code>\n\n\nI needed to change the name of the user in the SQL file to match the database/username in the PostgreSQL database, or I could have created a database/user with the same name as in the SQL file. But to change all of the names I did a one line perl call:\n\n    \n    <code class=\"hljs\">perl -pi bak -e 's/alec/test1/g' 20130930_dbdump_Alec.sql\n    </code>\n\n\nThis will change all instances of 'alec' to 'test1' in the file, and create a backup of the file.\n\nNow you can import the file by running:\n\n    \n    <code class=\"hljs\">psql database < filename\n    </code>\n\n\n\n\n## QGIS\n\n\nQGIS is a free alternative to ArcMaps. Install from their website. Installation is pretty straight forward.[http://qgis.org/en/site/forusers/download.html](http://qgis.org/en/site/forusers/download.html)\n\n\n### Connect to PostgreSQL database\n\n\n\n\n\n\t\n  * A tutorial: [http://gis-techniques.blogspot.com/2012/04/how-to-connect-spatial-databasepostgis.html](http://gis-techniques.blogspot.com/2012/04/how-to-connect-spatial-databasepostgis.html)\n\n\nTo connect to the PostgreSQL database, you'll actually add the database as a layer. Go to the Layer menu -> Add Layer -> Add PostGIS Layers.\n\n\n\n[![Screen Shot 2015-05-19 at 12.56.01 PM](http://scholarslab.org/wp-content/uploads/2015/05/Screen-Shot-2015-05-19-at-12.56.01-PM-300x144.png)](http://scholarslab.org/wp-content/uploads/2015/05/Screen-Shot-2015-05-19-at-12.56.01-PM.png)\n\nUnder the Connections section, click on the New button.\n\n    \n    <code class=\"hljs\">Name = anything you want\n    Host = localhost\n    User = the same as you used above in the createuser command\n    </code>\n\n\n[![Screen Shot 2015-05-19 at 12.56.21 PM](http://scholarslab.org/wp-content/uploads/2015/05/Screen-Shot-2015-05-19-at-12.56.21-PM-217x300.png)](http://scholarslab.org/wp-content/uploads/2015/05/Screen-Shot-2015-05-19-at-12.56.21-PM.png)\n\n\n\nNow select the tables you need and click the \"Add\" button.\n\n[![Screen Shot 2015-05-19 at 12.56.38 PM](http://scholarslab.org/wp-content/uploads/2015/05/Screen-Shot-2015-05-19-at-12.56.38-PM-300x193.png)](http://scholarslab.org/wp-content/uploads/2015/05/Screen-Shot-2015-05-19-at-12.56.38-PM.png)\n\n\n\nEnjoy multiple layers on your map!\n"},{"id":"2015-06-11-summer-in-the-makerspace-mucha-smart-dress-part-i","title":"Summer in the Makerspace: Mucha Smart Dress, Part I","author":"julia-schrank","date":"2015-06-11 10:59:18 -0400","categories":["Experimental Humanities","Grad Student Research"],"url":"summer-in-the-makerspace-mucha-smart-dress-part-i","content":"Bonjour à tous!\n\n\nI haven't written in a long time because I wanted to wait for something very special to share. With Laura and Purdom's encouragement, I am undertaking the creation of a wearable technology piece that I have dubbed (tentatively), the \"Mucha Smart Dress,\" which I hope to finish by mid-July...so I can wear it on my birthday!\n\nIn this post, I will describe the project and detail the goals, inspirations, and supplies I have in mind.\n\n\n## The Project\n\n\n[caption id=\"attachment_11975\" align=\"aligncenter\" width=\"500\"][![Mucha Smart Dress](http://scholarslab.org/wp-content/uploads/2015/05/Mucha-Smart-Dress-1024x981.jpg)](http://scholarslab.org/wp-content/uploads/2015/05/Mucha-Smart-Dress.jpg) My sketch of the \"Mucha Smart Dress\"[/caption]\n\nAlthough [Alphonse Mucha](http://www.muchafoundation.org/gallery/browse-works/object_type/posters/object/80) ultimately rejected the \"Art Nouveau\" label his critics ascribed to his œuvre, he is nonetheless considered one of the founding fathers of this movement. A wildly popular style in the decades of 1890 to 1910, Art Nouveau marked an effort towards _gesamtkunstwerk_, or \"total art.\" According to Mucha, his contributions to the style sought to surround society with beautiful works of art in order to inspire happiness and general well-being. He also maintained that objects' design, in order to bring this tranquility, had to indicate and contribute to the object's use.\n\nIt is with this fusion of the inspirational and the functional that I wanted to make an Alphonse Mucha-styled wearable.  I have conceived of a \"Smart Dress,\" based on an amalgam of Mucha designs, that will not only pay homage to Mucha's stunning legacy, but also be useful, wearable, and approachable. Despite the complexity of the design, I hope to make it a piece that I can wear on a normal day by toning it down with monochromatic colors and a more modern silhouette. The technology of the project will come in the form of a FLORA microcomputer housed in the dress's center decoration (to be 3D printed with Ninjaflex) that will operate a series of LEDs sewn into the drapes of the fabric. With temperature changes, different color combinations of LEDs will light up, illuminating the silk overlay. This function is not entirely practical, yet it is firmly anchored in the physical world while also fusing the four panels of Mucha's \"The Seasons\" quadriptych (see more details and a photo below) into one garment.\n\nWhile creating this project, I hope to also think critically about the utility (or lack thereof) of wearable technology in my research on Alphonse Mucha's \"French period\" of decorative art and poster designs. For example, I'm already asking myself, \"Are these clothes practical for everyday life?\" in 2015 and in 1900, and \"Is this practical (or impractical) nature a part of the designs' appeal?\"\n\n\n## Goals\n\n\n\n\n\n\t\n  * Create a wearable technology piece to be displayed in the Makerspace.\n\n\t\n  * Overcome my Arduino anxiety (!)\n\n\t\n  * Make a floor-length dress for summer that actually fits and flatters me.\n\n\t\n  * Bring an Alphonse Mucha design to life.\n\n\n\n\n## Inspirations\n\n\n\n\n\n\t\n  * [This dress by Electromode](http://fashioningtech.com/profiles/blogs/electromodewearable-tech) showed me that Arduino-enhanced clothing could be chic. I truly admire all of the geekery-inspired pieces out there, but I couldn't see myself incorporating them into my existing wardrobe.\n\n\t\n  * I got the idea for a temperature-sensitive focus from Alphonse Mucha's 1896 lithographic quadriptych of  \"The Seasons.\"\n\n\n![Screen Shot 2015-06-04 at 1.36.02 PM](http://scholarslab.org/wp-content/uploads/2015/05/Screen-Shot-2015-06-04-at-1.36.02-PM.png)\n\n\n\n\t\n  * The dress shape and much of its design I owe to Mucha's 1898 print \"The Arts: Painting.\" I have, however, adapted the dress to cover the breasts, since in our society this sort of garment is not – strictly speaking – legal to wear in public.\n\n\n[![Screen Shot 2015-06-04 at 1.37.32 PM](http://scholarslab.org/wp-content/uploads/2015/05/Screen-Shot-2015-06-04-at-1.37.32-PM.png)](http://scholarslab.org/wp-content/uploads/2015/05/Screen-Shot-2015-06-04-at-1.37.32-PM.png)\n\n\n\n\t\n  * I was also inspired by \"The Moon and the Stars,\" with particular emphasis on the \"Moon\"[ ](http://www.muchafoundation.org/gallery/search-works/display/results/object/156)since her dress looks like it already has LEDs on it, non?\n\n\n[![Screen Shot 2015-06-04 at 1.37.12 PM](http://scholarslab.org/wp-content/uploads/2015/05/Screen-Shot-2015-06-04-at-1.37.12-PM.png)](http://scholarslab.org/wp-content/uploads/2015/05/Screen-Shot-2015-06-04-at-1.37.12-PM.png)\n\n* [The Mucha Foundation](http://muchafoundation.org), upon my meeting with them, specified that they do not generally accept unauthorized reproductions of the artist's work and \"fair use\" is not the same in the Czech Republic. Therefore I have posted screenshots from the Foundation's website.\n\n\n## Supplies (selected)\n\n\n\n\n### Arduino supplies\n\n\n\n\n\n\t\n  * [adafruit's FLORA](https://www.adafruit.com/products/659), optimized for wearables\n\n\t\n  * [Conductive thread](https://www.sparkfun.com/products/11791)\n\n\t\n  * [Neopixel Diffused 5mm Through-Hole LEDs](http://www.adafruit.com/products/1938), 50-count\n\n\n\n\n### 3D Printing\n\n\n\n\n\n\t\n  * [Ultimaker PLA Silver-Metallic](http://fbrc8.com/collections/ultimaker-filament/products/ultimaker-pla-silver-metallic)\n\n\n\n\n### Materials\n\n\n\n\n\n\t\n  * Thrifted dress, for the prototype (see below)\n\n\t\n  * [Dark Silver Habotai Silk](http://www.moodfabrics.com/dark-silver-china-silk-habotai-pv2000-192.html), for the final copy\n\n\t\n  * Repurposed leather (From my time à Paris)\n\n\t\n  * [2\" Double-faced Silk Ribbon](http://www.mjtrim.com/silk/50mm-double-faced-silk-ribbon#sthash.4rVqAzGb.dpbs), Charcoal-purple and black\n\n\n[caption id=\"attachment_12000\" align=\"aligncenter\" width=\"683\"][![Before Dress](http://scholarslab.org/wp-content/uploads/2015/05/Before-Dress-683x1024.jpg)](http://scholarslab.org/wp-content/uploads/2015/05/Before-Dress.jpg) I'll turn this thrifted find (in Makerspace Red!) into a hacked 19th-century design![/caption]\n\nPlease let me know if you have any suggestions, alterations, or comments on this plan at any point during my work on this project!\n"},{"id":"2015-06-29-todo-introduce-code-concepts","title":"//TODO - Introduce Code Concepts","author":"wayne-graham","date":"2015-06-29 06:59:37 -0400","categories":["Research and Development"],"url":"todo-introduce-code-concepts","content":"One of the most fun (and challenging) things I get to do is to introduce people to programming concepts. I've done this in a lot of different environments ranging from intensive week-long courses with [Humanities Intensive Learning and Training](http://www.dhtraining.org/hilt2015/), to year-long apprenticeships our [Praxis Program](http://praxis.scholarslab.org), to day-long intros with events like [Rails Girls](http://railsgirls.com/) and [Rails Bridge](http://www.railsbridge.org/). All of these have different trade offs in getting people introduced to writing for computers (software development). But one of the best ways I've found to work with someone who is interested in learning about how to write software is [pair programming](https://en.wikipedia.org/wiki/Pair_programming), and it's something that is done in the Scholars' Lab a lot (though I don't get to do as much as I would like).\n\nIf you are unfamiliar with pair programming, basically it takes two people sitting side-by-side (there are remote versions of this too) where one person types (the driver), and the other \"tells\" the driver what to type (the navigator). I have found this a hugely effective way to get people to start hearing how to approach a software problem, particularly how to break up a particular problem in to smaller problems. This is a skill developers learn over time, but is perhaps one of the most opaque to novice developers. Some of this has to do with the abstract way in which software languages work, the level of syntax knowledge a developer has, and the fact that experienced developers start skipping steps along the way based on their experience. Novice developer might look at a problem and begin to develop a solution, they often get locked in to their thinking that this is the only way. When this path doesn't pan out as expected (which is usually the case), this can lead to a lot of frustration, and \"Stack Overflow\" code. As a seasoned developer, I'll usually see several different paths through a problem, and when I hit that first roadblock, alter my approach.\n\nLast week I joked with one of our [LAMI fellows](http://www.theleadershipalliance.org/Programs/SummerResearch/MellonInitiative/tabid/371/Default.aspx) on our IRC channel that she should write a plugin for one of our bots so we could generate memes with the [memegen.link API](http://memegen.link/). She responded back that she'd actually love to learn how, and in the spirit of [the Ten Thousand](https://xkcd.com/1053/), we met the next day to work this out. One of the bots in our IRC is [HUBOT](https://hubot.github.com/), which uses [CoffeeScript](http://coffeescript.org/) to interact with a [node.js](https://nodejs.org/) server. While I typically write in JavaScript for these purposes, I think CoffeeScript has some real advantages for a novice developer (particularly with closures and some of the funkier parts of JavaScript).\n\nThe way I broke this down was to say we need a few things for this to do. First, we want to be able to see which meme templates can used, and a method to get a URL back with the meme. Using the conventions of HUBOT, I suggested an interface like this\n\n[code]\nuser> hubot meme list\nhubot> afraid - Afraid to Ask\nhubot>...\n[/code]\n\nand\n\n[code]\nuser> hubot meme me afraid \"top message\" \"bottom message\"\nhubot> afraid - Afraid to Ask\nhubot>...\n[/code]\n\nThe meme list seemed to be easiest so we tackled that first. We looked at [the list of templates](http://memegen.link/templates/) and needed to simplify. Here I just wanted the keys and a label, so we built our own CoffeeScript object with this pattern:\n\n[code]\nmemes =\n  \"afraid\": \"'Afraid to Ask'\",\n  \"blb\": \"Bad Luck Brian\",\n  \"buzz\": \"X, X Everywhere\",\n  ...\n[/code]\n\nWith the list of memes, we just needed a way to get this to the user. Using the template that all hubot-scripts use, we added:\n\n[code]\nmodule.exports = (robot) ->robot.respond /meme list/i, (msg) ->\n    for code, meme of memes\n      msg.send \"#{code} - #{meme}\"\n[/code]\n\nWhen the bot hears \"hubot meme list\", it iterates over each line of the memes and prints out the code (on the left) and the meme (on the right).\n\nAfter we got this working we shipped the feature. To actually get the input to generate a meme, I knew we were going to have to do something I try not to do too much with novice developers...regular expressions. There's a joke that goes, \"if you have a problem that you have to solve with regular expressions, you now have two problems.\" And the one we need to parse the text input is particularly obscure. So much so that I had to pair with another developer (thanks Eric) to get it correct. I won't go in to detail, but this is the line that we came up with to parse the pattern \"hubot meme me afraid \"top message\" \"bottom message\"'\n\n[code]\nrobot.respond /meme me (\\w+) (\\\"[^\"]+\\\") (\\\"[^\"]+\\\")/i, (msg) ->\n[/code]\n\nAs we were testing this out, former Scholars' Lab fellow [Alex Gil](https://twitter.com/elotroalex) came in to ask some questions about Neatline and generated this\n\nhttps://twitter.com/elotroalex/status/614147822116581380\n\nAfter we got everything working, I took the code @lilybeth and I wrote and abstracted it in a way that could be easily used by others, publishing it on [npm](https://www.npmjs.com/package/hubot-memes), opening the source code on [GitHub](https://github.com/waynegraham/hubot-memes), and writing automation routines in Travis CI to automagically update the npm site when I push a new release to Github (this last part [is insanely easy with Travis](https://github.com/waynegraham/hubot-memes/blob/master/.travis.yml#L6-L13)).\n\nThere are many things we can do to make this toy better (like better error handling text input), but for now this is an example of how a thing went from a \"I'd like to know how to do this\" to a published thing being used by more than just us (according to the download count). And, if you'd like to run your own hubot (or waynebot as we affectionately call it), I [shared the code](https://github.com/waynegraham/waynebot-openshift) to run this on OpenShift with the RedisCloud cartridge addon which is based on code by [Katie Miller](https://github.com/codemiller/hubot-openshift).\n"},{"id":"2015-07-06-physical-computing-at-dhsi-2015","title":"Physical Computing at DHSI 2015","author":"ethan-reed","date":"2015-07-06 06:00:38 -0400","categories":["Digital Humanities","Experimental Humanities"],"url":"physical-computing-at-dhsi-2015","content":"In the beginning of June I had the pleasure of attending the [Digital Humanities Summer Institute](http://dhsi.org/) at the University of Victoria for the second year running. My experience this year was so good that I wanted to write a quick post sharing some of the highlights – so if you’re thinking of going to DHSI, are a regular interested in a class they’ve never taken, or have never heard of it and want to learn more, I hope this post sheds some light on the kinds of things people get up to at one of my favorite scholarly events of the year (and why I want to go again next summer)!\n\n\n\n\nI spent my week in Victoria in a class called [“Physical Computing and Desktop Fabrication,”](http://dhsi.org/courses.php) taught by the folks at [UVic’s Maker Lab](http://maker.uvic.ca/): Nina Belojevic, Devon Elliot, Shaun Macpherson, and Jentery Sayers. If you’ve read any of my other posts or talked to me about the kind of stuff I do in the Makerspace here at UVA,  I’ve taken a large number of cues from these folks in the past year as I’ve been learning more about critical making and physical computing. \n\n\n\n\nMeeting every day for five days in a row, the class covered a huge range of things that fall generally under the heading “physical computing,” which I think of broadly as just the interactions between computers and the physical world.  We worked with [Arduinos](https://www.arduino.cc/), wearables, a 3D printer ([Replicator 2](https://store.makerbot.com/replicator2.html)), a laser cutter ([Epilog](https://www.epiloglaser.com/)), [SketchUp](http://www.sketchup.com/), [photogrammetry](http://www.agisoft.com/), and more. If you are interested in exploring the full inventory of what’s available at the Maker Lab, you can [find that inventory here as a post](http://maker.uvic.ca/inventory/), or [here as a spreadsheet](https://docs.google.com/spreadsheets/d/1Hran2uky2vnXbjTfQ0RPsLhOmaVYKS2cY3X6Q9qMeSo/edit#gid=220944623). These technologies are capable of pretty amazing stuff - examples shared with us ranged from [interactive paintings](http://technolojie.com/pu-gong-ying-tu-dandelion-painting/), [wooden mirrors](https://www.youtube.com/watch?v=BZysu9QcceM), and [plants that tell you when they are thirsty](http://www.botanicalls.com/), to [technologies at CanAssist at UVic](http://www.canassist.ca/EN/main/programs/video-gallery.html) impacting the lives of those with disabilities.  If you want to check out the whole week’s schedule for a more detailed account of what went on (along with more of the examples shared by our instructors) check out the index at [the class’s GitHub page](https://github.com/uvicmakerlab/dhsi2015/blob/master/index.md).\n\n\n\n\nAlong with trying out hardware, software, and discussing the larger implications of this kind of work, we also had a specific project for the week: to make a box. More specifically, our mission (quoted here from [our class GitHub index](https://github.com/uvicmakerlab/dhsi2015/blob/master/index.md#theme-prototyping-a-box-material--metaphor)) was to “prototype your own box and explain how it operates both as a material and metaphor.” A deceptively simple task! But one that ended up producing awesome results - if you poke around on [Twitter and #physcomp](https://twitter.com/hashtag/physcomp) or [some pictures Jentery posted from the class](http://maker.uvic.ca/dhsi2015/), you can see some of the amazing things others made over the course of the week.\n\n\n\n\nSo what did I make? I called it… the gloom box.\n\n\n\n\n[![IMG_1878](http://scholarslab.org/wp-content/uploads/2015/07/IMG_1878-300x225.jpg)](http://scholarslab.org/wp-content/uploads/2015/07/IMG_1878-e1436181481295.jpg)\n\n\n\n\nSo what is the box and what does it do? Like our prompt for the week, the gloom box is simple on the outside: it is a small box with a sad face looking helplessly at a button and an LCD screen. The LCD screen displays the words “BATTERY LIFE” with a large number underneath. When you push the button, the number under “BATTERY LIFE” decreases, and when it hits zero the whole thing turns off - that’s it. The gloom box has no other functions.\n\n\n\n\nBefore getting into why I made this thing (box as metaphor), a little more info on the thing itself (box as material).\n\n\n\n\n[![IMG_1864](http://scholarslab.org/wp-content/uploads/2015/07/IMG_1864-e1436181168548-225x300.jpg)](http://scholarslab.org/wp-content/uploads/2015/07/IMG_1864-e1436181449114.jpg)\n\n\n\n\nThe box was cut using an [Epilog laser cutter](https://www.epiloglaser.com/) from a large sheet of baltic birch. I made a simple frame design for the six sides of the box using [BoxMaker](http://boxmaker.connectionlab.org/), then Nina and Shaun worked some magic to make it the correct dimensions for what I needed to put inside. The image on the top was something I made in a few seconds in Paint, then converted into a raster file that the laser cutter could read and engrave into the box.\n\n\n\n\nLike the outside of the box, the inside is also relatively simple:\n\n\n\n\n[![IMG_1857](http://scholarslab.org/wp-content/uploads/2015/07/IMG_1857-e1436181405354-225x300.jpg)](http://scholarslab.org/wp-content/uploads/2015/07/IMG_1857-e1436181469118.jpg)\n\n\n\n\nAn [Arduino Uno](https://www.arduino.cc/en/Main/arduinoBoardUno) hooked up behind the scenes via a breadboard and a bunch of jumper cables to the button, battery, breadboard, and LCD screen seen on the outside. To get the button to work, I ended up writing about thirty lines of code after borrowing from some examples in the Arduino library and other people working out similar problems online (and plenty of help from Devon!).  Unfortunately, the number underneath “BATTERY LIFE” didn’t actually correspond to the 9-volt’s battery - the code for that turned out to be too complicated. So I had it “simulate” the loss instead, setting “BATTERY LIFE” to a very high number and decreasing it by a random value from 1-9 every time the button was pushed. In what was described as a “delicious” twist of irony, when walking the gloom box over to share our work with other classes on Friday, the battery actually died before anyone had a chance to play with it. The line between “simulating” a battery’s energy expenditures and actually expending said energy can be very thin, apparently.\n\n\n\n\nThat covers the box as material, but what about the box as metaphor? Why make a box whose only purpose is to inform you of how your interaction with it has depleted its battery life?\n\n\n\n\nMy original goal had been to make a box that was “aware” at some level of some of the material costs of its own production, operation, or maintenance, ideally one that would highlight these costs in a way unique to the kinds of things physical computing lets you do. Two recent pieces of writing had me thinking this way: the first is an amazing post of Bethany’s from about a year ago titled [“Digital Humanities in the Anthropocene”](http://nowviskie.org/2014/anthropocene/); the second was an article given as background reading for my DHSI class that I can’t find a link to online as it is still forthcoming, but can give the info for here: it’s called “Between Bits and Atoms: Physical Computing and Desktop Fabrication in the Humanities” and is written by Devon, Bethany, Jentery, and Kari Kraus and William J. Turkel in _The New Blackwell Companion to Digital Humanities. _These two articles, coupled with some other reading going on around the same time (including Timothy Morton’s [_Hyperobjects_](https://www.upress.umn.edu/book-division/books/hyperobjects)_ _and N. Katherine Hayles’s [_How We Became Posthuman_](http://press.uchicago.edu/ucp/books/book/chicago/H/bo3769963.html)), made the problem feel like an interesting one worth investigating.\n\n\n\n\nThis small project turned out to be a great opportunity to explore certain problems I’ve been thinking about more generally. For example, the energy costs of “invisible” transactions that we make every day but don’t really see, like information moving from place to place, or from digital to analog - exchanges that feel like they aren’t physically embodied anywhere but actually are (a problem Hayles thinks through wonderfully in _How We Became Posthuman_). Here I was thinking of the kinds of micro-expenditures that happen when we send a text, take a picture, or run some code. A box that made these “invisible” or “blackboxed” expenditures big, open, and obvious - even with something as basic as pushing a button - seemed like an interesting idea. Given the time frame I had to work on it, I ended up with something simple, a little silly, but also frustrating in potentially productive ways. A small gloomy energy puzzle, seemingly aware of its own helplessness in the face of its dilemma. \n\n\n\n\nIt is reminiscent in simplicity and spirit of another example shared with us in class, [the most useless machine](http://makezine.com/projects/the-most-useless-machine/). But perhaps more philosophically, it taps into questions of inevitability and the environment Roy Scranton asked in his 2013 NYT article, [“Learning How to Die in the Anthropocene”](http://opinionator.blogs.nytimes.com/2013/11/10/learning-how-to-die-in-the-anthropocene/?_r=0), where he takes a question like “How do we make meaningful choices in the shadow of our inevitable end?” and asks it on the scale of civilization itself.\n\n\n\n\nSo: is it a box-as-metaphor for planet Earth (no one person pushes the button enough for it to zero out, but enough people over enough time can make it happen – energy use understood collectively)? For the problem of energy in a more-or-less closed system (the only way to keep the box going is to introduce a new battery)?  For existential gloom in general, applied not just to humans and societies but objects as well? I only had so much time to think the thing through, but thankfully I have the schematics and code and can remake it in our Makerspace if I want to keep tinkering around with it and the ideas that it – or projects like it – might represent.\n\n\n\n\nI hope this gives a window into some of the kinds of things people get into at DHSI. And I want to thank again all the folks at DHSI for making this amazing week possible, as well as my instructors and the other wonderful people in our class who made it such a welcoming and enlightening experience. Thanks for making a great space to tinker around in, and hopefully I will see you next year!\n"},{"id":"2015-08-19-uva-library-fall-2015-gis-workshops","title":"UVa Library Fall 2015 GIS Workshops","author":"chris-gist","date":"2015-08-19 09:28:32 -0400","categories":["Announcements","Events","Geospatial and Temporal"],"url":"uva-library-fall-2015-gis-workshops","content":"All sessions are one hour and assume attendees have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials with expert assistance.  All sessions will be taught on Thursdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421 (adjacent to the Scholars’ Lab).  Sessions are free to attend and are open to the UVa and larger Charlottesville community.\n\nSeptember 3rd\n**Making your First Map**\n\nGetting started with new software can be intimidating.  This workshop introduces the skills you need to work with spatial goodness.  Along the way you’ll get a taste of Earth’s most popular geographic software and a gentle introduction to map making.  You’ll leave with your own cartographic masterpiece and tips for learning more in your pursuit of mappiness at UVa.\n\nSeptember 10th\n**Getting Your Data on a Map**\n\nDo you have GPS points or a list of latitude and longitude you would like to show as points on a map?  This session will show you how to turn your data into map layers and how to connect them to make lines and polygons as well.\n\nSeptember 17th\n**Points on Your Map: Street Addresses and More Spatial Things**\n\nDo you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis.\n\nSeptember 24th\n**Georeferencing – Putting Old maps and Aerial Photos on Your Map**\n\nHave an old map or an aerial photograph that you would like to use as a spatial layer?  This session will teach you techniques to properly place your data and make it usable in GIS software.  We will also demo similar techniques for Google Earth.\n\nOctober 1st\n**Taking Control of Your Spatial Data:  Editing in ArcGIS**\n\nUntil we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on.\n\nOctober 8th\n**Collecting Your Own Spatial Data**\n\nResearch projects often rely on fieldwork to build new datasets.  In this workshop we’ll focus on tools for spatial data collection. First we’ll take a quick look behind the curtain to see how GPS really works and how to use that knowledge to our advantage.  Then we’ll evaluate free or low-cost options to gather locations and associated attributes using handheld GPS devices, smartphones, and apps.  This workshop will introduce you to a range of devices and methods for mobile spatial data collection.\n"},{"id":"2015-09-08-augmented-reality-and-simulation","title":"Augmented Reality and Simulation","author":"wayne-graham","date":"2015-09-08 07:20:15 -0400","categories":["Research and Development"],"url":"augmented-reality-and-simulation","content":"A few weeks ago the Scholars' Lab went on a field trip to the School of Architecture's \"[FabLab](http://www.arch.virginia.edu/fabrication)\" to check out a project Chris Gist and Melissa Goldman had been working on, a sand table that has a projector and a Kinect connected to a computer that projects a topology on to the sand.\n\n[![IMG_0383](http://scholarslab.org/wp-content/uploads/2015/08/IMG_03831-1024x768.jpg)](http://scholarslab.org/wp-content/uploads/2015/08/IMG_03831.jpg)[\n](http://scholarslab.org/wp-content/uploads/2015/08/IMG_0383.jpg)\n\nThis is an amazing tool that was developed by [a lot of different organizations](http://idav.ucdavis.edu/~okreylos/ResDev/SARndbox/), primarily to teach earth science concepts related to watershed science. However, it's also an amazing tool for teaching landscape and topography skills to architects (not to mention seriously fun to play with -- especially when you start to think about the possibilities of replacing the sand with [kinetic sand](https://www.youtube.com/watch?v=9Uc9uglgVAI)).\n\nWhile we were there, several people mentioned how cool it would be to capture and 3D print these landscapes created in the classroom. I've seen dozens of programs to build models from the structured light sensors in the Kinect ([Skanect](http://skanect.occipital.com/) is a good example), so how hard could this be? In an R&D shop, these can be famous last words...\n\nI started brushing off my really rusty OpenGL (this system is written for Linux) and started poking around with the [Kinect library](http://idav.ucdavis.edu/~okreylos/ResDev/Kinect/) that the project uses. Then I remembered how much \"fun\" OpenGL programming can be (coupled with rusty C++ skills) as I crashed the graphics drivers pretty spectacularly more than once (you can always tell because the screen goes blank and you can't see anything anymore).\n\nAfter checking on the forums, the maintainer clued me in on a feature in the Kinect utilities that enables this feature, but needed a bit of a patch to work. From the `KinectViewer` utility you can record the output from the camera, and there is another utility that is not built by default that allows you to convert frames of this stream in to the Lightwave Object format (you may remember that from the Commodore Amiga days, if you're that old).\n\nRight now (this will be folded in to the next release of the software), you need to clone my fork of the code ([https://github.com/waynegraham/Kinect](https://github.com/waynegraham/Kinect)) and to run a special `make` task:\n\n[code]\n\n$ cd path/to/clone\n$ make PACKAGES=MYKINECT LWOWriter\n\n[/code]\n\nThe basic workflow was to use `SARndbox` to project the topology and then switch over to the `KinectViewer` utility to create the output. I was hoping this could be done simultaneously, but unless you have two Kinects, you can only use one at a time. When we record the stream, you get something that looks like this:\n\n\n\nThen, using the `LWOWriter` to pull a single frame, you get single Lightwave Object file (which can be read in most 3D packages). After a bit of clean up (I used [Blender](https://www.blender.org/) and [Meshlab](http://meshlab.sourceforge.net/)), you get something that looks a bit more like the sand in the table.\n\n\n\n\n[Sarndbox Test Output](https://sketchfab.com/models/e4126b035cf64fb8adb8cc2850bb509f?utm_source=oembed&utm_medium=embed&utm_campaign=e4126b035cf64fb8adb8cc2850bb509f)\nby [waynegraham](https://sketchfab.com/waynegraham?utm_source=oembed&utm_medium=embed&utm_campaign=e4126b035cf64fb8adb8cc2850bb509f)\non [Sketchfab](https://sketchfab.com?utm_source=oembed&utm_medium=embed&utm_campaign=e4126b035cf64fb8adb8cc2850bb509f)\n\n\nI also wrote a short program to help process the movies in to Lightwave (there are more instructions on this gist as well, so check it out):\n\n[gist id=0e36c812ca64291797c1 file=converter.rb]\n"},{"id":"2015-09-10-virginia-woolf-natural-language-processing-and-the-quotation-mark","title":"Virginia Woolf, Natural Language Processing, and the Quotation Mark","author":"brandon-walsh","date":"2015-09-10 07:19:52 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"virginia-woolf-natural-language-processing-and-the-quotation-mark","content":"_[Cross-posted on [my personal blog](http://bmw9t.github.io/blog/2015/09/10/woolf-and-the-quotation-mark/)]_\n\nFor my fellowship in the Scholars' Lab this year I'll be working with [Eric](http://scholarslab.org/people/eric-rochester/) to expand a project we began last year on Virginia Woolf and natural language processing. My dissertation focuses on sound recordings and modernism, and this year I will focus on how Woolf's quotation marks offer evidence of her engagement with sound as a textual device. In my reading, the quotation mark is the most obvious point at which sound meets text, the most heavily used sound recording technology in use by writers. Patterns in quotation mark usage across large corpora can tell us a lot about the role that sound plays in literature, but, as you might expect, there are _lots _of quotation marks - hundreds or thousands in any given text. Computational methods can help us make sense of the vast number and turn them into reasonable objects of study.\n\nYou can find more information in [this post](http://scholarslab.org/digital-humanities/hearing-silent-woolf/) about my thinking on quotation marks and some preliminary results from thinking about them in relation to Woolf. As I discuss there, finding quotation marks in a text is not especially challenging, but this year Eric and I will be focusing on a particular wrinkle in Woolf's use of the marks, best conveyed in _The Hours_, Michael Cunningham's late-century riff on Virginia Woolf. In _The Hours_, Cunningham offers a fictionalized version of Woolf meditating on her composition process:\n\n\n<blockquote>She passes a couple, a man and woman younger than herself, walking together, leisurely, bent towards each other in the soft lemon-colored glow of a streetlamp, talking (she hears the man, “told me _something something something_ in this establishment, _something something_, harrumph, indeed”) (166).</blockquote>\n\n\nThe repeated \"_somethings_\" of the passage suggest the character's imperfect experience of the conversation as well as the limits of her senses. As the moment is conveyed through the character's perspective, the conversation will always be incomplete. Recording technology was largely unreliable during the early days of the twentieth century, and, similarly, the sound record of this conversation as given by the text is already degraded before we hear it. Cunningham points to how the sounded voice is given character in the ears of the listener, and, in a print context, in the pen of the writer. A printed voice can speak in a variety of ways and in a variety of modes.\n\nCunningham's passage contains echoes of what will eventually be the famous first sentence of Woolf's _Mrs. Dalloway_: \"Mrs. Dalloway said she would buy the flowers herself.\" The text implies that Mrs. Dalloway speaks, but it does not mark it as such: the same conversational tone in Cunningham remains here, but the narrator does not differentiate sound event from narrative by using quotation marks. We see moments of indirect speech like this all the time, when discourse becomes submerged in the texture of the narrative, but it doesn’t disappear entirely. Speech implies a lot: social relations, the thoughts of a speaking body, among others. Things get muddy when the line between narrative voice and speech becomes unclear. If quotation marks imply a different level of speech than unquoted speech, might they also imply changes in the social relations they represent?\n\n_Mrs. Dalloway_ is filled with moments like these, and this year I'll be working to find ways to float them to the surface of the text. Examining these moments can tell us how conversation changes during the period, what people are talking about and for, how we conceive of the limits of print and sound, and about changing priorities in literary aesthetics. The goal this year is to train the computer to identify moments like this, moments that a human reader would be able to parse as spoken but that are not marked as such. Our first pass will be to work with the quoted material, which we can easily identify to build a series of trigger words that Woolf uses to flag speech as sound (said, asked, called, etc.). With this lexicon, we can then look for instances in her corpus where they pop up without punctuation. Teaching the computer to classify these passages correctly will be a big task, and this process alone will offer me lots of new material to work with as I untangle the relationship between modernist print and sound. In upcoming posts I'll talk more about the process of learning natural language processing and about some preliminary results and problems. Stay tuned!\n\nWorks Cited:\n\nCunningham, Michael. _The Hours_. New York: Picador USA : Distributed by Holtzbrinck Publishers, 2002. Print.\n\nWoolf, Virginia. _Mrs. Dalloway_. 1st Harvest/HBJ ed. San Diego: Harcourt Brace Jovanovich, 1990. Print.\n"},{"id":"2015-09-21-music-genre-and-spotify-metadata","title":"Music Genre and Spotify Metadata","author":"brandon-walsh","date":"2015-09-21 06:48:25 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"music-genre-and-spotify-metadata","content":"_Cross-posted on [my personal site](http://bmw9t.github.io/blog/2015/09/20/music-genre-and-spotify-metadata/)_\n\nFor the last couple weeks, I have been exploring APIs useful to sound studies for a sound recording and poetry project I am working on with former Scholars’ Lab fellow [Annie Swafford](https://annieswafford.wordpress.com/). I was especially drawn to playing around with [Spotify](https://www.spotify.com/us/), which has an [API](https://developer.spotify.com/web-api/) that allows you to access metadata for the large catalog of music available through their service. The experiment described below focuses on genre: a notoriously messy category that we nonetheless rely on to tell us how to process the materials we read, view, or hear. Genre tells us what to expect from the art we take in, and our construction and reception of generic categories can tell us a lot about ourselves. In music, especially, genres and subgenres can activate fierce debates about authenticity and belonging. Does your favorite group qualify as \"authentic\" jazz? What composers do you have to know in order to think of yourself as a real classical music aficionado? Playing with an artist's metadata can expose a lot of the assumptions that were made in its collection, and I was especially interested in the ways in which Spotify models relations among artists.\n\nI wanted to explore Spotify’s metadata in a way that would model the interpretive messiness of generic categories. To do so, I built a program that bounces through Spotify’s metadata to produce multiple readings of the idea of genre in relation to a particular artist. Spotify offers a fairly robust API, and there are a number of handy wrappers that make it easier to work with. I used a Python module called [Spotipy](http://spotipy.readthedocs.org/en/latest/) for the material below, and you can find [the code for my little genre experiment over on my GitHub page](https://github.com/bmw9t/spotify/blob/master/genre_machine.py). If you do try to run this on your own machine, note that you will need to clone Spotipy’s repository and manually install it from the terminal with the following command from within the downloaded repository:\n\n`$ python setup.py install`\n\nPip will install an older distribution of the code that will only run in Python 2, but Spotipy's GitHub page has a more recent release that is compatible with Python 3.\n\nWhen run, the program outputs what I like to think of as the equivalent of music nerds arguing over musical genres. You provide an artist name and a number, and the terminal will work through Spotify’s API to produce the specified number of individual \"mappings\" of that artist’s genre as well as an aggregate list of all their associated genres. The program starts by pulling out all the genre categories associated with the given artist as well as those given to artists that Spotify flags as related. Once finished, the program picks one of those related artists at random and continues to do the same until the process returns no new genre categories, building up a list of associated genres over time.\n\nSo, in short, you give the program an artist and it offers you a few attempts at describing that artist generically using Spotify's catalog, the computational equivalent of instigating an argument about genre in your local record store. Here are the results for running the program three times for the band New Order:\n\n\n\n* * *\n\n\n\n`Individual genre maps`\n\n`Just one nerd's opinions on New Order:`\n\n`['dance rock', 'new wave', 'permanent wave', 'new romantic', 'new wave pop', 'hi nrg', 'europop', 'power pop', 'album rock']`\n\n`Just one nerd's opinions on New Order:`\n\n`['dance rock', 'new wave', 'permanent wave', 'gothic metal', 'j-metal', 'visual kei', 'intelligent dance music', 'uk post-punk', 'metropopolis', 'ambient', 'big beat', 'electronic', 'illbient', 'piano rock', 'trance', 'progressive house', 'progressive trance', 'uplifting trance', 'quebecois', 'deep uplifting trance', 'garage rock', 'neo-psychedelic', 'space rock', 'japanese psychedelic']`\n\n`Just one nerd's opinions on New Order:`\n\n`['dance rock', 'new wave', 'permanent wave', 'uk post-punk', 'gothic rock', 'discofox', 'madchester', 'britpop', 'latin', 'latin pop', 'teen pop', 'classic colombian pop', 'rai', 'pop rap', 'southern hip hop', 'trap music', 'deep rai']`\n\n`Aggregate genre map for New Order:`\n\n`['dance rock', 'new wave', 'permanent wave', 'new romantic', 'new wave pop', 'hi nrg', 'europop', 'power pop', 'album rock', 'gothic metal', 'j-metal', 'visual kei', 'intelligent dance music', 'uk post-punk', 'metropopolis', 'ambient', 'big beat', 'electronic', 'illbient', 'piano rock', 'trance', 'progressive house', 'progressive trance', 'uplifting trance', 'quebecois', 'deep uplifting trance', 'garage rock', 'neo-psychedelic', 'space rock', 'japanese psychedelic', 'gothic rock', 'discofox', 'madchester', 'britpop', 'latin', 'latin pop', 'teen pop', 'classic colombian pop', 'rai', 'pop rap', 'southern hip hop', 'trap music', 'deep rai']`\n\n\n\n* * *\n\n\n\nIn each case, the genre maps all begin the same, with the categories directly assigned to the source artist. Because the process is slightly random, the program eventually maps the same artist’s genre differently each time. For each iteration, the program runs until twenty randomly selected related artists return no new genre categories, which I take to be a kind of threshold of completion for one understanding of an artist’s genre.\n\nThe results suggest an amalgam of generic influence, shared characteristics, common lineages, and overlapping angles of approach. The decisions I made in how the program interacts with Spotify’s metadata suggest a definition of genre like the one offered by Alastair Fowler: “Representatives of a genre may then be regarded as making up a family whose septs and individual members are related in various ways, without necessarily having any single feature shared in common by all” (41). Genre is fluid and a matter of interpretive opinion - it is not necessarily based on objective links. The program reflects this in its results: sometimes a particular generic mapping feels very coherent, while at other times the script finds its way to very bizarre tangents. The connections do exist in the metadata if you drill down deeply enough, and it is possible to reproduce the links that brought about such output. But the more leaps the program takes from the original artist the more tenuous the connections appear to be. As I wrote this sentence, the program suggested a connection between garage rock revivalists The Strokes and big band jazz music: such output looks less like a conversation among music nerds and more like the material for a Ph.D. dissertation. As the program illustrates, generic description is the beginning of interpretation - not the ending.\n\nOf course, the program does not actually search all music ever: it only has access to the metadata for artists listed in Spotify, and some artists like Prince or the Beatles are notoriously missing from the catalog. Major figures like these have artist pages that serve as stubs for content drawn largely from compilation CDs, and the program can successfully crawl through these results. But this wrinkle points to a larger fact: the results the program produces are as skewed as the collection of musicians in the service’s catalog. Many of the errors I had to troubleshoot were related to the uneven nature of the catalog: early versions of the script were thrown into disarray when Spotify listed no related artists for a musician. On occasion, the API suggested a related artist who did not actually have an artist page in the system (often the case with new or less-established musicians). I massaged these gaps to make this particular exercise work (you’ll now get a tongue in cheek “Musical dead end” or “Artist deleted from Spotify” output for them), but the silences in the archive offer significant reminders of the commercial politics that go into generic and archival formation, particularly when an archive is proprietary. I can imagine tweaking things slightly to create a script that produces _only_ those archival gaps, but that is work for another day. In the meantime, I'll be trying to figure out how [Kanye West might be considered Christmas music](https://en.wikipedia.org/wiki/Yeezus).\n\nWorks Cited:\n\nFowler, Alastair David Shaw. _Kinds of Literature: An Introduction to the Theory of Genres and Modes_. Repr. Oxford: Clarendon Press, 1997. Print.\n"},{"id":"2015-09-23-introducing-the-2015-2015-scholars-lab-fellows","title":"Introducing the 2015-2016 Scholars' Lab Fellows","author":"purdom-lindblad","date":"2015-09-23 10:05:45 -0400","categories":["Announcements","Grad Student Research"],"url":"introducing-the-2015-2015-scholars-lab-fellows","content":"The Lab has been a busy place lately with 8 new fellows from across the arts, humanities, and social sciences!  Our [Graduate Fellows in the Digital Humanities](http://scholarslab.org/graduate-fellowships/) and [Praxis Program Fellows](http://praxis.scholarslab.org/) join a distinguished community of [past fellows](http://scholarslab.org/people/).\n\n\n## Graduate Fellowship in the Digital Humanities\n\n\nThis year, we are delighted to work closely with Veronica Ikeshoji-Orlati and Brandon Walsh.\n\n[gallery columns=\"2\" ids=\"12229,10140\"]\n\n[Veronica](http://scholarslab.org/people/veronica-ikeshoji-orlati/) is in her 6th year of the Classical Art & Archaeology program in the McIntire Department of Art. Her dissertation project, entitled “Music, Performance, and Identity in 4th century BCE South Italian Vase-Painting,” explores connections between the iconography of musical performance and constructions of identity in Apulia. During her fellowship year, she will be working with Wayne Graham to optimize her dissertation database and develop visualizations of her research, as well as experiment with integrating digital tools into the practice of iconographical analysis.\n\n[Brandon's](http://scholarslab.org/people/brandon-walsh/) research focuses on modern and contemporary literature and digital humanities. His dissertation entitled \"AudioTextual: Modernism, Sound Recording, and Networks of Reception\" examines how authors, amateur readers, and sound artists have used sound recording to invent and reinvent modernist literature throughout the last century. During his fellowship year, he will be working with Eric Rochester to apply machine learning and natural language processing techniques to speech in Virginia Woolf's novels.\n\n\n## Praxis Fellows\n\n\n[gallery ids=\"12233,12236,10883,12234,12231,12238\"]\n\nWe are entering our 5th (!) year of the Praxis Program and are welcoming six talented graduate fellows:\n\n\n\n \t\n  * James Ascher (English)\n\n \t\n  * Bremen Donovan (Anthropology)\n\n \t\n  * Ethan Reed (English)\n\n \t\n  * Gillet Rosenblith (History)\n\n \t\n  * Rachel Trapp (Music, Composition)\n\n \t\n  * Lydia Warren (Music, Critical and Comparative Studies)\n\n\nThis year’s cohort will be exploring various ways to represent time, both physically and digitally. We have [guiding questions](http://praxis.scholarslab.org/charter/charter-2015-2016/), but will be scoping the project as we go. Keep track of our experiments on the [Time page](http://praxis.scholarslab.org/time/) of the Praxis Program and our on [blog](http://scholarslab.org/archives/).\n\nPast Praxis teams developed [Prism](http://prism.scholarslab.org/), a web application for crowd-based interpretations of texts, and re-imagined [Ivanhoe](http://ivanhoe.scholarslab.org/), a platform for playfully (and collaboratively) interpreting texts and artifacts.\n"},{"id":"2015-09-23-time-and-praxis-2015-2016","title":"Time and Praxis: 2015-2016","author":"ethan-reed","date":"2015-09-23 10:46:14 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"time-and-praxis-2015-2016","content":"Time is a massive concept. If you were asked to think about it – how it works, feels, changes, what it looks like, how people go about talking about it, or representing it – where would you start?\n\nAs a person interested and invested in critical theory, my initial reflex would be to go to philosophers, phenomenologists – writers like [Martin Heidegger](http://plato.stanford.edu/entries/heidegger/#TemTem), [Maurice Merleau-Ponty](http://plato.stanford.edu/entries/merleau-ponty/), and so on. Or check in with narrative theorists, think again about what [Gérard Genette](https://en.wikipedia.org/wiki/G%C3%A9rard_Genette#Order) says about time and narrative in _Narrative Discourse_.\n\nMy second reflex would be to look at art and cultural objects. I've been trying to watch and/or reconsider as many time-travel movies as I had time for, everything from _[Primer](https://www.youtube.com/watch?v=4CC60HJvZRE)_ or _[Donnie Darko](https://www.youtube.com/watch?v=8wqVHjK2bQs) _to something more straightforward like _[The Terminator](https://www.youtube.com/watch?v=c4Jo8QoOTQ4)_. Sometimes you find something unexpected – for example, trying to [untangle](https://xkcd.com/657/) _Primer _after watching it last week, Gillet (a fellow Praxis member) commented that manipulating time in films can have a certain scare-factor, almost like it’s a specific branch of horror movies. I’d never thought about it that way before. Time, and the manipulation of time, has the power to frighten us.\n\nAlso, games: like [Braid](http://braid-game.com/), where manipulating time is the only way to solve puzzles and progress through the narrative. Or (at Jeremy’s suggestion) checking out Ian Bogost’s [A Slow Year](http://bogost.com/games/aslowyear/).\n\nAnd of course, literature. I thought first to a classics like H.G. Wells’ 1895 _[The Time Machine](https://en.wikipedia.org/wiki/The_Time_Machine) _or Ray Bradbury’s 1952 short story [“A Sound of Thunder,”](https://en.wikipedia.org/wiki/A_Sound_of_Thunder) where tourist big-game hunters go back in time to kill dinosaurs, but (spoilers) someone accidentally steps on a butterfly and changes the whole course of history. Then to Kurt Vonnegut’s 1969 [_Slaughterhouse Five _](https://en.wikipedia.org/wiki/Slaughterhouse-Five)(probably because it’s the first book I ever wrote a book report on), where a character becomes “unstuck in time” as he dips in and out of his life experiences. Or even to something like Ruth Ozeki’s 2013 _[A Tale for the Time Being](http://www.ruthozeki.com/writing-film/a-tale-for-the-time-being/) _(which I’m reading for my oral exams)_, _where relationships with past and present “time beings” might, in some ways, influence one another, even when those cross-temporal relationships are mediated textually.\n\nThere are so many movies, games, novels, short stories, poems, and plays I could go to here it’s nuts – not even looking at works with entangled narrative structures like the film [_Memento_ ](http://www.imdb.com/title/tt0209144/)or Faulkner's _[Absalom, Absalom!](https://en.wikipedia.org/wiki/Absalom,_Absalom!) _(for example). These above are just the first that come to mind, each of which interacts with time and experiences time a little differently.\n\nThis is my first time writing here as a Praxis Fellow for the 2015-2016 year. Along with [producing a charter](http://praxis.scholarslab.org/charter/charter-2015-2016/), this year we’re all trying to think about _time_ in as many different ways as possible, staying wide open with it and willing to explore any new possibilities, each coming at it from our own unique angle. Throughout the year, in our work and in posts like this, I'll try to keep track of and share what I feel like I am able to bring to our work from my own unique perspective (apparently even if sometimes that means just talking about a bunch of time-travel stories I'm excited about). We’re only a few weeks in, and I can already say how excited I am to be working with the Praxis team and all the folks at the Scholars’ Lab this coming year. I can’t wait to see what we come up with!\n"},{"id":"2015-09-30-preserving-reconstructing-teaching-in-3d","title":"Preserving, Reconstructing, & Teaching in 3D","author":"jennifer-grayburn","date":"2015-09-30 11:52:49 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"preserving-reconstructing-teaching-in-3d","content":"_[Cross-posted on my personal blog.](http://jennifernicolegrayburn.com/2015/10/13/preserving-reconstructing-and-teaching-in-3d/)_\n\nThe destruction of historic monuments has been a frequent topic in the news lately due to the Syrian and ISIS conflicts. The destruction of [historic mosques](http://www.theguardian.com/world/gallery/2013/apr/25/syria-umayyad-mosque-destroyed-pictures) and, most recently, the [Temple of Baal in Palmyra](http://www.nytimes.com/2015/09/01/world/middleeast/isis-militants-severely-damage-temple-of-baal-in-palmyra.html?_r=0) have sent shockwaves through the international community. The public outcry for cultural casualties has been so overwhelming that it has [prompted backlash and criticism ](http://www.bbc.com/news/blogs-trending-33111424)questioning the value of such monuments when confronted with overwhelming human suffering and death. This is not the place to debate the value of historical monuments and I cannot speak for anyone other than myself, but I anticipate that the focus of horror expressed by many institutions and academics is not the rejection of human suffering, but rather an acknowledgement of our own limitations to make a difference. Art and architecture play a key role in political legitimization and cultural identity, especially during wartime when these factors can bolster or inhibit support. The ability to preserve the history and cultural heritage of this region through our scholarship expresses a hope for a time when such artifacts (and the history they represent) can be studied and valued openly again. The variety of digital tools and methodologies embraced by the digital humanities has offered a small, but powerful way to preserve what we still can. [Project Mosul](http://projectmosul.org/), for example, is using crowd-sourced photographs to reconstruct monuments digitally. Harvard and Oxford have also [created their own \"monuments men\"](http://www.telegraph.co.uk/news/worldnews/islamic-state/11829761/Archaeologists-plan-to-use-3D-imagery-to-preserve-antiquities-under-threat-from-Islamic-State.html) to scan some of the most threatened monuments. Artist [Morehshin Allahyari](http://www.morehshin.com/2015/05/25/material-speculation-isis/) is also considering the relationship between preservation, destruction, and technology in her art as she reconstructs and 3D prints the artifacts lost to ISIS destruction.\n\nThis summer, my own research took me as far as possible from the war-torn Middle East to the quiet fields of Iceland. The [Monasticism in Iceland](https://www.facebook.com/klausturrannsokn) archaeology project provided me with an opportunity to work directly with artifacts and grapple with the complicated considerations of heritage preservation for the first time. Iconoclasm is not an issue in Iceland as it is in the Middle East; rather, site isolation and exposure to Iceland’s volatile climate create difficulties for sharing and preserving Iceland’s material past. A beautifully carved twelfth-century stone from Hítardalur represents these concerns in a particularly striking way. Likely a remnant of a failed medieval monastery, this unique mustachioed face lies in the field where it was discovered, open to the elements. When first told about this unique carving, I assumed it would be safely locked in a museum, climate controlled and secured. I did not expect to find this rare stone in a field adjacent to a private farmhouse, exposed to weather, theft, and accidents. Over the years, this open exposure has weathered the details of the face and two additional sculptures of similar composition were lost. I have never worked with artifacts outside of a museum setting and it was difficult for me to grasp that there are scores of objects that museums cannot accommodate, that the removal of artifacts—even for their protection and accessibility—can be interpreted as illegitimate or even criminal overreach. In fact, it raises multiple questions about artifact and heritage ownership that I cannot even begin to answer. As the cases of the [Parthenon/Elgin Marbles](http://www.telegraph.co.uk/news/uknews/11274713/Why-are-the-Elgin-marbles-so-controversial-and-everything-else-you-need-to-know.html) and [Kennewick Man](http://www.pcc.edu/staff/pdf/818/whatisthekennewickmancontroversyabout.pdf) demonstrate, accessibility, preservation, and ownership do not always coincide.\n\n[caption id=\"attachment_12167\" align=\"aligncenter\" width=\"500\"][![Hítardalur steinn - Copy](http://scholarslab.org/wp-content/uploads/2015/09/Hítardalur-steinn-Copy.jpg)](http://scholarslab.org/wp-content/uploads/2015/09/Hítardalur-steinn-Copy.jpg) Hítardalur sculpture, photograph taken in the mid-twentieth century.[/caption]\n\n[caption id=\"attachment_12166\" align=\"aligncenter\" width=\"500\"][![2015-05-13 09.52.51](http://scholarslab.org/wp-content/uploads/2015/09/2015-05-13-09.52.51.jpg)](http://scholarslab.org/wp-content/uploads/2015/09/2015-05-13-09.52.51.jpg) Hítardalur sculpture, photograph taken in June 2015. Note the deterioration of facial details.[/caption]\n\nI admit that I am still grappling with these issues, as my priorities of accessibility and preservation are clearly based on my own academic training and affiliation. This concern, however, prompted me to consider ways that I can participate in this dialogue in my limited capacity as a foreign scholar with limited resources. With the Middle Eastern examples and mentorship of my colleagues in the Scholars’ Lab (including the work and expertise of [Edward Triplett](http://www.edwardtriplett.com/)), I jumped into the digital modeling and photogrammetry methods that have been so successfully implemented by larger art and archaeology projects to see what I could do personally. The resulting model and 3D print preserves the current state of the medieval Icelandic sculpture, but highlights both the potentials and limitations of these technologies for preservation and pedagogy.\n\nArmed only with my camera, I started by taking a number of photographs of the Hítardalur sculpture at varying heights and distances. My goal was to capture the sculptural relief and texture of the stone in as much detail as possible. After looking into different software, I invested in [Agrisoft PhotoScan ](http://www.agisoft.com/)to compile a point cloud and build the mesh into a digital model. The software makes this easier than I anticipated and I was pleased with my early results. Because the sculpture was too heavy to lift myself, I was not able to photograph the base and, as a result, the digital model was open on the bottom. This is not a problem in itself, but the shell of this model would have been too fragile and had too many overhangs to 3D print properly. I exported the model to [netfabb](http://www.netfabb.com/) and [meshm](http://www.meshmixer.com/)[ixer](http://www.meshmixer.com/)—both available for free—to make the model watertight (closed off on all sides) and reorient it to sit flat on a printer platform.\n\n[caption id=\"attachment_12210\" align=\"aligncenter\" width=\"500\"][![Screen Shot 2015-05-16 at 1.33.20 PM](http://scholarslab.org/wp-content/uploads/2015/09/Screen-Shot-2015-05-16-at-1.33.20-PM.png)](http://scholarslab.org/wp-content/uploads/2015/09/Screen-Shot-2015-05-16-at-1.33.20-PM.png) Position of the camera for the photographs used to make the Hítardalur model.[/caption]\n\n[caption id=\"attachment_12211\" align=\"aligncenter\" width=\"500\"][![Screen Shot 2015-05-16 at 1.32.36 PM](http://scholarslab.org/wp-content/uploads/2015/09/Screen-Shot-2015-05-16-at-1.32.36-PM.png)](http://scholarslab.org/wp-content/uploads/2015/09/Screen-Shot-2015-05-16-at-1.32.36-PM.png) Finished Hítardalur model.[/caption]\n\n[caption id=\"attachment_12212\" align=\"aligncenter\" width=\"500\"][![Screen Shot 2015-05-16 at 1.37.58 PM](http://scholarslab.org/wp-content/uploads/2015/09/Screen-Shot-2015-05-16-at-1.37.58-PM.png)](http://scholarslab.org/wp-content/uploads/2015/09/Screen-Shot-2015-05-16-at-1.37.58-PM.png) Finished Hítardalur model with texture added.[/caption]\n\nPrinting the model had its own difficulties stemming from technical issues with the printers. After two failed prints on [MakerBot](http://www.makerbot.com/)’s Replicater 2 caused by a ‘glitch’ in the SD card, I reformatted the card and switched to the [Ultimaker](https://ultimaker.com/) 2. Using the Ultimaker software, [Cura](https://ultimaker.com/en/products/cura-software), I shrank the model and set the slicer settings to a lower quality to print a quick, rough prototype. With this successful print, I increased the size and print quality to produce an approximately four-inch model.\n\n[caption id=\"attachment_12220\" align=\"aligncenter\" width=\"500\"][![2015-09-13 11.57.55](http://scholarslab.org/wp-content/uploads/2015/09/2015-09-13-11.57.55.jpg)](http://scholarslab.org/wp-content/uploads/2015/09/2015-09-13-11.57.55.jpg) First failed print of the Hítardalur sculpture using PLA and the Replicator 2.[/caption]\n\n[caption id=\"attachment_12223\" align=\"aligncenter\" width=\"500\"][![2015-09-13 11.58.57](http://scholarslab.org/wp-content/uploads/2015/09/2015-09-13-11.58.57.jpg)](http://scholarslab.org/wp-content/uploads/2015/09/2015-09-13-11.58.57.jpg) Larger of the two succesfully printed Hítardalur prototypes. Printed using PLA and the Ultimaker 2.[/caption]\n\nWith a successful model and print, I am now left with the burning question: So what? It is true that I have preserved the sculpture in its current form in case it ever disappears or further weathers away. The digitization also offers a better way to share and teach the sculpture in a multi-dimensional way across vast distances and languages. But the model’s value and efficacy are ultimately limited by its online accessibility. Museums and institutions are increasingly compiling vast open-access databases of digital images and models of their own collections, but an isolated model like this is easy to miss. This sculpture, for example, only appears in Iceland’s main [archival database](http://sarpur.is/Adfang.aspx?AdfangID=678367) as an unnamed feature in a photograph of the farm. A model like this would likely need to be contextualized in larger project database, perhaps one dedicated to medieval, monastic, or sculptural Icelandic works, to increase accessibility and public interest.\n\nThe limitations of the 3D print are perhaps more obvious than the limitations of the 3D model. While the model has texture and can be shared online, the print varies in material, texture, color, weight, detail, and size from the original. Yet, the print is not necessarily meant to duplicate or replace the original sculpture. Its value lies in its ability to capture physical characteristics lost in digital form. Right now, the main way to teach artifacts is by digital photographs (or models when available). In some cases, such privileged photographs and models provide a chance to get a larger and more detailed view of an object than you can in real life (consider the zoom features in the [Google Cultural Institute](https://www.google.com/culturalinstitute/u/0/project/art-project) and [Artstor](http://www.artstor.org/)). Still, nothing replaces the opportunity to experience an object or monument in context and in person. While the 3D print (especially the small prototypes) cannot reconstruct this experience, there is potential for 3D printing (especially when an artifact is printed in its actual dimensions) to mimic some of the _physical_ features of the original that are lost in digital form. Students, for example, can interact with it as an object (rather than an isolated image) and analyze its forms in new and critical ways.\n\nThe opportunity to teach historical content while simultaneously training students to look, analyze, and think critically about the physical world around them through 3D prints is enticing. The next step in this project is to slice the digital model I have and print it as close to life size as possible. Due to the smaller size of the printers, this will require some experimentation with slicing the model, printing enlarged sections separately, and reassembling the parts. I have also requested an order for [sandstone filament](http://www.formfutura.com/285mm-sandstone-laybrick.html)—which will better mimic the texture, if not the color of the natural sculpture’s stone. If this medium does in fact enhance pedagogical opportunities for material-based studies, there are almost unlimited opportunities for multiple disciplines to replicate real-world conditions and design more authentic teaching opportunities.\n\n\n\n[caption id=\"attachment_12221\" align=\"aligncenter\" width=\"799\"]![_DSC0022](http://scholarslab.org/wp-content/uploads/2015/09/DSC0022.jpg) All attempted prototype prints of the Hítardalur model.[/caption]\n\n[\n](http://scholarslab.org/wp-content/uploads/2015/09/Screen-Shot-2015-05-16-at-1.32.36-PM.png)\n\n\n"},{"id":"2015-10-19-inktober-105-three-sketches","title":"Inktober 10/5: Three Sketches","author":"ethan-reed","date":"2015-10-19 07:03:49 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"inktober-105-three-sketches","content":"[Cross-posted from [my post on our Praxis page](http://praxis.scholarslab.org/blog/2015/10/05/inktober-10-5-three-sketches/).]\n\nWanted to put up a few time sketches for our own version of [Inktober](http://mrjakeparker.com/inktober). I'm aiming for every other day or so, so here's three.\n\nAll three have to do with time and consumption.[![10.5.movietime](http://scholarslab.org/wp-content/uploads/2015/10/10.5.movietime-e1445266696847.jpg)](http://scholarslab.org/wp-content/uploads/2015/10/10.5.movietime-e1445266696847.jpg)I love going to the movies, and usually opt for the biggest popcorn possible (bucket) but wonder how differently and quickly the 'time of eating' passes depending on how big your eating vessel is (I'm sure plenty of others have written about this - just Googling around quickly I found an entire website dedicated to [using smaller plates](http://www.smallplatemovement.org/)).\n\n[![10.5pintbythehour](http://scholarslab.org/wp-content/uploads/2015/10/10.5pintbythehour-e1445266344190.jpg)](http://scholarslab.org/wp-content/uploads/2015/10/10.5pintbythehour-e1445266344190.jpg)\n\nThe second one is another thing some people do to pass time together - having a drink at a bar or restaurant, which can go slow or fast. Here I tried to draw the rings that beer leaves behind as a marker of how much time passed where for the beer in this particular glass. Beer time! Reminds me of [Slughorn's Hourglass](http://harrypotter.wikia.com/wiki/Slughorn's_Hourglass) in Harry Potter, which goes faster or slower depending on the quality of conversation.\n\n[![10.5cigarettebreak](http://scholarslab.org/wp-content/uploads/2015/10/10.5cigarettebreak-e1445266362847.jpg)](http://scholarslab.org/wp-content/uploads/2015/10/10.5cigarettebreak-e1445266362847.jpg)\n\nLast is a cigarette - read a poem for orals by [Sherman Alexie ](http://www.poetryfoundation.org/bio/sherman-alexie)with James Dean in it (called \"Tourists\"), got me thinking of the images of Dean [with a cigarette in his red jacket](http://images.amcnetworks.com/blogs.amctv.com/wp-content/uploads/2009/10/2jacket.jpg) (no color in that photo, but pretty sure that's the jacket) and got thinking of how so many people frame \"breaks\" from their day, little slices of time, around something like cigarette \"breaks.\" As though they were carrying a pack of minutes in their pocket.\n"},{"id":"2015-10-21-inktober-1013-time-pieces-and-graphs","title":"Inktober 10/13: Time Pieces and Graphs","author":"ethan-reed","date":"2015-10-21 10:04:30 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"inktober-1013-time-pieces-and-graphs","content":"[Cross-posted from [my post on our Praxis page.](http://praxis.scholarslab.org/blog/2015/10/13/inktober-10-13-time-pieces-graphs/)]\n\nHello all - three more images for Inktober.\n\nThe first is a continuation from [my previous Inktober post](http://praxis.scholarslab.org/blog/2015/10/05/inktober-10-5-three-sketches/), very simply, two tubes of toothpaste, both almost empty.\n\n[![10.13toothpaste](http://scholarslab.org/wp-content/uploads/2015/10/10.13toothpaste-e1445266386143.jpg)](http://scholarslab.org/wp-content/uploads/2015/10/10.13toothpaste-e1445266386143.jpg)\n\nWas still thinking of the consumption of objects, for two reasons: first, because of their importance to a historical thinker deeply interested in time (and on my orals list), one Karl Marx, who writes on everything from [the length of the working day](https://www.marxists.org/archive/marx/works/1867-c1/ch10.htm) to [labour-time itself here in the first chapter of _Capital_](https://www.marxists.org/archive/marx/works/1867-c1/ch01.htm#S1). And second, because a lot of people use toothpaste in their everyday life. It is something they consume very slowly, and often even forget that they have or that it's running out; they measure out its consumption bit by bit over the course of many weeks, but its presence over time is 'at hand,' or taken for granted in a certain way.\n\nTo start with Marx though, in the first chapter of _Capital_ (titled: Commodities) he defines the value of a commodity as \"the labour time socially necessary for its production.\" He then goes on to quote himself from the [Economic & Philosophic Manuscripts of 1844](https://www.marxists.org/archive/marx/works/1844/manuscripts/preface.htm) saying that \"As exchange values, all commodities are merely definite quantities of _congealed labour-time_.\"\n\nWhat a phrase! _Congealed labour-time_. As though the object in hand were physically made up of hours from the lives of others. The idea that an hour of life itself can be estranged from an individual and sold for monetary compensation, and that these hours of others then congeal into an object which sits on my bathroom sink for a set number of weeks is fascinating, and which Marx thinks through seriously, though too much to talk about here.\n\nMy second point though, that people use toothpaste everyday, has to do with the thinking of another philosopher I've been reading while studying for my oral exams - [Martin Heidegger](http://plato.stanford.edu/entries/heidegger/). Though he's interested [in his big work](http://plato.stanford.edu/entries/heidegger/#BeiTim) with ontological questions (like, what kind of being does the number 1 have vs. the color brown, a chair, the solar system, a historical event, my cat, etc) he also believes that things 'are' for humans (or _Dasein_ in his terminology, the being for whom being is an issue or question) in a way that is more natural. In his thinking, when I see a desk, I don't go for some abstract, philosophical, Aristotelian route and think of it as a substance with attributes (such as squareness or brownness), I think more of whether or not it is the right size, or close enough to a reading light, or if it is an heirloom that needs to be treated carefully. That these considerations actually make up, in many ways, the 'being' of the table itself for Dasein (humans) was fascinating to me when I first started reading.\n\nHence: toothpaste! I brush my teeth twice a day and use toothpaste from a tube that looks like one of these. It is something that fits in my hand and I can tell how much time I have left with this particular tube just by picking it up. Using a brand new tube is very different from the ones I have drawn here, crumpling up tubes to try to make them last, to _extend their lives_. The \"time\" of a tube of toothpaste, its lifespan, is relatively slow (or long) compared to, say, a quart of milk or loaf of bread. The same tube persists through many weeks of use, and there is even a certain feeling of 'letting go' and beginning again when I have to stop crushing up an old familiar tube and go buy new. And trying a new brand, flavor, or type of toothpaste is a strangely serious time commitment, a decision that persists in your everyday experience (and teeth) for weeks or months.\n\nThis connects with another picture, those of time pieces.\n\n[![10.13laptopwatchphone](http://scholarslab.org/wp-content/uploads/2015/10/10.13laptopwatchphone-e1445266680371.jpg)](http://scholarslab.org/wp-content/uploads/2015/10/10.13laptopwatchphone-e1445266680371.jpg)\n\nI wanted to draw the things I look at during the day to tell what time it is. When this Dasein thinks of time (me), he probably thinks first of where he looks to 'tell the time,' the things I use to make sure that I am not late, or that I am getting enough sleep, or that I remember to eat lunch. For me these are my computer and my phone. My window into Greenwich Mean Time is almost always small numbers on a screen that sometimes fail to update to Daylight Savings. A lot of people have watches though, so I wanted to draw one of them too. I wonder how these objects of mediators of this standard time influence how time actually feels for the humans who use them.\n\nLast is more abstract. I was thinking of Johanna Drucker's work in _[Graphesis: Visual Forms of Knowledge Production](http://www.hup.harvard.edu/catalog.php?isbn=9780674724938)_, and particularly an article she wrote for [DHQ](http://www.digitalhumanities.org/dhq/) called [\"Humanities Approaches to Graphical Display\"](http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html) in which she gives some examples of how humanists can use graphical representations. So I decided to draw some graphs about how we might experience time in a 24-hour period.\n\n[![10.13graphs](http://scholarslab.org/wp-content/uploads/2015/10/10.13graphs-e1445266290311.jpg)](http://scholarslab.org/wp-content/uploads/2015/10/10.13graphs-e1445266290311.jpg)\n\nI was envisioning that a given person would travel along the path of the line, x-axis being each hour passing, and y-axis being... something else. Experience? Not sure - point being, although the same number of 'hours' passed for each line, the length, trajectory, and experience of each line (or sets of lines) was very unique. Would love to draw some more of these, or at least think through them a little more seriously before the month is out.\n"},{"id":"2015-10-28-inktober-1021-when-things-break","title":"Inktober 10/21: When Things Break","author":"ethan-reed","date":"2015-10-28 08:40:22 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"inktober-1021-when-things-break","content":"[Cross-posted from [my post on our Praxis page](http://praxis.scholarslab.org/blog/2015/10/21/inktober-10-21-when-things-break/).]\n\nThe third of my four installments (here's [one](http://scholarslab.org/digital-humanities/inktober-105-three-sketches/) and [two](http://scholarslab.org/digital-humanities/inktober-1013-time-pieces-and-graphs/) of Inktober). And oh boy, get ready for some strange-looking sketches in this one. I tried to use a new kind of pen that has two tips and can make way thicker marks, which has resulted in many many failed drawings and a few semi-successful ones. I present to you the semi-successful ones. (For those interested in drawing implements, I've been using a [0.35mm Pigma Micron](http://sakuraofamerica.com/pen-archival) but this time tried out a [Tombow Dual Brush Pen, N15 black](http://tombowusa.com/dual-brush-pen-602.html)).\n\nThe problems I had with my tools this week, believe it or not, fits thematically with what I've been trying to draw: when things break. Or when things (tools) don't work the way you want them to.\n\nWhat does this have to do with time? Let's look at the first picture I'm sharing, a drawing of one of my shoes.[![10.23samba](http://scholarslab.org/wp-content/uploads/2015/10/10.23samba-e1445898376493.jpg)](http://scholarslab.org/wp-content/uploads/2015/10/10.23samba-e1445898376493.jpg)You probably can't tell from what I drew here, but this shoe is thrashed on the inside. The back heel has completely torn through the cloth and this bluish plastic is exposed. There's also a split starting on the outer-toe area through the leather but I couldn't really draw it that well. On a side note, looking so closely at my shoe for so long reminded me of [this scene from Don DeLillo's _Underworld_](https://books.google.com/books?id=Ug3ArDMHLnQC&pg=PA540&lpg=PA540&dq=delillo+underworld+shoe+name+the+parts&source=bl&ots=fWnNKYyQUi&sig=KI5UxZA50WbPPs_DtATUDEeNygE&hl=en&sa=X&ved=0CCgQ6AEwAmoVChMI8N3n85_UyAIVRHY-Ch08OAK9#v=onepage&q=delillo%20underworld%20shoe%20name%20the%20parts&f=false) - from \"Sometimes I think...\" to the bottom of p. 543 - in which a boy is asked to name as many parts of a shoe as possible. I, in particular, had problems with [the vamp](http://www.shoeguide.org/shoe_anatomy/).\n\nPoint being, the shoe is beat, from toe cap to insole. It's spent. Both of my shoes are. I've worn these [sambas](http://www.adidas.com/us/samba-classic-shoes/034563.html) practically every day since I bought them and often play soccer in them. I will have to buy a new pair very soon. What's interesting is the way in which I learn that they need to be replaced: I need to buy a new pair because the shoe, as a _tool_, has \"stopped working.\" It's a trigger of some kind that produces an almost automatic response: thing you need breaks? Time to get a new one! Normally, I don't think about my shoes at all. They're comfortable and easy to walk in, so they're just kind of there - they're \"present at hand\" (or \"at foot\"! Ha!). I know they exist, but I'm no more aware of them than I am of my feet, legs, or hips while walking around. That is, until something goes wrong - the felt wears out and chafes my heel; I start to get blisters; the bottom cushioning wears through; the shoe splits at a side.\n\nFor Heidegger, when a piece of equipment breaks or malfunctions [it acquires a new kind of visibility](http://plato.stanford.edu/entries/heidegger/#ModEnc), as though it were semi-transparent before while being used. (If you're wondering what Heidegger is doing here or want more context, check out my [first](http://scholarslab.org/digital-humanities/inktober-105-three-sketches/) and [second](http://scholarslab.org/digital-humanities/inktober-1013-time-pieces-and-graphs/) posts). Most importantly, this new visibility changes how and what the thing _is_ - the implication being that situational contexts actually change _what things are_ for us (Dasein) in the world. So when my shoe 'breaks,' it changes how it _is_ in the everyday-ness of my world. It stops being a [_shoe_ shoe](http://www.polyvore.com/cgi/img-thing?.out=jpg&size=l&tid=71957156) and starts being a [_trash_ shoe](http://www.celia-pardini.fr/_/rsrc/1316020452111/my-destroyed-shoes/destroyShoes3.jpg). Its time is up. In his 1935 essay [\"The Origin of the Work of Art,\"](http://plato.stanford.edu/entries/heidegger-aesthetics/#HeiForArtIntThrPilHeiUndArt) Heidegger himself actually looks at [a famous pair of shoes](http://harpers.org/wp-content/uploads/van-gogh-a-pair-of-shoes.jpg) as an example of how an artistic representation of them can pull together and have the power to _disclose_ a kind of life-world despite simply being 'equipment.' And it seems to me that a totally trashed pair of shoes does this very differently than a pair fresh out of the box.\n\nSo I wonder: when a thing's lifespan as a thing is over (a lifespan determined in large part by its use context: i.e. a pair of boots that sits in a closet all year vs. pair used by a carpenter or roofer on worksites almost every day), what kind of lifespan does it acquire as something broken? How does the life of a _trash_ shoe different from a _shoe_ shoe, assuming that it's possible these two shoes might _be_ different things for me? An illustrative example would be something like comparing how we think of the 'lifespan' of a [car battery](http://www.carsofamericainc.com/wp-content/uploads/2013/06/iStock_000013970322XSmall.jpg) with the 'lifespan' of a [dead car battery](http://www.fullcarservice.org/images/category/carblog/carnews/bat_last/car-battery1.jpg): the first has to do with human use, the possibility of travel, cold winter mornings, brand reliability, maintenance, usually a matter of years; the latter with molecular half-lives, recycling practices, landfills, all on a much longer scale.\n\nAnother sketch with the new pen, this of an abandoned vehicle, engine- and battery-less. With some color! (If you want to see the original I was trying to draw, check it out [here](http://i.ytimg.com/vi/j2SGlkFfH3Y/maxresdefault.jpg)).[![10.23carintheweeds](http://scholarslab.org/wp-content/uploads/2015/10/10.23carintheweeds-e1445898394399.jpg)](http://scholarslab.org/wp-content/uploads/2015/10/10.23carintheweeds-e1445898394399.jpg)It's a kind of mythical image for me. In his song [\"Bad As Me\"](http://www.tomwaits.com/songs/song/368/Bad_As_Me/) Tom Waits calls it simply \"the car in the weeds.\"\n\nSo two main questions: on the one hand, how do we conceive of time passing for things that are 'invisible' and 'present-at-hand' - things that are there in the background almost all the time, but that we take for granted until lost or broken? And on the other, how do we conceive of time passing for things that are out of sight _and_ mind - [things that have been cast aside](http://www.theatlantic.com/business/archive/2012/06/26-trillion-pounds-of-garbage-where-does-the-worlds-trash-go/258234/), like this car is now, or my shoe soon will be?\n\nOne more: a pencil with a broken tip, and a pencil that has been sharpened way past its point of practical usefulness.[![10.23pencil](http://scholarslab.org/wp-content/uploads/2015/10/10.23pencil-e1445898361502.jpg)](http://scholarslab.org/wp-content/uploads/2015/10/10.23pencil-e1445898361502.jpg)Sometimes when things break, when their 'time is up,' they acquire a certain kind of urgency. They come to the fore. It's almost like they come back into existence from some kind of constantly humming background noise. All of the sudden you have to decide what to do with this thing - do you fix it? Replace it with something else? Replace it with a new version of itself? Leave it in the weeds ([or the center of the Pacific](https://en.wikipedia.org/wiki/Great_Pacific_garbage_patch))?\n\nThese questions may feel simple, or perhaps too straightforward, but they are the kinds of questions that matter for Daisen (us, humans, beings for whom being is a question) when it comes to things in our world. And because they matter to us in this way, these questions matter for how these objects _are _or _are not_ in our worlds - and by world I mean the highly contingent world with which a given person is familiar and in which they live their day-to-day lives, one that is changing all the time.\n\nReminds me of another scene from another 20th century novel, the \"Time Passes\" section of Virginia Woolf's _[To the Lighthouse](https://en.wikipedia.org/wiki/To_the_Lighthouse)_ (much of which you can read [here on Google Books](https://books.google.com/books?id=ng0Tg0FhRggC&q=time+passes#v=snippet&q=time%20passes&f=false)). Describing a home left more-or-less uninhabited for years, she writes:\n\n\"_So with the house empty and the doors locked and the mattresses rolled round, those stray airs, advance guards of great armies, blustered in, brushed bare boards, nibbled and fanned, met nothing in bedroom or drawing-room that wholly resisted them but only hangings that flapped, wood that creaked, the bare legs of tables, saucepans and china already furred, tarnished, cracked. What people had shed and left - a pair of shoes, a shooting cap, some faded skirts and coats in wardrobes - those alone kept the human shape and in the emptiness indicated how once they were filled and animated; how once hands were busy with hooks and buttons; how once the looking-glass had held a face; had held a world hollowed out in which a figure turned, a hand flashed, the door opened, in came children rushing and tumbling; and went out again. ... Loveliness and stillness clasped hands in the bedroom, and among the shrouded jugs and sheeted chairs even the prying of the wind, and the soft nose of the clammy sea airs, rubbing, snuffing, iterating, and reiterating their questions - \"Will you fade? Will you perish?\" - scarcely disturbed the peace, the indifference, the air of pure integrity, as if the question they asked scarcely needed that they should answer: we remain._\"\n\nI'd like to think more on just how my busted shoe, the car in the weeds, and the too-short pencil go about 'remaining' in the world, and how this might differ from their still-in-use counterparts.\n"},{"id":"2015-11-06-podcast-james-neal","title":"Podcast: James Neal","author":"ronda-grizzle","date":"2015-11-06 05:40:39 -0500","categories":["Podcasts"],"url":"podcast-james-neal","content":"**Practicing Digital Humanities Speaker Series: James Neal**\n**Public Libraries and Academic Libraries: Digital Partners?**\n\nJames Neal\nDigital Services Librarian\nPrince George's County Memorial Library System\n\n\nSummary:\nThe growth and development of technology, computers, software, and the Internet have changed the ways in which libraries function, operate, and are being used by the communities they serve. Public libraries have been playing a bit of catch-up in many ways related to the growth of digital services due to the fact that public libraries are also still serving users whose information needs include more traditional resources. Is there a common ground in mission and scope that public libraries and academic libraries serve together? In what ways are these services complementary and what are the ways in which each of these institutions can learn from and share with one another?\n\nSpeaker Bio:\nJames is a graduate of the MLS program at the University of Maryland College of Information Science, Maryland's iSchool in the Information and Diverse Populations concentration.\n\nHis experience at the University of Maryland acquiring the Master of Library Science degree was highlighted and dominated by his participation in the Information and Diverse Populations program. This allowed him to address the issue of underrepresented groups in librarianship and successful ways of working with colleagues and patrons from diverse backgrounds. “The Information and Diverse Populations (IDP) specialization of the College of Information Studies at the University of Maryland focuses on instruction about and research into the design, development, provision and integration of information services, resources, technologies, and outreach that serve diverse populations” (from UMD website).\n\nJames maintains a strong interest in the future of public libraries, and began working at the Prince George's County Memorial Library System in August 2014, moving to his current position of Digital Services Librarian in August 2015.\n\nJames is very active on social media. You can follow him on [Twitter](http://www.twitter.com/james3neal), [Instagram](https://instagram.com/james3neal/), [Tumblr](http://james3neal.tumblr.com/), [Google+](http://gplus.to/james3neal), and [Pinterest](http://www.pinterest.com/james3neal/pins/).\n\nThe Practicing Digital Humanities Speaker Series is sponsored by UVa Library's Intellectual Crossroads Team, Scholars' Lab, and the Academic Engagement group.\n\nAs always, you can listen to (or [subscribe to](http://www.scholarslab.org/category/podcasts/)) our podcasts on the Scholars' Lab blog, or [on iTunesU](http://itunes.apple.com/us/itunes-u/scholars-lab-speaker-series/id401906619).\n\n[podloveaudio src=\"https://itunesu.assets.itunes.com/apple-assets-us-std-000001/CobaltPublic69/v4/39/06/ed/3906ede0-f193-b362-e669-0d1ff87a1a18/313-2999196782334166045-neal.mp3\"]\n"},{"id":"2015-11-12-when-old-technology-meets-the-new-accessing-windows-95-cd-roms-through-wine","title":"When Old Technology Meets The New: Accessing Windows 95 CD-ROMs through Wine","author":"nora-benedict","date":"2015-11-12 08:46:10 -0500","categories":["Grad Student Research"],"url":"when-old-technology-meets-the-new-accessing-windows-95-cd-roms-through-wine","content":"Up to this point in my academic career I have worked primarily with physical books and I certainly feel most comfortable with this medium. Anything remotely technological frightens me and I’m particularly inept when it comes to simple computer issues (think: getting my computer to talk to my printer, resolving internet connectivity issues, etc.). All of this changed drastically a few weeks ago when I found myself desperately in need of an Argentine literary supplement from the 1930s for my dissertation, “The Fashioning of Jorge Luis Borges: Newspapers, Magazines, and Print Culture in Argentina (1930-1951),” which deals with the physical features of Jorge Luis Borges’s works, read through the lens of analytical bibliography and genetic criticism. I would normally InterLibrary Loan the materials, but runs of this particular periodical are very scarce (and, as a result, not circulating). That being said, while researching in Argentina this summer, I happened to purchase a book about the literary supplement in question, complete with a digital edition on CD-ROM. In my naïve state, I thought I could simply pop this disc into any PC and be able to access the files. Wrong. This was a CD-ROM for Windows 95/98 and not adequately suited for the current Windows 10!\n\nI soon realized that I would not be able to solve this problem on my own. After firing off a round of frantic emails to recommended experts in old machines at UVa, I wandered into the Scholars’ Lab with my CD-ROM in the hopes that these experts could work their magic and help me gain access to the digitized images contained on it. We first tried simply putting the CD-ROM into a computer with the hopes that the files could easily be extracted or copied onto my machine. No such luck. There was some sort of encoding that resulted in an error message (“access denied!”) every time we clicked on the strange “.jpe” files. Word quickly spread in the Scholars’ Lab that some crazy student was trying to use a CD-ROM for Windows 95 and I met with several experts that were all excited to see the CD-ROM and give a crack at accessing its materials. One of these kind souls, Wayne Graham, was particularly interested in doing everything possible to help me. Soon after handing over the CD-ROM to Wayne, he was able to pull up the program on a PC in the Scholars’ Lab, and I proceed to jumped for joy upon seeing the following screens:\n\n[![Screen Shot 2015-11-12 at 9.55.14 AM](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.55.14-AM-300x209.png)](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.55.14-AM.png)\n\n[![Screen Shot 2015-11-12 at 9.55.19 AM](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.55.19-AM-300x207.png)](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.55.19-AM.png)\n\n[![Screen Shot 2015-11-12 at 9.55.23 AM](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.55.23-AM-300x192.png)](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.55.23-AM.png)\n\nThinking we (and by “we” I mean Wayne) had found a way into the files, we click into the index (“INDICE”) and found a listing of all of the titles published in this literary journal from 1933-1934:\n\n[![Screen Shot 2015-11-12 at 9.55.29 AM](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.55.29-AM-300x188.png)](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.55.29-AM.png)\n\nYES. THIS MUST BE IT! Nope. Another dead end. When Wayne clicked on any of the above listed files, the same error message (“access denied!”) popped up. Back to the drawing board I went.\n\nTrying not to become terribly discouraged, I shared my technological problem with a dear friend, James Ascher (Praxis Fellow 2015-2016), who immediately wanted to take a look at the CD-ROM. We talked about what Wayne had tried and he thought it might be fun to try an emulator. When he asked me to open terminal and I looked blankly at him as if he were speaking Greek, he saw this as the perfect teaching moment to walk me through all of the (complex!) steps Wayne had taken to try and get to those pesky files. James, thus, explained the basics of the command processor, Bash; I must confess that I have never felt more out of my element. I learned about simple codes like “pwd,” “cd Documents,” “ls,” and “cd ~,” among others. James had me play around with these commands for a bit until I felt comfortable enough to move in and out of directories with ease:\n\n[![Screen Shot 2015-11-12 at 9.59.04 AM](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.59.04-AM-300x193.png)](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.59.04-AM.png)\n\nHe also explained how “mkdir” and “rmdir” can be useful tools, but only when used cautiously since one incorrect key stroke could remove an entire directory. While I was getting familiar with giving my computer commands, James started to run the CD-ROM on an emulator (a program that allows one computer system to behave like another) on his computer. No luck. That same error message kept popping up again! After working through several other ideas and reading through the meta-data on the files, James came to the conclusion that we should copy all of the files from the CD-ROM to the UVa BOX for storage (and then I wouldn’t have to keep bringing the disc to grounds everyday). In the process James tried the emulator a second time with the copied files on his computer and we discovered that the error message had something to do with the physical disc itself. SUCCESS! Instead of receiving an error message when he clicked on a random file, we were both presented with the following:\n\n[![Screen Shot 2015-11-12 at 9.55.46 AM](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.55.46-AM-237x300.png)](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.55.46-AM.png)\n\nThe next challenge would be installing and teaching me how to emulate the program files on my own computer (i.e. the REAL challenge). My first task was to “install Homebrew” by copying the following command into Bash:\n\n[![Screen Shot 2015-11-12 at 9.59.14 AM](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.59.14-AM-300x53.png)](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.59.14-AM.png)\n\nEasy enough! I was on my own for the next set of instructions (the real test!). After brew was installed, I typed gave my computer the following command to install Wine, the emulator I would use to run the program:\n\n[![Screen Shot 2015-11-12 at 9.59.21 AM](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.59.21-AM-300x58.png)](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.59.21-AM.png)\n\nAll seemed to be going according to plan and, after letting the program install completely (about an hour or so), I restarted terminal and was very excited to start my very own simulation. Unfortunately, when I typed the command to run the program files through wine, I got a NEW error message:\n\n[![Screen Shot 2015-11-12 at 9.59.28 AM](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.59.28-AM-300x84.png)](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.59.28-AM.png)\n\nNO SUCH FILE OR DIRECTORY?? I tried not to panic and, instead, went about searching various directories for the installed wine. With some guidance from James, I eventually found it in the cellar (quite an apt place for wine, right??). Having located the program, I now needed to download the text editor, Atom, to tell my computer where to find wine (and thus run the emulator):\n\n[![Screen Shot 2015-11-12 at 9.59.36 AM](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.59.36-AM-300x27.png)](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.59.36-AM.png)\n\nAfter completing this intermediary step (there were a few other minor hiccups along the way), I was ready to roll! With a few simple keystrokes I now had access to the entire periodical run (1933-1934) from the convenience of my very own computer. Even though the images were digitized in 1999, I was extremely impressed by their quality and the ability to zoom in and out on each and every page. Eager to share my success with Wayne (and, of course, thank him immensely for his help and patience), I dropped by the Scholars’ Lab and showed how I could now access all of the material from my very own computer. While I was typing my commands into Bash, Wayne decided to help me write myself a shortcut to make the process even quicker:\n\n[![Screen Shot 2015-11-12 at 9.59.43 AM](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.59.43-AM-300x20.png)](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.59.43-AM.png)\n\nNow all I have to do is enter my terminal and type “rms” and the entire program launches. Mission accomplished!:\n\n[![Screen Shot 2015-11-12 at 9.55.58 AM](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.55.58-AM-300x188.png)](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.55.58-AM.png)\n\nI don’t think I can thank the Scholars’ Lab enough for all of their continued help and encouragement along the way. If it weren’t for them, I wouldn’t have the ability to access these files from the convenience of my own computer and describe the fascinating changes that Borges’s early fictions take from their first publications in this periodical (1933-1934) to later inclusion in book form (1935).\n\n[![Screen Shot 2015-11-12 at 9.56.08 AM](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.56.08-AM-300x142.png)](http://scholarslab.org/wp-content/uploads/2015/11/Screen-Shot-2015-11-12-at-9.56.08-AM.png)\n"},{"id":"2015-11-17-3d-printing-historical-objects-enhancing-the-qualities-inherent-to-the-past","title":"3D Printing Historical Artifacts: Enhancing the Qualities Inherent to the Past","author":"margaret-furr","date":"2015-11-17 10:28:50 -0500","categories":["Digital Humanities","Makerspace"],"url":"3d-printing-historical-objects-enhancing-the-qualities-inherent-to-the-past","content":"** **Earlier this fall semester, I ventured to test out the [Makerspace’s](http://scholarslab.org/makerspace/) 3D printer by reproducing a 3D version of [Kepler’s platonic solid model](http://www.georgehart.com/virtual-polyhedra/figs/kepler-spheres-1.jpg). This model was a historical object that I desired to examine in physical form while taking a class on the Scientific Revolution. I desired to study the artifact in such a way so that I would be able to hold the object in my hands and examine it from any perspective, rather than observe only a part of the object in an image, from one perspective. While I had this desire, I didn’t think that I would ever gain the opportunity to study the model in physical form. Everything changed when I became acquainted with both the [online 3D printing community](http://www.thingiverse.com/) and the Makerspace’s 3D printer. The online community enabled me to download the [model’s 3D version](http://thingiverse-production-new.s3.amazonaws.com/renders/bc/08/31/f5/c9/KeplerInnerPlanets_preview_featured.jpg) that a maker had already digitized, and the printer liberated me to output a 3D physical artifact from the 3D digital model. As I downloaded and printed Kepler’s platonic solid model, I began to question whether the replicated versions of objects preserved any of the qualities inherent to the original model.\n\n\n\nMy main question, as I downloaded and printed Kepler’s platonic solid model, was concerning the extent to which the printed model was genuine, and authentic, in comparison to the original image. I questioned whether the 3D printed versions preserved the historical work’s inherent quality or rather carried different qualities. I asked this question as (1) I contemplated how Kepler, in the time period he lived in, would never have been able to use digital technologies to design such a model in order to understand the universe’s nature, and (2) I worked to use the right types of supports and overhangs to print the model effectively. Encountering other applications of 3D printing technologies since asking this question, however, I have come to realize that rather than minimizing the extent to which a historical work’s quality is preserved, 3D printing expands the qualities inherent to the past, opening up the past to more people in new ways.\n\n\n\nThe [application of 3D printing technology](https://www.kickstarter.com/projects/3dphotoworks/bringing-the-worlds-greatest-art-to-blind-people) that I recently encountered was an example in which 3D technologies were used to help blind people see, or experience art through tactile means. After feeling inspired by this application of 3D printing technology, I challenged myself to extend my thoughts beyond how much a 3D print preserves a historical piece of work, like Kepler’s platonic solid model, which he drew out as an image, and rather consider the experience that people can gain from exploring something as tactile as a 3D print. By doing so, I returned to the initial reason for my desiring to print the model -- my hoping to touch the object and examine how it fit into the 16th-17th century from a physical perspective -- and decided that I think that 3D printing enhances the qualities inherent to historical works. I’m curious though, what has your experience been with 3D printing historical artifacts?\n"},{"id":"2015-11-18-a-former-fellows-new-adventures-in-data-science","title":"A Former Fellow's New Adventures in Data Science","author":"claire-maiers","date":"2015-11-18 06:51:23 -0500","categories":null,"url":"a-former-fellows-new-adventures-in-data-science","content":"Hello All.  I’m happy to report that after several years away, I’ll be blogging at the Slab again this year.  Thanks in large part to my experience with the [Praxis Fellowship](http://scholarslab.org/graduate-fellowships/), I was selected to participate in the [UVa Presidential Fellowship in Data Science](http://gradstudies.virginia.edu/node/46).  Over the course of the next year, I will be collaborating with a PhD student in systems engineering on a data-driven project.  From time to time, I’ll post updates about struggles, questions, and successes with the project.  Today, I’m going to start with a basic description of what we think we might be up to:\n\nIn a nutshell, Nick and I will be trying to model the patterns by which ideas move through social systems.  There already exists a robust literature within sociology that focuses on identifying how and when ideas will be adopted and spread.  But there are some shortcomings to this body of work.  Existing research tends to focus on the adoption and diffusion ideas.  There are not many studies that consider the persistence or the decline of an idea.  This means that we don’t have holistic models that capture the entire lifespan of ideas.\n\nNick and I will be taking a stab and remedying this situation by looking at just one social system; we’ll be tracking concepts and methods over time within academia.  Our first goal is to generate a typology of the common patterns by which ideas are adopted, persist, and decline.  If we manage that, our next task will be to develop a predictive model for when a concept or method will jump to a new discipline.\n\nWe will be using the text of articles contained in the [JSTOR database](http://dfr.jstor.org/), and we’ll be experimenting with a couple of methods.  One option is to use a supervised method.  We’ll generate a list of terms consisting of significant concepts and methods and then track those overtime through the database.  The second option is to use topic modelling to generate a list of topics and then track those over time.  If you’re thinking that this project has some similarities to Goldstone and Underwood’s PMLA project, you’re right. It does.  If you’re not familiar, check out their [excellent post ](http://andrewgoldstone.com/blog/2012/12/13/pmla/)to learn more about topic modeling.\n\nWe’ve already had a number of challenges in accessing and thinking about how to structure our data....look for more on that in future posts.\n\n"},{"id":"2015-12-03-reflections-on-a-year-of-dh-mentoring","title":"Reflections on a Year of DH Mentoring","author":"brandon-walsh","date":"2015-12-03 09:43:19 -0500","categories":["Digital Humanities","Grad Student Research","Research and Development"],"url":"reflections-on-a-year-of-dh-mentoring","content":"_[[Cross-posted on the Digital Humanities at Washington and Lee University blog](http://digitalhumanities.wlu.edu/blog/2015/12/03/reflections-on-a-year-of-dh-mentoring/)]_\n\nThis year I am working with [Eric Rochester](http://scholarslab.org/people/eric-rochester/) on a fellowship project that has me learning natural language processing (NLP), the application of computational methods to human languages. We're adapting these techniques to study quotation marks in the novels of Virginia Woolf (read more about the project [here](http://scholarslab.org/digital-humanities/virginia-woolf-natural-language-processing-and-the-quotation-mark/)). We actually started several months before this academic year began, and, as we close out another semester, I have been spending time thinking about just what has made it such an effective learning experience for me. I already had a technical background from my time in the Scholars' Lab at the beginning of the process, but I had no experience with Python or NLP. Now I feel most comfortable with the former of any other programming language and familiar enough with the latter to experiment with it in my own work.\n\nThe general mode of proceeding has been this: depending on schedules and deadlines, we meet once or twice every two weeks. Between our meetings I would work as far and as much as I could, and the sessions would offer a space for Eric and me to talk about what I had done. The following are a handful of things we have done that, I think, have helped to create such an effective environment for learning new technical skills. Though they are particular to this study, I think they can be usefully extrapolated to apply to many other project-based courses of study in digital humanities. They are primarily written from the perspective of a student but with an eye to how and why the methods Eric used proved so effective for me.\n\n\n**Let the Wheel Be Reinvented Before Sharing Shortcuts**\n\n\nI came to Eric with a very small program adapted from Matt Jockers's book on_ Text Analysis with R for Students of Literature_ that did little beyond count quotation marks and give some basic statistics. I was learning as I built the thing, so I was unaware that I was reinventing the wheel in many cases, rebuilding many protocols for dealing with commonly recognized problems that come from working with natural language. After working on my program and my approach to a degree of satisfaction, Eric pulled back the curtain to reveal that a commonly used python module, the Natural Language ToolKit ([NLTK](http://www.nltk.org/)), could address many of my issues and more. NLTK came as something of a revelation, and working inductively in this way gave me a great sense of the underlying problems the tools could address. By inventing my own way to read in a text, clean it to make its text uniformly readable by the computer, and breaking the whole piece into a series of words that could be analyzed, I understood the magic behind a couple lines of NLTK code that could do all that for me. The experience also helped me to recognize ways in which we would have to adapt NLTK for our own purposes as I worked through the book.\n\n\n**Have a Plan, but Be Flexible**\n\n\nAfter discussing NLTK and how it offered an easier way of doing the things that I wanted, Eric had me systematically work through the [NLTK book](http://www.nltk.org/book/) for a few months. Our meetings took on the character of an independent study: the book set the syllabus, and I went through the first seven chapters at my own pace. Working from a book gave our meetings structure, but we were careful not to hew too closely to the material. Not all chapters were relevant to the project, and we cut sections of the book accordingly. We shaped the course of study to the intellectual questions rather than the other way around.\n\n\n**Move from Theory to Practice / Textbook to Project**\n\n\nAs I worked through the book, I was able to recognize certain sections that felt most relevant to the Woolf work. Once I felt as though I had reached a critical mass, we switched from the book to the project itself and started working. I tend to learn from doing best, so the shift from theory to execution was a natural one. The quick and satisfying transition helped the work to feel productive right away: I was applying my new skills as I was still learning to feel comfortable with them. Where the initial months had more the feel of a traditional student-teacher interaction, the project-based approach we took up at this point felt more like a real and true collaboration. Eric and I would develop to-do items together, we would work alongside each other, and we would talk over the project together.\n\n\n**Document Everything**\n\n\nBetween our meetings I would work as far and as much as I could, carefully noting places at which I encountered problems. In some cases, these were conceptual problems that needed clarifying, and these larger questions frequently found their way into separate notes. But my questions were frequently about what a particular line of code, a particular command or function, might be doing. In that case, I made comments directly in the code describing my confusion. I quickly found that these notes were as much for me as for Eric--I needed to get back in the frame of mind that led to the confusion in the first place, and copious notes helped remind me what the problem was. These notes offered a point of departure for our meetings: we always had a place to start, and we did so based on the work that I had done.\n\n\n**Communicate in as Many Ways as Possible**\n\n\nWe met in person as much as possible, but we also used a variety of other platforms to keep things moving. Eric and I had all of our code on [GitHub](https://github.com/erochest/woolf) so that we could share everything that we had each been working on and discuss things from a distance if necessary. Email, obviously, can do a lot, but I found the chat capabilities of the Scholars' Lab's IRC channel to be far better for this sort of work. If I hit a particular snag that would only require a couple minutes for Eric to answer, we could quickly work things out through a web chat. With Skype and Google Hangouts we could even share the code on the other person's computer even from hundreds of miles away. All of these things meant that we could keep working around whatever life events happened to call us away.\n\n\n**Recognize Spinning Wheels**\n\n\nThese multiple avenues of communication are especially important when teaching technical skills. Not all questions or problems are the same: students can work through some on their own, but others can take them days to troubleshoot. Some amount of frustration is a necessary part of learning, and I do think it's necessary that students learn to confront technical problems on their own. But not all frustration is pedagogically productive. There comes a point when you have tried a dozen potential solutions and you feel as though you have hit a wall. An extra set of eyes can (and should) help. Eric and I talked constantly about how to recognize when it was time for me to ask for help, and low-impact channels of communication like IRC could allow him to give me quick fixes to what, to me at least, seemed like impossible problems. Software development is a collaborative process, and asking for help is an important skill for humanists to develop.\n\n\n**In-person Meetings Can Take Many Forms**\n\n\nWhen we met, Eric and I did a lot of different things. First, we would talk through my questions from the previous week. If I felt a particular section of code was clunky or poorly done, he would talk and walk me through rewriting the same piece in a more elegant form. We would often pair program, where Eric would write code while I watched, carefully stopping him each time I had a question about something he was doing. And we often took time to reflect on where the collaboration was going - what my end goal was as well as what my tasks before the next meeting would be. Any project has many pieces that could be dealt with at any time, and Eric was careful to give me solo tasks that he felt I could handle on my own, reserving more difficult tasks for times in which we would be able to work together. All of this is to say that any single hour we spent together was very different from the last. We constantly reinvented what the meetings looked like, which kept them fresh and pedagogically effective.\n\nThis is my best attempt to recreate my experience of working in such a close mentoring relationship with Eric. Obviously, the collaboration relies on an extremely low student-to-teacher ratio: I can imagine this same approach working very well for a handful of students, but this work required a lot of individual attention that would be hard to sustain for larger classes. One idea for scaling the process up might be to divide a course into groups, being training one, and then have students later in the process begin to mentor those who are just beginning. Doing so would preserve what I see as the main advantage of this approach: it helps to collapse the hierarchy between student and teacher and engage both in a common project. Learning takes place, but it does so in the context of common effort. I'd have to think more about how this mentorship model could be adapted to fit different scenarios. The work with Eric is ongoing, but it's already been one of the most valuable learning experiences I have had.\n"},{"id":"2015-12-03-the-ghost-in-the-graph-a-recap-on-time-things-and-entanglement","title":"The Ghost in the Graph: A Recap on Time, Things, and Entanglement","author":"ethan-reed","date":"2015-12-03 09:05:13 -0500","categories":["Digital Humanities","Experimental Humanities","Grad Student Research","Visualization and Data Mining"],"url":"the-ghost-in-the-graph-a-recap-on-time-things-and-entanglement","content":"_[This post is the protein-rich version of a series of related posts from [our Praxis site](http://praxis.scholarslab.org/), with fresh reflections on the process and product now that I’m done. If you want to see originals, check out [the project idea](http://praxis.scholarslab.org/memo/2015/11/02/11-2-week-project-time-through-things/), [the data itself as I recorded it](http://praxis.scholarslab.org/blog/2015/11/12/everything-i-used-in-a-seven-day-period/), a [first attempt at a visualization](http://praxis.scholarslab.org/blog/2015/11/18/visualizing-everything-i-used/), and a [second attempt at a series of visualizatio](http://praxis.scholarslab.org/blog/2015/11/20/visualization--2-of-everything-i-used-in-a-seven-day-period/)[ns](http://praxis.scholarslab.org/blog/2015/11/20/visualization--2-of-everything-i-used-in-a-seven-day-period/).]_\n\nTime through things. This was the motivating idea behind [a week-long project ](http://praxis.scholarslab.org/memo/2015/11/02/11-2-week-project-time-through-things/)I started at the beginning of November for Praxis. Everyone on the team decided to track, monitor, or experiment with lived time in one way or another for a full week. James looked at [clouds](http://praxis.scholarslab.org/memo/2015/11/11/cloud-data/); Lydia at [music](http://praxis.scholarslab.org/memo/2015/10/31/music-tracking-update/); Gillet at [time indoors and outdoors](http://praxis.scholarslab.org/memo/2015/11/18/outside-time/); Bremen at policing and affect; Rachel at [language use](http://praxis.scholarslab.org/blog/2015/11/18/f-bomb/). I chose to think about time through things. So I wrote down in a notebook [everything I used in a seven day period](http://praxis.scholarslab.org/blog/2015/11/12/everything-i-used-in-a-seven-day-period/), from when I woke up to when I went to bed. Obviously what counts as “use” and what counts as a “thing” gets conceptually gritty very quickly. To stay sane, I took them in their most intuitive, ordinary senses, which means Yes, my methodology was in some ways arbitrary, but also Yes, I managed not to go nuts while seeing it through. So the data is not perfect, but it’s there!\n\nWhy did I want to do this?\n\nIf you have a chance to look at my other posts this year related to Praxis's current explorations of time and ways of representing it ([When Things Break](http://scholarslab.org/digital-humanities/inktober-1021-when-things-break/), [Time Pieces](http://scholarslab.org/digital-humanities/inktober-1013-time-pieces-and-graphs/), and [Three Sketches](http://scholarslab.org/digital-humanities/inktober-105-three-sketches/) in particular) it becomes clear I’m interested in nonhuman and what some might call posthuman ways of thinking about time. For me, this means I’m thinking about the way time works for stuff, things, and entities that aren’t people. Which can be difficult because as humans we tend to anthropomorphize everything - humans think through human lenses. [Jacques Derrida famously argued](http://www.iep.utm.edu/met-phen/#H4) that Western philosophy itself is anthropomorphic (and ethnocentric) - others have [argued related things in different venues](http://press.uchicago.edu/ucp/books/book/chicago/M/bo3637992.html). And these ideas make a lot of sense. If the human species were physically different (blind but great sense of smell; two brains per body; a strain of bacteria; [four-dimensional](https://en.wikipedia.org/wiki/Tralfamadore#Slaughterhouse-Five)) our understandings of lots of things – basically everything – would be affected.\n\nBut even if it’s a difficult project, thinking of nonhuman and posthuman time also feels like an important project. For example, this kind of thinking might help us wrap our heads around our role in systems or entities bafflingly larger than us, like the geologic time of our planet and our power as a species to shape its geologic future. For a few of my favorite examples of this kind of thinking, see [Timothy Morton](https://www.upress.umn.edu/book-division/books/hyperobjects), [Dipesh Chakrabarty](http://www.jstor.org/stable/10.1086/596640), [Bethany Nowviskie](http://nowviskie.org/2014/anthropocene/) and [China Miéville](http://salvage.zone/in-print/the-limits-of-utopia/).\n\nA less “planetary” example would be re-thinking how we as decision-making entities are influenced or “made to do” things by the non-human entities that surround us. As [Bruno Latour wonders](http://www.jstor.org/stable/20167474?seq=1#page_scan_tab_contents): are you smoking the cigarette or does the cigarette smoke you? Well, as he says: both.\n\nSo how am I trying to think about posthuman or nonhuman time?\n\nTo answer this, let’s look at the data and what I did with it. My data, as [presented on the site](http://praxis.scholarslab.org/blog/2015/11/12/everything-i-used-in-a-seven-day-period/) in big blocks of words, almost looks like a kind of poetry (maybe [uncreative poetry](http://chronicle.com/article/Uncreative-Writing/128908/) in the vein of Kenneth Goldsmith). I took this strange data and did my best to represent it visually, both for practice with [d3 tools](http://d3js.org/) and also to eke more meaning out of what I’d done. For this visualization and the ones below, I'm posting images rather than the graphs themselves - if you want to play around with the sometimes sluggish originals, check out the links at the top of this post.\n\nFor my first attempted visualization, I borrowed from Jason Davies’ [“Parallel Sets”](https://www.jasondavies.com/parallel-sets/) visualization (with significant help from [Wayne](https://twitter.com/wayne_graham)). You can find here [the github page I took Davies' code from originally](https://github.com/jasondavies/d3-parsets), and his [license here](https://github.com/jasondavies/d3-parsets/blob/master/LICENSE).\n\nAlas, I began with his beautiful graph and turned it quickly into an incomprehensible scribble. Behold:[![visualization 1 12.3.2015](http://scholarslab.org/wp-content/uploads/2015/12/visualization-1-12.3.2015.png)](http://scholarslab.org/wp-content/uploads/2015/12/visualization-1-12.3.2015.png)As a visualization of data, what I made is pretty close to nonsensical. And also unwieldy - the original doesn't load right and can slow the page down significantly. This monstrosity didn’t come as a surprise to me, as I didn’t clean my data or prep it for what the code expected. I more or less just tried to crowbar my data into Davies’ code/setup until something came out the other end that looked anything remotely like a graph. So while I don’t know exactly what’s happening here, things _are _happening. Some sort of nightmare causality is at work, even if only my laptop really knows what’s going on (or _not _going on) as it tries to make sense of what I'm feeding it.\n\nBut when I take a step back, what it comes up with also feels kind of poetic, almost like the data itself. When you mouse over one of the catastrophic clusters and happen upon a single thread, the graph tries to produce a new narrative of objects for you. The pseudo-stories are wandering and garbled, but also charming and original. For example: “spoons -> mouse pad -> backpack -> chair -> yogurt -> metro card.” Or my personal favorite: “pillow -> sheets -> sheets -> sheets -> sheets -> sheets.”\n\nMore useful, I think, are my [second round of visualizations](http://praxis.scholarslab.org/blog/2015/11/20/visualization--2-of-everything-i-used-in-a-seven-day-period/) . These are [force-directed graphs](http://bl.ocks.org/mbostock/4062045), also from [d3js.org](http://d3js.org/), one of [Mike Bostock’s many visualizations](http://bost.ocks.org/mike/). As with Davies’ parallel sets graph, I used what Bostock had up on d3, replaced his data set with mine, and with a lot of help from [Eric](http://scholarslab.org/people/eric-rochester/) figured out how to get it to read my .csv file.\n\nWith this method, I produced one force-directed graph for each day and a sprawling, magnificent mess at the end combining all seven days. In the graphs, each node (or point) represents a “thing” that I used. If you mouse over the node you can see which thing it represents. Every edge (or connecting line) represents a connection between those things – in this case, a connection between A and B means that A was used right after or right before B in my linear data. This means that the linearity, as well as the order of use, is collapsed in these representations. What we’re left with, however, is something new and potentially worth looking at on its own.\n\nDifferent days have different shapes. Thursday’s things live in big billowing petals that loop out on long, solitary walks of minimal connection:[![visualization 2 thursday 12.3.2015](http://scholarslab.org/wp-content/uploads/2015/12/visualization-2-thursday-12.3.2015.png)](http://scholarslab.org/wp-content/uploads/2015/12/visualization-2-thursday-12.3.2015.png)Sunday’s things live in much tighter, centralized clusters – most activity is shared activity, a miniature city of things:\n\n\n[![visualization 2 sunday](http://scholarslab.org/wp-content/uploads/2015/12/visualization-2-sunday.png)](http://scholarslab.org/wp-content/uploads/2015/12/visualization-2-sunday.png)\n\n\nBut what do these static, force-direct graphs of relationships between things have to do with time?\n\nAccording to [what I gather on the subject](https://www.youtube.com/watch?v=YycAzdtUIko), modern physics has some counter-intuitive insights to offer regarding our ordinary bodily understanding of time, such that our intuited experience of it doesn’t necessarily correspond to how events happen in the nether realms of relativity. For example, according to [the relativity of simultaneity](https://en.wikipedia.org/wiki/Relativity_of_simultaneity), one observer might see A -> B -> C (with accurate measurements), while another with equal accuracy observes B -> C -> A.\n\nBut while orders of events may be muddled, the fact of [causality](https://en.wikipedia.org/wiki/Causality_(physics)#Cause_and_effect_in_physics) remains. So in one way of thinking, time for a person is less something that passes by moment to moment in a linear progression than it is a static line segment of every casually linked event, all existing simultaneously. In which case, static, simultaneous representations of multiple events might actually have an unusual kind of purchase when it comes to representing causal/temporal relationships.\n\nSo what happens when we try to think about this with regards to _stuff_? I'm thinking about causal relations between things that might not involve humans. For example, if humans were to disappear tomorrow (as in Weisman’s_ [The World Without Us](http://www.worldwithoutus.com/index2.html)_), _things _would very well continue to interact with one another. To expand on examples from my data: a pillow getting heavier, mustier, and moldier as moisture creeps into a dilapidating room, staining the sheets on which it rests, while both weigh down on a mattress whose metal springs start to rust and give (pillow -> sheets -> mattress); freeze-thaw cycles crack and crumble sections of road until a telephone pole tips and snaps onto the hood of a parked convertible, sending glass onto the street (road -> pole -> car); and so on. Other writers have thought about this before: in [a previous post](http://scholarslab.org/digital-humanities/inktober-1021-when-things-break/) I linked to Virginia Woolf’s depiction in _[To the Lighthouse](https://en.wikipedia.org/wiki/To_the_Lighthouse) _of a home left uninhabited for years. In Ray Bradbury’s [“There Will Come Soft Rains”](https://en.wikipedia.org/wiki/There_Will_Come_Soft_Rains_(short_story)) from _[The Martian Chronicles](https://en.wikipedia.org/wiki/The_Martian_Chronicles) _(not available as text online, but awesomely available as [a recording read by Leonard Nimoy](https://www.youtube.com/watch?v=LzhlU8rXgHc)), we watch from a disembodied point of view as a futuristic home in California, long abandoned, slowly breaks down, from its automated kitchenware and mouse-Roombas to the automated poetry-reading voice in the bedroom.\n\nPoint being, things can very well interact with one another without humans. There may be no humans to perceive them and thus classify these encounters as between discrete, meaningful things, but from our current vantage we can at least imagine them. In this sense, these thing-graphs are speculative.\n\nThat said, there is a ghost in the graph – a body, my body, invisible save for traces left in having connected thing-nodes through use-edges. But in these graphs, the time that body spent making those connections evaporates, like steam from a cup of coffee. And what’s left is the cup of coffee itself - and all its associates. Though these associates were used in a specific linear order, I wanted instead to think of them as bound together causally, all entangled simultaneously within a given frame of reference (24 hours). Is this a chronology? Is it a timeline? A time network?\n\nRegardless of observers arguing over linear orders-of-events, these things on this day have been strung together. A glass of water and a faucet, a toothbrush and toothpaste: somehow, someway, they were (are!) all tangled up with one another.\n\nCan I ever really think of “thing time” without using my very human body to think it through and write it all down? Of course not. The ghost is in the graph – the ghost (with a lot of help) put the graph online, is talking to you about the graph right now. But I can certainly try to imagine how we might think of these entanglements in less anthropomorphic ways, to de-center (in [the vein of Latour](https://en.wikipedia.org/wiki/Actor%E2%80%93network_theory)) the role of the human as sole prime agent, mover of all things. Rather, I have tried to show how these objects have lives of their own - and how, as active shaping forces, these lives are causally, temporally, entangled with ours.\n"},{"id":"2015-12-08-classical-archaeology-and-the-makerspace","title":"Classical Archaeology and the Makerspace","author":"jennifer-grayburn","date":"2015-12-08 08:06:41 -0500","categories":["Digital Humanities","Grad Student Research","Makerspace"],"url":"classical-archaeology-and-the-makerspace","content":"_[Cross-posted on my personal blog.](http://jennifernicolegrayburn.com/2015/12/08/classical-archaeology-and-the-makerspace/)_\n\nA few weeks ago, [R. Benjamin Gorham](http://www.virginia.edu/art/phd-program/current-students/11/ben-gorham), a Ph.D. candidate in [Classical Art & Archaeology](http://www.virginia.edu/art/phd-program/classical-art-archaeology/) at the University of Virginia, visited the Makerspace for a consultation on photogrammetry and 3D printing. Ben has been using GIS, drones, and photogrammetry during his summer excavations in Morgantina, Sicily and wanted to experiment with 3D printing his models. The physical reconstruction of archaeological sites offers exciting opportunities for both teaching and research, and I asked Ben to share a bit about his digital project:\n\n\nThe [American Excavations at Morgantina: Contrada Agnese Project](http://morgantina.org/) is an ongoing archaeological investigation at the ancient Graeco-Roman city of Morgantina, Sicily. As the Supervisor of Geospatial Studies at The Contrada Agnese Project, my goal is to translate the data from the field into useful, visual forms which can be studied, measured, and employed in publications and conferences. Part of this duty involves the creation, curation, and testing of a GIS database which serves as a nexus for all geographic data acquired in the field, including findspots, architectural features, and aerial imagery. Using a quadcopter drone we capture hundreds of images from the air every day, between 5 and 50 meters above our ongoing excavations. [Agisoft Photoscan](http://www.agisoft.com/) allows us to then combine these images with photos taken on the ground to create 3D models of our trenches and extant architecture, which are then hosted on our website and embedded in our GIS document. The Scholars’ Lab at UVa has allowed us to take this one step further, through the production of 3D-printed models of our trenches. We are using the Makerspace to generate hand-held versions of the buildings and trenches which are part of our ongoing excavations. This enables us to preserve every season's results in a physical form. Since archaeology is an inherently destructive science, we are constantly removing, changing, and re-burying the stratigraphic records in the soil which we study in order to reveal more about the past, and these 3D-printed models serve as instructive units which can be examined, shared, and explored long after our project has either backfilled or dug past interesting features and periods of ancient history. This creates a permanent physical record of our project which would otherwise be partially lost every time our season concludes for the summer.\n\nThe Contrada Agnese Project is currently taking applications for Summer 2016 student volunteers. Please contact Ben, [rbg8jn@virginia.edu](mailto:rbg8jn@virginia.edu), or consult the [application flier](https://docs.google.com/document/d/1iu6u3nQtyr710ImWoMDuydZ-pg_wmFlvc4vLgpubOTA/edit?usp=sharing) for more details.\n\n[caption id=\"attachment_12412\" align=\"aligncenter\" width=\"432\"][![Untitled](http://scholarslab.org/wp-content/uploads/2015/12/Untitled.jpg)](http://scholarslab.org/wp-content/uploads/2015/12/Untitled5.jpg) Photogrammetry can only model what is visible in photographs, and Ben’s initial model only included the surface layer of the ground and trenches. The topographical irregularities, however, would not be possible to print without substantial supports.[/caption]\n\n[caption id=\"attachment_12411\" align=\"aligncenter\" width=\"432\"][![Untitled1](http://scholarslab.org/wp-content/uploads/2015/12/Untitled1.jpg)](http://scholarslab.org/wp-content/uploads/2015/12/Untitled1.jpg) To close the model and make it suitable for 3D printing, we needed to extend the y-axis so it lay flat on the printing platform. We imported the model into Meshmixer and used the Extrusion editing function to extend the base to the depth of the trenches.[/caption]\n\n[caption id=\"attachment_12414\" align=\"aligncenter\" width=\"432\"][![Untitled3](http://scholarslab.org/wp-content/uploads/2015/12/Untitled3.jpg)](http://scholarslab.org/wp-content/uploads/2015/12/Untitled4.jpg) We uploaded our model into Cure to slice it and generate the gcode for the Ultimaker 2.[/caption]\n\n[caption id=\"attachment_12410\" align=\"aligncenter\" width=\"431\"][![Untitled4](http://scholarslab.org/wp-content/uploads/2015/12/Untitled4.jpg)](http://scholarslab.org/wp-content/uploads/2015/12/Untitled4.jpg) Unfortunately, this was the beginning of the end of our Ultimaker’s initial golden age of printing. We began experiencing extrusion problems, which is evident from our first attempt to print.[/caption]\n\n[caption id=\"attachment_12409\" align=\"aligncenter\" width=\"431\"][![Untitled5](http://scholarslab.org/wp-content/uploads/2015/12/Untitled5.jpg)](http://scholarslab.org/wp-content/uploads/2015/12/Untitled5.jpg) We switched to the Makerbot software and our Makerbot Dual printer to complete the print.[/caption]\n\n[caption id=\"attachment_12413\" align=\"aligncenter\" width=\"431\"][![Untitled6](http://scholarslab.org/wp-content/uploads/2015/12/Untitled6.jpg)](http://scholarslab.org/wp-content/uploads/2015/12/Untitled6.jpg) While the detail quality is not as clear as we would like, we were able to generate a successful print showing all of the trenches and topographical features. As we fix the Ultimaker 2, we will continue to experiment with printing size and quality to meet the project's needs.[/caption]\n\n[\n](http://scholarslab.org/wp-content/uploads/2015/12/Untitled4.jpg)[\n](http://scholarslab.org/wp-content/uploads/2015/12/Untitled.jpg) [\n](http://scholarslab.org/wp-content/uploads/2015/12/Untitled6.jpg)\n"},{"id":"2015-12-17-learning-to-use-3d-printers-for-the-digital-humanities","title":"Learning to Use 3D Printers for the Digital Humanities","author":"margaret-furr","date":"2015-12-17 11:03:21 -0500","categories":["Digital Humanities","Experimental Humanities"],"url":"learning-to-use-3d-printers-for-the-digital-humanities","content":"As someone who was primarily educated as a humanist and has also worked on projects involving data, I have experience in courageously facing the steep curve of learning new technologies. Curious about both the arts and the sciences and seeing where the two can enhance each other to assist in better, more critical thinking, I am passionate about learning technologies for the humanities.\n\nI must admit that when I first started learning how to use the 3D printers, which I was fascinated by from mechanical and philosophical viewpoints, I worried about breaking them or failing at a print. The day that someone, who was using one of the printers made me aware that the nozzle was clogged with filament was the day that I realized that when learning to fix printer and printing problems, I had to be fierce, have a sense of humor, and be willing to learn while not knowing 100%. I have carried this attitude into printing as I have overcome challenges related to cleaning out the nozzle, not being afraid to pull components apart, exploring alternative ways to develop a model, such as a model for visualizing data, figuring out how to print something in a way that removes stringiness, and addressing the problem of a too hot printer plate. Carrying this attitude pays off because as digital data and information are increasingly available for the humanities and also in other fields, new ways to understand that data and information through artistic prints can be of value. As humanists, including myself, continue to be printing warriors, facing initial fears such as “a 3D printer, what is this?” or “data? how does that relate to art and humanities scholarship?”, I think that necessary bridges between artists and humanists and technologists and engineers can be strengthened. I also see how appreciation for different disciplines and how disciplines work together can be increased.\n\nOne way that I myself did this semester was by printing data on DC population sizes across districts. As scholars refine data available on attributes of locations at different historical periods, I envision classroom engagement increasing by understanding historical trends across locations with 3D maps. Check out the map with stringiness on the left and the cleaner map on the right below!\n\n\n\n[![print1](http://scholarslab.org/wp-content/uploads/2015/12/print1-300x272.png)](http://scholarslab.org/wp-content/uploads/2015/12/print1.png)\n\n[![print2](http://scholarslab.org/wp-content/uploads/2015/12/print2-300x238.png)](http://scholarslab.org/wp-content/uploads/2015/12/print2.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},{"id":"2016-01-14-working-with-d3-part-one","title":"Working with D3, Part One.","author":"ammon-shepherd","date":"2016-01-14 08:15:41 -0500","categories":["Visualization and Data Mining"],"url":"working-with-d3-part-one","content":"# [![graph](http://scholarslab.org/wp-content/uploads/2016/01/graph.png)](http://scholarslab.org/wp-content/uploads/2016/01/graph.png)\n\n\n\n\n\n# Track-n-Treat\n\n\n\nHalloween is great. Free candy. And I have six kids to go out and get it for me. :)\n\nI cull some of the finest chocolates from their bags after trick-or-treating and enjoy them throughout the next week. We usually eat everything within a week...\n\nThis year I decided to track how much candy I ate, and which ones, in the week after Halloween. It was only because we needed to do something like this for [Praxis](http://praxis.scholarslab.org/memo/2015/10/30/track-the-treat/). Otherwise I eat candy without a second thought.\n\n\n\n## [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#getting-started-with-d3)Getting started with d3\n\n\n\nD3 is a Decent tool for visualizing Data, hence the name Data-Driven Documents. It is basically a JavaScript framework for making charts, graphs, maps, or anything you can images, based on data from a file, database, etc.\n\nThe best way to learn d3 is to practice it over and over. I suggest looking at one or both of these d3 tutorials first.\n\n\n\n\n    \n  * [https://www.dashingd3js.com/table-of-contents](https://www.dashingd3js.com/table-of-contents)\n\n    \n  * [http://alignedleft.com/tutorials/d3/about](http://alignedleft.com/tutorials/d3/about)\n\n\n\nAfter you've been through one, or both of them, try looking at my examples here:\n\n\n    \n  * [https://github.com/mossiso/track-n-treat](https://github.com/mossiso/track-n-treat)\n\n\n\n\n\n# [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#making-the-chart)Making the chart\n\n\n\nThis post is an expanded version of the actual code files which contain lots of comments.\n\n\n\n## [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#a-note-about-data)A note about data\n\n\n\nHow you structure the data is actually pretty important. It can make it super easy or super hard to get at the information you want. The most simple method usually is the best way to start.\n\nIn my case, every time I ate a piece of candy, I took a note of what day it was, what time, and which candy. Something like this:\n\n`day: 2015-11-01, time: 11:15, candy: Snickers`\n\nAs it so happens, this is a great format. Each day on it's own line. The data gets a little redundant, the day and time information are repeated many times, but it makes it super easy to read and we can easily manipulate it later with d3.\n\nFor d3 to use this, we have several options. CSV and JSON are the easiest to work with, so I'll pick one of them.\n\nA CSV option would look like this:\n\n\n    \n    <code>2015-11-01, 13:15, Snickers\n    2015-11-01, 15:24, KitKat\n    2015-11-02, 19:33, Twix\n    2015-11-02, 17:42, KitKat\n    2015-11-03, 21:51, Snickers\n    </code>\n\n\n\nA JSON option is a bit more intense. Here I'm creating an array of objects.\n\n\n\n\n\n    \n     [\n       {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>day<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2015-11-01<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>17:25<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candy<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Heath Bar<span class=\"pl-pds\">\"</span></span>},\n       {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>day<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2015-11-01<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>17:25<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candy<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Kitkat Bar<span class=\"pl-pds\">\"</span></span>},\n       {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>day<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2015-11-01<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>20:25<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candy<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Kitkat Bar<span class=\"pl-pds\">\"</span></span>},\n       {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>day<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2015-11-02<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>17:38<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candy<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Whopper<span class=\"pl-pds\">\"</span></span>},\n       <span class=\"pl-ii\">...</span>\n     ]\n\n\n\n\n\n\nThis is an array (designated with square brackets `[]`) of objects (designated with curly braces `{}`). The object consists of names (day, time, candy) and their associated values. If we assign the above to the variable 'array', we can access the first element of the array with `array[0]`. That would return the first line above.\n\n\n\n<blockquote>A good tutorial on JavaScript arrays is at Lynda.com (free subscription through the [library](https://www.library.virginia.edu/lynda/)). Search for 'Introducing the JavaScript Language with Joe Chellman'.</blockquote>\n\n\n\nTo access any element within the object, we can use dot notation.\n\n`array[0].day` would return `2015-11-01`.\n\n`array[0].time` would return `17:25`.\n\n`array[3].candy` would return `Whopper`.\n\n_What would array[2].time return?_\n\n_How would you get the value \"Kitkat Bar\" (there are two ways)?_\n\n\n\n## [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#setting-up-the-document)Setting up the document\n\n\n\nFirst off, we'll create a generic HTML page with the standard HTML\n\n\n\n\n\n    \n    <!DOCTYPE html>\n    <<span class=\"pl-ent\">html</span> <span class=\"pl-e\">lang</span>=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>en<span class=\"pl-pds\">\"</span></span>>\n        <<span class=\"pl-ent\">head</span>>\n            <<span class=\"pl-ent\">meta</span> <span class=\"pl-e\">charset</span>=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>utf-8<span class=\"pl-pds\">\"</span></span>>\n            <<span class=\"pl-ent\">title</span>>D3 Test</<span class=\"pl-ent\">title</span>>\n            <<span class=\"pl-ent\">link</span> <span class=\"pl-e\">href</span>=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>styles.css<span class=\"pl-pds\">\"</span></span> <span class=\"pl-e\">rel</span>=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>stylesheet<span class=\"pl-pds\">\"</span></span> <span class=\"pl-e\">type</span>=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>text/css<span class=\"pl-pds\">\"</span></span> />\n    <span class=\"pl-s1\">        <<span class=\"pl-ent\">script</span> <span class=\"pl-e\">type</span>=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>text/javascript<span class=\"pl-pds\">\"</span></span> <span class=\"pl-e\">src</span>=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>d3/d3.v3.js<span class=\"pl-pds\">\"</span></span>></<span class=\"pl-ent\">script</span>></span>\n        </<span class=\"pl-ent\">head</span>>\n        <<span class=\"pl-ent\">body</span>>\n        </<span class=\"pl-ent\">body</span>>\n    </<span class=\"pl-ent\">html</span>>\n\n\n\n\n\n\nEverything is pretty standard. The `<script>` tag is what pulls in the d3 library from the folder named 'd3'.\n\n`<script type=\"text/javascript\" src=\"d3/d3.v3.js\"></script>`\n\nNext we create a couple of divs, one for placing some text and links, and another that we'll use to attach our graph to. This code goes in between the `<body>` and `</body>` tags in the code above.\n\n\n\n\n\n    \n          <<span class=\"pl-ent\">div</span> <span class=\"pl-e\">id</span>=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>wrap<span class=\"pl-pds\">\"</span></span>>\n            <<span class=\"pl-ent\">h1</span>>Track-n-Treat</<span class=\"pl-ent\">h1</span>>\n            <<span class=\"pl-ent\">p</span>>How many candies did I eat each day the week after Halloween?</<span class=\"pl-ent\">p</span>>\n            <<span class=\"pl-ent\">div</span> <span class=\"pl-e\">id</span>=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>graph<span class=\"pl-pds\">\"</span></span>></<span class=\"pl-ent\">div</span>>\n          </<span class=\"pl-ent\">div</span>>\n\n\n\n\n\n\n\n\n## [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#diving-into-d3)Diving into D3\n\n\n\nThe rest of the code is written in JavaScript. The JavaScript can live anywhere in the code: in between the `<header>` and`</header>` tags, or anywhere in between the `<body>` and `</body>` tags. I just put it after the above code after the `<body>`tag.\n\nTo have JavaScript in the body of the HTML document, we'll surround it with `<script>` tags like so:\n\n\n\n\n\n    \n    <span class=\"pl-s1\"><<span class=\"pl-ent\">script</span> <span class=\"pl-e\">type</span>=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>text/javascript<span class=\"pl-pds\">\"</span></span>></span>\n    <span class=\"pl-s1\"> <span class=\"pl-c\">/* Add some JavaScript here.</span></span>\n    <span class=\"pl-s1\"><span class=\"pl-c\">    '/*' starts a multi-line comment, and the next line ends it.</span></span>\n    <span class=\"pl-s1\"><span class=\"pl-c\">*/</span></span>\n    \n    <span class=\"pl-s1\"><span class=\"pl-c\">// This is a single line comment</span></span>\n    <span class=\"pl-s1\"></<span class=\"pl-ent\">script</span>></span>\n\n\n\n\n\n\nOur first line of JavaScript, and d3, is one to pull in a file that has the data in it. It is the first line within our `<script>` tags above.\n\n\n    \n    <code>d3.json(\"data/track-n-treat.json\", function(data) {\n      // More d3 code will go in here\n    \n    }\n    </code>\n\n\n\nThis function surrounds all of the d3 code that makes the graph. Supply the path (relative to this html file) and a variable name for the data (within the function's parenthesis) Here we pull in the data from a separate file. `d3` calls the main d3 method, `.json` calls d3's json method that takes care of loading all of the data from the json file. The json function needs a file path (data/track-n-treat.json) that is relative to where this file is, and a function that creates an internal variable/object to hold the data.\n\nSo now all of the data from the json file is available as a variable, in our case it is named 'data'. (We could change that to anything we want.)\n\nIt's as if we had this in the code:\n\n\n\n\n\n    \n    data <span class=\"pl-k\">=</span> [\n       {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>day<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2015-11-01<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>17:25<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candy<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Heath Bar<span class=\"pl-pds\">\"</span></span>},\n       {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>day<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2015-11-01<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>17:25<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candy<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Kitkat Bar<span class=\"pl-pds\">\"</span></span>},\n       {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>day<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2015-11-01<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>20:25<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candy<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Kitkat Bar<span class=\"pl-pds\">\"</span></span>},\n       {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>day<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2015-11-02<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>17:38<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candy<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Whopper<span class=\"pl-pds\">\"</span></span>},\n       ...\n     ]\n\n\n\n\n\n\n\n\n### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#some-variables)Some Variables\n\n\n\nNext, we'll set some variables to use later.\n\nThe 'margin' variable is really an object, so we can call the elements within using dot notation like so: `margin.top` returns '40', or `margin.right` will return '40'.\n\n\n\n\n\n    \n    <span class=\"pl-k\">var</span> margin <span class=\"pl-k\">=</span> {top<span class=\"pl-k\">:</span> <span class=\"pl-c1\">40</span>, right<span class=\"pl-k\">:</span> <span class=\"pl-c1\">40</span>, bottom<span class=\"pl-k\">:</span> <span class=\"pl-c1\">40</span>, left<span class=\"pl-k\">:</span><span class=\"pl-c1\">40</span>},\n        width <span class=\"pl-k\">=</span> <span class=\"pl-c1\">700</span>,\n        height <span class=\"pl-k\">=</span> <span class=\"pl-c1\">300</span>,\n        workingHeight <span class=\"pl-k\">=</span> height <span class=\"pl-k\">-</span> <span class=\"pl-smi\">margin</span>.<span class=\"pl-c1\">top</span> <span class=\"pl-k\">-</span> <span class=\"pl-smi\">margin</span>.<span class=\"pl-c1\">bottom</span>;\n\n\n\n\n\n\n\n\n### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#axes)Axes\n\n\n\nD3 can take care of a lot of the functionality of creating and placing the x and y axes. D3 puts the axis in the right spot, puts the tick marks on, spaces them appropriately, and labels the tick marks.\n\nThere are two parts to creating an axis. First, create a scale. Second, apply the scale to the axis.\n\nLet's start by making the x axis.\n\n\n\n#### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#the-x-axis-scale)The X-Axis (scale)\n\n\n\nWe want the x axis to show the days.\n\nWe'll use a time scale, since we're plotting days. We'll first set 'x' to be a function that converts dates that we plug into it, into pixels on the screen. Think of this as a range or scale converter.\n\nThe domain represents the minimum and maximum values that exist in the data. The range is the minimum and maximum values as represented on the web page. This will basically be a date to pixel converter.\n\n\n\n<blockquote>For a great write up on how d3 scales work, look here:\n\n> \n> \n    \n>   * [http://www.jeromecukier.net/blog/2011/08/11/d3-scales-and-color/](http://www.jeromecukier.net/blog/2011/08/11/d3-scales-and-color/)\n> \n\n</blockquote>\n\n\n\nOur next line of code creates the x function and assigns the domain and range for the scale. This code goes right after the variables we created above.\n\n\n\n\n\n    \n    <span class=\"pl-k\">var</span> x <span class=\"pl-k\">=</span> <span class=\"pl-smi\">d3</span>.<span class=\"pl-smi\">time</span>.<span class=\"pl-en\">scale</span>()\n              .<span class=\"pl-en\">domain</span>([ <span class=\"pl-k\">new</span> <span class=\"pl-en\">Date</span>(data[<span class=\"pl-c1\">0</span>].<span class=\"pl-smi\">day</span>), <span class=\"pl-smi\">d3</span>.<span class=\"pl-smi\">time</span>.<span class=\"pl-smi\">day</span>.<span class=\"pl-en\">offset</span>(<span class=\"pl-k\">new</span> <span class=\"pl-en\">Date</span>(data[<span class=\"pl-smi\">data</span>.<span class=\"pl-c1\">length</span> <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>].<span class=\"pl-smi\">day</span>), <span class=\"pl-c1\">1</span>)])\n              .<span class=\"pl-en\">rangeRound</span>([<span class=\"pl-c1\">0</span>, width <span class=\"pl-k\">-</span> <span class=\"pl-smi\">margin</span>.<span class=\"pl-c1\">left</span> <span class=\"pl-k\">-</span> <span class=\"pl-smi\">margin</span>.<span class=\"pl-c1\">right</span>]);\n\n\n\n\n\n\nLet's walk through this in a bit more detail.\n\nD3's [time.scale](https://github.com/mbostock/d3/wiki/Time-Scales) function takes a 'domain' and a 'range', which both take a minimum and maximum value. We plug the minimum and maximum dates into the domain section and we set the pixel limits in the range section.\n\nThe minimum date is the first day I ate candy, the maximum date is the last day I ate candy. To calculate the first day, we can get the date from the data array: 'data[0].day' corresponds with the first element in the 'data' array (which is an object), and the value of that objects 'day' key.\n\nSince the json file is assigned to the 'data' variable, we can get the first 'day' by accessing `data[0].day`. We put that in the default JavaScript `Date` function to return the date as a String.\n\nGetting the last day is similar, we can also get the last day from the data in the json file. We just need to get the last object element in the array, and get the value of the day element. But how do we specify which is the last element in the array if we don't know how many elements there are? We could count, but what if we change the data?\n\nWe can do a little math to calculate the last element in the array. The JavaScript builtin `.length` method gives us how many elements are in an array. Since array elements begin counting at 0, we just get the length, number of elements, minus 1 to give us the index of the last element. We can then we put that into d3's `time.day.offset` function which adds or subracts a given amount of days from the day that you input. In our case we'll offset by one day, so that the axis goes from the first day until the day after the last day that I ate candy.\n\nThe range is basically the width we specify above, but subtract some of the padding. So the range would be from 0 to 620 (700 - 40 - 40).\n\nA visualization of what the domain to range looks like\n\n\n\n\n\n    \n    2015-11-1      2015-11-4         2015-11-8\n       <span class=\"pl-k\">|</span>---------------<span class=\"pl-k\">|</span>-----------------<span class=\"pl-k\">|</span>\n      /                /                  \\\n     /                /                    \\\n    <span class=\"pl-k\">|</span>----------------<span class=\"pl-k\">|</span>----------------------<span class=\"pl-k\">|</span>\n    0                  88.5                620\n    \n\n\n\n\n\n\n\n\n#### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#the-x-axis-axis)The X-Axis (axis)\n\n\n\nWe create an x axis by calling the `d3.svg.axis` method, and assigning it to a variable. Let's call it 'xAxis'.\n\n\n\n\n\n    \n    <span class=\"pl-k\">var</span> xAxis <span class=\"pl-k\">=</span> <span class=\"pl-smi\">d3</span>.<span class=\"pl-smi\">svg</span>.<span class=\"pl-c1\">axis</span>()\n                  .<span class=\"pl-en\">scale</span>(x)\n                  .<span class=\"pl-en\">orient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>bottom<span class=\"pl-pds\">\"</span></span>)\n                  .<span class=\"pl-en\">ticks</span>(<span class=\"pl-smi\">d3</span>.<span class=\"pl-smi\">time</span>.<span class=\"pl-smi\">days</span>, <span class=\"pl-c1\">1</span>)\n                  .<span class=\"pl-en\">tickFormat</span>(<span class=\"pl-smi\">d3</span>.<span class=\"pl-smi\">time</span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>%a %d<span class=\"pl-pds\">'</span></span>));\n\n\n\n\n\n\nAgain, this can all be written out on one line, but we separate each chained method onto its own line to make the code more legible.\n\nWe plug in the scale created above using the `.scale` method, and we assign an orientation using the `.orient` method.\n\n`.ticks` sets the ticks or marks on the x axis. `d3.time.days` sets a range of days within the dates used in the scale (above). '1' means show each day in that range (a '2' would show every other day in the range).\n\n`.tickFormat` sets the format for the tick to be a date using the `d3.time.format` function in the form 'DDD ##'.\n\n\n\n#### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#y-axis-scale)Y-Axis (scale)\n\n\n\nThe y-axis represents how many candies eaten each day. The height of this axis is determined by the maximum number of candies eaten in a single day. A few times I ate multiple candies at the same time. We'll need to get the number of candies for each time period in a day and add them all up. This will determine the max height of the y axis.\n\nWe'll use the d3 `nest` function which manipulates the data array. The `key` function pulls out all of the separate days as a key, the value is all of the times that are associated with that day.\n\nWe can then use the `rollup` function to turn the values into something else. In this case it returns the length of the values array, which is the number of candies eaten that day.\n\n\n\n\n\n    \n    <span class=\"pl-k\">var</span> timesPerDay <span class=\"pl-k\">=</span> <span class=\"pl-smi\">d3</span>.<span class=\"pl-en\">nest</span>()\n                        .<span class=\"pl-en\">key</span>(<span class=\"pl-k\">function</span>(<span class=\"pl-smi\">d</span>) { <span class=\"pl-k\">return</span> <span class=\"pl-smi\">d</span>.<span class=\"pl-smi\">day</span>; })\n                        .<span class=\"pl-en\">rollup</span>(<span class=\"pl-k\">function</span>(<span class=\"pl-smi\">t</span>) { <span class=\"pl-k\">return</span> <span class=\"pl-smi\">t</span>.<span class=\"pl-c1\">length</span>; })\n                        .<span class=\"pl-en\">entries</span>(data);\n\n\n\n\n\n\nThe `key` part of the `nest` function turns the data from this:\n\n\n\n\n\n    \n    [\n    {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>day<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2015-11-01<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>17:25<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candy<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Heath Bar<span class=\"pl-pds\">\"</span></span>},\n    {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>day<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2015-11-01<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>17:25<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candy<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Kitkat Bar<span class=\"pl-pds\">\"</span></span>},\n    {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>day<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2015-11-01<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>20:25<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candy<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Kitkat Bar<span class=\"pl-pds\">\"</span></span>},\n    {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>day<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2015-11-02<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>17:38<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candy<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Whopper<span class=\"pl-pds\">\"</span></span>},\n    {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>day<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2015-11-02<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>18:38<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candy<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Whopper<span class=\"pl-pds\">\"</span></span>},\n    ...\n    ]\n\n\n\n\n\n\nInto this:\n\n\n\n\n\n    \n    [\n      {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>key<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-01<span class=\"pl-pds\">'</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>values<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> [\n        {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>day<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-01<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>time<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>17:25<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>candy<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Heath Bar<span class=\"pl-pds\">'</span></span>},\n        {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>day<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-01<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>time<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>17:25<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>candy<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Kitkat Bar<span class=\"pl-pds\">'</span></span>},\n        {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>day<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-01<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>time<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>20:25<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>candy<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Kitkat Bar<span class=\"pl-pds\">'</span></span>},\n        ]\n      },\n      {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>key<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-02<span class=\"pl-pds\">'</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>values<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> [\n        {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>day<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-02<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>time<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>17:38<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>candy<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Whopper<span class=\"pl-pds\">'</span></span>},\n        {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>day<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>2015-11-02<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>time<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>18:38<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>candy<span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Whopper<span class=\"pl-pds\">\"</span></span>},\n        ...\n        ]\n      },\n      ...\n    ]\n\n\n\n\n\n\nAnd the `rollup` part of the `nest` function further converts the data into this:\n\n\n\n\n\n    \n    [\n      {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>key<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-01<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>values<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-c1\">3</span> },\n      {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>key<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-02<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>values<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-c1\">2</span> },\n      ...\n    ]\n\n\n\n\n\n\n`.entries` is where we plug in which data is to be 'key'ed and 'rollup'ed. We assign the result to the variable 'timesPerDay'.\n\nNext we need to pull out the highest number from this array.\n\n\n\n\n\n    \n    <span class=\"pl-k\">var</span> maxEats <span class=\"pl-k\">=</span> <span class=\"pl-smi\">d3</span>.<span class=\"pl-c1\">max</span>(timesPerDay, <span class=\"pl-k\">function</span>(<span class=\"pl-smi\">v</span>) { <span class=\"pl-k\">return</span> <span class=\"pl-smi\">v</span>.<span class=\"pl-smi\">values</span>; })\n\n\n\n\n\n\nThe `d3.max` function looks at an array and returns the highest value. It takes an array and a function. The array is the 'timesPerDay' array we created above. The function allows us to specify which part of the array to count. Here the `v` stands for the array of day objects, and `v.values` is the 'values' element within each day object, which holds the number of times a candy was eaten for that day. So `d3.max` is now just looking at the different 'values' fields and returns the highest number.\n\nSince we're just using numbers (not dates), we use a regular linear scale for the y axis. We use the maximum number of candy in a day as the maximum for the domain.\n\n\n\n\n\n    \n    <span class=\"pl-k\">var</span> y <span class=\"pl-k\">=</span> <span class=\"pl-smi\">d3</span>.<span class=\"pl-smi\">scale</span>.<span class=\"pl-en\">linear</span>()\n              .<span class=\"pl-en\">domain</span>([<span class=\"pl-c1\">0</span>, maxEats ])\n              .<span class=\"pl-en\">range</span>([workingHeight, <span class=\"pl-c1\">0</span>]);\n\n\n\n\n\n\nThe range is the working height (the height minus the padding) to 0.\n\n\n\n<blockquote>What happens if the range is 0 to `workingHeight`</blockquote>\n\n\n\n\n\n#### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#the-y-axis-axis)The Y-Axis (axis)\n\n\n\nNo we can create the y axis using the scale above.\n\n\n\n\n\n    \n    <span class=\"pl-k\">var</span> yAxis <span class=\"pl-k\">=</span> <span class=\"pl-smi\">d3</span>.<span class=\"pl-smi\">svg</span>.<span class=\"pl-c1\">axis</span>()\n                  .<span class=\"pl-en\">scale</span>(y)\n                  .<span class=\"pl-en\">orient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>left<span class=\"pl-pds\">'</span></span>)\n                  .<span class=\"pl-en\">tickPadding</span>(<span class=\"pl-c1\">8</span>);\n\n\n\n\n\n\nMost of this is self-explanatory. The `.scale` calls the y scale we made above. The `.orient` sets the axis on the left hand side. `.tickPadding` determines the space between the tick marks. We set all of this to the variable name 'yAxis'.\n\nWe'll use the variables 'xAxis' and 'yAxis' later in the code.\n\n\n\n### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#put-it-all-together)Put it all together\n\n\n\n\n\n#### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#the-main-svg-element)The main SVG element\n\n\n\nThis is where the magic happens. First, we create a variable/object for the svg elements to live under, because we want to add to it later. `d3` calls the main d3 method. `select` specifies which part of the DOM we want to target. We're going to target an HTML div tag with the id 'graph'.\n\n\n\n\n\n    \n    <span class=\"pl-k\">var</span> svg <span class=\"pl-k\">=</span> <span class=\"pl-smi\">d3</span>.<span class=\"pl-c1\">select</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>#graph<span class=\"pl-pds\">'</span></span>)\n                .<span class=\"pl-en\">append</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>svg<span class=\"pl-pds\">'</span></span>)\n                  .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>graph<span class=\"pl-pds\">'</span></span>)\n                  .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>width<span class=\"pl-pds\">'</span></span>, width)\n                  .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>height<span class=\"pl-pds\">'</span></span>, height)\n                .<span class=\"pl-en\">append</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>g<span class=\"pl-pds\">'</span></span>)\n                  .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>transform<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>translate(<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-smi\">margin</span>.<span class=\"pl-c1\">left</span> <span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>, <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-smi\">margin</span>.<span class=\"pl-c1\">right</span> <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>)<span class=\"pl-pds\">'</span></span>);\n\n\n\n\n\n\nThis targets, or selects, the HTML tag with the id of 'graph'. We then append an svg tag to the div tag, give it a class of 'graph', and set the width and height.\n\nThen we append an svg group element `g` where the chart will reside. We're going to offset this group to create some padding where the axes will go. To better understand the `transform` and `translate` functions, take a minute to read through the section on 'SVG Transform as a Coordinate Space Transformation'.\n\n\n\n\n    \n  * [https://www.dashingd3js.com/svg-group-element-and-d3js](https://www.dashingd3js.com/svg-group-element-and-d3js)\n\n\n\n\n\n#### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#the-graph-section)The graph section\n\n\n\nWe attach the graph using the [selectAll ](https://github.com/mbostock/d3/wiki/Selections#d3_selectAll)method, and attaching to our previously created `svg` variable/object.\n\n\n\n\n\n    \n    <span class=\"pl-smi\">svg</span>.<span class=\"pl-en\">selectAll</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>.graph<span class=\"pl-pds\">'</span></span>)\n        .<span class=\"pl-c1\">data</span>(timesPerDay)\n        .<span class=\"pl-en\">enter</span>().<span class=\"pl-en\">append</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>rect<span class=\"pl-pds\">'</span></span>)\n          .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bar<span class=\"pl-pds\">'</span></span>)\n          .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>, <span class=\"pl-k\">function</span>(<span class=\"pl-smi\">d</span>) {<span class=\"pl-k\">return</span> <span class=\"pl-en\">x</span>(<span class=\"pl-k\">new</span> <span class=\"pl-en\">Date</span>(<span class=\"pl-smi\">d</span>.<span class=\"pl-smi\">key</span>)) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">14</span>; })\n          .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>, <span class=\"pl-k\">function</span>(<span class=\"pl-smi\">d</span>) { <span class=\"pl-k\">return</span> workingHeight <span class=\"pl-k\">-</span> (workingHeight <span class=\"pl-k\">-</span> <span class=\"pl-en\">y</span>(<span class=\"pl-smi\">d</span>.<span class=\"pl-smi\">values</span>)); })\n          .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>width<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">24</span>)\n          .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>height<span class=\"pl-pds\">'</span></span>, <span class=\"pl-k\">function</span>(<span class=\"pl-smi\">d</span>) { <span class=\"pl-k\">return</span> workingHeight <span class=\"pl-k\">-</span> <span class=\"pl-en\">y</span>(<span class=\"pl-smi\">d</span>.<span class=\"pl-smi\">values</span>) });\n\n\n\n\n\n\nThe data we use in the `data` method is the `timesPerDay` array created above. The `.enter` method represents the elements that will be appended to the svg group. The elements that we will append are rectangles, `rect`. Basically, this allows us to loop through the array and places the bar on the graph depending on the x and y positions.\n\nThe x position for the bar uses the `x` function created above, the 'date' and '14' for some padding. We give it the date, because the x scaler that we created above will turn that into a pixel within the range that we can use.\n\nThe y axis starts from the top of the screen, so we set the y location of the bar to start at the highest value of the height (so it starts at the bottom of the screen), then subract from that value to move the bar up the screen. We move it up the height of the axis minus the number of candies that day. The height is the workingHeight minus the number of candies.\n\n\n\n### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#attach-the-axes)Attach the Axes\n\n\n\nThis just attaches the x axis to a svg group element\n\n\n\n\n\n    \n    <span class=\"pl-smi\">svg</span>.<span class=\"pl-en\">append</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>g<span class=\"pl-pds\">'</span></span>)\n        .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>x axis<span class=\"pl-pds\">'</span></span>)\n        .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>transform<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>translate(0, <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> workingHeight <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>)<span class=\"pl-pds\">'</span></span>)\n        .<span class=\"pl-c1\">call</span>(xAxis);\n\n\n\n\n\n\nAttach the y axis\n\n\n\n\n\n    \n    <span class=\"pl-smi\">svg</span>.<span class=\"pl-en\">append</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>g<span class=\"pl-pds\">'</span></span>)\n        .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>y axis<span class=\"pl-pds\">'</span></span>)\n        .<span class=\"pl-c1\">call</span>(yAxis);\n\n\n\n\n\n\n\n\n## [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-1.md#finish-the-javascript-code-and-html-page)Finish the JavaScript code and HTML page\n\n\n\nWe need to close the function we started above, and close the HTML script tag\n\n\n\n\n\n    \n    });\n    </<span class=\"pl-ent\">script</span>>\n    \n    </<span class=\"pl-ent\">body</span>>\n    </<span class=\"pl-ent\">html</span>>\n\n\n\n\n\n\nAnd there you have it. A bar graph showing how many candies I ate each day for the week after Halloween. [See it here.](http://mossiso.com/d3-instructions/eats.html)\n\nCheck here for all of the code together without comments: [https://github.com/mossiso/track-n-treat/blob/master/eats-no-comments.html](https://github.com/mossiso/track-n-treat/blob/master/eats-no-comments.html)\n\nNext up, a graph to show how many times a day and which candies I ate for each day.\n"},{"id":"2016-01-19-working-with-d3-part-2","title":"Working with D3, Part 2","author":"ammon-shepherd","date":"2016-01-19 07:03:46 -0500","categories":["Visualization and Data Mining"],"url":"working-with-d3-part-2","content":"# [![candies](http://scholarslab.org/wp-content/uploads/2016/01/candies.png)](http://scholarslab.org/wp-content/uploads/2016/01/candies.png)\n\n\n\n\n\n# When did I eat all those candies?\n\n\n\nThis second visualization will answer the above question, and also which candies I ate.\n\nThis visualization will show each day, and within each day it will show the time period that I had candy, and an image of the candy will designate what kind of candy, and how many at that time period.\n\n\n\n# [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-2.md#d3-code)D3 code\n\n\n\nThis uses the exact same data file as the previous chart, `data/track-n-treat.json`, but we build it in an entirely different way. We'll also spice it up with some images.\n\n\n\n## [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-2.md#begin-the-html-document)Begin the HTML document\n\n\n\nWe start this file much like the previous.\n\n\n\n\n\n    \n    <!DOCTYPE html>\n    <<span class=\"pl-ent\">html</span> <span class=\"pl-e\">lang</span>=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>en<span class=\"pl-pds\">'</span></span>>\n        <<span class=\"pl-ent\">head</span>>\n            <<span class=\"pl-ent\">meta</span> <span class=\"pl-e\">charset</span>=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>utf-8<span class=\"pl-pds\">'</span></span>>\n            <<span class=\"pl-ent\">title</span>>Track-n-Treat</<span class=\"pl-ent\">title</span>>\n            <<span class=\"pl-ent\">link</span> <span class=\"pl-e\">href</span>=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>styles.css<span class=\"pl-pds\">'</span></span> <span class=\"pl-e\">rel</span>=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>stylesheet<span class=\"pl-pds\">'</span></span> <span class=\"pl-e\">type</span>=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>text/css<span class=\"pl-pds\">'</span></span> />\n            <<span class=\"pl-ent\">link</span> <span class=\"pl-e\">href</span>=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>https://fonts.googleapis.com/css?family=Inknut+Antiqua<span class=\"pl-pds\">'</span></span> <span class=\"pl-e\">rel</span>=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>stylesheet<span class=\"pl-pds\">'</span></span> <span class=\"pl-e\">type</span>=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>text/css<span class=\"pl-pds\">'</span></span>>\n    <span class=\"pl-s1\">        <<span class=\"pl-ent\">script</span> <span class=\"pl-e\">type</span>=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>text/javascript<span class=\"pl-pds\">'</span></span> <span class=\"pl-e\">src</span>=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>d3/d3.v3.js<span class=\"pl-pds\">'</span></span>></<span class=\"pl-ent\">script</span>></span>\n        </<span class=\"pl-ent\">head</span>>\n        <<span class=\"pl-ent\">body</span>>\n          <<span class=\"pl-ent\">h1</span>>Track-n-Treat 2015</<span class=\"pl-ent\">h1</span>>\n          <<span class=\"pl-ent\">p</span>>Tracking how many candies I ate in the week after Halloween.</<span class=\"pl-ent\">p</span>>\n    \n          <<span class=\"pl-ent\">div</span> <span class=\"pl-e\">id</span>=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>wrapper<span class=\"pl-pds\">'</span></span>>\n            <<span class=\"pl-ent\">div</span> <span class=\"pl-e\">id</span>=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>chart<span class=\"pl-pds\">'</span></span>></<span class=\"pl-ent\">div</span>>\n          </<span class=\"pl-ent\">div</span>>\n\n\n\n\n\n\nA new line here that wasn't in the previous document is the 'link' tag pulling in a Google font. We'll add that as the default font, to give the text a more Halloweeny look.\n\nThe two HTML div tags are what the svg will be attached to. Having them allows the svg to scroll because the graph will be longer horizontally than the screen.\n\n\n\n### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-2.md#begin-the-javascript-and-d3)Begin the JavaScript and D3\n\n\n\nWe start the JavaScript and D3 code the same way as before.\n\n\n\n\n\n    \n    <span class=\"pl-s1\"><<span class=\"pl-ent\">script</span> <span class=\"pl-e\">type</span>=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>text/javascript<span class=\"pl-pds\">'</span></span>></span>\n    <span class=\"pl-s1\">  <span class=\"pl-smi\">d3</span>.<span class=\"pl-en\">json</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>data/track-n-treat.json<span class=\"pl-pds\">'</span></span>, <span class=\"pl-k\">function</span>(<span class=\"pl-smi\">data</span>) {</span>\n    <span class=\"pl-s1\">    <span class=\"pl-k\">var</span> w <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2500</span>;</span>\n    <span class=\"pl-s1\">    <span class=\"pl-k\">var</span> h <span class=\"pl-k\">=</span> <span class=\"pl-c1\">300</span>;</span>\n    \n    <span class=\"pl-s1\">    <span class=\"pl-k\">var</span> barW <span class=\"pl-k\">=</span> <span class=\"pl-c1\">60</span>;</span>\n    <span class=\"pl-s1\">    <span class=\"pl-k\">var</span> barH <span class=\"pl-k\">=</span> <span class=\"pl-c1\">55</span>;</span>\n\n\n\n\n\n\nWe start with the d3 method to pull in the json file. This includes a function that surrounds the rest of the d3 code.\n\nWe also set an arbitrary width and height. We'll use these variables later for the dimensions of the main svg element. We also set the width and height that will be used as a representation of a candy bar.\n\n\n\n### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-2.md#massaging-the-data)Massaging the data\n\n\n\nWe need to nest the data, and group the candy by day and time. We'll use the d3 'nest' method and set the new array to the variable 'nested'\n\n\n\n<blockquote>For more help on understanding nesting see: [http://bl.ocks.org/shancarter/raw/4748131/](http://bl.ocks.org/shancarter/raw/4748131/)</blockquote>\n\n\n\nNesting our data will reformat the array from looking like this:\n\n\n\n\n\n    \n     [\n       {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>day<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-01<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>time<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>17:25<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>candy<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Heath Bar<span class=\"pl-pds\">'</span></span>},\n       {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>day<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-01<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>time<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>17:25<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>candy<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Kitkat Bar<span class=\"pl-pds\">'</span></span>},\n       {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>day<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-01<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>time<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>20:25<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>candy<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Kitkat Bar<span class=\"pl-pds\">'</span></span>},\n       {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>day<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-02<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>time<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>17:38<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>candy<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Whopper<span class=\"pl-pds\">'</span></span>},\n       <span class=\"pl-k\">...</span>\n     ]\n\n\n\n\n\n\nTo look like this:\n\n\n\n\n\n    \n     [\n       { <span class=\"pl-s\"><span class=\"pl-pds\">'</span>key<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-01<span class=\"pl-pds\">'</span></span>,\n         <span class=\"pl-s\"><span class=\"pl-pds\">'</span>values<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> [\n                     {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>key<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>17:25<span class=\"pl-pds\">'</span></span>,\n                      <span class=\"pl-s\"><span class=\"pl-pds\">'</span>values<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> [\n                                  {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>candy<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Heath Bar<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>day<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-01<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>time<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>17:25<span class=\"pl-pds\">'</span></span>},\n                                  {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>candy<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Kitkat Bar<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>day<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-01<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>time<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>17:25<span class=\"pl-pds\">'</span></span>}\n                                ]\n                     },\n                     {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>key<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>20:25<span class=\"pl-pds\">'</span></span>,\n                      <span class=\"pl-s\"><span class=\"pl-pds\">'</span>values<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> [\n                                  {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>candy<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Kitkat Bar<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>day<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-01<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>time<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>20:25<span class=\"pl-pds\">'</span></span>}\n                                ]\n                     }\n                   ]\n       },\n       {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>key<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-02<span class=\"pl-pds\">'</span></span>,\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>values<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> [\n                    {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>key<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>17:38<span class=\"pl-pds\">'</span></span>,\n                     <span class=\"pl-s\"><span class=\"pl-pds\">'</span>values<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> [\n                                {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>candy<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Whopper<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>day<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2015-11-02<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>time<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">:</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>17:38<span class=\"pl-pds\">'</span></span>}\n                               ]\n                    }\n                  ]\n       }\n       <span class=\"pl-k\">...</span>\n     ]\n\n\n\n\n\n\nHere is the code to do that:\n\n\n\n\n\n    \n    <span class=\"pl-k\">var</span> nested <span class=\"pl-k\">=</span> <span class=\"pl-smi\">d3</span>.<span class=\"pl-en\">nest</span>()\n        .<span class=\"pl-en\">key</span>(<span class=\"pl-k\">function</span>(<span class=\"pl-smi\">d</span>) { <span class=\"pl-k\">return</span> <span class=\"pl-smi\">d</span>.<span class=\"pl-smi\">day</span>; })\n        .<span class=\"pl-en\">key</span>(<span class=\"pl-k\">function</span>(<span class=\"pl-smi\">t</span>) { <span class=\"pl-k\">return</span> <span class=\"pl-smi\">t</span>.<span class=\"pl-smi\">time</span>; })\n        .<span class=\"pl-en\">entries</span>(data);\n\n\n\n\n\n\nBasically, this makes a group of all the lines in the original array that have the same day. Then within that group, it makes a group for each time. It's like an onion, with different layers of data.\n\n\n\n### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-2.md#making-the-graph)Making the graph\n\n\n\n\n\n\n\n    \n    <span class=\"pl-k\">var</span> svg <span class=\"pl-k\">=</span> <span class=\"pl-smi\">d3</span>.<span class=\"pl-c1\">select</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>#chart<span class=\"pl-pds\">'</span></span>).<span class=\"pl-en\">append</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>svg<span class=\"pl-pds\">'</span></span>)\n        .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>graph<span class=\"pl-pds\">'</span></span>)\n        .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>width<span class=\"pl-pds\">'</span></span>, w)\n        .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>height<span class=\"pl-pds\">'</span></span>, h)\n      .<span class=\"pl-en\">append</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>g<span class=\"pl-pds\">'</span></span>)\n        .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>transform<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>translate(20,20)<span class=\"pl-pds\">'</span></span>);\n\n\n\n\n\n\nThis creates the actual SVG canvas/element where the graph will be displayed. Select the div element with an id of 'graph', append an svg tag/element, give it a CSS class of 'graph', and set the width to the 'w' variable and the height to the 'h' variable as we assigned above.\n\nFinally, append a group tag/element. This group is offset 20 pixels down and 20 pixels over from the original origin point. To better understand transform and translate read the section on SVG Transform as a Coordinate Space Transformation at[https://www.dashingd3js.com/svg-group-element-and-d3js](https://www.dashingd3js.com/svg-group-element-and-d3js)\n\nThe 20 pixels down and over are for padding, especially useful if there are axis lines and markers. We don't have axes in this visualization, but the padding is still useful.\n\n\n\n### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-2.md#doing-it-old-school)Doing it old school\n\n\n\nThis is where all of the magic really happens. The data is used to place the points on the graph, create the bars, etc. Instead of using built in d3 functions for looping through the data, we'll use default JavaScript control structures, the `forEach` loop.\n\nFirst we'll assign a default x position. This will determine the first position on the x-axis, which should start at 0.\n\n\n\n\n\n    \n    <span class=\"pl-k\">var</span> xPos <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>;\n\n\n\n\n\n\nLet's also declare some variables that we'll use later.\n\n\n\n\n\n    \n    <span class=\"pl-k\">var</span> prevDayTime, thisDayTime, thisDay, prevDay;\n\n\n\n\n\n\n\n\n#### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-2.md#loop-the-loop)Loop the loop\n\n\n\nNext we'll do a series of loops through the data, each loop associated with the groups created in the nested array that was created above.\n\nThe first level is the day object. The JavaScript 'forEach' is a relatively new built in function specifically for iterating over arrays.\n\nThe first time through 'forEach' returns an object where the 'key' is the date and the 'values' is an array of objects.\n\nIn the second 'forEach', we go through the 'values' of the first level. This returns an object, where the 'key' is the time and the 'values' is an array of objects that contain the date, time and candy.\n\nNext, the third 'forEach' of the levelTwo 'values' array gives us each instance when the candy was eaten.\n\nHere is the basic structure of the 'forEach' loops. We'll add more code in the next steps.\n\n\n\n\n\n    \n    <span class=\"pl-smi\">nested</span>.<span class=\"pl-en\">forEach</span>(<span class=\"pl-k\">function</span>(<span class=\"pl-smi\">dayObject</span>) {\n        <span class=\"pl-smi\">dayObject</span>.<span class=\"pl-smi\">values</span>.<span class=\"pl-en\">forEach</span>(<span class=\"pl-k\">function</span>(<span class=\"pl-smi\">timeObject</span>, <span class=\"pl-smi\">dayIndex</span>) {\n            <span class=\"pl-smi\">timeObject</span>.<span class=\"pl-smi\">values</span>.<span class=\"pl-en\">forEach</span>(<span class=\"pl-k\">function</span>(<span class=\"pl-smi\">candyObject</span>, <span class=\"pl-smi\">timeIndex</span>) {\n            });\n        });\n    });\n\n\n\n\n\n\nIn the function section of the last two 'forEach' methods, we create an object and an index. The 'index' for each of these is the index number of the array element. For the last 'forEach', the 'candyObject' contains the day, time and candy.\n\n\n\n#### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-2.md#making-the-graph-1)Making the graph\n\n\n\nThe first thing we'll need to do is create an svg group element to contain each day.\n\n\n\n\n\n    \n    <span class=\"pl-smi\">nested</span>.<span class=\"pl-en\">forEach</span>(<span class=\"pl-k\">function</span>(<span class=\"pl-smi\">dayObject</span>) {\n        dayGroup <span class=\"pl-k\">=</span> <span class=\"pl-smi\">svg</span>.<span class=\"pl-en\">append</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>g<span class=\"pl-pds\">'</span></span>);\n    \n      <span class=\"pl-smi\">dayObject</span>.<span class=\"pl-smi\">values</span>.<span class=\"pl-en\">forEach</span>(<span class=\"pl-k\">function</span>(<span class=\"pl-smi\">timeObject</span>, <span class=\"pl-smi\">dayIndex</span>) {\n        <span class=\"pl-smi\">timeObject</span>.<span class=\"pl-smi\">values</span>.<span class=\"pl-en\">forEach</span>(<span class=\"pl-k\">function</span>(<span class=\"pl-smi\">candyObject</span>, <span class=\"pl-smi\">timeIndex</span>) {\n\n\n\n\n\n\nThen we need to pull some information from that specific instance of eating a candy. We need the day and the day and time together. We don't need to declare these variables with `var` because we did that already above.\n\n\n\n\n\n    \n          thisDayTime <span class=\"pl-k\">=</span> <span class=\"pl-smi\">candyObject</span>.<span class=\"pl-smi\">day</span> <span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span> <span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span> <span class=\"pl-smi\">candyObject</span>.<span class=\"pl-smi\">time</span>;\n          thisDay <span class=\"pl-k\">=</span> <span class=\"pl-smi\">candyObject</span>.<span class=\"pl-smi\">day</span>\n\n\n\n\n\n\nAs we loop through each of the instances, we'll need to check several things.\n\n\n\n\n    \n  * Is this the last instance at this time?\n\n    \n  * Is this the last instance for this day?\n\n    \n  * Is this the same time and day, but a separate instance? (Sometimes I had two or three candy bars within the same minute, [ziggy-piggy](http://klipd.com/watch/bill--teds-excellent-adventure/ziggy-piggy-scene).)\n\n\n\nIf the current day and time is the same as the previous day and time, then the x position should be the same. Later in the code we add 25 to the current x position, so that the next time through the loop, the x position of the box is further away from previous box. If it's the same time and day, then we subtract that 25 we add on later to put it back to the same position as the previous time.\n\n\n\n\n\n    \n          <span class=\"pl-k\">if</span> (thisDayTime <span class=\"pl-k\">===</span> prevDayTime) {\n            xPos <span class=\"pl-k\">=</span> xPos <span class=\"pl-k\">-</span> barW <span class=\"pl-k\">-</span> <span class=\"pl-c1\">5</span>;\n          }\n\n\n\n\n\n\n\n\n#### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-2.md#the-candy-man-cometh)The candy man cometh\n\n\n\nNext we start creating the candy elements of the graph. We could use boring bars and boxes, or we could use images of actual fun size candy bars and those yummy fun size packets of M&M's and Reese's Pieces.\n\nWe can do this by appending an image to the `dayGroup` svg group element we created before.\n\n\n\n\n\n    \n          <span class=\"pl-smi\">dayGroup</span>.<span class=\"pl-en\">append</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>image<span class=\"pl-pds\">'</span></span>)\n              .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>xlink:href<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>images/<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-smi\">candyObject</span>.<span class=\"pl-smi\">candy</span> <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>.jpg<span class=\"pl-pds\">'</span></span>)\n              .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>bar<span class=\"pl-pds\">'</span></span>)\n              .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>, xPos)\n              .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>, h <span class=\"pl-k\">-</span> (barH <span class=\"pl-k\">*</span> timeIndex <span class=\"pl-k\">+</span> <span class=\"pl-c1\">190</span>))\n              .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>width<span class=\"pl-pds\">'</span></span>, barW)\n              .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>height<span class=\"pl-pds\">'</span></span>, barH);\n\n\n\n\n\n\nThe first `.attr` tells d3 where to find the image to use; the `xlink:href` states there is a path to follow. The next part is the path to the image. We use the name of the candy supplied from the `candyObject` object. All of the image files have the same name as those used in the data (the json file), so each different candy can use the specific image for that candy. So`candyObject.candy` returns 'Kitkat', or 'Snickers' or 'Twix', etc. The `+` plus signs in the code combine the string parts (those within the single quotes) with the variable.\n\nAll of the jpg files are in the 'images' directory, which is in the same directory where this file is.\n\nNext, assign the svg element a class of 'bar' so we can style it if we need to.\n\nThe x position is dealt with elsewhere. The first time through the loop, for the very first instance, it is zero. Subsequent instances have an increased x position, or the same x position if the time is the same as the previous time.\n\nThe next bit of trickery is where to place the image vertically. Remember the y axis starts from the top of the screen, so we set the y location of the candy bar box to start at the highest value of the height (so it starts at the bottom of the screen), then subtract from that value to move the box up the screen. Some time periods have multiple candies (I ate two or three within one minute), so their x position is the same, but the y position needs to be higher.\n\nTo do this we'll use the array's index number to provide a variable in a made up formula for increasing the height. We want the stacked candy boxes to start at a specific point, lets say 190 pixels above the bottom of the svg 'canvas' that we create. The first candy for a time will then start at '400 - (35 * 1 + 190)'. That means starting at the bottom of the screen '400', we'll subtract 190, so half way up the canvas, plus move it up 35 for some padding. Halfway up the canvas gives us room for the times and date bars. Subsequent candy boxes will be bumped up '35 * 2' or '35 * 3' providing a stacked look.\n\n\n\n<blockquote>Play with the numbers to see how the boxes move up or down.</blockquote>\n\n\n\n\n\n#### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-2.md#time-for-some-labels)Time for some labels\n\n\n\nWe need to add the time that each candy was eaten. We do this in a similar manner as adding the candy images, but appending text rather than images.\n\n\n\n\n\n    \n          <span class=\"pl-smi\">dayGroup</span>.<span class=\"pl-en\">append</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>text<span class=\"pl-pds\">'</span></span>)\n             .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>time<span class=\"pl-pds\">'</span></span>)\n             .<span class=\"pl-c1\">text</span>(<span class=\"pl-smi\">candyObject</span>.<span class=\"pl-smi\">time</span>)\n             .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>, xPos)\n             .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>, h <span class=\"pl-k\">-</span> (barW <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>))\n             .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>width<span class=\"pl-pds\">'</span></span>, barW)\n             .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>height<span class=\"pl-pds\">'</span></span>, barH);\n    \n\n\n\n\n\n\nThis is very similar to the above. We append a text element to the `dayGroup` element we created before. The `.text` line is where we add the text. The text is just the time, which we can get from the `candyObject`.\n\n`candyObject = {'day': '2015-11-xx', 'time': 'xx:xx', 'candy': 'Xxxx'}`\n\nEach `candyObject` object has this info associated with it. The `x`'s in the example above are replaced each time it goes through the loop.\n\nThe x position, `xPos`, is set and the same as the candy wrapper image.\n\nThe y position will be the same for each time period. We'll set it initially at the height of the 'canvas', so it starts at the very bottom. Then we'll subtract some amounts to get it to a good spot. We can use the width variable for a set number, and then let's multiply that by 2 for fun. We could have just used a static number, like '100', but this way the height adjusts to any changes in the other aspects of the graph. If we change how big the candy bar wrappers are, then the time labels automatically adjust.\n\n\n\n<blockquote>Play around with those numbers, replace the 'h' and 'barW' with actual numbers.</blockquote>\n\n\n\n\n\n#### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-2.md#a-box-for-each-day)A box for each day\n\n\n\nWe're placing something on the screen for the candy wrapper image and the time each time we loop through. For the instances when there are multiple candies at the same time, there are actually multiple times overlayed. They are in the exact same place, so you only see the one. This won't work for the boxes that represent the days. We only want one box.\n\nOne way to solve this is to only draw the day box the very first time an instance of that day comes up in our loop. This is where our `thisDay` and `prevDay` variables come into play. We can check the current date with the previous date. If the current date is the same as the `prevDate` variable, then it is the same day, so don't draw a box. The only time `thisDay` will not equal `prevDay` is when `thisDay`'s instance is the next day.\n\nHmm, that's really hard to explain, and this probably confuses more than clarifies.\n\n\n\n\n\n    \n          <span class=\"pl-c\">// add the day boxes</span>\n          <span class=\"pl-k\">if</span> (thisDay <span class=\"pl-k\">!==</span> prevDay) {\n            <span class=\"pl-smi\">dayGroup</span>.<span class=\"pl-en\">append</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>rect<span class=\"pl-pds\">'</span></span>)\n               .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>day<span class=\"pl-pds\">'</span></span>)\n               .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>, xPos)\n               .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>, h <span class=\"pl-k\">-</span> <span class=\"pl-c1\">110</span>)\n               .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>width<span class=\"pl-pds\">'</span></span>, (<span class=\"pl-smi\">dayObject</span>.<span class=\"pl-smi\">values</span>.<span class=\"pl-c1\">length</span> <span class=\"pl-k\">*</span> barW <span class=\"pl-k\">+</span> barW<span class=\"pl-k\">/</span><span class=\"pl-c1\">2</span>))\n               .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>height<span class=\"pl-pds\">'</span></span>, barH);\n    \n\n\n\n\n\n\nHere we append a rectangle to the `dayGroup` object, give it a CSS class, set the x position to `xPos`, and the y position to a set number. This time we took the lazy way and just subtracted a static number from the height.\n\nThe width is the tricky part here. We want the width of the bar to be the length of all of the time elements for that day. We can get the number of times for a day from the `dayObject` object. The key is the date, and the value is an array of times. So we can get the number of times in a day with dot notation and the array `length` method, `dayObject.values.length`. That gives us a single digit, so we need to multiply that by the width of the candy bar image, `dayObject.values.length * barW`. Then add some more for padding. We can dynamically get a number by using the candy bar image width divided by 2,`barW/2`. That gives us a number that approximates the width taken up by all of the time elements for a day.\n\nMaking the box is only half the job. We should add some text so we know which day it is that we are adding. Let's use the month and day in the format 'MMM DD', so the month name using three letters, and the two digit day of the month.\n\n\n\n\n\n    \n            dateForm <span class=\"pl-k\">=</span> <span class=\"pl-smi\">d3</span>.<span class=\"pl-smi\">time</span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>%b %d<span class=\"pl-pds\">'</span></span>)\n            <span class=\"pl-smi\">dayGroup</span>.<span class=\"pl-en\">append</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>text<span class=\"pl-pds\">'</span></span>)\n               .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>date<span class=\"pl-pds\">'</span></span>)\n               .<span class=\"pl-c1\">text</span>(<span class=\"pl-en\">dateForm</span>(<span class=\"pl-k\">new</span> <span class=\"pl-en\">Date</span>(thisDayTime)) )\n               .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>, xPos <span class=\"pl-k\">+</span> <span class=\"pl-c1\">5</span>)\n               .<span class=\"pl-en\">attr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>, h <span class=\"pl-k\">-</span> barW <span class=\"pl-k\">-</span> <span class=\"pl-c1\">10</span>);\n          }\n    \n\n\n\n\n\n\nTo get the date we'll use d3's default [time formating method](https://github.com/mbostock/d3/wiki/Time-Formatting), `d3.time.format`.\n\n`dateForm = d3.time.format('%b %d')` creates a function that will take a date object and return it in the format 'Month ##'\n\n`.text( dateForm(new Date(thisDayTime)) )`\n\nFrom the inside out: `thisDayTime` is the year-month-day and time variable we create above.\n\nWe use Javascript's `new Date()` function to return a string from the time.\n\nThen we use the `dateForm` function we created to format the date the way we want.\n\n\n\n#### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-2.md#make-a-little-buffer)Make a little buffer\n\n\n\nIf it's the last time of the day, and the last candy for that time, add in a space bumper. Without it the days are squished together. This adds a visually appealing space between the days.\n\n\n\n\n\n    \n          <span class=\"pl-k\">if</span> ( ( dayIndex <span class=\"pl-k\">==</span> (<span class=\"pl-smi\">dayObject</span>.<span class=\"pl-smi\">values</span>.<span class=\"pl-c1\">length</span> <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>) ) <span class=\"pl-k\">&&</span> ( timeIndex <span class=\"pl-k\">==</span> (<span class=\"pl-smi\">timeObject</span>.<span class=\"pl-smi\">values</span>.<span class=\"pl-c1\">length</span> <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>) ))\n          {\n            xPos <span class=\"pl-k\">=</span> xPos <span class=\"pl-k\">+</span> barW;\n          }\n\n\n\n\n\n\nWe use an if statement to check if it's the last day and the last time for that day. If so we need to increase the x position by the width of the candy bar image. Otherwise it will be at the same position as the last time. We don't need to add an actual svg element, just increase the x position for the next time through the loop.\n\n\n\n#### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-2.md#increment-the-incrementables)Increment the incrementables\n\n\n\nFinally, we need to increment the x position and set the `prevDayTime` variable to `thisDayTime` and the `prevDay` variable to`thisDay`.\n\n\n\n\n\n    \n          xPos <span class=\"pl-k\">=</span> xPos <span class=\"pl-k\">+</span> barW <span class=\"pl-k\">+</span> <span class=\"pl-c1\">5</span>;\n          prevDayTime <span class=\"pl-k\">=</span> thisDayTime;\n          prevDay <span class=\"pl-k\">=</span> thisDay;\n    \n\n\n\n\n\n\n\n\n### [](https://github.com/mossiso/track-n-treat/blob/master/D3-Instructions-Part-2.md#finish-it)Finish it\n\n\n\nClose off the `forEach` loops\n\n\n\n\n\n    \n                    });\n                  });\n                });\n\n\n\n\n\n\nClose off the encompassing d3 function, and close off the HTML tags to finish of the file.\n\n\n\n\n\n    \n              });\n            </<span class=\"pl-ent\">script</span>>\n        </<span class=\"pl-ent\">body</span>>\n    </<span class=\"pl-ent\">html</span>>\n\n\n\n\n\n\nYou can see the visualization here: [http://mossiso.com/d3-instructions/day.html](http://mossiso.com/d3-instructions/day.html)\n\nAnd get the code without comments here: [https://github.com/mossiso/track-n-treat/blob/master/day-no-comments.html](https://github.com/mossiso/track-n-treat/blob/master/day-no-comments.html)\n"},{"id":"2016-02-02-apply-for-2016-2017-graduate-fellowship-in-digital-humanities","title":"Apply for 2016-2017 Graduate Fellowship in Digital Humanities","author":"purdom-lindblad","date":"2016-02-02 04:48:47 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"apply-for-2016-2017-graduate-fellowship-in-digital-humanities","content":"The Scholars’ Lab is proud to announce that applications for our prestigious [Graduate Fellowship in the Digital Humanities](http://scholarslab.org/graduate-fellowships/) are being accepted for the 2016-2017 academic year. Applications are due **February 26, 2016**.\n\nThe fellowship supports ABD graduate students doing innovative work in the digital humanities at the University of Virginia. The Scholars’ Lab offers Grad Fellows advice and assistance with the creation and analysis of digital content, as well as consultation on intellectual property issues and best practices in digital scholarship and DH software development.\n\nFellows join [our vibrant community](http://scholarslab.org/people/), have a voice in intellectual programming for the Scholars’ Lab, make use of our dedicated grad office, and participate in one formal colloquium at the Library per semester.\n\nSupported by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew & Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities, the Graduate Fellowship in Digital Humanities is designed to advance the humanities and provide emerging digital scholars with an opportunity for growth.\n\n**Eligibility**\nApplicants must be ABD, having completed all course requirements and been admitted to candidacy for the doctorate in the humanities, social sciences or the arts at the University of Virginia.\n\nApplicants must be enrolled full time in the year for which they are applying.\n\nA faculty advisor must review and approve the scholarly content of the proposal.\n\n_Applicants are strongly encouraged to have Praxis Program or equivalent experience_. Experience can include work on a collaborative digital project, comfort with programing and code management, public scholarship, and critical engagement with digital tools.\n\n**How to Apply**\nEmail a complete application package will include the following materials to [Purdom Lindblad](mailto:jpl8e@virginia.edu):\n\n\n\n\t\n  * a cover letter, addressed to the selection committee;\n\n\t\n  * [a Graduate Fellowship Application Form](http://scholarslab.org/wp-content/uploads/2016/02/dhfellowsappform.pdf);\n\n\t\n  * a dissertation abstract;\n\n\t\n  * a summary of the applicant’s plan for use of digital technologies in his or her dissertation research;\n\n\t\n  * a summary of the applicant’s experience with digital projects;\n\n\t\n  * a description of UVa library digital resources (content or expertise) that are relevant to the proposed project;\n\n\t\n  * and 2-3 letters of nomination and support, at least one being from the applicant’s dissertation director.\n\n\n**Deadline: February 26, 2016\nNotifications: March 31, 2016**\n\nQuestions about Grad Fellowships and the application process should be directed to [Purdom Lindblad](mailto:jpl8e@virginia.edu), Head of Graduate Programs at the Scholars’ Lab.\n"},{"id":"2016-02-02-bigger-nozzles-faster-printing","title":"Bigger nozzles, faster printing","author":"shane-lin","date":"2016-02-02 18:39:56 -0500","categories":["Makerspace"],"url":"bigger-nozzles-faster-printing","content":"This week at the SLab Makerspace, we've been experimenting with faster 3D printing at lowering resolutions with larger extruder nozzles.\n\nThe diameter of the standard Ultimaker 2 nozzle/block assembly is 0.4mm. When we recently installed Anders Olsson's upgraded heater block (after the stock thermocouple end came off inside of our OEM heater block), we gained the ability to swap in E3D compatible nozzles. The Olsson block itself came with an assortment of sizes, from 0.25mm to 0.8mm.\n\nThe flow rate of Cura's normal quickprint settings with the 0.4mm nozzle is **2mm^3/s**.  This figure is calculated by multiplying together the nozzle diameter, the layer height, and the print speed (0.4mm x 0.1mm x 50mm/s), basically the volume of a line of filament deposited onto a model or build platform. With the stock nozzle, Cura will produce a warning when trying to extrude more than 8mm^3/s. With stock configuration, our Ultimaker 2 worked most reliably printing PLA at around 5-6mm^3/s or less at 210C temperature.\n\nAfter installing the 0.8mm filament, we've been able to push this up to **20mm^3/s** and potentially higher by printing at 0.5mm layer height at 50mm/s. On a practical level, this means that we can now print a low resolution version of this cute octopus model in 27 minutes instead of the 3 hours estimated under our previous settings. Printing with substantially higher layer height required that we recalibrate the bed to a somewhat farther distance, but was otherwise very straightforward. The filament used here is Colorfabb Leaf Green PLA/PHA, which we've had very good luck with.\n\n![2016-02-01 18.47.43](http://scholarslab.org/wp-content/uploads/2016/02/2016-02-01-18.47.43.jpg)![2016-02-01 18.47.43](http://scholarslab.org/wp-content/uploads/2016/02/2016-02-01-18.47.43-300x225.jpg)\n\nThe lower z-resolution is obvious in these prints, but I think we can use the larger nozzle with shorter layer heights to achieve less pronounced quality differences, still with a modest gain in speed, because the reduction in X-Y axis resolution does not seem to be as dramatic. The larger nozzle diameter will offer faster print speeds and allow more efficient toolpaths. The standard 0.8mm walls will now only require a single pass to lay down.\n\nI think this is a case where the larger 2.85mm filament offers some much-needed leeway. The feeder would have to insert the thinner 1.75mm filament at nearly 10mm/s to keep up with this kind of flow rate; I'm not at all confident that our Replicators would be up to that.\n\nIn the coming weeks, we'll try to push the flow even higher and experiment with improving quality with thin-layer wide-nozzle settings. We'll also be testing prints with extremely fine detailing using the 0.25mm nozzle. In any case, the ability to easily change out nozzles on our UM2 opens up the possibility of tailoring our printer to the specific needs of the print. Pushing speed limits will allow us to produce truly rapid prototypes and to demonstrate the basic operation of 3D printers to many more students over the course of the day.\n"},{"id":"2016-02-02-ready-for-praxis-apply-by-february-26-for-the-2016-2017-cohort","title":"Ready for Praxis? Apply by February 26 for the 2016-2017 cohort","author":"purdom-lindblad","date":"2016-02-02 04:37:47 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"ready-for-praxis-apply-by-february-26-for-the-2016-2017-cohort","content":"UVa grad students! Apply by  **February 26** for a unique, funded opportunity to work with an interdisciplinary group of your peers, learning digital humanities skills from the experts, and collaboratively designing and executing an innovative digital project. The 2015-2016 Praxis cohort is in full swing, thanks to a generous support by [UVa Library](http://www.library.virginia.edu) and [GSAS](http://gsas.virginia.edu).\n\nEach year, the Scholars’ Lab [Praxis Program](http://praxis.scholarslab.org) provides six UVa graduate students with hands-on experience in digital humanities collaboration and project creation. Team members work to take a shared project from conception to design and development, and finally to publication, community engagement, and analysis. Praxis is a unique and well-known training program in the international digital humanities community. Our fellows blog about their experiences and develop increased facility with project management, collaboration, and the public humanities, even as they tackle (most for the first time, and with the mentorship of our  faculty and staff) new programming languages, tools, and digital methods. Praxis aims to prepare fellows with digital methodologies to apply both the the fellowship project and their future research.\n\nIn 2012-2013, the Scholars’ Lab joined with like-minded institutions to create the [Praxis Network](http://praxis-network.org), made up of allied but differently-inflected humanities education initiatives from around the world, mainly focused on graduate training, and all engaged in rethinking pedagogy and campus partnerships in relation to the digital humanities. The Praxis Network Student Directory showcases Praxis Program alumni have traveled diverse career paths, including tenure-track teaching positions and digital humanities positions within academic libraries and research centers.\n\nWe will welcome six new, competitively-selected Praxis students in late August 2016. _**The Praxis fellowship replaces teaching responsibilities for the academic year. Fellows are expected to devote approximately 10 hours per week in the fall and spring semesters**_, to learning together and building a collaborative digital humanities project in the Scholars’ Lab. Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, and can make use of a dedicated grad lounge.\n\nAll University of Virginia graduate students working within or committed to humanities disciplines are eligible to apply to join the 2016-17 cohort. We particularly encourage applications from women, LGBT students, and people of color, and will be working to put together an interdisciplinary and intellectually diverse team.\n\nThe application process for Praxis is simple! You apply individually, and we assemble the team, through a process that includes group interviews (_which will be scheduled for 3/7/2016, 3/9/2016, and 3/11/2016_) as well as input from peers.\n\nTo start, we only ask for a letter of intent, sent to [Purdom Lindblad](mailto:jpl8e@virginia.edu). The letter should include:\n\n- the applicant’s research interests;\n- summary of the applicant’s plan for use of digital technologies in your research;\n- summary of what skills, interests, methods the applicant will bring to the Praxis Program\n- summary of what the applicant hope to gain as a Praxis Fellow\n\n\nQuestions about Praxis Fellowships and the application process should be directed to: [Purdom Lindblad](mailto:jpl8e@virginia.edu)\n"},{"id":"2016-02-11-welcome-alison-booth","title":"Welcome, Alison Booth!","author":"laura-miller","date":"2016-02-11 04:03:10 -0500","categories":["Announcements","Digital Humanities"],"url":"welcome-alison-booth","content":"We have some big news! The Scholars' Lab is thrilled to welcome Alison Booth as our new academic director!\n\n\n\n\nProfessor Alison Booth, a Department of English faculty member and a preeminent scholar of digital humanities, has been appointed academic director of the University of Virginia’s Scholars’ Lab, Arts & Sciences Dean Ian Baucom and Dean of Libraries Martha Sites announced Monday.\n\n[caption id=\"attachment_12492\" align=\"alignright\" width=\"300\"]![ABooth](http://scholarslab.org/wp-content/uploads/2016/02/ABooth-300x206.jpg) Professor Alison Booth, the new academic director of the Scholars’ Lab[/caption]\n\n\n\n\nLocated in Alderman Library, the Scholars’ Lab provides students and researchers from across disciplines opportunities to partner with expert staff as they explore the digital humanities, geospatial information, and other scholarly discoveries at the intersection of digital, textual and physical worlds.\n\n“We are resetting the director of the Scholars’ Lab to align more closely with faculty research interests” Sites said. “Professor Booth will join the College and the Library in building a digital humanities environment at UVA that will spur innovative scholarship and exciting collaborations.”\n\nBooth has been a faculty member at the University of Virginia since 1986. In addition to digital humanities, her specialties include transatlantic Victorian studies, biography, and women’s history. Booth also directs Collective Biographies of Women (CBW), a digital project based on her book, _How to Make It as a Woman: Collective Biographical History from Victoria to the Present_ (University of Chicago Press). With a National Endowment for the Humanities startup grant in the Institute for Advanced Technology in the Humanities (IATH), the CBW project is developing tools and visualizations for analyzing the narratives and the networks of documents that represent historical women.\n\nBooth’s books include _Greatness Engendered: George Eliot and Virginia Woolf_ and “Homes and Haunts: Touring Writers’ Shrines and Countries” (forthcoming, Oxford University Press, 2016). She is the editor of _Famous Last Words: Changes in Gender and Narrative Closure_, and the Longman Cultural Edition of _Wuthering Heights_, as well as co-editor of the Norton _Introduction to Literature_ (10th edition).\n\n\n\n\n“We are fortunate to have Alison at the helm of the Scholars’ Lab,” said Baucom. “Her leadership will forge connections and inspire the type of creative research that advances knowledge within and beyond the liberal arts.”\n\nBooth will begin her term as director of the Scholars’ Lab immediately and will balance this appointment with her existing responsibilities in the College.\n\n“This is a wonderful chance to refresh UVA’s pioneering role in a fast-paced field, said Booth. “The Scholars’ Lab in the Library is the right place to draw in students, fellows, and faculty for interdisciplinary collaborations across Grounds. I look forward to working with the Institute for Advanced Technology in the Humanities (IATH), Sciences, Humanities & Arts Network of Technological Initiatives (SHANTI), Advanced Research Computing Service (ARCS), and all the units and leaders who have a stake in advanced computing for research and learning.”\n\n\n\n\n\n"},{"id":"2016-02-22-3d-printing-in-the-classroom-course-assignments-and-the-makerspace","title":"3D Printing in the Classroom: Course Assignments and the Makerspace","author":"jennifer-grayburn","date":"2016-02-22 05:08:01 -0500","categories":["Digital Humanities","Makerspace"],"url":"3d-printing-in-the-classroom-course-assignments-and-the-makerspace","content":"_[Cross-posted on my personal blog. ](http://jennifernicolegrayburn.com/2016/02/22/3d-printing-in-the-classroom-course-assignments-and-the-makerspace/)_\n\nDuring the first week of the spring semester, the [Makerspace](http://scholarslab.org/makerspace/) was a flurry of activity, our Ultimaker 2 printing feverishly throughout the day. Groups of students came in and out, selecting and slicing models, checking on their 3D prints, and assembling different components. This week marked the beginning of a new semester-long assignment created by Slavic Librarian Kathleen Thompson, Slavic Lecturer Jill Martiniuk, and myself to better integrate 3D fabrication and physical computing resources in the classroom. After months of discussing course objectives and possible digital components, including website creation, Arduino programming, and 3D printing, Kathleen and Jill determined that 3D printing would allow students to engage physically with 'icons' of Russian identity for Yuri Urbanovich's 'Understanding Russia: Symbols, Myths, and Archetypes of Identity' course and to think critically about how such icons are made, circulated, and contextualized/decontextualized.\n\nLogistically, this assignment only required that we block off enough time for the students to stop by and print their selected models (mostly selected from [Thingiverse](http://www.thingiverse.com/)). Most the students had never been to the Makerspace before and this offered a great opportunity to introduce the equipment and resources to a new audience. It was clear that the process and time required to make these objects encouraged the students to think critically about their objects, with some questioning whether or not an object was \"too negative\" or even if certain models they selected would print properly. The students were able to control various aspects of their prints, including filament color, print scale, and assembly options, and many students took this opportunity to think symbolically about what and how their object communicated. As students have the opportunity to change or modify their prints later in the semester, they will again be confronted with how these features contribute to its message. I asked Kathleen to share more about her assignment, the implementation of this first phase, and how the students applied their interpretations in the classroom:\n\n\"Bringing Symbols to Life\" is intended to provide students with new ways of examining and expressing the symbolic world of Russian self-identification. Symbols take on new meaning when we can actually manipulate and interact with them, rather than simply seeing them on a page. Interacting with an object allows us to differentiate between ‘symbol’ as an abstract and ‘object’ as a thing; if an object is decontextualized and recontextualized, is it still a symbol of Russianness? What assumptions do we make about symbols of Russianness, and how can we challenge those assumptions?\n\n[caption id=\"attachment_12588\" align=\"alignleft\" width=\"225\"]![2016-01-26 13.21.57](http://scholarslab.org/wp-content/uploads/2016/02/2016-01-26-13.21.57-225x300.jpg) The 'scales of justice' as it prints. The students also printed two cubes--one white, one red--to represent conflicting forces on each side of the scale.[/caption]\n\nTo answer these questions, groups of 6-7 students collaborated to choose and print a 3D object that, to them, represents Russia. We chose to have students print 3D objects, rather than simply view photographs or slides, because we suspected that having a tangible physical object that they would have to create and then handle would give them a new perspective on how Russians think of themselves. They presented their objects during the second week of class, along with a brief justification of the object they chose and its usefulness as a symbol of Russian identity. The six groups chose the following objects: a bust of Joseph Stalin, a Rubik'scube, the onion dome of an Orthodox cathedral, a Soyuz rocket, St. Basil's cathedral, and a set of unbalanced scales containing a white box and a red box on each side. Having the items present in class, and able to be touched and passed around, fostered what we think was a more robust discussion of identity than a set of images on a page -- for example, being able to see up close the many intricate parts of St. Basil's cathedral that make what one group called it a symbol of \"chaotic Russia\", where cultures and traditions coexist yet also clash, seemed more meaningful than simply looking at it on a screen. The Rubik's cube and the scales spurred the most discussion, as students noted not only the scale's metaphorical representation of a power imbalance between Russia and the West, but also its literal imbalance and fragility as a model (it had to be handled very carefully due to its delicate construction). The Rubik's cube, which was meant to represent what its group called the \"polyperipheral\" environment, history, and geography, inspired students to discuss scientific and mathematical components of Russian identity and the ever-present Russian fear of feeling \"left behind\" in technological advancements.\n\n[caption id=\"attachment_12591\" align=\"alignright\" width=\"225\"]![2016-01-25 17.14.42](http://scholarslab.org/wp-content/uploads/2016/02/2016-01-25-17.14.42-225x300.jpg) St. Basil's Cathedral, iconic landmark in Moscow.[/caption]\n\n[caption id=\"attachment_12590\" align=\"alignright\" width=\"225\"]![2016-01-25 17.15.57](http://scholarslab.org/wp-content/uploads/2016/02/2016-01-25-17.15.57-225x300.jpg) The onion dome of St. Basil's Cathedral before assembly.[/caption]\n\nStudents are currently designing and curating two physical exhibits of their objects: one in the Makerspace in Alderman Library, and another in the Slavic Department's hallway on the second floor of New Cabell Hall. Since creating two exhibits means that objects have to be printed twice, students may alter (or switch entirely) their objects if they choose, provided they give justification. The Rubik's cube group, for example, expressed interest in re-printing their cube to make the parts movable, which they said would better reflect the changes that \"polyperipheral\" Russia has experienced especially in the last twenty years. Throughout the semester, students will also revisit their objects to examine how they might function as expressions of Russian identity in various contexts, and at the end of the semester, they will write and present longer reflections on their objects as useful symbols.\n\nStarting February 25th, all printed icons of the \"Bringing Symbols to Life\" exhibition will be on display in the Slavic Department display case, located outside the Slavic Department offices in 258 New Cabell Hall.\n\n\n\n\n\n\n\n\n\n\n"},{"id":"2016-03-02-teaching-archaeology-of-the-middle-east-in-the-time-of-daesh-the-merits-of-incorporating-allahyaris-material-speculation-with-3d-printing","title":"Teaching Archaeology of the Middle East in the Time of Daesh: the Merits of  Incorporating Allahyari’s “Material Speculation” with 3D Printing","author":"jennifer-grayburn","date":"2016-03-02 08:58:35 -0500","categories":["Digital Humanities","Grad Student Research","Makerspace"],"url":"teaching-archaeology-of-the-middle-east-in-the-time-of-daesh","content":"_[Cross-posted on my personal blog.](http://jennifernicolegrayburn.com/2016/03/02/teaching-archaeology-of-the-middle-east-in-the-time-of-daesh-the-merits-of-incorporating-allahyaris-material-speculation-with-3d-printing/)_\n\nAnthropology Ph.D. Candidate [Sue Ann McCarty](http://anthropology.virginia.edu/gradstudents/profile/sm9eb) frequently visits the Makerspace to print archaeological artifacts. Over multiple conversations, we've discovered that we share a similar passion for 3D modeling and printing in the classroom. Sue Ann recently applied her research to a course she taught at James Madison University, and I asked her to share more about the benefits and challenges of integrating digitally-oriented assignments and methods in the classroom:\n\nThe adjoining regions of southeastern Turkey, northern Syria and northern Iraq—in which my dissertation research is focused—share a rich musical tradition of lamentations within a genre known in Turkish as the _uzun hava_: long, winding melodic meditations on suffering and loss that express pain as a heartfelt wail of mourning that nevertheless remains beautiful. This musical tradition has never been more apt than in 2015 when the Islamic State (better known under the acronyms ISIS, ISIL or, as used here, Daesh) systematically targeted the region’s cultural heritage sites for destruction and broadcast their actions globally as a form of promotional propaganda. Archaeological sites that were first excavated in the 1870s such as the Assyrian cities of Nimrud and Nineveh, the Mosul Museum and more recent religious shrines from both the Muslim and Syriac Christian traditions have all been looted, severely damaged or destroyed.\n\nGiven these recent events, I found myself faced with a challenge during the Fall 2015 semester: how do you engage students in an emotionally wrenching topic…particularly when the subjects of study stand a decent chance of being destroyed before your eyes during the semester? How do you teach a class focused on the archaeology of a region where—just before the beginning of the semester—the elderly archaeologist of Palmyra Khaled al-Asaad was publicly beheaded by Daesh; how can you best honor his memory and the memory of all the other lives lost in the current regional conflict? Should the on-going destruction even be discussed in a class focused on the ancient world, and, if so, how? These were the questions I faced while re-writing the syllabus for my Fall 2015 Archaeology of the Middle East class.\n\nI discovered that destruction of memory is one of the key themes underlying these acts—and the one that ultimately provided a lifeline for my class. While planning my syllabus I encountered the [“Material Speculation” project of artist Morehshin Allahyari.](http://www.morehshin.com/2015/05/25/material-speculation-isis/) Ms. Allahyari’s creative and insightful approach to the topic uses photogrammetry—reconstructed 3D scans of photos and objects—as well as 3D printing to reproduce artifacts destroyed by Daesh in the Mosul Museum and at archaeological sites like Palmyra, Hatra, Nineveh and Nimrud. Each 3D printed reproduction is embedded with a memory card that contains information about the original context of the artifact. In an ironic and clever twist, Ms. Allahyari notes that the plastic from which each new reproduction is printed is a petroleum product and that control of petroleum resources is one of the main sources of conflict in the region. (It’s worth noting that the PLA—Polylactic Acid—plastic that is used in many small-scale 3D printers is eco-friendly and made of cornstarch.) Ms. Allahyari plans to make public the materials necessary for anyone who can gain access to a 3D printer to create, re-create and remember the lost cultural heritage of Syria and Iraq; she posted her first public plan in February 2016. With “Material Speculation” the artifacts destroyed by Daesh are detached from their original material boundaries; they become infinitely reproducible, capable of being distributed globally and accessed across cultural, linguistic and economic strata. In an age of refugees, the destroyed art itself has become detached from its homeland. Printing is an empowering but also peaceful response to violent acts of intolerance.\n\n[caption id=\"attachment_12620\" align=\"alignleft\" width=\"300\"]![151119_007](http://scholarslab.org/wp-content/uploads/2016/03/151119_007-300x225.jpg) Peyton Fitzgerald and Emilie Gregory learn how to use the printing software.[/caption]\n\nMs. Allahyari’s work inspired me to incorporate a 3D printing project into my class. At the end of the semester—by which time, I grimly assumed, more sites would probably have been destroyed by Daesh—student “excavation teams” would each be responsible for printing objects from a specific damaged site and would present an “excavation report” in the style of a conference paper describing its history and damage. How were we going to do the 3D printing, though?\n\nAlthough I am a doctoral candidate in the University of Virignia’s Department of Anthropology—working with Pati Wattenmaker at the UVA excavations at Kazane Höyük in Şanlıiurfa, Turkey, 30 miles from the Syrian border post at Akçakale/Sabi Abyad—I was teaching my Archaeology of the Middle East class as an adjunct instructor at [James Madison University](http://www.jmu.edu/) in Harrisonburg. This presented a unique opportunity to observe the pedagogical benefits and problems associated with two very different ways of incorporating 3D printing into student life as well as two different 3D printing labs: [UVA’s Makerspace in Alderman Library’s Scholar’s Lab](http://scholarslab.org/makerspace/) and [James Madison University’s 3SPACE Lab](https://sites.jmu.edu/3SPACE/), each of which generously contributed to my efforts. I realized throughout the semester that each school’s 3D printing approach has its benefits and challenges.\n\nAt both schools 3D printers are located in a number of different nodes around their respective campuses—most notably, in their engineering schools, their health sciences departments and in a facility accessible to undergraduate College of Arts and Sciences (UVA) or General Education (JMU) students. There is no single resource at either school for finding out where 3D printers are located, to whom they are accessible or how to access them.\n\n[caption id=\"attachment_12618\" align=\"alignright\" width=\"300\"]![151119_012](http://scholarslab.org/wp-content/uploads/2016/03/151119_012-300x225.jpg) Front row: Dominic Traver, Alexandra Bowen, Brianna McDonald, Samantha Hill and Megan Walmsley plan their printing projects; back left, standing Center for Instructional Technology Instructional Designer Jamie Calcagno-Roach trains Emilie Gregory and Peyton Fitzgerald how to use the printing software. Also pictured, Hannah Sullivan, Chris Molloy, Carlisa Childress and Colton Wells talking about their printing plans.[/caption]\n\nThe accessibility and security of the 3D printers are approached very differently at UVA and JMU. At UVA, the most easily-accessible printers are located in the Alderman Library Scholar’s Lab Makerspace under the wider umbrella of the Digital Humanities. Because it is part of a publically-accessible research library, the Makerspace ethos encourages casual experimentation and creativity. It is accessible for drop-in users or by appointment as part of the regular services provided by the library. At the moment, it contains one Ultimaker and 2 Makerbot printers (with more varieties on the way) and individuals using them can book printers for multiple days to print if needed. (This is useful because complex 3D printing projects can take a day or more to print fully.) _All_ 3D printers at the current stage in their technological development are glitchy; being able to monitor whether they’re working and fix printing errors or problems encourages a successful print. One of the wildly knowledgeable and friendly Makerspace staff members is always present to answer questions and fix the machines if necessary. The open access to the Makerspace means that students can drop by at their convenience between 1:00 and 5:00 (or by appointment at other times).\n\nBy contrast, JMU’s printing lab—located in Burruss Hall in the center of campus—is locked when not in use and is only accessible during scheduled classes, by special appointment when an attendant is present or during 3D printing club weekend hours. This is partly a product of security concerns due to the number of computers and printers present in the room. It is also a product of safety concerns, because printer users are required to be formally trained by someone who can inform them of the hazards associated with the machines (like the very hot printing tips). Unlike the Makerspace, JMU’s printing lab is designed for entire classes of students to use the printers simultaneously in a classroom setting; it includes nine Apple computers attached to Affinia printers and another printer for more detailed varieties of printing. (My class really had the maximum number of people possible present in the lab with 3-4 students at each printer for our training session.) All users must receive at least one class session of formal training in machine function and safety before using the printers. This classroom space can accommodate a larger number of students at the same time than the Makerspace is able to handle; however, printing projects can only be created during the hours when the lab is open. This limits users to projects that can be printed in an hour or two unless someone is available to unlock the lab at a later time. The amount of PLA printing filament used in the 3SPACE lab is also more tightly controlled than the Makerspace; students are required to weigh their projects and report the weight of the plastic used after each session so that the cost of supplies can be accounted for due, in part, to the higher volume of students using the machines. Large or detailed individual projects that aren’t for specific classroom purposes might involve a fee; semester-long classes in the lab include a lab materials fee for spools of PLA filament.\n\n[caption id=\"attachment_12619\" align=\"alignleft\" width=\"300\"]![151119_008](http://scholarslab.org/wp-content/uploads/2016/03/151119_008-300x225.jpg) Front to back on right: David Szady, Ximena Calvo and Catherine Grimes wait for their first prints to start in the JMU 3SPACE Lab.[/caption]\n\nFrom a pedagogical perspective, the differences between these print labs presented both opportunities and challenges—and I made a number of very significant mistakes of my own along the way. Chief among the latter was the assumption when I designed the course syllabus that students would have easy access outside of classroom hours to JMU’s 3SPACE printers in order to print their projects, particularly if the individual object was complicated and would take a long time to print. I incorrectly assumed that they had the same relatively open-access policy that the UVA Makerspace applied to its printers. This meant that I had to schedule extra hours—as it turned out, on a Sunday—for the students to print their group projects. I also had to scale down the requirements for the final group printing project as a result. The size of my class and its need for extended formal training were also issues that I should have anticipated through better coordination with the friendly and helpful people at JMU’s very interesting and supportive [Center for Instructional Technology](https://cit.jmu.edu/) and the [Institute for Visual Studies and Math](https://www.jmu.edu/ivs/), which run the 3SPACE Lab collaboratively. I am grateful for the patience, enthusiasm and dedication of the staff members in both the Center for Instructional Technology and in the Scholar’s Lab, especially Jamie Calcagno-Roach, Jennifer Grayburn and Shane Lin.\n\nFrom the students’ perspectives, how did the 3D printing project turn out? From the first class, a number of skeptical students stayed enrolled specifically in order to learn 3D printing. One of the great joys of the semester was listening to their excitement during our first 3SPACE training session as they talked with one another about plans for their own future projects, dreaming up new ideas for fun things they could print, both personal and academic. Students were actively dreaming of new ways that 3D printing might be beneficial for archaeological use such as creating new comparative collections of animal bones for zooarchaeologists, making artifacts from museum collections more widely available for indigenous peoples who approve their distribution, teaching stratigraphy by printing each layer of an excavation separately, reconstructing features or just simply allowing the public to inspect a reproduction of an artifact up close in a way that might damage the original.\n\n[caption id=\"attachment_12616\" align=\"alignright\" width=\"300\"]![151119_013](http://scholarslab.org/wp-content/uploads/2016/03/151119_013-300x225.jpg) Carlisa Childress shows off her first 3D printed object, a JMU logo.[/caption]\n\nThe final student projects focused on the badly damaged ancient cities of Dura Europos, Nineveh, Nimrud, and Palmyra. I had hoped originally for students to print objects derived from these specific sites. However, Ms. Allahyari’s “Material Speculation” plans were still unavailable and there are relatively few other 3D printing plans for relevant objects or buildings published. In the last weeks we had to generalize and print whatever was available (for example, the Temple of Baalshamin and the Temple of Baal from Palmyra courtesy  of [Thingiverse](http://www.thingiverse.com/), as well as a winged human-headed lion lamassu sculpture from the British Museum’s 3D printing collection). The students were frustrated that they didn’t get more time in the lab. (At the end of the semester we only managed to have two printing sessions, one of which had to be scheduled for a Sunday morning so not everyone could come.) We were also unable to print individual objects for each student by the end of the semester. (Due to our final time and space limitations, I required one print per “excavation team”.)\n\nPerhaps the most important question is—was there an actual educational benefit to incorporating 3D printing into the class (other than that it was _fun_)? Did the students understand the intellectual connection between 3D printing, Daesh’s cultural heritage destruction and the reproduction of memory? As the museum exhibition artist Gary Staab recently said in a Smithsonian Magazine article that described an exhibition model in which he combined his own sculpting with 3D printing to recreate the Neolithic mummy known as Ötzi, “I also find the physical act of making stuff is such a great memory aid. If you want to learn something, you draw it. If you want to know it, you sculpt it. If you have to physically make it in three-dimensions, that burns it into your memory and those facts stay hard and fast,” (Wei-Haas 2016). Three-D printable objects are mnemonic devices, acting as tactile, visceral, ontological connections to their progenitors while also incorporating something new: the labor of the student who reproduces, remembers, touches and observes these objects, physical phantoms of their former selves. The student physically contributes to their memory and reproduction, and it is this, I think, that makes them powerful tools not only in the classroom but as a global tool for fighting the kind of destruction promulgated by Daesh. With 3D printing, memory of sites and objects isn’t just widely distributed, observed, studied and remembered; it has to be actively physically reproduced with time, materials and labor. Each act of reproduction individually chips away just a tiny bit at the destructive force that obliterated Version 1.0, empowering the maker, the memory and the other globally-distributed reproducers. Ömür Harmanşah (2015) points out that Daesh’s destruction isn’t just iconoclasm; it is more like reality TV, an ever-escalating attempt to grab the world’s attention by enacting The Unthinkable. This also humiliates populations whose pride is intertwined with protecting and remembering local monuments in front of a global audience, like schoolyard bullies who tease those they perceive as weak in front of their peers. Like schoolyard bullies, the best way to shut down aggressors is for everyone, united, to join together in opposition, to respect and remember the individuals, sites and objects they attempt to desecrate, both ancient and modern. Obviously our priorities today must fall firmly on the living people who are suffering the deprivations of warfare and occupation; but it’s important, too, to remember that the value of these sites and objects doesn’t really lie in their physical remains. It lies in the fact that this is all we have to remember the dead, to remember the acts of past peoples—workers and kings, everyday families, mothers and children, farmers, travelers and priests—real human beings whose lives are commemorated only by the tactile remains they crafted. Daesh attempts to destroy the honor of the living and the memory of the dead through these acts but the resilience, persistence and memory of the living people of Syria and Iraq is stronger than their oppressors.\n\n[caption id=\"attachment_12617\" align=\"alignnone\" width=\"1024\"]![IMG_2579_ClassPhoto](http://scholarslab.org/wp-content/uploads/2016/03/IMG_2579_ClassPhoto-1024x768.jpg) James Madison University Anth 395 Archaeology of the Middle East Class Photo: the students are holding their 3D printed objects related to material destroyed at Palmyra, Nimrud, Nineveh and Dura Europos after their final presentations. Pictured, front row l-r: Erin Woods, Rani Bertram, Carlisa Childress, Peyton Fitzgerald, Catherine Grimes, Alexandra Bowen, Brianna McDonald, Jaime Lantzy, Courtney Bryce, Haile Bennett, Ximena Calvo; Middle Row l-r: Patrick Jones, Lauryn Poe, Timmis Maddox, Samantha Hill, David Szady; Back row l-r: Dylan Hickey, Dominic Traver, Haile Bennett, Colton Wells. Not pictured: Connor Amano, Emily Gregory, Chris Molloy, Hannah Sullivan[/caption]\n\n**References Cited**\n\nAllahyari, Morehshin. “Material Speculation: ISIS (Work in Progress).” 2015. Accessed 02/22/2016. [http://www.morehshin.com/2015/05/25/material-speculation-isis/](http://www.morehshin.com/2015/05/25/material-speculation-isis/).\n\nHarmanşah, Ömür. “ISIS, Heritage, and the Spectacles of Destruction in the Global Media.” _Near Eastern Archaeology_, Vol. 78, No. 3, Special Issue: The Cultural Heritage Crisis in the Middle East (September 2015): 170-177.\n\nWei-Haas, Maya. “An Artist Creates a Detailed Replica of Ötzi, the 5,300-Year-Old ‘Iceman’.”_ Smithsonian Magazine_, February 17, 2016. Accessed 02/22/2016. [http://po.st/gy5vhe](http://po.st/gy5vhe).\n"},{"id":"2016-03-22-eggs-and-baskets-lessons-on-data-foraging","title":"Eggs and Baskets: Lessons on Data Foraging","author":"claire-maiers","date":"2016-03-22 05:48:00 -0400","categories":["Grad Student Research"],"url":"eggs-and-baskets-lessons-on-data-foraging","content":"It’s been a (long) while since my inaugural post on my Data Science Fellowship project.  This post takes the form of a piece of advice for other soon-to-be data gathers, and it comes down to this: don’t put all your eggs in one basket.\n\nIt sounds cliché, and—in retrospect at least—extremely obvious.  But it is an important piece of advice nevertheless.  What I’m talking about is the way in which we secure data for research projects.\n\nNick and I built our proposal around a single database.  Before submitting an official request for data, we talked with the folks that ran the database, we solicited advice from others who have worked with text as data, and we thought carefully about what we were requesting.  We knew the approximate timeline for receiving a data set, and we worked out a deadline that would allow us enough time to complete our analysis.  I felt that we were being thorough and doing the best we could to ensure quick delivery of our data.  Originally, the developers at the database told us to expect the data as much as a month in advance of that deadline.  That deadline has long come and gone.  Recently, we learned that due to some complexities of our request, they may not be able to deliver the data set before our fellowship concludes.\n\n(If I am able to speak with the developers, I’ll update this post at a later with more details about exactly what made extracting a data set so difficult.  Right now, what I’ve come to understand is that the size of our request may have been unprecedented and revealed some problems with running large inquiries.)\n\nMy advice is to pursue as many sources as possible until you have a workable data set in your hands.  This is especially important if you are working with a strict deadline, as so often the case for those of us writing dissertations or completing fellowships.  I recognize that there may not always be that luxury, but if there is more than one potential data source for your work, try to work with more than one.\n\nI want to emphasize that this post is in no way meant as a criticism of the developers who tried to deliver our data.  Sometimes, despite the best of intentions, things don’t work out.  Nick and I had great interactions with our contacts there, and we do feel that they did everything in their power to meet our deadline. I would not hesitate to request data from them again in the future. The silver lining is that we do expect to get the data eventually and hope to execute our proposal at that point (though this will be after the our institutional support comes to an end).  While Nick and I had discussed and prepared for a number of obstacles to our research, failure to gather data was not one of them.  You can be sure that the next time I go data gathering, I’ll take more than one basket.\n\n\n"},{"id":"2016-04-11-saving-arduino-sensor-data","title":"Saving Arduino Sensor Data","author":"ammon-shepherd","date":"2016-04-11 07:16:18 -0400","categories":["Makerspace"],"url":"saving-arduino-sensor-data","content":"![IMG_20160411_103622522_HDR](http://scholarslab.org/wp-content/uploads/2016/04/IMG_20160411_103622522_HDR-1024x768.jpg)\n\nWe had a need to take the temperature of an environment over a period of time, and record those temperatures for later analysis.\n\nThere are a number of options for recording sensor data.\n\n\n\n\t\n  1. If connected to a computer, the data can be saved by reading the serial output and storing that in a file.\n\n\t\n  2. If there is an SD card connected to the Arduino, the data can be saved directly to the SD card.\n\n\nIn this case, the Arduino needed to function by itself without being connected to a computer, so the sensor data needed to be saved to an SD card. We also needed the temperature sensor to be quite a distance from the Arduino and batteries, to minimize radiant heat affecting the temperature, so I soldered the sensor on to lengths of wire.\n\nA number of extension shields offer SD card readers. We had a WiFi shield with an SD card reader, so that is the one I used.\n\nThere are some limitations with this set up. The biggest issue is that this set up does not include a time stamp with the temperatures. In order to get a timestamp, it is best to record the data with the Arduino connected to a computer, or a Raspberry Pi. See here for code to capture the sensor data on a computer (like a Raspberry Pi) using Python: [https://github.com/mossiso/arduino-tuts/tree/master/raspberry-pi](https://github.com/mossiso/arduino-tuts/tree/master/raspberry-pi)\n\nAnother way is to use a dedicated SD card shield with an RTC (Real Time Clock) built in, such as this one from adafruit: [https://www.adafruit.com/products/1141](https://www.adafruit.com/products/1141)\n\n\n\n\n## Hardware\n\n\n\n\n\n\t\n  * Arduino Uno\n\n\t\n  * Wifi Shield or other shield with an SD card reader.\n\n\t\n    * This sketch uses a Wifi shield with SD card. This one uses the Adafruit CC3000 shield.\n\n\t\n    * Instructions for installation here: [https://learn.adafruit.com/adafruit-cc3000-wifi/cc3000-shield](https://learn.adafruit.com/adafruit-cc3000-wifi/cc3000-shield)\n\n\t\n    * Tips for installing male headers or stackable headers: [https://learn.sparkfun.com/tutorials/arduino-shields#installing-headers-preparation](https://learn.sparkfun.com/tutorials/arduino-shields#installing-headers-preparation)\n\n\n\n\n\t\n  * SD card\n\n\t\n  * Temperature sensor\n\n\t\n    * [https://www.sparkfun.com/products/10988](https://www.sparkfun.com/products/10988)\n\n\n\n\n\t\n  * Battery pack\n\n\t\n    * This set up works well with an external battery pack.\n\n\t\n    * [https://www.adafruit.com/products/248?&main_page=product_info&products_id=248](https://www.adafruit.com/products/248?&main_page=product_info&products_id=248)\n\n\n\n\n\n\n\n\n## Wiring Diagram\n\n\n![temperature-tmp36](http://scholarslab.org/wp-content/uploads/2016/04/temperature-tmp36-1024x664.png)\n\n\n\n\n## Code\n\n\n\n\n[code lang=\"c\"]\n// SPI and SD libraries. SPI for connecting SD card to SPI bus.\n#include <SPI.h>\n#include <SD.h>\nconst int sdPin = 4;\n\n// Temperature pin set to analog 0\nconst int temPin = 0;\n\n// Delay time. How often to take a temperature reading, in miliseconds\n// 20 minutes = 1200000 milliseconds\nconst int delayTime = 1200000;\n\n// File variable\nFile tempsFile;\n\n\n\nvoid setup() {\n  // Serial output for when connected to computer\n  Serial.begin(9600);\n  while (!Serial) {\n    ; // wait for serial port to connect. Needed for native USB port only\n  }\n\n  Serial.print(\"Initializing SD card...\");\n  if(!SD.begin(sdPin)) {\n    Serial.println(\"initialization failed!\");\n    return;\n  }\n  Serial.println(\"Initialization done.\");\n\n  tempsFile = SD.open(\"temps.txt\", FILE_WRITE);\n\n  if (tempsFile) {\n    Serial.println(\"Printing temperatures\");\n    tempsFile.println(\"Printing temperatures:\");\n    tempsFile.close();\n    Serial.println(\"Done.\");\n  } else {\n    Serial.println(\"Error opening file in setup.\");\n  }\n\n}\n\nvoid loop() {\n  /********************/\n  // Open SD card for writing\n  tempsFile = SD.open(\"temps.txt\", FILE_WRITE);\n\n  if (tempsFile) {\n    // Temperature readings\n    float voltage, degreesC, degreesF;\n    voltage = getVoltage(temPin);\n    degreesC = (voltage - 0.5) * 100.0;\n    degreesF = degreesC * (9.0/5.0) + 32.0;\n\n    // write temps to Serial\n    Serial.print(\"Celsius: \");\n    Serial.print(degreesC);\n    Serial.print(\" Fahrenheit: \");\n    Serial.println(degreesF);\n\n    // write temps to SD card\n    tempsFile.print(\"Celsius: \");\n    tempsFile.print(degreesC);\n    tempsFile.print(\" Fahrenheit: \");\n    tempsFile.println(degreesF);\n\n    // close the file\n    tempsFile.close();\n  } else {\n    Serial.println(\"Error opening file in loop.\");\n  }\n\n\n  delay(delayTime);\n\n}\n\nfloat getVoltage(int pin)\n{\n  return (analogRead(pin) * 0.004882814);\n}\n[/code]\n\n"},{"id":"2016-05-17-reading-speech-virginia-woolf-machine-learning-and-the-quotation-mark","title":"Reading Speech: Virginia Woolf, Machine Learning, and the Quotation Mark","author":"brandon-walsh","date":"2016-05-17 07:03:28 -0400","categories":["Digital Humanities","Experimental Humanities","Grad Student Research"],"url":"reading-speech-virginia-woolf-machine-learning-and-the-quotation-mark","content":"_[Cross-posted on the my [personal blog](http://bmw9t.github.io/blog/2016/05/17/reading-speech/) as well as the [WLUDH blog](https://digitalhumanities.wlu.edu/blog/2016/05/17/reading-speech-virginia-woolf-machine-learning-and-the-quotation-mark/). What follows is a slightly more fleshed out version of what I presented this past week at [HASTAC 2016](https://hastac2016.org) (complete with my memory-inflected transcript of the Q&A). I gave a bit more context for the project at the event than I do here, so it might be helpful to read my past two posts on the project [here](https://bmw9t.github.io/blog/2015/03/23/woolf-huskey/) and [here](http://bmw9t.github.io/blog/2015/09/10/woolf-and-the-quotation-mark/) before going forward. This talk continues that conversation.]_\n\nThis year in the [Scholar’s Lab](https://www.scholarslab.org) I have been working with Eric on a machine learning project that studies speech in Virginia Woolf’s fiction. I have written elsewhere about the [background for the project](https://bmw9t.github.io/blog/2015/03/23/woolf-huskey/) and [initial thoughts towards its implications](http://bmw9t.github.io/blog/2015/09/10/woolf-and-the-quotation-mark/). For the purposes of this blog post, I will just present a single example to provide context. Consider the famous first line of _Mrs. Dalloway_:\n\n\n<blockquote>Mrs Dalloway said, “I will buy the flowers myself.”</blockquote>\n\n\nNothing to remark on here, except for the fact that this is not how the sentence actually comes down to us. I have modified it from the original:\n\n\n<blockquote>Mrs Dalloway said she would buy the flowers herself.</blockquote>\n\n\nMy project concerns moments like these, where Woolf implies the presence of speech without marking it as such with punctuation. I have been working with Eric to lift such moments to the surface using computational methods so that I can study them more closely.\n\nI came to the project by first tagging such moments myself as I read through the text, but I quickly found myself approaching upwards of a hundred instances in a single novel-far too many for me to keep track of in any systematic way. What’s more, the practice made me aware of just how subjective my interpretation could be. Some moments, like this one, parse fairly well as speech. Others complicate distinctions between speech, narrative, and thought and are more difficult to identify. I became interested in the features of such moments. What is it about speech in a text that helps us to recognize it as such, if not for the quotation marks themselves? What could we learn about sound in a text from the ways in which it structures such sound moments?\n\nThese interests led me towards a particular kind of machine learning, supervised classification, as an alternate means of discovering similar moments. For those unfamiliar with the concept, an analogy might be helpful. As I am writing this post on a flight to HASTAC and just finished watching a romantic comedy, these are the tools that I will work with. Think about the genre of the romantic comedy. I only know what this genre is by virtue of having seen my fair share of them over the course of my life. Over time I picked up a sense of the features associated with these films: a serendipitous meeting leads to infatuation, things often seem resolved before they really are, and the films often focus on romantic entanglements more than any other details. You might have other features in mind, and not all romantic comedies will conform to this list. That’s fine: no one's assumptions about genre hold all of the time. But we can reasonably say that, the more romantic comedies I watch, the better my sense of what a romantic comedy is. My chances of being able to watch a movie and successfully identify it as conforming to this genre will improve with further viewing. Over time, I might also be able to develop a sense of how little or how much a film departs from these conventions.\n\nSupervised classification works on a similar principle. By using the proper tools, we can feed a computer program examples of something in order to have it later identify similar objects. For this project, this process means training the computer to recognize and read for speech by giving it examples to work from. By providing examples of speech occurring within quotation marks, we can teach the program when quotation marks are likely to occur. By giving it examples of what I am calling ‘implied speech,’ it can learn how to identify those as well.\n\nFor this machine learning project, I analyzed Woolf texts downloaded from [Project Gutenberg](https://www.gutenberg.org/wiki/Main_Page). Eric and I put together scripts in Python 3 that used a package known as the [Natural Language Toolkit](https://nltk.org/)] for classifying. All of this work can be found at the project’s [GitHub repository](http://https;//www.github.com/bmw9t/woolf).\n\nThe project is still ongoing, and we are still working out some difficulties in our Python scripts. But I find the complications of the process to be compelling in their own right. For one, when working in this way we have to tell the computer what features we want it to pay attention to: a computer does not intuitively know how to make sense of the examples that we want to train it on. In the example of romantic comedies, I might say something along the lines of \"while watching these films, watch out for the scenes and dialogue that use the word 'love.'\" We break down the larger genre into concrete features that can be pulled out so that the program knows what to watch out for.\n\nTo return to Woolf, punctuation marks are an obvious feature of interest: the author suggests that we have shifted into the realm of speech by inserting these grammatical markings. Find a quotation mark-you are likely to be looking at speech. But I am interested in just those moments where we lose those marks, so it helps to develop a sense of how they might work. We can then begin to extrapolate those same features to places where the punctuation marks might be missing. We have developed two models for understanding speech in this way: an external and an internal model. To illustrate, I have taken a single sentence and bolded what the model takes to be meaningful features according to each model. Each represents a different way of thinking about how we recognize something as speech.\n\nExternal Model for Speech:\n\n\n<blockquote>\"I love walking in London,\" **said Mrs. Dalloway**.  \"Really it’s better than walking in the country.\"</blockquote>\n\n\nThe external model was our initial attempt to model speech. In it, we take an interest in the narrative context around quotation marks. In any text, we can say that there exist a certain range of keywords that signal a shift into speech: said, recalled, exclaimed, shouted, whispered, etc. Words like these help the narrative attribute speech to a character and are good indicators that speech is taking place. Given a list of words like this, we could reasonably build a sense of the locations around which speech is likely to be happening. So when training the program on this model, we had the classifier first identify locations of quotation marks. Around each quotation mark, the program took note of the diction and parts of speech that occurred within a given distance from the marking. We build up a sense of the context around speech.\n\nInternal Model for Speech:\n\n\n<blockquote>\"**I love walking in London**,\" said Mrs. Dalloway. \"**Really it’s better than walking in the country**.\"</blockquote>\n\n\nThe second model we have been working with works in an inverse direction: instead of taking an interest in the surrounding context of speech, an internal model assumes that there are meaningful characteristics within the quotation itself. In this example, we might notice that the shift to the first-person 'I' is a notable feature in a text that is otherwise largely written in the third person. This word suggests a shift in register. Each time this model encounters a quotation mark it continues until it finds a second quotation mark. The model then records the diction and parts of speech inside the pair of markings.\n\nEach model suggests a distinct but related understanding for how sound works in the text. When I set out on this project, I had aimed to use the scripts to give me quantifiable evidence for moments of implied speech in Woolf’s work. The final step in this process, after all, is to actually use these models to identify speech: looking at texts they haven't seen before, the scripts insert a caret marker every time they believe that a quotation mark should occur. But it quickly became apparent that the construction of the algorithms to describe such moments would be at least as interesting as any results that the project could produce. In the course of constructing them, I have had to think about the relationships among sound, text, and narrative in new ways.\n\nThe algorithms are each interpretative in the sense that they reflect my own assumptions about my object of study. The models also reflect assumptions about the process of reading, how it takes place, and about how a reader converts graphic markers into representations of sound. In this sense, the process of preparing for and executing text analysis reflects a certain phenomenology of reading as much as it does a methodology of digital study. The scripting itself is an object of inquiry in its own right and reflects my own interpretation of what speech can be. These assumptions are worked and reworked as I craft algorithms and python scripts, all of which are as shot through with humanistic inquiry and interpretive assumptions as any close readings.\n\nFor me, such revelations are the real reasons for pursuing digital study: attempting to describe complex humanities concepts computationally helps me to rethink basic assumptions about them that I had taken for granted. In the end, the pursuit of an algorithm to describe textual speech is nothing more or less than the pursuit of deeper and enriched theories of text and speech themselves.\n\n**Postscript**\n\nI managed to take note of the questions I got when I presented this work at HASTAC, so what follows are paraphrases of my memory of them as well as some brief remarks that roughly reflect what I said in the moment. There may have been one other that I cannot quite recall, but alas such is the fallibility of the human condition.\n\nQ: You distinguish between speech and implied speech, but do you account at all for the other types of speech in Woolf’s novels? What about speech that is remembered speech that happened in earlier timelines not reflected in the present tense of the narrative's events?\n\nA: I definitely encountered this during my first pass at tagging speech and implied speech in the text by hand. Instead of binaries like quoted speech/implied speech, I found myself wanting to mark for a range of speech types: present, actual; remembered, might not have happened; remembered incorrectly; remembered, implied; etc. I decided that a binary was more feasible for the machine learning problems that I was interested in, but the whole process just reinforced how subjective any reading process is: another reader might mark things differently. If these processes shape the construction of the theories that inform the project, then they necessarily also affect the algorithms themselves as well as the results they can produce. And it quickly becomes apparent that these decisions reflect a kind of phenomenology of reading as much as anything: they illlustrate my understanding of how a complicated set of markers and linguistic phenomenon contribute to our understanding that a passage is speech or not.\n\nQ: Did you encounter any variations in the particular markings that Woolf was using to punctuate speech? Single quotes, etc., and how did you account for them?\n\nA: Yes - the version of _Orlando_ that I am working with used single quotes to notate speech. So I was forced to account for such edge cases. But the question points at two larger issues: one authorial and one bibliographical. As I worked on Woolf I was drawn to the idea of being able to run such a script against a wider corpus. Since the project seemed to impinging on how we also understand psychologized speech, it would be fascinating to be able to search for implied speech in other authors. But, if you are familiar with, say, Joyce, you might remember that he hated quotation marks and used dashes to denote speech. The question is how much can you account for such edge cases, and, if not, the study becomes only one of a single author’s idiosyncrasies (which still has value). But from there the question spirals outwards. At least one of my models (the internal one) relies on quotation marks themselves as boundary markers. The model assumes that quotation marks will come in pairs, and this is not always the case. Sometimes authors, intentionally or accidentally, omit a closing quotation mark. I had to massage the data in at least half a dozen places where there was no quotation mark in the text and where its lack was causing my program to fail entirely. As textual criticism has taught us, punctuation marks are the single most likely things to be modified over time during the process of textual transmission by scribes, typesetters, editors, and authors. So in that sense, I am not doing a study of Woolf’s punctuation so much as a study of Woolf’s punctuation in these particular versions of the texts. One can imagine an exhaustive study that works on all versions of all Woolf’s texts as a study that might approach some semblance of a correct and thorough reading. For this project, however, I elected to take the lesser of two evils that would still allow me to work through the material. I worked with the texts that I had. I take all of this as proof that you have to know your corpus and your own shortcomings in order to responsibly work on the materials - such knowledge helps you to validate your responses, question your results, and reframe your approaches.\n\nQ: You talked a lot about text approaching sound, but what about the other way around - how do things like implied speech get reflected in audiobooks, for example? Is there anything in recordings of Woolf that imply a kind of punctuation that you can hear?\n\nA: I wrote about this extensively in my dissertation, but for here I will just say that I think the textual phenomenon the questioner is referencing occurs on a continuum. Some graphic markings, like pictures, shapes, punctuation marks, do not clearly translate to sound. And the reverse is true: the sounded quality of a recording can only ever be remediated by a print text. There are no perfect analogues between different media forms. Audiobook performers might attempt to convey things like punctuation or implied speech (in the audiobook of *Ulysses*, for example, Jim Norton throws his voice and lowers his volume to suggest free indirect discourse). In the end, I think such moments are playing with an idea of what my dissertation calls audiotextuality, the idea that all texts recordings of texts, to varying degrees, contain both sound and print elements. The two spheres may work in harmony or against each other as a kind of productive friction. The idea is a slippery one, but I think it speaks to moments like the implied punctuation mark that come through in a particularly powerful audiobook recording.\n"},{"id":"2016-05-23-3d-printing-in-the-classroom-outcomes-and-reflections-on-a-slavic-course-experiment-12","title":"3D Printing in the Classroom: Outcomes and Reflections on a Slavic Course Experiment (1/2)","author":"jennifer-grayburn","date":"2016-05-23 11:54:56 -0400","categories":["Makerspace"],"url":"3d-printing-in-the-classroom-outcomes-and-reflections-on-a-slavic-course-experiment-12","content":"_[Cross-posted on my personal blog.](https://jennifernicolegrayburn.com/2016/05/23/3d-printing-in-the-classroom-outcomes-and-reflections-on-a-slavic-course-experiment-12/)_\n\nIn a [previous post](http://scholarslab.org/uncategorized/3d-printing-in-the-classroom-course-assignments-and-the-makerspace/), UVA's Slavic Librarian, Kathleen Thompson, and Slavic Lecturer, Jill Martiniuk, outlined the early stages of a 3D printing assignment for Yuri Urbanovich’s ‘Understanding Russia: Symbols, Myths, and Archetypes of Identity’ course. Kathleen and Jill now describe the unexpected obstacles and opportunities of this assignment in a two part blog post:\n\nIf we had to describe this project in two words or fewer, we would definitely do so using the phrase “learning process”. Two days after the original blog post about the project and the accompanying exhibit went live, we had students curate the exhibit of their objects in the hallway of the Slavic Department, on the second floor of Cabell Hall. We had arranged for use of half of a display case that had been occupied by faculty books and items of Slavic ephemera, which we had permission to remove and re-arrange as needed.\n\n[caption id=\"attachment_12713\" align=\"alignleft\" width=\"225\"]![3D-exhibit-curating](http://scholarslab.org/wp-content/uploads/2016/05/3D-exhibit-curating-225x300.jpg) Students putting the final touches on their exhibit.[/caption]\n\nThe exhibit curation coincided with the Slavic Department’s biweekly Russian Tea & Conversation event, in which students, faculty, staff, and community members interested in speaking Russian or learning about Russian culture gather to eat, drink tea, converse, and occasionally listen to speakers on special topics. The 2470 students brought their objects to this event, and after a brief introduction to the project from us, they presented their objects to the group and explained why they chose the objects they did. The students then took their objects to the display case and set up the exhibit (for which we had printed a brief blurb and some attributive text).\n\nGeneral response to the project, and the objects, at this event seemed positive, which is why we were quite surprised the next day to hear that one faculty member had a very strong negative reaction to one of the objects in the display case. After some deliberation, we decided to remove the object from the exhibit, and a few weeks later (due to class not meeting because of Spring Break) we discussed this reaction with the students in class.\n\nThe object in question was a [bust of Joseph Stalin](https://www.thingiverse.com/thing:516756), which the group printed in red. The faculty member who raised the objection is well-known for his work on Stalin’s system of forced labor camps, known as the Gulag. His most recent book on the topic features an image of Stalin on the front cover, which was displayed in the same case in which the students’ exhibition was curated. For an American student whose only experience of Stalin comes from textbooks, using this bust to represent Russia was a positive choice, because Stalin is seen in the U.S. as a strong leader who helped defeat Hitler during World War II. For the objecting faculty member, whose family has personal experience with the effects of Stalin’s policies, the bust carries entirely different connotations: genocide, violence, and turbulence. When we shared this with the entire class, the group that printed the Stalin bust maintained that they stood by their choice, and some of their classmates expressed the common sentiment that Germany is not usually characterized only by Hitler’s negative aspects, so why should Russia be characterized only by Stalin’s? We wanted to emphasize that the students were not in trouble, nor were their project grades affected, and we led them through a discussion of the ways in which their objects can be interpreted as symbols by different groups with various degrees of distance from those symbols. This discussion raised some salient points about the effect of a two-dimensional representation versus a three-dimensional representation, and (for us, at least) really drove home the point that 3D objects absolutely can bring symbols to life in new and unexpected ways.\n\n[caption id=\"attachment_12712\" align=\"aligncenter\" width=\"700\"]![3D-exhibit-final-product](http://scholarslab.org/wp-content/uploads/2016/05/3D-exhibit-final-product.jpg) The finished product. Note the placement of the Stalin bust next to the book at the left.[/caption]\n\nAbout a month after that discussion, towards the end of the semester, we gave students the option to re-print their objects if they wished, since at least one group had expressed interest in doing so – the group that printed the Rubik’s cube, for example, had thought about re-making their object to have movable parts. None of the groups chose this option, and headed into the work of their final projects with their original objects intact.\n\nThe final project asked groups to complete two assignments. First was a 1,000-word essay addressing the ways in which the group’s view of their object changed, highlighting the following questions: In what ways does your object serve (or no longer serve) as a symbol of Russia? What might better symbolize Russia, or how could your symbol be improved? Did you have any misconceptions in your initial plan to work with this object? What were they? Where did they come from? Is this object ‘loaded’ with any preconceived notions from a Western perspective? From a Russian perspective? The second assignment was to create and give a 3-minute presentation addressing the following questions: What do the objects chosen by the entire class, and the ways the class arranged those objects, say about how we view Russia? What do these objects have in common? How do you think Russians might symbolically represent their views of the United States?\n\nGenerally, student groups were satisfied with their objects as symbols of Russia, though each group recognized shortcomings in their initial conceptions of their objects and was able to articulate ways to make their object a stronger symbol. The Stalin group, for example, would have printed a figure of Stalin to represent him as a human being, rather than as a figurehead or an icon on a pedestal, based on the visceral reaction to the bust that could be seen as “glorifying” a figure whose history is murky at best. The group that printed St. Basil’s cathedral felt that their object lacked political symbolism, which they would have changed by adding a 3D model of the Kremlin to surround the church. The onion dome group offered two ideas for improvement: first, making their onion dome a dynamic object with multi-colored layers that represent what they call “the multiple forces at work in Russian society”, or second, printing a model of a nesting doll (_matryoshka_) to represent the seemingly outwardly-united Russia that depends on one leader at its core. The group with the Soyuz rocket would have changed the color of their object; rather than the white that represents Russia’s multicultural identity, they would have printed a red rocket to emphasize the importance of Communism and revolutions in Russia’s history. They might also have added an astronaut to the rocket, or adorned it with embellishments, to portray Russian opulence created by human hands. The Rubik’s cube group had initially expressed a desire to enhance their object with moving parts; this desire still held true at the end of the semester, though an acceptable compromise would have been to scatter the colors throughout the cube so that it did not have a uniform appearance and could thus express their claim that the puzzle that is Russia has not yet been “solved” in their lifetimes. Finally, the group that printed the scales stated that their object could have been more meaningful if the scales had been movable, rather than fixed, and if each side had had multiple boxes that could be moved to portray the fluctuating power balance between the U.S. and Russia.\n\nAs a final note, a few days after classes ended for the semester, I (Kathleen) came across a link on [my Twitter feed](https://twitter.com/rusalkat?lang=en) to a story about upcoming Victory Day celebrations in the Siberian city of Novosibirsk. (Russians celebrate Victory Day every May 9 to commemorate the end of World War II, which they refer to as “The Great Patriotic War”). The story focused on a set of billboards being erected in the city for the celebrations, [all of which prominently featured images of Joseph Stalin](https://meduza.io/en/news/2016/05/04/novosibirsk-installs-billboards-with-stalin-s-portrait-across-city-in-preparation-for-victory-day). This story adds another twist to our discussion of the use of Stalin as a symbol of Russia; for many Russians, he is still seen as a positive figure in the country, despite his treatment of Soviet citizens during his rule. Like the Russian _matryoshka _doll, the use of Stalin as a representation of Russia is complex and multi-layered, and can be unpacked and re-arranged in a myriad of ways.\n\nIn our next post, we will address which parts of this project worked and which did not, students’ and the instructor’s final thoughts on the project, and ideas for improving the implementation of such a project in the future.\n\n\n"},{"id":"2016-06-08-3d-printing-in-the-classroom-outcomes-and-reflections-on-a-slavic-course-experiment-22","title":"3D Printing in the Classroom: Outcomes and Reflections on a Slavic Course Experiment (2/2)","author":"jennifer-grayburn","date":"2016-06-08 07:00:54 -0400","categories":["Makerspace"],"url":"3d-printing-in-the-classroom-outcomes-and-reflections-on-a-slavic-course-experiment-22","content":"_[Cross-Posted on my personal blog.](https://jennifernicolegrayburn.com/2016/06/08/3d-printing-in-the-classroom-outcomes-and-reflections-on-a-slavic-course-experiment-22/) _\n\nUVA's Slavic Librarian, Kathleen Thompson, and Slavic Lecturer, Jill Martiniuk, conclude their [two-part evaluation](http://scholarslab.org/makerspace/3d-printing-in-the-classroom-outcomes-and-reflections-on-a-slavic-course-experiment-12/) of the [3D printing assignment for Yuri Urbanovich's 'Understanding Russia' course](http://scholarslab.org/uncategorized/3d-printing-in-the-classroom-course-assignments-and-the-makerspace/). Considering both student and professor feedback, Kathleen and Jill offer suggestions to continue and improve this interactive assignment for future courses:\n\nSince this project was an experiment, some parts of it were bound to work better than others, and while we can say that this has generally been a positive experience for both students and instructors, both we and Prof. Urbanovich have several ideas on what could be improved for future iterations of a similar assignment.\n\nOverall, we think that this experiment was a success. It achieved its initial goals of getting students to think differently about symbols and their power and use, and the reaction to the one particular object almost certainly would not have been so strong if the depiction of that particular concept had not been in 3D. ** **We base this conclusion on both instructor and student evaluations of both the course and the project, and the ways in which the project was integrated into the course.\n\nStudents were given two evaluation forms to fill out at the end of the semester: one was the standard online evaluation through UVaCollab, and the other was a written evaluation handed out during the final exam. Not surprisingly, participation in the written evaluation was stronger than the online evaluation; only 12 of the 35 students filled out the online evaluation, and only one of those mentioned the 3D project. That comment suggested more class time to discuss the project in the week leading to the final presentation. Overall, however, students rated the course as worthwhile (81% said that they “strongly agreed” with that sentiment, and 9% “agreed”), and felt that they had learned a great deal in the course (66% said that they “strongly agreed” with that sentiment, and 33% “agreed”).\n\nWe were** **more interested to see the written course evaluations because Prof. Urbanovich emphasized that students should mention the 3D projects as part of their evaluation.** **31 of the 35 students submitted those written evaluations during the final exam! Of those, 17 mentioned the 3D project. Of the 17 that mentioned the 3D project, the average score for the course was 4.941 (out of 5); the overall average for the course was 4.548, so the evaluations that mentioned the project gave the course a higher rating than evaluations that did not. Course ratings remained high even among students who did not find the project very meaningful.\n\nCommon negative reactions touched upon one of our biggest concerns, which was the integration of the project into the course: “a bit irrelevant”; “seemed unnecessary”; and “it was interesting but maybe didn’t fit well with this course” were a few notes of feedback. Most responses were quite positive, with students citing the project’s ability to spur creative thinking about Russian identity as a plus: “It was good to see how my perspective of Russia changed over the course of [the project]”; “[It] provided a more open ended approach to my exploration and learning about Russia”; and “I liked it in terms of analyzing preconceived notions and then applying model to material learned throughout the semester” were a few notes of feedback in this case. A couple of students made suggestions for improving the project in the future, such as making the groups smaller and providing clearer expectations of the final presentation. Finally, one student offered this advice: “Keep the 3D project, we all love it!”\n\nWith this last note come two caveats: one, we realize that one student’s positive feedback about the project does not a collective opinion make. Two, these evaluations were written in the context of a final examination for which the professor was present, which, although anonymous, may not lend itself to the most honest criticism. Online evaluations are completely anonymous and not compulsory, so their efficacy as true measures of a group’s feeling about a course remains questionable.\n\n_What worked:_ Discussions generated by the objects were thought-provoking, and making the students’ work public compelled them to do a good job of selecting and justifying their objects. Having the students curate their own exhibit gave them ownership of the project. Not forcing them to discuss their objects every class session probably helped them not feel burned out by the project, too. Even the backlash from that one object was a valuable learning experience, as it gave students some insight on how symbols are perceived by certain groups of people, and how powerful even the smallest or most innocently-intended representations can be.\n\n_What didn’t work:_ The 3-minute presentation was supposed to be a soundbite, but the extra work created by having students record and re-record their presentations proved too cumbersome. General student reaction to the idea of a soundbite was negative, whereas reaction to a brief presentation was more positive (and, we think, more inclusive – it allows students to listen to one another in real time, rather than after the fact).\n\nWe had originally thought about having students curate an online exhibit of their objects to explore how (if at all) digital exhibits reflect meaning in ways different from physical exhibits, using the Tumblr platform. Student reaction to this idea was lukewarm at best; after setting up the Tumblr account for the class, we abandoned the idea partially due to this reaction, and partially because we could not assign any tangible pedagogical value to the blog. We’re still mulling over whether or not Tumblr is a viable pedagogical tool; it may be, but perhaps not for a project like this, where the 3D component of the exhibit is vital to understanding how perceptions of objects as symbols change according to the medium.\n\nCurating a second exhibit in Alderman Library was neither useful nor interesting to the students or to us, since they – and we – did not see any value in putting up another display in an area with light foot traffic in which attention is not directed at the space in which the display would have been.\n\nSince this wasn’t our own class, it felt a little odd popping in to Prof. Urbanovich’s class once a week, or every couple of weeks, to talk to students about project components and issues that arose along the way. We wonder if students took us, and the project, as seriously as they might have if Prof. Urbanovich had been the sole conductor of the experiment, because some students may have perceived that they were test subjects (for lack of a better term), even though we were careful to explain early on that this was an experimental project.\n\nA few days after Part I of this blogpost went live, I (Kat) spoke to Prof. Urbanovich about his thoughts on the scholarly value of the project, and whether or not he would continue to include the project in future iterations of this course. He said that he would absolutely keep the project as part of the course, and suggested the following changes to it:\n\n- A more detailed explanation of the goals and aims of the project, and more discussion of the groups’ presentations and final papers.****\n\n- In general, more in-class discussion of the symbols that students chose; Prof. Urbanovich noted that the most useful class meetings were the ones driven by student discussion, rather than instructor lecturing.****\n\n- Pursuant to that, a teaching assistant or “project leader” who would not only introduce the project and work with students to print the models, and guide them through the exhibit-building and presentations, but would also facilitate bi-weekly in-class discussions relating symbols to that week’s topics.****\n\n- Possibly leave room for comparison of U.S. and Russian symbols expressing a particular idea, and greater room for discussion of how Russians symbolize the U.S.****\n\n- More discussion of the various meanings conveyed by 2D symbols, as opposed to 3D symbols.****\n\nOne major change that Prof. Urbanovich suggested would be to delay the initial printing of 3D objects in favor of showing symbols that already exist in Russia, and asking students to discuss their perceptions of those symbols. He gave the example of the Allies Monument that stands in [Moscow’s Victory Park](http://www.panoramio.com/photo/118252667). He finds that students are usually shocked by the idea of a monument portraying soldiers from France, the U.S., the U.K., and the U.S.S.R. as allies, because they do not immediately make the connection between them as the group united against Hitler in World War II. Even more surprising to students is the fact that this monument was erected in 2005, some sixty years after the war ended!\n\nGiving students an idea of what symbols already exist, and then asking them to devise their own, might help give students new theoretical approaches to inform their choices, and lend increased significance to their cultural interactions.\n\nFinally, concerning the scholarly value of such a project, Prof. Urbanovich said that this project was very timely given the current tenuous state of U.S.-Russian relations, which he does not necessarily foresee stabilizing in the near future. For that reason alone, this project is worthwhile, because he has repeatedly and enthusiastically supported the facilitation of open discussion about the ways in which cultures perceive one another as a way of coming to a mutual understanding. Those who might question the need for a 3D project when a 2D project might suffice need only examine what happened when the bust of Stalin was printed – in a sense, “came to life” – to understand that such a project has fairly deep implications indeed.  \n\nFor now, we continue to process our thoughts on this project, and will work to improve its structure and implementation for a more robust classroom experience. We will be participating in a roundtable, “Digital Humanities In and Out of the Classroom” at the Association for Slavic, East European, and Eurasian Studies (ASEEES) Annual Convention in Washington, D.C., in November 2016 to discuss this project with Slavic scholars working in the digital realm. We are also developing a 2-week undergraduate course called “Making a Digital Museum”, using online museum collections as a basis for creation of physical and virtual Russian-culture exhibit. We hope to tie this course to the return of the Fabergé eggs to the Virginia Museum of Fine Arts in Richmond, Va., in late October, for which the museum is preparing a renovated and expanded exhibit space. \n"},{"id":"2016-06-08-using-dh-to-explore-movement-and-meaning","title":"Using DH to Explore Movement and Meaning","author":"kelli-shermeyer","date":"2016-06-08 07:52:32 -0400","categories":["Grad Student Research"],"url":"using-dh-to-explore-movement-and-meaning","content":"_**Enjoy this guest post by Kelli Shermeyer, Doctoral candidate in the UVA English department, in which she describes her work with Professor Holly Pickett's English 380 course at Washington & Lee. This work is supported by ASC grant expanding [collaboration between Washington & Lee and the Scholars' Lab](http://scholarslab.org/announcements/scholars-lab-grads-partner-with-washington-lee-university/). Cross-posted on the [Washington & Lee's DH blog](http://digitalhumanities.wlu.edu/blog/2016/06/08/kelli-shermeyer-on-using-dh-to-explore-movement-and-meaning/).**_\n\n“Playwrights write plays for the stage, not the study,” or so Roland Broude reminds us. Yet in my field of English literature, it’s quite common to study a play primarily as a textual object rather than a performance whose final form, tone, and affect all rely on extra-textual features. We don’t typically account for changes in play’s text during its first rehearsals (often these changes are implemented after the play text has been sent to print!), refinements in timing and intonation that occur during a show’s run, or even accidental line drops, forgotten words, or ad libs contrived by actors in reaction to something that happened during a particular performance. The reality of theater is that plays are constantly rewritten in a multitude of ways and we don’t have a lot of good ways to talk about that beyond acknowledgement.\n\nIn our world of the single-author study and the copyright, one of the consequences of seeing dramatic texts primarily as “literature” is the following assumption that the play is entirely the property of its author, who, as Broude argues, “exercises over it a droît moral: his is the sole right to establish the text, and, once it has been established, to alter it.” Teaching from this paradigm limits engagement with the performer or designer’s role in creating the play’s affect or meaning.\n\nMy work as a teacher, researcher, and theater director is to employ the digital humanities to help create ways that empower students to see a play as a complex nexus of decisions rather than a static textual object (for even the text itself is not stable). The problem that quickly surfaces is that performance (in many of its forms) is actually rather tricky to write about, because while we may have access to many versions and editions of the textual object (script), each enactment of that script is essentially ephemeral—a portion of it remains unrecoupable.\n\nPeggy Phelan has claimed that the ontology of performance is essentially its irreproducibility and she acknowledges the difficulty this presents in analyzing performance art. We can try to fix parts of performance in a variety of non-performative forms such as narrative, photograph, or video recording, but those other media can only offer ekphrasis, not full reproduction. The ephemerality of performance gives it much of its affective weight and political potential. While we may not be able to entirely recapture performance outside of ekphrasis, my hope is that we can develop tools and methods for examining dramatic texts and performances that can help us to translate some of the harder-to-capture elements of performance into forms on which we can engage in various kinds of analysis or reflection. One of the questions I am currently thinking through is how can we “read” movement?\n\nThere’s some interesting work from the dance world that begins to think through these issues. Choreographer William Forsythe’s work with the Ohio State University (called [Synchronous Objects](http://synchronousobjects.osu.edu/)) is particularly fascinating. Earlier work by choreographers such Rudolf Laban developed notational systems for dancers based on certain ideas about the body in space (Labanotation, for example). But I’ve been struggling to try to find a way that connects movement and text (like a script) in a meaningful way. How do certain textual features invite us to think about certain movements? What in the text tells us to move to a particular place or in a particular way? Asking students these questions is also a way of approaching the critical practices like the close reading and formal analysis which still remains important to much (but not all!) of our work in literary studies.\n\nAs a way to experiment with the relationship between movement and language, I worked with Holly Pickett’s English 380 class on two activities to help us discuss the relationship between the text and blocking of a scene. (Blocking is both a noun and a verb: it describes both the pattern of movement in a given scene and the act of directing/designing those movement patterns in rehearsal). First, I gave them the “to be or not to be” monologue from Hamlet Act 3, scene 1. I selected this text because I thought it was one they may be marginally familiar with and one that doesn’t contain many stage directions within the language (for example, when a character says “come here,” cuing another actor’s movement).\n\nI instructed them to draw Hamlet’s path throughout the monologue—where does he start, end, and where does he move throughout the speech? I did not give them any instructions on how to notate pauses, changing positions or how long Hamlet took to walk somewhere as I was interested in seeing how they would choose to notate this.\n\nI also asked them to use [Prism](http://prism.scholarslab.org/?locale=en) to mark up the monologue, indicating where Hamlet started moving, stopped moving, or changed position in their blocking. I did not tell them in which order to do these tasks just that they had to do both of them. At the end of the allotted 20 minutes, I taped all of the drawings on the board and pressed the visualization button on Prism to see what we found. The Prism results revealed that there was a great variety in blocking styles, yet there were definable loci of energy around certain parts of the text. (Here’s the [full visualization](http://prism.scholarslab.org/prisms/b72767fa-1396-11e6-bdec-005056b3784e/visualize?locale=en))\n\n![prism1](http://scholarslab.org/wp-content/uploads/2016/06/prism1.png)\n\nIn this first image, you can see that a lot of students notated something around “end them? To die; to sleep,” but there disagreement as to what Hamlet was doing at those moments.\n\nWe zeroed in on the word “end” as Prism showed there was some debate as to what movement happened on that word. The Prism showed that students either had Hamlet change position without changing location (indicated by the blue) or stop moving all together (indicated by the red). No one had Hamlet begin moving on this word (which would have been indicated by green).\n\n![prism2](http://scholarslab.org/wp-content/uploads/2016/06/prism2.png)\n\nThroughout the whole monologue, “die” and “death” continued to appear as words where students thought some kind of movement or position change should occur, but we couldn’t agree as to what that movement should be:\n\n![prism3](http://scholarslab.org/wp-content/uploads/2016/06/prism3.png)\n\n![prism4](http://scholarslab.org/wp-content/uploads/2016/06/prism4.png)\n\nI have no definitive way to explain this: only a director’s hunch that there’s some sort of affective energy around the word and concept of death that we associate with anxiety that incites us to movement—we (or at least most of us) do not want to be still when facing death. Part of my future work is figuring out how to interpret these results.\n\nThe other part of the activity—the drawings of Hamlet’s path—are much harder to read. Most drawings started Hamlet out on the center of the stage, not accounting for the first bit of text printed in the monologue directing that “Hamlet enters.” To me, this suggests some kind of connection between what we know about Hamlet, the role of this speech in the play, and center stage—we know Hamlet is the central figure and associate this important monologue with the center stage. But aside from that, the patterns varied. Most were well-balanced with Hamlet spending time on both sides of the stage (my mentors would be proud that both sides of the audience would get an equally good view of the actor). Some incorporated gestures or moments of intentional stillness. Many contained loops. Professor Pickett explained that she chose to do this in her drawing because of how she views the speech as rhetorically winding and wanted to create a movement pattern that reflected her reading.\n\nSeveral of the students actually marked words on their drawings as well connecting the text directly to their movement patterns:\n\n![move1](http://scholarslab.org/wp-content/uploads/2016/06/move1.png)\n\n![move2](http://scholarslab.org/wp-content/uploads/2016/06/move2.png)\n\nSome used no text at all and focused on the shape of Hamlet’s movements:\n\n![move3](http://scholarslab.org/wp-content/uploads/2016/06/move3.png)\n\nAnd here’s one that was purposely playing with Hamlet’s winding rhetoric:\n\n![move4](http://scholarslab.org/wp-content/uploads/2016/06/move4.png)\n\nSo how do we make meaning out of all of this data?\n\nI’m in the process (the slow, painful process) of developing a tool (or likely, a set of tools) to help students visualize the connection between play text and movement patterns. By considering the way language suggests movement will, I hope, allow for a richer consideration of the formal stylistics of particular plays, but also in the long run create corpus of data on the way people see theatre texts. I’m interested in what new areas of inquiry open up if I can use a digital tool to process many blocking patterns of the same scene (i.e. perform a kind of distant reading on the movement patterns that I had the students create). At the least, we can get students to think more deeply about the way that the dramatic text is a living document brought to life, challenged, and enriched by a consideration of the ways its interacts with the body, and embodiment.\n\nThis is important work for me both as a literary scholar and a theater director because of the reciprocal relationship between movement and interpretation. The director interprets the text to find places to block movement, and then the audience uses those movements to interpret certain moments on stage. Thinking about the relationship between movement and interpretation helps us to counter the belief that the playwright alone fixes the meaning of his or her “original text” and to recognize the larger networks of people, practices, traditions, and texts that make theater mean something.\n\n---\n\n\n1. Broude, Roland. Performance and the Socialized Text. Textual Cultures: Texts, Contexts, Interpretation, Volume 6, Number 2, 2011, 24.\n\n\n\n\n2. Ibid. 25.\n\n\n\n\n3. Phelan, Peggy. Unmarked : The Politics of Performance. London ; Routledge, 1993.\n\n\n\n\n4. It’s entirely worth noting that there should be a lively debate about if there are elements of performance that should not be recorded or analyzed as well. Is the kind of work we’re doing creating a richer context for talking about performances, or are we violently decontextualizing aspects of performance that can’t be understood without the full (but sadly unrecoverable) picture? (Many thanks to [Purdom Lindblad](http://scholarslab.org/people/purdom-lindblad/) for first asking me a version of this question!)\n\n\n\n\n5. I was made aware of this interesting work in dance through sitting in on “The Art of Dance” taught by Kim Brooks Mata in the summer of 2015.\n"},{"id":"2016-06-11-managing-director","title":"Managing Director","author":"alison-booth","date":"2016-06-11 05:57:51 -0400","categories":["Announcements","Digital Humanities"],"url":"managing-director","content":"We are so excited to be inviting applications for the position of Managing Director of Scholars' Lab, in the University of Virginia Library. See [posting 0618965](https://jobs.virginia.edu/applicants/Central?quickFind=79369) on the UVA HR site [jobs.virginia.edu](https://jobs.virginia.edu/applicants//jsp/shared/frameset/Frameset.jsp) for more details (if need be, go to \"Search Postings\" link on upper left, enter the posting number).\n\nWe did slightly reword the posting from June 9, which said \"Ph.D. required.\"  See the 6-10-16 edition: \"Graduate study (Ph.D. preferred) in the humanities, library/information sciences, social sciences or related discipline.\" We were always open to the experienced, advanced applicant, Ph.D. requirement having been in HR wording but not in spirit. Now letter and spirit match.\n\nWe do hope for familiarity with graduate education and doctoral scholarship, for the good reason that Scholars' Lab trains Ph.D. students in advanced research in digital humanities through the Praxis program and several DH dissertation fellowships each year. So graduate study is essential in the background of the new managing director. Scholars' Lab engages in R&D, notably Neatline and the Makerspace, various projects by staff such as Take Back the Archive and Collective Biographies of Women, and a fascinating array of research and pedagogy in many disciplines, from GIS to textual analysis, including collaboration with the Carter G. Woodson Institute for African-American and African Studies.\n\n[UVA Library](http://www.library.virginia.edu/) is a hub of diverse and innovative digital work, with teams working with many methods and platforms in addition to Scholars' Lab. It's going to be dazzling in the coming years, with our new University Librarian, John Unsworth. The Scholars' Lab is fostering a community of practice by hosting the dh.virginia.edu site (in beta). Just this week in the Library, the Rare Book School course on digitization, run by Bethany Nowviskie and my colleague Andy Stauffer, hosted an engaging presentation by the Scholars' Lab's Jeremy Boggs and Purdom Lindblad on feminist interface and rich-prospect browsing. In other words, there is an active spirit of collaboration and a deep bench of expertise. As Academic Director, I see an auspicious alignment of stars in the current administration of Arts and Sciences, ITS, and other schools. It will be [a great moment](https://facultynews.library.virginia.edu/news/summer-2016/) to unite energies and participate in this next phase of open-access humanities research, more library-centered than ever.   .\n\n[Laura Miller](http://scholarslab.org/people/laura-miller/) and I will be happy to answer any questions about the position.\n"},{"id":"2016-06-12-are-you-our-new-managing-director","title":"Are you our new Managing Director?","author":"laura-miller","date":"2016-06-12 05:10:28 -0400","categories":["Announcements","Digital Humanities"],"url":"are-you-our-new-managing-director","content":"We are thrilled to announce an exciting job opportunity on the leadership team here in the Scholars' Lab. Read on for more details!\n\nAre you an innovative leader with a deep research portfolio and strong background in the management of digital humanities projects and practitioners?\n\nThe University of Virginia Library seeks a **Managing Director** to help shape and steward our internationally respected Scholars' Lab. This individual will work closely with the Academic Director to develop and support the [deep resources for digital scholarship](http://scholarslab.org/announcements/managing-director/) within the vibrant intellectual community at UVA. The Managing Director will inform the vision of the Scholars' Lab and ensure its smooth and effective operation as a pan-university center within the Library, overseeing Scholars' Lab staff, resources, and budget. He or she will also help to integrate Scholars' Lab goals and activities into the service profile of the University Library.\n\n**Primary Responsibilities: **\nIn collaboration with the Academic Director, the Managing Director will: shape digital humanities services for the UVA Library; provide oversight of day-to-day operations of the Scholars' Lab; develop, oversee, and retain talented staff; coordinate the use of all Lab resources (funding, space, personnel); develop the Lab's budget, in collaboration with the Academic Director; help to prepare and submit grants; help to plan and organize intellectual programming, fellowships, pedagogical initiatives, and outreach; represent the Scholars' Lab, serving as a liaison with University staff, faculty, and affiliated centers, as well as national and international peers; engage in collaborative planning with colleagues across the Library to promote and support digital scholarship; assist in designing and shaping projects; and help to establish high-level goals, intake processes, workplans, and MOUs for digital project collaborations. The Managing Director will keep abreast of new methodologies and practices relevant to digital humanities and will engage in professional development and their own (often collaborative) research projects related to the mission of the Lab, culminating in publication of results and/or presentation at appropriate venues.\n\n**Specialized Knowledge and Skills: **\n[Graduate study (Ph.D. preferred)](http://scholarslab.org/announcements/managing-director/) in the humanities, library sciences, social sciences, or a related discipline; experience in an administrative position that includes supervision of personnel, experience with a technical area of digital humanities and the ability to advise on hardware and software purchasing and implementation; excellent oral and written communication skills; excellent organizational skills and ability to manage multiple priorities; demonstrated leadership, problem-solving, and decision-making skills; familiarity with recent scholarship and emergent best practices in the digital humanities; ability to teach in workshop or classroom setting and experience with effective digital pedagogy.\n\n**Salary and Benefits:**\nSalary is commensurate with experience, and expected to range between approximately $95 and $105k per annum. Excellent benefits, TIAA/CREF and other retirement plans along with generous funding for travel and professional development.\n\nTo Apply:\nReview of applications to begin immediately and continue until position is filled. Apply through [the posting](https://jobs.virginia.edu/applicants/Central?quickFind=79369) on the University of Virginia's online employment website. (If you need to search the [Jobs@UVa](https://jobs.virginia.edu/) portal, the posting number is **0618965**.)  Complete application, and attach cover letter and CV, with contact information for three current, professional references.\n"},{"id":"2016-07-01-whats-the-scholars-lab","title":"What's the Scholars' Lab?","author":"alison-booth","date":"2016-07-01 09:59:41 -0400","categories":["Announcements"],"url":"whats-the-scholars-lab","content":"Obviously, there are lots of ways to find out what this center is and does, including an exploration of this website. I encourage anyone to see our Charter (not far from here, http://scholarslab.org/about/charter/).\n\nBut if you want to know how we have been recently identifying ourselves, here's a go:\n\nWithin the University of Virginia Library, the Scholars’ Lab since 2006 has been a center for advanced digital scholarship in humanities, information and library science, social sciences, and related fields, emphasizing interpretative and theoretical as well as technological innovation and inquiry. The Scholars’ Lab consists of a collaborative staff of professionals serving a flexible community of graduate fellows, faculty, and staff whose activities intersect here.\n\nWho? Ten (now nine) people, and soon to be eleven or twelve; by some counts, two more. Academic Director, Head of Research and Development, Head of Public Programs, Information Architect, GIS Specialist, Project Manager, two Developers. Searching for Managing Director. Posting for second GIS Specialist in process. Our Head of Graduate Programs is irreplaceable, but we hope for a renewal of this position. Two additional specialists (you should see what they do with drones, Occulus Rift, photogrammetry!) have affiliated with the Digital Media Lab, Content Stewardship, and Scholars' Lab.\n\nMembers of the Scholars’ Lab reach out across the Library and schools of the University, frequently teaching workshops or courses here and elsewhere and presenting their research in conferences or publications.\n\nSLab, open to anyone, exists as much in communications and events as in a place located on the fourth floor of Alderman library, with its offices and graduate lounge, classrooms (shared by the Library), workstations in the public space, presentation facility, and Makerspace.  This is not a drop-off service center, but rather an incubator for consultation and potential sustained collaboration, helping to identify or provide the resources, expertise, and tools that a project needs to become a reality.  In supporting faculty research, we work in alliance especially with the Institute for Advanced Technology in the Humanities (IATH), a center for faculty fellowships and advanced digital research, and with Sciences, Humanities, and Arts Network of Technological Initiatives (SHANTI), a center that develops platforms, tools, and networks for teaching and research.  The Scholars’ Lab hosts and convenes an advisory committee for a website, dh.virginia.edu (in development), an online crossroads leading any user, within the university and outside it, into the network of diverse activities in digital humanities at UVA and elsewhere (with lists of people and projects and a calendar).  We continue to offer a series of lectures, visiting workshops, and short or long courses open to all.  In our service to faculty and students at the University, we collaborate with the subject liaisons of the Library and with several centers based in the Library and schools, including, for example, Research Data Services and the Carter G. Woodson Institute for African-American and African Studies.\nCurrent projects include:\n·      a summer program, Leadership Alliance Mellon Initiative, introducing minority undergraduates to research that may lead to graduate study;\n·      annual cohorts of six Praxis Fellows who learn while creating a group project (graduate students in various fields)\n·      several DH dissertation fellowships annually\n·      Consulting and collaboration with a range of faculty- and student-initiated projects from many fields and schools\n·      Neatline, an Omeka plug-in that has built on years of Scholars’ Lab innovation in visualizing geo-temporal data in humanities research;\n·      A Makerspace for 3D printing and innovative design\n·      Research on “adovocacy by design,” feminist interface, and rich-prospect browsing, in collaboration with Take Back the Archive (on the history of sexual assault at the University of Virginia) and Collective Biographies of Women (a study of the history of representations of women in printed collections of biographies)\n·      GIS consultation and instruction\n·      Affiliated staff working on photogrammetry, augmented reality, and various digital multimedia tools\n·      Continuing innovations in textual editing and literary history as well as textual analysis such as topic modeling\n·      collaboration with IATH and SHANTI as well as VP for IT Ron Hutchins and the Deans of the Library and Arts and Sciences on a conference on DH at UVA, Fall 2016;\n·      With the Woodson Institute and others, a series of lectures in 2016-2017 on digital diversity, accessibility, race, and gender.\n"},{"id":"2016-07-03-what-has-scholars-lab-been-up-to-or-a-community-of-practice-communicating","title":"What has Scholars' Lab been up to? Or, a community of practice, communicating.","author":"alison-booth","date":"2016-07-03 12:57:44 -0400","categories":["Announcements"],"url":"what-has-scholars-lab-been-up-to-or-a-community-of-practice-communicating","content":"Like any active research/development/teaching/service team, the Scholars' Lab faces a challenge keeping up with each others' diverse collaborations, and communicating what we do.  Since January, I've learned about the Library's systems of recording interactions with students and faculty, and I've encountered various applications that will somehow track our data, but I think we need more than CRM (customer relationship management).  We've grown our own self-recognition system, as simple as: monthly activity statements, in a template of categories, shared on Box.  (And Slack is working for us.)  Happily, it turns out that staff enjoy writing these very short notes (_gee, I actually have done more than I remembered!_) and reading each other's (_oh, I'm glad to know she's doing that_; or, _oh, I talked to that faculty member separately, hmmm_).  We have biweekly scrums, but there's much more detail in the shared self-reflective accounts.\n\n[caption id=\"\" align=\"alignnone\" width=\"573\"]![](http://giving.virginia.edu/campaign/wp-content/uploads/sites/2/2013/10/librarymain.jpg) Alderman Library (not in July)[/caption]\n\nI post some excerpts of my \"seasonal\" compilation of activities in April and May, 2016, from the staff's monthlies.  I don't think this somehow scores points compared to other flourishing parts of this library, or other flourishing centers of digital humanities, but I think it profiles a community of practice centered in our practice of communicating.\n\nI was genuinely surprised and excited by the depth and breadth of the \"monthlies\" by Ammon, Chris, Eric, Jeremy, Laura, Purdom, Ronda, and Scott.  Now, if I could only find the monthly time to add my own chronicles of collaboration! Thanks to Ron Hutchins, Worthy Martin, Judy Thomas, Martha Sites, David Germano, and Rennie Mapp for the regular meetings and exciting planning, this spring.\n\n**April and May, 2016:**\n\n**Consultations/Collaborations, Faculty and Students, Library Colleagues—indicating people/projects, not hours or frequency of repeat sessions**\n\n\n\n \t\n  * **Nineteen** consultations and collaborations with **faculty from 12 departments/programs**: Women and Gender Studies, Carter G. Woodson Center for African-American and African Studies, History, English, Curry Education School, German, Kinesiology, Religious Studies, Biology, Sociology, Archeology/Art History, Slavic\n\n \t\n  * At least **twenty** students from **doctoral to undergraduate (in addition to Praxis)**, in such fields as Music and Anthropology: **individual consultations**, not including hands-on aid in Makerspace; from learning Python to project design to GIS\n\n\n**Scholars’ Lab as Starting Point, Incubator, Training Center, and Curator of Projects in DH **\n\nWith a wealth of centers and nodes of digital expertise and activity at UVA, Scholars’ Lab has taken initiative to improve collaboration and communication, launching [http://dh.virginia.edu](http://dh.virginia.edu), **a website**, in May (in development).  Many faculty and student projects may be born and in due course put to rest/sustainable pasture through Scholars’ Lab, and we guide people to find the collaborators and resources they need in between.  We are a consultation hub and work closely with IATH, SHANTI, Research Data Services, Arts and Sciences and Provost’s Office administration, Library leadership, and ITS, among others.\n\n**Some of the Continuing Projects: **Neatline; Geoblacklight; Makerspace; Take Back the Archive (American Studies); Collective Biographies of Women (with IATH); For Better for Verse; Participatory Media; Salem Witch Trials; text analysis (18th-c. English); projects in NLP, GIS, 3D printing, augmented reality, data mining from social media, sonification\n\n**Speakers and Events Series.**  In Spring, 2016, we held eight events open to the public and the Library, three of which focused on Praxis fellows (current or alum) or the Visiting Fellow presenting on their work.  In spring, 2016, we began planning a DH@UVA fall conference and a lecture series for 2016-2017 on diversity and access issues in DH.\n\n**Teaching Courses and Support of Course Projects** Members of the staff collaborate with the Teaching and Learning group in the Library and the subject liaisons, particularly Chris Ruotolo, to offer workshops on Neatline and GIS, or to offer short and for-credit courses.  One staff member taught an undergraduate course in American Studies.  SLab staff taught workshops for undergraduate and graduate courses in English, Architecture, German, and other fields.  Staff have collaborated on a course-based project on the archeology of Flowerdew Hundred.\n\n**Research, Presentations, Publications**\n\nMembers of Scholars’ Lab are active in professional development; for instance, taking a course in Design Thinking; learning new methods of topic modeling; keeping up with the latest in bots and 3-D printers as well as drones and VR; presenting research that is informed by narrative theory, critical race studies, queer theory, or cultural geography; adapting research on user experience and information design.  Our work will be presented at DH2016 in Krakow, by six members of the staff, at a pre-conference workshop on biographical data and several other sessions.  SLab members presented at the British Library and at Yale University this spring.  A co-authored essay was proposed, accepted, and is now in process for publication in DH+Lib.\n"},{"id":"2016-07-11-lower-the-stakes","title":"Lower the Stakes","author":"ronda-grizzle","date":"2016-07-11 07:02:53 -0400","categories":["Technical Training"],"url":"lower-the-stakes","content":"I recently received the greatest compliment on my technical training ability that I've ever gotten.\n\nWhen the Seminar on the Acquisition of Latin American Library Materials ([SALALM](http://salalm.org/)) conference was held here at UVA this past May, I led a Neatline workshop for 45 attendees of the conference. For 90 minutes, we worked through the Lab's standard introduction to the Neatline interface together, solving network issues and technical glitches as we went (something that I could not have accomplished without the able assistance of my Scholars' Lab colleagues, who all volunteered to serve as \"back of the room\" tech support!), and at the end of the session my 45 students all had a small working Neatline exhibit and ideas about how they might apply that digital tool in their own research and at their own institutions.\n\nThe compliment? One of my students told me that she'd had so much fun, that she wasn't aware of the time passing.\n\nSo how does that happen? A novice level student having so much fun that she experiences [a flow state](http://www.pursuit-of-happiness.org/history-of-happiness/mihaly-csikszentmihalyi/) while learning a complex interface, in a very crowded, noisy classroom where people are trying to follow the instruction, or get their wireless passwords to let them connect to the network, or to figure out why their browser did that weird thing they weren't expecting?\n\nWe lower the stakes. We take the training very seriously, but we make it as much fun for the students as possible.\n\nOur Neatline workshop is designed to reduce risk for the students. We always teach in our sandbox installation of Neatline, using a pre-populated set of Omeka records and a carefully scripted workflow that introduces students to every part of the interface with just enough repetition that concepts are reinforced without inducing boredom. Students are not working on a site where their work will later be graded, or will be in public view. They're not working with data that matters to them professionally or academically. We deliberately lower the stakes so that they don't feel as much pressure as they otherwise might.\n\nPlayfulness is a good path to learning. Making the workshop as playful as I can helps people to let go of their tension around the vulnerability that learning requires (adult learners can be especially stressed about this!), and a less stressed student is a student who is more able to stay engaged, and who later retains more of what they've learned.\n\nAre you a trainer or instructor? How do you lower the stakes for your students?\n\n"},{"id":"2016-07-22-robocamp-2016","title":"Robocamp 2016","author":"ammon-shepherd","date":"2016-07-22 12:02:04 -0400","categories":["Research and Development"],"url":"robocamp-2016","content":"I had the pleasure of spending a week with the folks in the Architecture School learning and playing with their Kuka robot (named Karl, [http://www.robotsinarchitecture.org/](http://www.robotsinarchitecture.org/)). This was the first run of a hopefully recurring camp to introduce faculty and staff to the robot arm in the Fab-lab in the A-school. Most of the participants were A-school faculty, but there was also me from the Library, someone from the Music school's Maker space, and someone from the Bio-engineering school (they just purchased a Kuka KL 3000, a much bigger bot).\n\n![IMG_20160722_140917](http://scholarslab.org/wp-content/uploads/2016/07/IMG_20160722_140917-949x1024.jpg)\n\nOver four days we learned how to make the robot do marvelous and wonderful tricks.\n\n\n### Day 1: Push some clay around.\n\n\nThe object of this day was to learn the basics of the software that runs the Kuka robot.\n\nWe picked a tool that we would have the robot push into the clay at a certain number of intervals. The idea being to pour plaster onto the clay to create a permanent design:\n\n[gallery link=\"file\" size=\"medium\" columns=\"2\" ids=\"12823,12824\"]\n\nThis was the most basic tool, just an extension of the arm itself, with different shapes on the end.\n\n\n### Day 2: Cut a foam block\n\n\nThis day we used the robot to cut a foam block with a hot wire. We drew a line in the software that would correspond to a long vertical cut in the block. What we didn't know at the beginning, was that after the first cut the block is rotated 90 degrees, and the cut is made again.\n\nThis produced four long pieces that could be rotated to form a column with a unique profile on each side.\n\n![IMG_20160720_154748](http://scholarslab.org/wp-content/uploads/2016/07/IMG_20160720_154748-758x1024.jpg)\n\nI wrote my name with the line, which produced this:\n\n![IMG_20160720_154713](http://scholarslab.org/wp-content/uploads/2016/07/IMG_20160720_154713-1024x758.jpg)\n\nThe focus of this day was to help us think spatially. We could add twists to the curves which led to columns that were hard to mentally visualize how they would turn out. Here is a sample of the columns our group generated:\n\n[gallery columns=\"2\" size=\"medium\" ids=\"12835,12836,12834,12833\"]\n\n\n### Day 3: Light writing\n\n\nWe used a little bit more sophisticated tool end this day. It was an LED light that could switch colors. We designed a shape and assigned a color to each line or curve or segment.\n\nI didn't get very creative on this one, but others had great ideas:\n\n[gallery size=\"medium\" ids=\"12839,12837,12838,12840,12841,12842,12843\"]\n\n\n### Day 4: 3D extruding\n\n\nThis day was a more demonstrative than hands-on. We went through the basics of how to get the robot to use a 3D pen to print filament in 3D space. Rather than limiting printing to a series of layers on top of each other, the printing can happen in true 3D space.\n\nFinally, we sat together and discussed how robotics can and should impact our fields, and how the robo-camp went in general.\n\nThe discussion was light, and basically just touched on the following topics, or simply asked these questions. No deep discussion or trying to answer, just a quick question- and thought-dump session.\n\n\n\n \t\n  * What can robots do that humans can't, besides going faster and more accurate?\n\n \t\n  * What can robots do for humanities research? How can they improve and help data visualization?\n\n \t\n  * What are the components necessary for the robot to function? The robot has motion, but it needs sensory input, data, in order to act on it.\n\n \t\n  * One of the former students presented an example from their year of working with the robot and noted a frustration that perhaps Libraries can and should be able to help with. This group wanted to use the robot to print cement in true 3D (not like traditional 3D printers that basically print 2D layers, and then print layer upon layer). In order to accomplish this, they had to design and build their own tool. The idea was to have a nozzle that changed shape while extruding the cement, thereby adding another layer of control and design to the printed object. The group was frustrated in the many hours spent in figuring out how to get motors to operate, when there are people out there (and probably students on campus) who know how to do this in their sleep. They spent time developing the tool instead of refining the product. Sometimes creating the tool is a beneficial and important step in creating a product. Other times it detracts from the end goal. Libraries could/should be a great resource for connecting two groups that can benefit from each others collaboration. The Scholars' Lab can/should be such a hub of networking and connection.\n\n\nAll in all it was a great experience. Mind opening and enlightening.\n"},{"id":"2016-08-12-neatline-implementation-grant","title":"Neatline Implementation Grant","author":"eric-rochester","date":"2016-08-12 05:29:52 -0400","categories":["Announcements"],"url":"neatline-implementation-grant","content":"![Screen Shot 2016-08-11 at 4.23.37 PM](http://scholarslab.org/wp-content/uploads/2016/08/Screen-Shot-2016-08-11-at-4.23.37-PM.png)\n\nYou may have noticed on Twitter or elsewhere that the [NEH announced funding for almost 300 humanities projects](http://www.neh.gov/news/press-release/2016-08-09). Congratulations to all!\n\n\n\nOne of the projects awarded was our [Neatline](http://neatline.org/) Omeka plugin! We're really excited by the possibilities that this will open up for this project and the ways that we're planning on improving it.\n\n\n\nSo what do we hope to accomplish? What should you be looking for from the future of Neatline?\n\n\n\nPrimarily, we're going to focus on _graphesis_ and _sustainability_.\n\n\n\nFirst, Neatline has always been an experiment in trying to embody the principles of [graphesis](https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxlbmdhZ2VtZW50YnlkZXNpZ258Z3g6NzY1ZGY3ZWRmMmRkYTY3NA). It's motivated by the belief that interacting with your project and your data in a visual, hands-on, and messy way informs and changes the way that you think about your project. Of course, modern technology is limited in how well it can enact these principles. As mobile platforms and tablets have matured, however, they offer a more hands-on, material experience. We'd like to incorporate these new technologies and leverage them to improve the experience of creating and exploring Neatline exhibits.\n\n\n\nRelated to this, we want to improve the [Neatline Text](https://omeka.org/add-ons/plugins/neatlinetext/) add-on. This allows Neatline exhibits to incorporate and interact with long-form text. Unfortunately, the editing interface is still very rough. Improving this would go a long way to making Neatline a more compelling platform for creating geotemporal exhibits around a text.\n\n\n\nAnd in general, we'll revisit the editing interface for Neatline to streamline it for common tasks, while making less used features still easily accessible.\n\n\n\nSecond, Neatline is itself also maturing, and like any software project, it's aging gracefully in some ways, and—ahem—less so in others. We'd like to take this opportunity to make Neatline a more sustainable project. This is a broad goal, so let's tease it apart. What do I mean by this?\n\n\n\n\n\n  * The Omeka team at [RRCHNM](http://chnm.gmu.edu/) are working on a new version of the platform, called [Omeka S](https://github.com/omeka/omeka-s), that will make it significantly easier to host and managing multiple Omeka instances. This will involve significant changes to the plugin architecture, however, so we'll update Neatline to work with this new version of Omeka.\n\n\n  * Currently, Neatline uses [OpenLayers](http://openlayers.org/) for its mapping component. After we included it in Neatline, however, OpenLayers released version 3, which is a major rewrite of this component for modern browsers. Because of licensing issues, it also no longer supports Google maps. We'll take this opportunity to evaluate upgrading to OpenLayers 3 or going with a different mapping component altogether, such as [LeafletJS](http://leafletjs.com/).\n\n\n  * For the timeline component, Neatline uses the [SIMILE Timeline](http://www.simile-widgets.org/timeline/). This software is also showing its age, so we'll look at using a different timeline component, possibly creating one ourselves. This will also give us the chance to evaluate the data models that we use to represent time and change them to accommodate ambiguity and fuzzy dates better. This will also make this part of Neatline better suited to messy humanities data.\n\n\n  * But the most important sustainability feature that we'd like to work on is building a more sustainable and active community. This includes better documentation, better tutorials, and better support for getting new developers set up to work on Neatline. We'd also like to make it easier for users to contribute in many ways to the project, whether through code, documentation, design, or other ways.\n\n\n\n\nWe're really excited about the opportunities that this grant opens up for us, and we're so grateful to the NEH ODH for providing the resources for this.\n\n"},{"id":"2016-08-23-programmatically-building-high-level-charts-with-bokeh","title":"Programmatically Building High-level Charts with Bokeh","author":"scott-bailey","date":"2016-08-23 05:43:39 -0400","categories":["Technical Training","Visualization and Data Mining"],"url":"programmatically-building-high-level-charts-with-bokeh","content":"A couple of months ago, while preparing for the Digital Humanities 2016 conference, I was trying to build a series of charts to visualize data results from some topic modeling I had done. Specifically, I had a data file in which each row was a document and the columns were topic proportions. Reading across any row, you could understand the document’s composition according to the twenty topics of the topic model. Reading down a column, you could understand how a topic was more or less present across the text corpus, in this case a series of logically and chronologically ordered documents. For the charts, I wanted to create a separate chart for each column of the data table so that I could _see_ the distribution of the topic across documents and understand how it was more or less present in different sections of the corpus. I figured bar graphs would be a straightforward way to do this, with each bar representing the topic proportion for a single document.\n\nI initially tried to do this with [D3](https://d3js.org/), a Javascript library frequently and regularly used to create interactive visualizations. I’ve used D3 before, and while I do find it somewhat tedious, it’s usually not too difficult to craft the more straightforward types of graphs, like bar graphs. I ran into an issue, though, which is that I wanted to take my single data file and produce twenty different bar graphs, and I wanted to do it programmatically rather than having to write code for each graph. This is certainly possible in D3, but while I made progress toward it, my Python loving self grew quickly frustrated with how hard it seemed to be to iterate through the columns of the data table, something I knew I could do quickly and easily with Python and [pandas](http://pandas.pydata.org/), perhaps the most regularly used data analysis library in Python.\n\nI decided, then, that it was a great opportunity to learn to do a bit of data visualization in Python. There are a number of different libraries available to you: [matplotlib](http://matplotlib.org/), [Seaborn](https://stanford.edu/~mwaskom/software/seaborn/), [Bokeh](http://bokeh.pydata.org/en/latest/), and more. I chose to use Bokeh due to its easy integration with pandas and its multi-level API. With Bokeh you can quickly and easily create [high-level charts](http://bokeh.pydata.org/en/latest/docs/user_guide/charts.html), such as bar graphs, box plots, or histograms, with a minimal amount of code and configuration. Or you can work with Bokeh’s [mid-level interface](http://bokeh.pydata.org/en/latest/docs/user_guide/plotting.html), creating figure elements such as circles and adding them to figures you create. Finally, if you want complete control over every bit of the chart, you can work directly with Bokeh’s [models](http://bokeh.pydata.org/en/latest/docs/reference/models.html#bokeh-models).\n\nSince I just wanted to create a series of bar charts, I worked directly with Bokeh’s high-level chart API. Here’s the script I wrote with extensive comments.\n\n[code language=\"python\"]\n# import pandas to handle data\nimport pandas as pd\n# from bokeh's high-level charts api, import Bar to create the bar graph,\n# output_file to save the generate html files, and show to immediately show\n# the generated html files\nfrom bokeh.charts import Bar, output_file, show\n\n\ndef break_names(df):\n    \"\"\"Parse filenames into paragraph numbers and titles, adding those\n    into the dataframe\"\"\"\n    # grab the column with filenames and convert it to a list\n    filenames = df['file'].tolist()\n    # create empty lists to hold paragraph numbers and titles\n    # every document in the corpus has a unique paragraph number and title\n    para_nums = []\n    titles = []\n    # iterate over the filenames, split them, and push each component into the\n    # correct list\n    for filename in filenames:\n        num_and_name = filename.split('/').pop()\n        chunks = num_and_name.split('-')\n        para_nums.append(chunks[2])\n        titles.append(chunks[3])\n    # create new dataframe columns from the lists of paragraph numbers\n    # and titles\n    df['para_nums'] = pd.Series(para_nums)\n    df['titles'] = pd.Series(titles)\n    # return the dataframe with the added columns\n    return df\n\n\ndef main():\n    # read in the csv with the data\n    data = pd.read_csv('barth_composition.csv')\n    # create the dataframe with paragraph number and title columns\n    cleaned_data = break_names(data)\n    # get the column names from the dataframe\n    columns = cleaned_data.columns.values\n    # use a list comprehension to create a list of all the column names that\n    # contain the word 'topic', e.g. 'topic-01'\n    topics = [topic for topic in columns if 'topic' in topic]\n    # for each topic/column create a bar graph\n    # see documentation here:\n    # http://bokeh.pydata.org/en/latest/docs/user_guide/charts.html#bar-charts\n    for topic in topics:\n        plot = Bar(\n            cleaned_data,\n            'para_nums',\n            values=topic,\n            ylabel='Proportion',\n            xlabel='Paragraph Number',\n            title=\"Proportion per pararaph of \" + topic\n        )\n        # save the html output file\n        output_file('charts/' + topic + '.html')\n        # open the html output file in the browser\n        show(plot)\n\n\nif __name__ == '__main__':\n    main()\n[/code]\n\nI really like Bokeh, and creating a series of bar charts was incredibly straightforward. There is a definite limitation with Bokeh's output options though. At the time I wrote this script, the primary output type is an HTML file, which is entirely reasonable given that Bokeh describes itself as \"a Python interactive visualization library that targets modern web browsers for presentation.\" It is deliberately positioned as a Python alternative to writing D3 yourself. There have been requests for the Bokeh team to add the ability to directly save output as either SVG or PNG, but, given [this Github issue thread](https://github.com/bokeh/bokeh/issues/538), it seems like there are significant technical blocks to implementing this in the near future. That said, the interactive graph window does allow you to click an icon to save a static version of the graph as it currently appears in the browser as a PNG. While this isn't particularly efficient, it works just fine when you need to generate a few static figures for papers or presentations. Since I'm thinking of creating a website that includes these visualizations, I'm not bothered by the HTML output, though in the future, when I'm just creating figures for a presentation, I might use Seaborn, built on matplotlib, which does allow you to save directly to PNG and other formats. On the whole, especially if you're already a Python person and find writing D3 a bit tedious, I strongly recommend taking a look at Bokeh.\n"},{"id":"2016-08-26-fall-2016-uva-library-gis-workshops-series","title":"Fall 2016 UVa Library GIS Workshops Series","author":"chris-gist","date":"2016-08-26 06:16:27 -0400","categories":null,"url":"fall-2016-uva-library-gis-workshops-series","content":"**Fall 2016 UVa Library GIS Workshop Series**\n\nAll sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on **Wednesdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421** (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.\n\nSeptember 21st\n**Making Your First Map with ArcGIS\n**Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa.\n\nSeptember 28th\n**Georeferencing a Map - Putting Old maps and Aerial Photos on Your Map\n**Would you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS.\n\nOctober 5th\n**Getting Your Data on a Map\n**Do you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map.\n\nOctober 12th\n**Points on Your Map: Street Addresses and More Spatial Things\n**Do you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis.\n"},{"id":"2016-09-09-coming-soon-dhuva-conference-oct-1415-2016","title":"Coming soon: DH@UVA Conference Oct. 14&15, 2016","author":"alison-booth","date":"2016-09-09 10:53:39 -0400","categories":null,"url":"coming-soon-dhuva-conference-oct-1415-2016","content":"After you enjoy NEH@50 celebrations, and in between this fall's workshops and speakers, come to an intimate conference on digital humanities, right here on Grounds, Friday and Saturday October 14 and 15, 2016.  We've been developing an interesting conference program aimed at building the DH community at UVA.  At this event we'll also be further introducing the design and purposes of the website (in development): [http://dh.virginia.edu/](http://dh.virginia.edu/).  This event's reflection on the present and future is a joint effort of ITS, the Library, Arts & Sciences, and the Provost's Office; [Scholars' Lab](http://scholarslab.org/), [IATH](http://www.iath.virginia.edu/), and [SHANT](http://shanti.virginia.edu/)I.\n\nEvents are free and open to the public. We'll be looking for faculty and graduate students to participate in the program, so please reply to Rennie (see below) or to me, booth@virginia.edu if interested in a lightning talk, participation in a roundtable, or otherwise.\n\nFor more preliminary detail, here is the Save the Date message prepared by the Project Manager of the conference, Rennie Mapp:\nDear members of the UVa DH community:\n\nPlease plan to attend DH@UVa 2016 on Friday, October 14 and Saturday, October 15 in the Harrison/Small Auditorium of the Special Collections Library. We'll be considering the present and future of the Digital Humanities at the University of Virginia.\n\nConference sessions will include panel discussions with outside speakers, lightning talks, break-out conversations, pop-up brainstorming, and opportunities to talk informally with others at UVa who are interested in the Digital Humanities (broadly speaking). The public, faculty, staff of the libraries and other organizations, and students at the University are most welcome!\n\nSoon I'll be notifying you about open registration for these events on our website. The conference is FREE but we appreciate notice that you'll attend so that we can plan food, refreshments, and space. In the meantime, please mark your calendar and consider giving a lightning talk or formulating a good topic for a break-out conversation. If you have any questions or thoughts about the conference, please contact me, Rennie Mapp (conference manager), at rcmapp@gmail.com\n\nThis conference is sponsored by Ron Hutchins, VP for Information Technology. It is organized by Alison Booth, Director of Scholars' Lab; David Germano, Director of SHANTI; and Worthy Martin, Director of IATH--in consultation and with the active support of Archie Holmes, Associate Provost for Academic Affairs; John Unsworth, Dean of Libraries and University Librarian; and Francesca Fiorani, Associate Dean for Arts and Humanities.\n"},{"id":"2016-09-21-are-you-our-next-head-of-graduate-programs","title":"Are you our next Head of Graduate Programs?","author":"alison-booth","date":"2016-09-21 11:24:07 -0400","categories":null,"url":"are-you-our-next-head-of-graduate-programs","content":"\"We build up people and practices more than products.\" That's part of our [charter](http://scholarslab.org/about/charter/), and the Head of Graduate Programs is essential to this mission.  We in the Scholars' Lab and University of Virginia Library are thrilled to alert you to the new job posting [https://jobs.virginia.edu/applicants/Central?quickFind=80148](https://jobs.virginia.edu/applicants/Central?quickFind=80148)\n\nAs the description on the University of Virginia job site emphasizes, the primary responsibilities focus on the [Praxis Program](http://praxis.scholarslab.org) and other graduate DH Fellowships.  \"Mentoring, managing day-to-day operations, and coordinating staff support for both team-based and individual graduate fellowship programs at U.Va. Library. Developing intellectual programming in the digital humanities for the Scholars' Lab and building community among emerging scholars at U.Va. Fostering collaboration on humanities training and research support with internal and external partners.\"  The Head of Graduate Programs works closely with graduate programs around the university to arrange fellowships and collaborates with librarians in enhancing the digital curriculum.  Feel free to imagine how you might weave your own interests and research into these responsibilities (we make time for staff members to pursue research projects).  We look for talent in communication and community building, experience in the context of digital research and instruction, and familiarity with graduate studies in your discipline(s): \"Master's degree or equivalent in the humanities or humanistic aspects of information science.\"\n\nThis is a great time to join the broad and deep teams of digital scholarship at the University of Virginia, with our new Dean of Libraries John Unsworth and the University's reconfirmed commitment to innovative humanities research in the biggest tent of DH.  Scholars' Lab collaborates with the [Carter G. Woodson Institute for African-American and African Studies ](http://woodson.virginia.edu/)and the [Institute of the Humanities and Global Cultures](http://ihgc.as.virginia.edu/).  The Head of Graduate Programs can help shape both the research agenda and the series of workshops and public programs that serve the entire University community, along with the Scholars' Lab's directors, the Head of Public Programs, and others in this collaborative center within the Library.\n\n**Salary and Benefits:**\nSalary is commensurate with experience, and expected in the $65,000 - $75,000 range per annum. Excellent benefits, TIAA/CREF and other retirement plans along with generous funding for travel and professional development.\n\nTo Apply:\nReview of applications to begin immediately and continue until position is filled. (If you need to search the [Jobs@UVa](https://jobs.virginia.edu/) portal, the posting number is 0619604).  It is easy to apply: complete the application, and attach cover letter and CV, with contact information for three current, professional references.\n"},{"id":"2016-11-09-3d-printing-for-fun-and-presentation","title":"3D printing for fun and presentation","author":"ammon-shepherd","date":"2016-11-09 04:30:56 -0500","categories":["Makerspace"],"url":"3d-printing-for-fun-and-presentation","content":"This post is a quick update on a couple of projects by students who frequent the makerspace. All of the students who use the space are doing amazing things, and we hope to highlight some of those projects like this more often.\n\nThe project write ups are written by the students themselves. So without further introduction, here are Brandon Phan and his Pokédex phone case and Arian Azizi and civil war bullets for a class presentation.\n\n[Header image source: https://commons.wikimedia.org/wiki/File:International_Pok%C3%A9mon_logo.svg]\n\n\n## Brandon Phan - Pokédex\n\n\n\n\n3D printing a pokedex case has been the highlight of my 3rd week as a first year. I learned so much about the process and the materials used in 3D printing. It's really mesmerizing to watch the nozzle create something you saw on the computer screen. Everyone who worked there appreciated my eagerness and were did their best to help me finish my case and learn more about the machine. We messed up a few times but we were always able to fix it without having to redo the whole case. I learned the difference between PLA and ABS. I learned how different machines work. I learned about nozzles and layer sizes. I learned about how to print. Along the way of my educational adventure, I got a pokedex case for my phone that helps me catch pokemon on Pokemon Go. Whenever, I walk with Pokemon Go open, some people ask where I got it. I point them in the direction of the MakerSpace lab in Alderman West Wing. Many people don't know about it, and while it has been my secret for a little bit, I hope more people come as long as they don't take all my printing timeslots!\n\n\n\n\nAlso it saves me a lot of money as a first year who doesn't have a car. Instead of buying guitar picks, I can just make my own!\n\n\n\n\n[caption id=\"attachment_13006\" align=\"aligncenter\" width=\"1024\"]![pokedex](http://scholarslab.org/wp-content/uploads/2016/11/pokedex-1024x768.jpg) Image source: Brandon Phan[/caption]\n\n\n## \n\n\n\n\n## Arian Azizi - Civil War Bullets\n\n\n\n\nThe most important part of any presentation is of course...the presentation. No matter how comprehensive and long the content, if the actual delivery of the material itself fails to deliver, the entire project itself falters. In order to create a robust presentation then, the perception of the information must engage the audience. And what better way to get the attention of viewers than providing them with something more than a mere PowerPoint slide? \n\n\n\n\nHaving a hands-on experience always leaves an impression on a group that simple text will never be able to match. Thus when I was pondering over how best to capture the attention of my audience, the notion of tangible setpieces sprung to mind. In my presentation, which had the aim of conveying all of the emotion, strategy, and information of the Civil War in a mere 10 minutes, I pulled back to one very basic question: how was the war fought? The simple answer of \"guns and bullets\" may have checked the box off information-wise, but would it really make the audience invested in the trials and tribulations of the soldiers? It was then that I considered the idea of having the audience be able to actually look at these very tools up close. It is one thing to see pictures of bullets in photographs centuries old, but it is another thing entirely to hold one such model personally. To turn a rifle bullets over in your hands and imagine the destruction that such a small object could inflict. The chosen bullet in question, the Minié ball, was such an improvement over the older musket balls that instead of merely lodging inside of the target, it would tear muscles and cause bones to shatter. These very bullets, popularized through the Civil War itself, marked a transition in the art of warfare itself. The 3D printing of these bullets was an interesting component of the project. Created out of plastic filament, these scale models were built from the ground up, crafting an entire replica of a metal bullet in under an hour. \n\n\n\n\nThe presentation itself went exceptionally well. Beginning with passing about the bullets and discussing their influence on the conflict, I went through the content and conveyed the importance of the battles. With bullets in hand, my audience were able to actually imagine the very bullet models in their hands being shot out of rifles into enemy soldiers.  In fact the presentation went so well that my final grade was a 100 on this assignment, with my instructor citing the bullets as \"clinching the brilliance\" of the entire presentation. Not only did I illustrate the material in a graphic way, but I allowed for the audience to become engaged themselves in this narrative I unfolded around the Civil War. \n\n\n\n\nPerhaps it was no surprise then that immediately afterwards, the class gathered around me and asked how I had created these bullets, and what else they would be able to 3D print. And as I subsequently told them, only their imaginations were their limit.\n\n\n\n\n[caption id=\"attachment_13014\" align=\"aligncenter\" width=\"1024\"]![civil war bullets](http://scholarslab.org/wp-content/uploads/2016/11/IMG_20161109_112829970-1024x576.jpg) Image source: Ammon Shepherd[/caption]\n"},{"id":"2016-12-03-hybrid-literature-ruth-ozekis-a-tale-for-the-time-being","title":"Hybrid Literature: Ruth Ozeki’s A Tale for the Time Being","author":"christian-howard","date":"2016-12-03 09:10:04 -0500","categories":["Digital Humanities","Experimental Humanities","Grad Student Research"],"url":"hybrid-literature-ruth-ozekis-a-tale-for-the-time-being","content":"As a scholar of contemporary literature, I have naturally been drawn to the incredible literary innovation that has exploded in the wake of digital developments. I’m certainly not alone in my interest, and critics such as Katherine Hayles, Marie-Laure Ryan, Wolfgang Hallet, and Jan-Noël Thon have discussed the role of new media in literary studies, including video games, hypertext fiction, and comic books. Yet here I want to focus on contemporary writers who have begun exploiting new technologies in even more subtle ways, using technology as a means of supplementing more traditional printed books. More specifically, these texts employ a “both-and” approach, relying upon traditional publishing platforms (no matter how international the dissemination) while including new media elements that extend beyond print to reach the burgeoning generation of digital readers. Such works range from Indra Sinha’s online stories and sketches that extend the fictional world of his _Animal’s People _to Ali Smith’s incorporation of images and “Google poems” in _Artful_ and David Mitchell’s creation of a live Twitter account for one of his characters in _Slade House_. Even as these hybrid texts experiment with new technologies and print platforms, so do they use new technology for the purposes of publishing and branding, in order to reach a different audience, and as a means of developing a new, innovative aesthetic. So let’s look more closely at one of these hybrid texts in particular, namely, Ruth Ozeki’s 2013 novel, _A Tale for the Time Being_.\n\nSo here’s the back-story: Ozeki’s _A Tale for the Time Being_ pivots between the fictional worlds of Naoko Yasutani (“Nao,” pronounced “Now”) and a fictional persona known as Ruth Ozeki, who is modeled upon Ozeki herself. The story takes place on a small Canadian island, where Ruth discovers the diary of a sixteen year-old Japanese girl preserved within a Hello Kitty lunchbox. Ruth, who is herself part Japanese and able to translate the diarist’s – Nao’s – scrawl, subsequently begins reading Nao’s diary, and _A Tale for the Time Being_ alternates between recording Ruth’s English translation (complete with footnotes) of Nao’s diary and Ruth’s own life struggles as she engages with Nao’s text.\n\nPublished simultaneously not only in hardback and paperback editions, but also as an e-book and an audio download, Ozeki’s novel stands as a pioneering work in terms of experimental publication. That is, Canongate (the publisher of the first published edition of _A Tale_ in the UK) was particularly attentive to creating a brand for Ozeki’s novel that reached across print and digital audiences. As such, the covers of all these editions feature the face of a girl superimposed on a landscape, which is partially concealed behind a peeled-back red circle (reminiscent of the Japanese flag) – see Figure 1 below.\n\n[caption id=\"attachment_13035\" align=\"alignnone\" width=\"300\"]![Figure 1. Screen grab from the Canongate website](http://scholarslab.org/wp-content/uploads/2016/12/bundle_visual-300x185.jpg) Figure 1. Screen grab from the Canongate website[/caption]\n\nCate Cannon, the head of marketing at Canongate, has remarked on this focus during the publication planning of Ozeki’s novel. She says: “The animation and design translates across our digital outdoor advertising, our website and all our editions, creating a brand identity for this novel that is enriching, engaging and progressive” (Montgomery). Nonetheless, the real innovative aspect of Canongate’s publishing lies not in the similarities between each edition, but rather in the subtle differences, for each of the covers of these versions differed slightly based on medium. The hardback version, for instance, was published with the complete image of the girl’s face printed on the cover; this image was wholly concealed by a red sticker that the reader could physically peel back. This format was unique to the hardback version, and the paperback edition simply printed the image as pictured above, with the red circle partially concealing the girl’s face. Yet the paperback also employed technology in a unique way to create a kind of hybrid cover: this has resulted in the first “interactive book cover.” Book designer Zoë Sadokierski explains: “Published by Canongate, art director Rafi Romaya collaborated with creative agency Big Active. ....The ‘interactive’ aspect of the physical book cover involves Blippar technology (augmented reality) – using the camera on a smart phone/tablet, you can link to audio/visual material, via the Blippar app” (Sadokierski). In other words, if you have a paperback copy and have downloaded the Blippar app, you can point your smartphone or tablet at the cover: a virtual sticker will appear on your device, which you can “peel back” to reveal the girl’s face; this face has even been animated to move for 15 seconds (you can view the Blippar “interactive cover” here: [https://www.youtube.com/watch?v=N7vpb7UPK6E](https://www.youtube.com/watch?v=N7vpb7UPK6E)).\n\nThis attention to technology and branding is further manifested in Ozeki’s creation of a supplementary “book trailer,” in which Ruth (played by Ozeki) discovers Nao’s diary (see Ozeki’s website, _OzekiLand_: [http://www.ruthozeki.com/](http://www.ruthozeki.com/)). Even as it blends media by applying a form previously reserved for television shows and movies to a novel, this book trailer appeals to Ozeki’s tech-savvy audience of e-readers, and indeed, the e-book links to YouTube videos, including interviews with Ozeki and video reviews of the novel. While these various covers and animations may sound gimmicky, we can nonetheless see how media and technology have become increasingly important both for authorial branding and in order to reach a larger, more tech-savvy audience.\n\nAs I’ve examined Ozeki’s supplementary use of technology in her otherwise fairly traditional print novel, I’ve been struck by several questions: How does the use of augmented reality – both through applications such as Blippar and videos of fictional works in which the author and main character appear as the same individual – alter our understanding of the reality-fictional divide? Do digital spaces provide a kind of “hyper reality,” and if so, how might we interpret this hyper-reality? In what other ways are “hybrid” literary works experimenting with technology, and what implications does such experimentation have upon the development of literature? In short, I’m excited to see how contemporary writers continue to engage with digital developments!\n\n\n\n\n\nWorks Cited\n\nMontgomery, Angus. “An Interactive Book Cover From Canongate and Big Active.” _Design Week_. N.p., 22 Feb. 2013. Web. 30 Aug. 2016.\n\nOzeki, Ruth. _A Tale for the Time Being_. New York: Penguin Books, 2013. Print.\n\nSadokierski, Zoe. “On Publication Design: Interactive Book Covers.” _zoesadokierski.blogspot.com_. N.p., 23 Feb. 2013. Web. 30 Aug. 2016.\n\n\n"},{"id":"2016-12-13-discussions-in-the-digital-humanities-and-learning-new-technologies-in-the-scholars-lab","title":"Discussions in the Digital Humanities and Learning New Technologies in the Scholars’ Lab","author":"sarah-mceleney","date":"2016-12-13 06:25:55 -0500","categories":null,"url":"discussions-in-the-digital-humanities-and-learning-new-technologies-in-the-scholars-lab","content":"Over the course of the Fall 2016 semester, Praxis fellows participated in weekly meetings to discuss key topics relevant to the field of the digital humanities.  At the beginning of the academic year, we considered the numerous debates about the definition of the digital humanities, relying on the collection of articles, _Debates in the Digital Humanities_ (2016).  Digital humanities, as an academic discipline, has been a notoriously tricky field to define, and conflicting definitions abound in the literature, leading to a vast range of ways that different people understand the discipline.\n\nAfter our discussions and study of the digital humanities as a discipline, we began to learn about important technologies for digital humanists. Over the course of the fall semester, we practiced using HTML and CSS, as well as some Javascript.  Praxis fellows were introduced to the Javascript library, JQuery, as well as the CSS frameworks, Bootstrap and Materialize. We also worked extensively with the version control system Git as a method for working collaboratively on projects.  None of the fellows had used Git before, and after the tutorials and practice sessions led by Scholar’s Lab staff, we attained the ability to confidently use Git for our projects.\n\nThis semester’s introduction to the digital humanities has been has helped us realize how many ways that the discipline of digital humanities is defined, with some scholars focusing on social activism, some on technological aspects, and others on the qualitative analysis of digital media.  Of course, the different angles of digital humanities scholarship can and do overlap and intersect, which make the digital humanities, as I see it, such a dynamic and engaging interdisciplinary field.\n"},{"id":"2016-12-13-praxis-on-choosing-a-subject-of-study-or-how-did-we-come-to-the-kardashians","title":"Praxis on Choosing a Subject of Study, or, How did we come to the Kardashians?","author":"alicia-caticha","date":"2016-12-13 07:28:04 -0500","categories":null,"url":"praxis-on-choosing-a-subject-of-study-or-how-did-we-come-to-the-kardashians","content":"[caption id=\"attachment_13076\" align=\"alignright\" width=\"300\"]![img_2038_1024](http://scholarslab.org/wp-content/uploads/2016/12/img_2038_1024-300x225.jpg) Keeping up with the Praxis 2016-2017 cohort.[/caption]\n\n\nUnlike previous cohorts of Praxis, we did not have a topic or project assigned to us when we came into the program. Rather, our fearless leaders at the Scholars Lab took a chance. Our assignment: “time, temporality, go!” The first impulse was to directly challenge the traditional conceptions of time, linear time. Our cohort, made up of scholars of English literature, Russian literature, US history, and European art history, came to the topic of cyclical time from multiple different viewpoints. Joseph, whose academic work focuses on music and collective memory, immediately grasped the nature of cyclical time. Similarly, Alyssa and Jordan understood cyclical time’s ramifications in memory, story telling, and formal literary construction. I, on the other hand, an art historian who works with ephemeral objects, objects that are seen by humans and then over time melt, break, and disintegrate… I had trouble wrapping my head around the concept. Once an object is gone, how can it be viewed and perceived by the viewer and human subject? After meetings in which we discussed Gilles Deleuze, George Kubler, Albert Einstein, among others, I finally found myself opening up to the fact that time, as we humans perceived, _is_ a construction—one that I was having trouble removing myself from because of the conditioning I had received as a member of a western-centric, euro-centric society.  As an art historian, my views had been directly—if unknowingly—shaped by the history of my field. Emmanuel Kant, after all, established the notion that time is something we experience.  A key step to this breakthrough was our discussion of the density of time and brainstorming its many variations: \n\n\n\n\n- literary time \n\n\n\n\n- cinematic time\n\n\n\n\n- visual time\n\n\n\n\n- collective memory\n\n\n\n\n- nostalgia\n\n\n\n\nAs a group, we kept coming back to the topic of collective memory and nostalgia. Where was the line between the two? Was nostalgia a conservative form of collective memory? The concept of “radical nostalgia” challenged this notion. Radical, or insurgent, nostalgia, reframes nostalgia as an act not inherently conservative, especially when harnessing radical or progressive events and movements of the past (see Peter Glazer’s _Radical Nostalgia: Spanish Civil War Commemoration in Americ__a_, 2005). In our discussion, it became radically apparent that our own experience with the development of contemporary collective memory, how it was shaped in our daily lives, was the product of one dominant force: **social media**.\n\n\n\n\nWe were in the trenches of the 2016 election cycle. Donald Trump’s tweets were in the news everyday. Conservatives were accusing Facebook’s content editors of having a liberal bias. How was social media shaping how we perceived the 24-hour news cycle, and thereby our collective understanding of culture and politics? How could we visualize the social media landscape? How could we use social media as a data set? Was there a cohesive data set across all media that we could mine for a productive analysis of social media usage and its cultural ramifications?\n\n\n\n\nEnter the Kardashian family empire. Reality TV, tabloid coverage, Twitter, Instagram, Snapchat, iPhone and android apps, online blogs… in many ways it emerged as a story of archive creation, expansive real time narrative construction, and yes, even a study of collective memory. \n\n\n\n\nWho are the Kardashians? The Kardashians are an Armenian-American family living in Los Angeles headed by the matriarch Kris Jenner, who married Robert Kardashian in 1978 (divorced in 1991) and later the Olympian Bruce Jenner, now Caitlyn Jenner, in 1991 (divorced in 2015). The family first came to public attention in 1995, during the high profile murder trial of O.J. Simpson during which Robert Kardashian, was part of the defense’s legal team. They found themselves in the tabloids again in 2007 when a sex tape was leaked of daughter Kim Kardashian and her then-boyfriend Ray J. Taking advantage of such notoriety the family debuted their reality television series _Keeping Up with the Kardashians_ later that year. By embracing nearly ever media platform at their disposal, the Kardashians have promoted their brand and developed their family narrative in real time, garnering interest in how they will ‘cover’ certain life events in their TV show airing months later.\n\n\n\n\nCommenting on news events, promoting awareness of the Armenian genocide, while producing extravagant television specials, the Kardashian family has harnessed the 24/7 media landscape in new and unprecedented ways; their rise to fame paralleling the rise of social media and the internet as we know it. \n\n\n\n\nStay tuned as we, the Praxis 2016-2017 cohort, dissect and analyze the Kardashian media ecology….\n"},{"id":"2016-12-13-the-state-of-dh-in-slavic-studies-by-kathleen-thompson","title":"The state of DH in Slavic Studies, by Kathleen Thompson","author":"ammon-shepherd","date":"2016-12-13 05:42:33 -0500","categories":["Digital Humanities","Makerspace"],"url":"the-state-of-dh-in-slavic-studies-by-kathleen-thompson","content":"In a final wrap up, of what has become a four-part series of blog posts on using 3D-printing in a humanities course, Kathleen Thompson reports back on the ASEEES conference and the state of DH in Slavic studies.\n\n\nThe previous posts can be read here:\n\n\n\n\nPart 1: [Printing in the Classroom: Course Assignments and the Makerspace](http://scholarslab.org/uncategorized/3d-printing-in-the-classroom-course-assignments-and-the-makerspace/)\n\n\n\n\nPart 2: [3D Printing in the Classroom: Outcomes and Reflections on a Slavic Course Experiment (1/2)](http://scholarslab.org/makerspace/3d-printing-in-the-classroom-outcomes-and-reflections-on-a-slavic-course-experiment-12/) \n\n\n\n\nPart 3: [3D Printing in the Classroom: Outcomes and Reflections on a Slavic Course Experiment (1/2)](http://scholarslab.org/makerspace/3d-printing-in-the-classroom-outcomes-and-reflections-on-a-slavic-course-experiment-22/)\n\n\n\n\nKathleen Thompson also provided three videos of printing the onion dome pictured above:\n\n\n[embed]https://youtu.be/AThw04OFa6o[/embed]\n\n\nOnion Dome movie 1\n\n\n[embed]https://youtu.be/WpUtw0_IB40[/embed]\n\n\nOnion Dome movie 2\n\n\nhttps://youtu.be/NOVC8kWo33Y\n\n\nOnion Dome movie 3\n\n\n\n\nAfter receiving a PhD from the University of Virginia's Slavic department in August 2015, Kathleen Thompson worked as the interim Slavic librarian for the 2015-2016 academic year. Since the conclusion of that position, she has been working as a research assistant in the Slavic department. Her current research interests involve digital pedagogy, 21st-century Russian women writers, culture studies, and digital networks.\n\n\n\n\n\n\n* * *\n\n\n\nIn late November, Jill Martiniuk and I attended the 48th annual Association for Slavic, East European, and Eurasian Studies (ASEEES) Convention (link: http://aseees.org/convention) to present our project on 3D printing in the classroom. This convention is one of two major conferences in the field of Slavic studies, and is regarded as the largest and most well-attended because it includes scholars and students from a wide range of specializations, at all levels of their careers, discussing issues germane to locales ranging from the Caucasus to the Baltics, from Crimea to Siberia, and everywhere in between.\n\n\n\nThis year’s convention played host to four debuts: one, the inaugural meeting of the newly-formed Slavic Digital Humanities affiliate group (link: [http://slavic-dh.org/](http://slavic-dh.org/)); two, the debut of the ASEEES Commons repository ([https://aseees.hcommons.org/](https://aseees.hcommons.org/)); three, a THATCamp meeting (link: [http://aseees2016.thatcamp.org/](http://aseees2016.thatcamp.org/)) preceding the convention; and four, a panel stream of seven sessions (link: [http://www.slavic-dh.org/panels2016/](http://www.slavic-dh.org/panels2016/)) devoted to digital humanities in our field. Jill and I both attended THATCamp, and participated in the sixth DH session, “Digital Humanities In and Out of the Classroom”, which was a roundtable highlighting projects linking DH and pedagogy. At both THATCamp and the roundtable, we were eager to share our findings from our spring-semester project, get feedback from other instructors or scholars working with similar technology, and learn about projects in different media and pedagogical contexts.\n\n\n\nSince we were unsure how familiar our audience would be with 3D printing, not to mention DH in general, Jill and I prepared a slideshow (link: [https://dl.dropboxusercontent.com/u/64498698/3D-printing-in-Slavic-classroom-presentation.pptx](https://dl.dropboxusercontent.com/u/64498698/3D-printing-in-Slavic-classroom-presentation.pptx)) to highlight the creation, implementation, and outcomes of the project. We also included photos of the students’ work, as well as of the Makerspace itself and a couple of videos I shot of an onion dome being printed. I brought the onion dome with me to the roundtable and passed it around to the 25+ audience members and panelists, most of whom seemed to enjoy handling it. Unfortunately, after we briefly discussed our project and made some comments about issues we had to consider when creating and implementing the project – the standard “why are we doing this and what are our learning goals?” questions – the discussion moved towards other projects and their creators’ responses to similar issues. As this was a roundtable on general approaches to pedagogical issues with DH projects, Jill and I were not too bothered by this shift, though we were slightly disappointed not to receive more feedback.  Even so, throughout the weekend, we heard from many convention-goers and THATCamp participants that ours was the first 3D printing project they had heard of in the field, and interest in our project on a one-to-one basis was surprisingly high.\n\n\n\nWe came away from the conference encouraged and excited about the state of digital humanities in the Slavic field, and the direction in which it seems to be trending. Slavic scholars are engaging in thoughtful, critical work in several areas, if the titles of the THATCamp breakout sessions and DH panels are any indication. The THATCamp sessions were:\n\n\n\n \t\n  * Introduction to DH\n\n \t\n  * Digital Public Scholarship\n\n \t\n  * Digital Pedagogy\n\n \t\n  * Digital Archives\n\n \t\n  * Course Development\n\n \t\n  * Programming for Humanists\n\n \t\n  * GIS\n\n \t\n  * Databases & Visualizations\n\n \t\n  * Topic Modeling\n\n \t\n  * Network Analysis\n\n \t\n  * Textual Analysis & TEI\n\n \t\n  * Working with Eastern European Languages – Character Recognition, Encoding, and other Issues.\n\n\n\n\nThe DH panel and roundtable titles were:\n\n \t\n  * Platforms for Digital Scholarship\n\n \t\n  * The Researcher-Librarian Interface in Digital East European Studies\n\n \t\n  * Seeing Through Data: How Does DH Change How We View Culture?\n\n \t\n  * Computational Poetics: Digital Approaches to the Analysis of Rhyme, Meter, and Text Length\n\n \t\n  * Locating Text and Image in the Digital Humanities\n\n \t\n  * DH In and Out of the Classroom\n\n \t\n  * Mapping and GIS in the Slavic and Eurasian Humanities\n\n\n\n\nApproximately 80 convention attendees signed up for THATCamp, with probably 50-60 actual attendees at the meeting. About 30 convention attendees were present at the Slavic DH affiliate group business meeting, with several expressing interest in future topic streams and initiatives at next year’s convention as well as serving on the group’s board of elected officers. The panels and roundtables that Jill and I attended garnered audiences ranging in size from 10-40 people, with the most interest focused on digital archives, content management systems and blogs, network analysis, and topic modeling. Interestingly, the launch of the ASEEES Commons site seemed to rouse a little less enthusiasm (at present, it has 14 members and 5 public groups), though that could be due to a general lack of knowledge about the “whys” and “whats” of the site – or exhaustion from an already fulfilling and enriching weekend of scholarship.\n\nSome of my main takeaway points from the convention follow:\n\n \t\n  * Right now, it seems that the majority of Slavic scholars practicing DH are historians, though there is also a sizable number of librarians, linguists, sociologists, cultural scholars, and literature scholars – many of whom are collaborating with one another across disciplines. Even so, few of us humanists have coding skills beyond the basics, so some of the most valued relationships are between us and the computer-savvy scholars (often graduate or undergraduate students) who make our project dreams a reality.\n\n \t\n  * WordPress and Omeka are the two most-used platforms for DH projects across most disciplines, mostly because of low buy-in and learning curves, variety of presentation options, and sustainability over the long term.\n\n \t\n  * Questions that were repeatedly raised about practicing DH in the Slavic field were “why?”, “how?”, “who else is doing this?”, and (the ever-present) “how can we get our work to be taken seriously?” In light of that last question, I noted a serious push to move the conversation from “legitimizing the field” to “legitimizing the outcome”, suggesting that we as a field may have finally validated DH as a worthwhile pursuit, and are focusing more on its sustainable, valuable practice.\n\n \t\n  * The excitement level about DH work in our field seems high enough to support a wide range of initiatives, collaborations, and scholarship, though the phrases “critical mass” and “bursting bubble” came up more than a few times throughout the convention.\n\n \t\n  * Some future issues to consider include: how do we make sure that graduate and undergraduate students receive proper credit, and training, for their past and future work on DH projects? How can we use DH as a platform for public scholarship to reach our wider communities? How can we start local, and expand to the global? How does DH help build and change relationships between faculty, researchers, students, and communities?\n\n\nIf you’re interested in seeing some of the projects we learned about, check out this list:\n\n_Digital Public Scholarship _\n[https://1917resources.aseees.hcommons.org/](https://1917resources.aseees.hcommons.org/) - A resource on all things 1917 for the upcoming 100th anniversary of the Russian Revolution; will eventually be open-source\n[https://notevenpast.org/](https://notevenpast.org/) - A resource on local history research at the University of Texas-Austin, which includes the popular podcast 15-Minute History ([https://15minutehistory.org](https://15minutehistory.org)) and projects that have emerged from Not Even Past ([https://thinkinginpublic.org/](https://thinkinginpublic.org/))\n[http://soviethistory.msu.edu/](http://soviethistory.msu.edu/) - 17 Moments in Soviet History, a digital archive of primary sources across Soviet history\n[http://blackseanetworks.org/](http://blackseanetworks.org/) - A collaborative initiative on the Black Sea as a network\n\n_Digital Pedagogy _\n[http://dhrees.yale.edu/](http://dhrees.yale.edu/) - Digital Humanities and Russian and East European Studies at Yale\n[http://campuspress.yale.edu/emigres/](http://campuspress.yale.edu/emigres/) - Avant-Gardes and Émigrés: Slavic Studies and Digital Humanities\n[https://utopiaafterutopia.com/](https://utopiaafterutopia.com/) - a contemporary culture research initiative on politics and aesthetics in the post-Soviet world\n[http://blackandbluedanube.wordpress.com/](http://blackandbluedanube.com/) - _The Black and Blue Danube_ symposium and “Legacies of the Second World” working group, out of Colgate University\n\n_Digital Archives_\n[http://www.laits.utexas.edu/txczechproject/home](http://www.laits.utexas.edu/txczechproject/home) - the Texas Czech Legacy Project\n[http://chnmdev.gmu.edu/rpi/](http://chnmdev.gmu.edu/rpi/) - Russian Perspectives on Islam\n[http://cultural-opposition.eu/](http://cultural-opposition.eu/) - Courage: Connecting Collections\n[https://perspectives.ushmm.org/](https://perspectives.ushmm.org/) - Jewish Perspectives on the Holocaust\n[http://luchsveta.org/](http://luchsveta.org/) - Luch Sveta, an online repository of video clips for learning Russian language at all levels\n\n_Data and Visualizations_\n[http://104.236.44.207/notes/](http://104.236.44.207/notes/) - sandbox of Andrew Janco (Digital Scholarship Librarian, Haverford College)\n\n_Topic Modeling_\n[http://agoldst.github.io/dfr-browser/demo/#/model/list](http://agoldst.github.io/dfr-browser/demo/#/model/list) - _PMLA_ topic modeling, designed for browser\n[http://sprout025.sprout.yale.edu/topics/sr/200/#/model](http://sprout025.sprout.yale.edu/topics/sr/200/#/model) - _Slavic Review_ topic modeling, designed for browser\n\n_Blogging_\n[http://russianhistoryblog.org/](http://russianhistoryblog.org/) - An experiment in digital Russian history\n[https://joanneuberger.wordpress.com/](https://joanneuberger.wordpress.com/) - Joan Neuberger, co-author of Not Even Past\n[http://amynelson.net/](http://amynelson.net/) - Amy Nelson, co-author of 17 Moments in Soviet History; see more DH-specific posts at [https://siriusreflections.org/](https://siriusreflections.org/)\n[http://www.soundingthespacerace.com](http://www.soundingthespacerace.com) - Sounding The Space Race, coming December 2016\n"},{"id":"2016-12-13-time-twitter-and-keeping-up-with-the-kardashians","title":"Time, Twitter, and Keeping Up with the Kardashians","author":"alyssa-collins","date":"2016-12-13 07:54:42 -0500","categories":["Grad Student Research"],"url":"time-twitter-and-keeping-up-with-the-kardashians","content":"We started this semester thinking about time and the ways time is structured, pathologized, and altered. And when it came to finding an access point for these questions, a project, if you will, we found it in an unlikely source:\n\n<!-- [caption id=\"\" align=\"aligncenter\" width=\"434\"] -->\n![A cover of Cosmopolitan in which the Kardashians are pictured. It includes the title ](http://www.awesomelyluvvie.com/wp-content/uploads/2015/10/Cosmopolitan-Kardashians.jpg) Cosmo names the Kardashians as America's \"First Family\"\n<!-- [/caption] -->\n\n\n\nInitially, as *serious* grad students, we were a bit resistant/hesitant  to stake our entire project on such an unlikely primary source. However, after several elaborate, energized, and playful discussions, we came to the conclusion that  Kim Kardashian and her family (a.k.a \"the Kardashians\") are interesting set of figures who actively make and re-make their own history through a network of media platform (both new and traditional/\"mainstream\"). Because they loom large on the landscape of American contemporary culture and have connected themselves to so many people, the Kardashians are situated within a nexus of not only conversations about time and information dissemination, but also conversations about race, gender, and sexuality.  For our cohort, the Kardashians also are  an exciting place of research where we can pair **intellectual play and rigor**. So, eventually we all got on board. \n\n<!-- [caption id=\"attachment_13077\" align=\"aligncenter\" width=\"441\"] -->\n![Brainstorming Session](http://scholarslab.org/wp-content/uploads/2016/12/IMG_7612-copy-300x235.jpg) Brainstorming Session\n<!-- [/caption] -->\n\n\n\nWe decided that by looking at the ways their reality TV show, *Keeping Up With the Kardashian* (KUTWK), we can not only ask what it means to \"keep up *with*\" these people (what apps you must buy, what headlines you must read, what games you must play, and what places you must go to be a good fan/follower), but also (and maybe more interestingly) how \"history\" or cultural moments are visited and revisited through the conjuction of social media and television show.  Moments are first visited in tweets that are happening IRL (in real life) and IRT (in real time) and then revisited after a period of three months or time it takes for production to complete an episode of _KUWTK_. \n\n\n\n\n\n\n\nSo this semester (and next) we will be asking: \n\n\n\n\n\n\n  1.  Why do people like the Kardashians? What do they offer?\n\n\n  2. How does _Keeping Up With the Kardashians_ move in and out of time? \n\n\n  3. This includes looking at “historical Kardashian moments” that seem to take up a great deal of historical narrative space. How are these stories told and retold? We think that if we pair two of these media timelines: Twitter and television (KUWTK) we can begin to uncover the way these narratives work together (or even against each other).  Here are some possible moments we might look to: \n\n\n    * The OJ Simpson trial (1994)\n\n\n    * Kim’s sex tape (leaked in 2007) \n\n\n    * The Kardashian’s trip to Armenia (2015)\n\n\n    * Khloe’s marriage to Lamar Odom and his very public illness (2015)\n\n\n    * Caitlyn Jenner’s coming out (2015)\n\n\n    * Kanye West’s ongoing beef with Taylor Swift (2009 - 2016)\n\n\n    * The robbery in Paris and the possible retirement of Kim Kardashian from public life. (2016) In the ten years it has been on television, how has _KUWTK_ move in and out of different genres?\n\n\n\n\n\n  4. How do the Kardashians publicly enter and exit discourses: race, gender, sexuality, and class. On the other hand, how are the Kardashians situated in these discourses by fans, scholars, and journalists?  It seems that nothing they do is uncontroversial and considering the public response to events in the Kardashians' lives is also an important to understanding how they fit into the American historical landscape. \n\n\n\n\nLastly, in thinking about the Kardashians we are interested to think about how their use and presentation of history funneled through different media platforms operates as a model for other kinds and forms of celebrity (both insidious and benign). How might we see their use of both mediums as a test case for the ways in which news cycles and history has possibly sped up in the 21st century? \n\n\n\n\n\n\n\nWe are excited to see what we come up with and we hope you come along for the ride! \n"},{"id":"2016-12-13-why-not-build-another-digital-humanities-tool","title":"Why not build another digital humanities tool?","author":"joseph-thompson","date":"2016-12-13 07:50:24 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"why-not-build-another-digital-humanities-tool","content":"One of the recurrent issues I noticed when our Praxis cohort began discussing the meaning of the digital humanities was the field’s need to justify its existence. At the beginning of the semester, we read articles about digital humanities as a “tactical term” and the kind of institutional, financial affiliations necessary to sustain DH labs and staff. All of this background on the history of the field proved useful in understanding where DH stands within academia and why it has received so much recent criticism as an instrument in the further neoliberalization (insert your personal definition here) of the university. Scholars within and outside the field have noted, with some justification, how DH might further the financialization and quantification of, well, everything and how that might lead us further away from the inquiries that should ostensibly drive our intellectual pursuits in the humanities.\n\n\n\n\n \n\n\n\n\nSo, if there is this “dark side” of DH, then what does it look like and what can we do to push against it?\n\n\n\n\n \n\n\n\n\nAn emphasis on empirical, quantitative projects and the “mining” of data and texts has characterized many (most?) DH projects, creating easy fodder for critics concerned with the ever-growing reach of neoliberalism. These types of projects have served as the very foundation for many DH literary studies and have yield generative results into topics like genre devices, authorial style, and the history of the novel. But beyond yielding new insights into literary texts, these types of projects offered a way to justify the field of DH. If humanities scholars could use computational analysis to produce projects illustrated with graphs, charts, and numerical data, then perhaps the humanities could maintain its relevance in a political/academic climate that seemingly values STEM over the arts. \n\n\n\n\n \n\n\n\n\nThe other way that DH seems to claim its relevancy is by emphasizing the production of tools. Our own Scholars’ Lab staff has created new tools like [Neatline](http://neatline.org/) and previous Praxis cohorts have developed [Prism](http://prism.scholarslab.org/pages/about) and [Ivanhoe](http://ivanhoe.scholarslab.org/), making useful, open-sourced tools and bringing attention to the work being done here at UVA. Of course, there’s no problem with producing such tools and many will say that’s exactly what DH should do. The problem will arise if every DH lab in the world starts producing new tools for every new project. At that point, the future of DH might look a lot like the iTunes app store, more a marketplace of things rather than ideas.\n\n\n\n\n \n\n\n\n\nSo, will this year’s Praxis cohort create a slick new tool for the DH marketplace? You guessed it, no.\n\n\n\n\n \n\n\n\n\nFor one reason, our cohort simply does not have the time or expertise to build the coolest new tool on the web. Several of us entered this program with little to no knowledge of coding, web design, or software development. My goal is to gain an introduction to if not a handle on those skills while I’m here.\n\n\n\n\n \n\n\n\n\nThe other reason is political.\n\n\n\n\n \n\n\n\n\nDuring a DH presentation at the 2016 American Studies Association conference, I heard a panelist describe the Internet as the “island of misfit toys.” I took this to mean that in a push for innovation and the need to justify the field’s existence a proliferation of DH projects have led to a mass of broken, unmaintained, and otherwise unusable tools. This stress on tool creation has fed into the critiques of DH as a neoliberal undertaking bent on the production of quantifiable and marketable results rather than the humanistic analysis that might yield more ideas than products.\n\n\n\n\n \n\n\n\n\nOur cohort won’t be producing a fancy new tool, and that’s okay. But we will use existing digital tools in innovative ways and maybe offer inspiration for other scholars with similar amounts of time and DH experience to pursue projects of their own.\n"},{"id":"2016-12-16-working-with-an-archive-of-the-now","title":"Working with an Archive of the ‘Now’","author":"jordan-buysse","date":"2016-12-16 05:28:16 -0500","categories":["Experimental Humanities","Grad Student Research","Visualization and Data Mining"],"url":"working-with-an-archive-of-the-now","content":"Given our subject matter for the 2016-17 Praxis cohort, we recognized early on that we would be grappling with a very different sort of archive than we’ve grown accustomed to as humanists. Instead of the stacks, journal databases, manuscripts, and historical objects, we’d have to take a serious look at Facebook, Instagram, Vine, Snapchat, and Twitter. The following is a brief summary of some of our efforts with Twitter, a platform which we realized early on would be key in meaningfully keeping up with the Kardashians.\n\nWe’ve centered our thinking around Twitter because at first glance, given its relatively open API, the possibilities for analysis seemed endless. What we’ve found is that this openness is a bit deceptive. While it’s easy enough to click around and ask questions of our archive through regular browsing, it’s another matter entirely to start answering those questions quantitatively. It’s hard to turn data that you can see into data you can use. It’s one thing to know this intellectually, but quite another to experience the many roadblocks of a proper social media data set.\n\nThe first thing we found in attempting to wrangle data from the platform is that Twitter’s API prioritizes an archive of the ‘now’. One of the ways to sample twitter data is with the R package rtweet, which allows the user to search tweets based on keywords or user data such as followers, user ‘likes’, and retweets. Because rtweet and other tools like it use Twitter’s “streaming API,” which searches for tweets as they happen, the experience is a bit like trying to sip from a firehose. A query of the name ‘Kardashian’ on a given Wednesday morning yields output like this:\n\n[![keyword kardashian](http://scholarslab.org/wp-content/uploads/2016/12/keyword-kardashian.bmp)](http://scholarslab.org/wp-content/uploads/2016/12/keyword-kardashian.bmp)\n\nThe CSV format allows us to view the data in a slightly more manageable format as follows:\n\n[![keyword kardashian2](http://scholarslab.org/wp-content/uploads/2016/12/keyword-kardashian2.bmp)](http://scholarslab.org/wp-content/uploads/2016/12/keyword-kardashian2.bmp)\n\nTo be sure, there’s more we could do to clean up this data, but to what end? Using social media networks to answer our questions about celebrity culture means making a choice. It’s either the comprehensive and immense archive of ‘now’, or some more carefully and narrowly selected data set.\n\nWe’re moving towards the latter. Indeed, our training as humanists has primed us to theorize and uphold the significance of the small data set closely read and applied. So what does that archive look like? It seems like a simple enough change; instead of watching the deployment of the Kardashian name across twitter in real time, why not focus in on key users themselves? Even if we were to make a generous coterie of, say, thirty Kardashian and Kardashian-adjacent twitter accounts, fresh problems arise. ‘Now’-ness is baked into the tools available to us. Our efforts to delve into deeper histories of this celebrity hit a snag about a year and a half back from the present moment; you can only reach back 3,200 tweets into a given account before twitter’s API cuts you off. The workarounds are either inelegant or expensive. Twitter is a uniquely profitable and profit-driven archive, and the analytics tools to reach back further than those 3,200 tweets are out of our price range.\n\nBut for our purposes, there’s a workaround somewhere between the streaming API’s technical affordances and the browser’s ease of use. It is possible to reach back into twitter’s deep archives simply using the site’s search feature, where users can search for tweets by user across certain date ranges, all of the way back to a user’s first tweets. These, in turn, can be scraped from the web and archived using a browser. The solution to our data-gathering questions, it seems, lies in moving between the user-centric practice of ‘browsing’ and the computational problem of data wrangling. Our efforts will involve translating the archive between these modes. To do so, we’ll have to rely on our best instincts developed from traditional humanist archives in tandem with the technical affordances and scalability of the archive of the ‘now’.\n"},{"id":"2017-01-12-blippar-and-augmented-reality-literature","title":"Blippar and Augmented Reality Literature","author":"christian-howard","date":"2017-01-12 11:39:05 -0500","categories":null,"url":"blippar-and-augmented-reality-literature","content":"As I was writing about [Ruth Ozeki’s _A Tale for the Time Being_](http://scholarslab.org/digital-humanities/hybrid-literature-ruth-ozekis-a-tale-for-the-time-being/) and the various technological innovations that Ozeki’s novel employs, I became increasingly interested in Blippar, an augmented reality application. Blippar is by no means restricted to literary projects, but as this is my primary interest, I naturally focused my search on ways in which literary works are incorporating augmented reality into their design. I found the results as fascinating as the implications are endless.\n\nLiterary works are currently employing Blippar in two primary ways: As a means of advertising and as a way of garnering user/reader participation and interactivity. The two overlap, of course, but the degrees vary according to purpose. Ravinder Singh, for instance, has made the cover of his novel, _This Love that Feels Right…_ blippable. According to Blippar’s website: “By blipping the cover of the book you can see a miniature Ravinder Singh come to life in 3D. He introduces his book and prompts the reader to share a review or take a selfie to win the chance to meet him and discuss love, life and books over a steaming hot cuppa.”\n\n\n\n![Screen Shot 2016-12-03 at 12.38.42 PM](http://scholarslab.org/wp-content/uploads/2017/01/Screen-Shot-2016-12-03-at-12.38.42-PM-300x220.png)\n\nFigure 1. Screen-grab from Blippar’s website illustrating Ravinder Singh’s _This Love that Feels Right…_\n\n\n\nYet extending beyond advertising, graphic artists and novelists are employing Blippar much more intensively, in some cases even supplementing almost every panel of their comics with augmented-reality blipps. Ram Devineni, Lina Srivastava, and Dan Goldman have created a storytelling project titled [_Priya’s Shakti_](http://www.priyashakti.com/) that aims to both educate men and women about sexual violence even as it fights against rape in India.\n\n\n\n![Screen Shot 2016-12-03 at 12.41.22 PM](http://scholarslab.org/wp-content/uploads/2017/01/Screen-Shot-2016-12-03-at-12.41.22-PM-300x226.png)\n\nFigure 2. Screen-grab from Blippar’s website illustrating the comic book, _Priya’s Shakti_\n\n\n\n_Priya’s Shakti _is available for [free download on Amazon ](https://www.amazon.com/Priyas-Shakti-Ram-Devineni-ebook/dp/B00Q1YQWUG), so, curious, I downloaded the comic and read it using Blippar. Not only has _Priya’s Shakti _given India its [first female “super-hero”](http://www.bbc.com/news/world-asia-india-30288173), but the comic itself is a work of art that comes to life with Blippar’s augmented reality, providing further information, links to videos, and even sound effects. This is, I believe, just the beginning of augmented reality’s dance with literature.\n\n\n\n![](http://scholarslab.org/wp-content/uploads/2017/01/2016-12-03-17.49.55-e1484256439261-229x300.png)\n\nFigure 3. Screen-shot of Blippar’s enhanced graphics of a page of _Priya’s Shakti_\n\n\n\n![2016-12-03 17.50.20](http://scholarslab.org/wp-content/uploads/2017/01/2016-12-03-17.50.20-e1484256535722-300x170.png)\n\nFigure 4. Screen-shot from a video available through Blippar’s augmentation of _Priya’s Shakti_\n\n\n\nIn addition to enhancing the literary scene, Blippar has likewise expanded to education, sports, retail, healthcare, and business, among others. Perhaps most interestingly, Blippar recently launched the first “augmented reality retail destination” in Covent Garden (read about it on Blippar’s blog: [https://blippar.com/en/resources/blog/2016/11/22/covent-garden-becomes-worlds-first-augmented-reality-retail-destination-powered-blippar/](https://blippar.com/en/resources/blog/2016/11/22/covent-garden-becomes-worlds-first-augmented-reality-retail-destination-powered-blippar/)).\n\nBlippar also encourages its users to become “blippbuilders” by creating their own interactive augmented reality experiences. Naturally, I had to play around with blippbuilding myself, so I created a blipp for the UVA Makerspace, with images of our 3D printers and Makerspace technology, and links to both a video about Makerspaces and the UVA Scholars’ Lab website. Here are a few screenshots of my blipp:\n\n\n\n![Screenshot_2017-01-09-22-05-15](http://scholarslab.org/wp-content/uploads/2017/01/Screenshot_2017-01-09-22-05-15-e1484256610611-250x300.png)\n\nFigure 5. Blippbuilding in progress\n\n\n\n![Screenshot_2017-01-09-22-04-03](http://scholarslab.org/wp-content/uploads/2017/01/Screenshot_2017-01-09-22-04-03-e1484256645143-300x169.png)\n\nFigure 6. Uploading the blipp\n\n\n\n![Screenshot_2017-01-09-22-04-30](http://scholarslab.org/wp-content/uploads/2017/01/Screenshot_2017-01-09-22-04-30-e1484256687975-300x263.png)\n\nFigure 7. The completed blipp of the UVA Scholars’ Lab Makerspace\n"},{"id":"2017-01-20-reading-the-kardashians","title":"Reading the Kardashians","author":"joseph-thompson","date":"2017-01-20 05:11:57 -0500","categories":["Digital Humanities","Grad Student Research"],"url":"reading-the-kardashians","content":"Have you ever wondered what it would be like to read the dialogue that passes between members of the Kardashian family on _Keeping Up with the Kardashians_? To have those seemingly intimate conversations and confessions in the form of a literary production that is open to analysis, interpretation, and text mining?\n\nMe neither. Not until recently, that is.\n\nOur Praxis cohort has asked this very question to find out what the Kardashian conversations and their fourth wall monologues might reveal about that the ways they control and create the narrative around their lives.\n\nOf course, answering this question requires access to the transcripts of the shows. While I half assumed some _KUWTK_ fan might have transcribed the shows already, my searches for this material came up empty but feel free to point us in that direction if you know where they are. Without such a trove, a few of us began searching for ways to borrow the closed captioning service from the DVDs. There are programs that do that, but one problem that immediately arose is that our library only holds the first season of _KUWTK_ on DVD. True, we could ask the library to purchase the remaining 11 seasons, but justifying that purchase and then waiting for their arrival might seriously slow us down. Another potential problem comes from the sheer time it would take to download physical copies of the show. Again, time is a serious concern here since our tenure as Praxis fellows expires at the end of this semester, and we still have to complete some sort of analysis of this material.\n\nFollowing a brief search of other options, Alyssa Collins and I decided to try [CCExtractor](http://www.ccextractor.org/doku.php?id=start), a program that allows the collection of closed captioning transcripts from streaming services and hosts it’s code in [GitHub](https://github.com/CCExtractor). Not only would this save us the time and money required to gather transcripts from DVDs, but it also offered us a way to practice the technical skills we’ve been developing in Praxis.\n\nAlyssa and I jumped into CCExtractor, simply following the tutorial available on their website and using the documentation provided in their GitHub repository. We used my computer since I had installed Homebrew already. This allowed us to substitute “brew” for “apt-get” in the command line and run the CCExtractor scripts. After a few hiccups, which we overcame with the assistance of Eric Rochester and Jeremy Boggs (thanks!), we downloaded the closed captioning transcripts for _KUWTK_’s first episode, titled “I’m Watching You” and originally aired on October 14, 2007, into srt and vtt files. We then opened the vtt file with Atom and had the entire transcript in a readable form, complete with time stamps for each person’s line.\n\nWhat will we do with this information? I don’t have an answer for that yet, but just looking at the data opened up a lot of questions. Who speaks the most for the Kardashians? Does this speaking time correlate to on-screen time? In other words, are there family members who appear with regularity but don’t speak? How can we use this text material to consider the literary devices at work in an episode of reality television? There are a lot of unknowns, but I’m excited at the prospect of having this data to analyze.\n"},{"id":"2017-01-24-spring-2017-uva-library-gis-workshop-series","title":"Spring 2017 UVa Library GIS Workshop Series","author":"chris-gist","date":"2017-01-24 05:30:42 -0500","categories":["Announcements","Events","Geospatial and Temporal"],"url":"spring-2017-uva-library-gis-workshop-series","content":"![](http://coloradogeologicalsurvey.org/wp-content/uploads/2013/07/gis_data.jpg)All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be taught on **Wednesdays from 2PM to 3PM in the Alderman Electronic Classroom, ALD 421** (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!\n\nFebruary 1st\n**Making Your First Map with ArcGIS\n**Here’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa.\n\nFebruary 8th\n**Georeferencing a Map - Putting Old maps and Aerial Photos on Your Map**\nWould you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS.\n\nFebruary 15th\n**Getting Your Data on a Map**\nDo you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map.\n\nFebruary 22nd\n**Points on Your Map: Street Addresses and More Spatial Things**\nDo you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis.\n\nMarch 1st\n**Taking Control of Your Spatial Data: Editing in ArcGIS**\nUntil we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on.\n\nMarch 15th\n**Easy Demographics**\nNeed to make a quick demographic map or religious adherence?  This workshop will show you how easily navigate Social Explorer.  This powerful online application makes it easy to create maps with contemporary and historic census data and religious information.\n\nMarch 22nd\n**Historic Census Data**\nWould you like to map the poverty in Philadelphia around the turn of the 20th Century?  How about a racial breakdown by state in the 1860s?  This workshop will focus on how to download historic census boundary and tabular data to make historic demographic maps.\n\nMarch 29th\n**Introduction to ArcGIS Online**\nWith ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, customize the ArcGIS Online website, and view status reports. You can also use ArcGIS Online as a platform to build custom location-based apps.\n"},{"id":"2017-01-31-call-for-applicants-for-tuition-fellowships-for-the-digital-humanities-summer-institute","title":"Call for Applicants for Tuition Fellowships for the Digital Humanities Summer Institute","author":"laura-miller","date":"2017-01-31 07:19:03 -0500","categories":["Technical Training"],"url":"call-for-applicants-for-tuition-fellowships-for-the-digital-humanities-summer-institute","content":"![DHSI logo](http://scholarslab.org/wp-content/uploads/2017/01/logo-dhsi-110x110.gif)Want to learn more about the skills, methods, and inquiry entailed in digital humanities?  The [Digital Humanities Summer Institute](http://www.dhsi.org/) (DHSI) at the University of Victoria has a tradition of transformative training. The University of Virginia, as a sponsoring institution, provides five tuition-free fellowships for the summer of 2017. This deal ends April 1. **Apply ASAP.  The Committee will begin selection on February 6.**\n\nPlease look at [DHSI site](http://www.dhsi.org/) for the available [courses](http://www.dhsi.org/courses.php).  Spaces in the workshops are filling up.  The selection committee, organized by the Scholars’ Lab, will receive applications. To apply, please email [scholarslab@virginia.edu](mailto:scholarslab@virginia.edu?subject=Application%20for%202017%20DHSI%20Fellowship) with responses to the following items.  We may be in touch for further information.\n\n\n\n \t\n  * your name\n\n \t\n  * a one page resume or CV with contact information in addition to email\n\n \t\n  * your University of Virginia affiliation and status (e.g. 3rd year PhD in art history; job title in library or department);\n\n \t\n  * availability for June 5-9 or 12-16, 2017 (or both)\n\n \t\n  * your preferred courses\n\n \t\n  * a short, specific statement (300-500 words) about your experience, qualifications, research interests, and why you would like to attend DHSI\n\n \t\n  * other sources of funding you could apply for that would cover travel and housing\n\n\n"},{"id":"2017-02-03-graduate-applications-for-the-praxis-program-2017-2018","title":"Graduate Applications for the Praxis Program, 2017-2018","author":"alison-booth","date":"2017-02-03 12:04:36 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"graduate-applications-for-the-praxis-program-2017-2018","content":"![2014-15 Praxis Fellows](http://scholarslab.org/wp-content/uploads/2017/02/Praxis15-16Fellows-300x189.jpeg)UVa graduate students! **Apply by** **February 28** for a unique, funded opportunity to work with an interdisciplinary group of your peers, learning digital humanities skills from the experts, and collaborating on the design and execution of an innovative digital project. The 2016-2017 Praxis cohort is in full swing, thanks to generous support by UVa Library and GSAS.\n\nEach year, the Scholars’ Lab, UVa Library’s center for advanced digital research in the humanities, runs a [Praxis Program](http://scholarslab.org/graduate-fellowships/). This program provides six UVa graduate students with hands-on experience in digital humanities collaboration and project creation. Team members work to take a shared project from conception to design and development, and finally to publication, community engagement, and analysis. Praxis is a unique and well-known training program in the international digital humanities community. Our fellows blog about their experiences and develop increased facility with project management, collaboration, and the public humanities, even as they tackle (most for the first time, and with the mentorship of our faculty and staff) new programming languages, tools, and digital methods. Praxis prepares fellows to apply their own digital skills and methods to the fellowship project and to research and careers in the future.\n\nIn 2012-2013, the Scholars’ Lab joined with like-minded institutions to create the [Praxis Network](http://praxis-network.org/), made up of allied humanities education initiatives from around the world, mainly focused on graduate training, and all engaged in rethinking pedagogy and campus partnerships in relation to the digital humanities. The Praxis Network Student Directory showcases Praxis Program alumni who have traveled diverse career paths, including tenure-track teaching positions and digital humanities positions within academic libraries and research centers.\n\nWe will welcome six new, competitively-selected Praxis students in late **August 2017. The Praxis fellowship replaces teaching responsibilities for the academic year. Fellows are expected to devote approximately 10 hours per week in the fall and spring semesters, to learning together and building a collaborative digital project in the Scholars’ Lab.** Fellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, and can make use of a dedicated grad lounge.\n\n**Eligibility**: All University of Virginia doctoral students studying in any humanities and arts fields (architecture, media studies, interpretative social sciences, education, or other fields will also be considered). In general, studio or performing arts or creative writing are less appropriate applicant fields. Feel free to inquire about your disciplinary area and its fit with the Praxis program. We particularly encourage applications from women, LGBT students, and people of color, and will be working to put together an interdisciplinary and intellectually diverse team.\nThe application process for Praxis is simple! You apply individually, and we assemble the team, through a process that includes group interviews scheduled in early March, as well as input from peers.\n**First step to apply**: submit a letter of intent to [scholarslab@virginia.edu.](mailto:scholarslab@virginia.edu)\n**Letter of Intent** includes:\n– the applicant’s research interests, with a title and a sentence about an example of recent work such as a thesis or essay;\n– summary of the applicant’s plan for use of digital technologies in your research;\n– summary of what skills, interests, methods, or experience the applicant will bring to the Praxis Program;\n– summary of what the applicant hopes to gain as a Praxis Fellow.\n\nQuestions about Praxis Fellowships and the application process should be directed to [scholarslab@virginia.edu](mailto:scholarslab@virginia.edu)\n"},{"id":"2017-02-03-graduate-fellowships","title":"Digital Humanities Fellows","author":"alison-booth","date":"2017-02-03 12:15:32 -0500","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"graduate-fellowships","content":"The Scholars’ Lab is proud to announce that applications for our prestigious [Graduate Fellowship in the Digital Humanities](http://scholarslab.org/graduate-fellowships/) are being accepted for the 2017-2018 academic year. Applications are due **February 28, 2017**.\n\nThe fellowship supports ABD graduate students doing innovative work in the digital humanities at the University of Virginia. The Scholars’ Lab offers Grad Fellows advice and assistance with the creation and analysis of digital content, as well as consultation on intellectual property issues and best practices in digital scholarship and DH software development.\n\nFellows join [our vibrant community](http://scholarslab.org/people/), have a voice in intellectual programming for the Scholars’ Lab, make use of our dedicated grad office, and participate in one formal colloquium at the Library per semester.\n\nSupported by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew & Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities, the Graduate Fellowship in Digital Humanities is designed to advance the humanities and provide emerging digital scholars with an opportunity for growth.\n\n**Eligibility**\nApplicants must be ABD, having completed all course requirements and been admitted to candidacy for the doctorate in the humanities, social sciences or the arts at the University of Virginia.\n\nApplicants must be enrolled full time in the year for which they are applying.\n\nA faculty advisor must review and approve the scholarly content of the proposal.\n\n_Applicants are strongly encouraged to have Praxis Program or equivalent experience_. Experience can include work on a collaborative digital project, comfort with programing and code management, public scholarship, and critical engagement with digital tools.\n\n**How to Apply**\nEmail a complete application package including the following materials to [scholarslab@virginia.edu](mailto:scholarslab@virginia.edu) :\n\n\n\n \t\n  * a cover letter, addressed to the selection committee, containing:\n\n \t\n    * a summary of the applicant’s plan for use of digital technologies in his or her dissertation research;\n\n \t\n    * a summary of the applicant’s experience with digital projects;\n\n \t\n    * a description of UVa digital resources (content or expertise) that are relevant to the proposed project;\n\n\n\n\n \t\n  * [a Graduate Fellowship Application Form](http://scholarslab.org/wp-content/uploads/2016/02/dhfellowsappform.pdf);\n\n \t\n  * a dissertation abstract;\n\n \t\n  * and 2-3 letters of nomination and support, at least one being from the applicant’s dissertation director.\n\n\n**Deadline: February 28, 2017\nNotifications: March 31, 2017**\n\nQuestions about Grad Fellowships and the application process should be directed to [scholarslab@virginia.edu](mailto:scholarslab@virginia.edu).\n\n\n"},{"id":"2017-02-09-why-study-popular-culture-why-study-the-kardashians","title":"Why Study Popular Culture? Why Study the Kardashians?","author":"alicia-caticha","date":"2017-02-09 10:27:57 -0500","categories":["Grad Student Research"],"url":"why-study-popular-culture-why-study-the-kardashians","content":"_“When I got my first television set, I stopped caring so much about having close relationships.” — Andy Warhol _\n\n\n[![](http://scholarslab.org/wp-content/uploads/2017/02/article-2517744-19D1E53000000578-676_634x755-252x300.jpg)](http://www.dailymail.co.uk/tvshowbiz/article-2519086/Kanye-West-didnt-ask-Andy-Warhols-cousin-paint-Kim-Kardashian.html)\n\n“In the future, everybody will be world famous for fifteen minutes,” is without a doubt Andy Warhol’s most famous quote. Eerily predictive of the twenty-first century's stars who are famous for “being famous,” this quote encapsulates the seemingly democratized nature of celebrity created by reality television, the internet, and social media. Despite his embrace of pop culture, Warhol’s oeuvre fits seamlessly into the Academy’s notions of “high” culture and specifically, “high” art. Even as Warhol’s work subverted the modernist cannon and the teleological evolution towards abstraction, this very critique cemented its value among cultural critics and academics, as well as the market.\n\nOnly recently, has the study of popular culture become an important touchstone for Academic inquiry. Scholars of media studies, material culture, and visual culture (a field in direct opposition to art history in concept, if not in practice) have certainly led the charge on this front.  However, others in the Academy still question the relevance of pop culture and address studies of it with skepticism, if not disdain. And so the question still remains: what is the value of studying popular culture? Why should Praxis spend a year thinking about and studying the Kardashian family and their media empire? Part of our goal for this project is to explore this question in its own right, but in the meantime, here are four reasons why it is critical that we study the Kardashians _now_: \n\n\n\n \t\n  1. In 2015 the season premiere of the Kardashians was the most viewed Sunday cable program—notably ahead of the finale of AMC’s Mad Men—averaging about 4.24 million viewers, ranking it the number one Sunday night program for adults 18-34, and women 18-49. By ignoring and degrading such a popular program, we are not just looking down upon the tv program in question, but disregarding its audience.\n\n \t\n  2. Television, a medium based on repetition, reinforces dominant social meanings and prevailing ideologies. Reality TV creates the illusion of a false intimacy between spectator and subject through the repetitive depiction of everyday tasks and conversations. If we understand television as a medium that cements dominant social ideologies through repetitive viewing, Reality TV and its pretense of the ‘real’ reinforces these ideologies two-fold. But upon closer look at the themes prevailing throughout _Keeping Up with the Kardashians_ complicate what we might perceive to be dominant social norms. The Kardashians have subverted the traditional family sitcom in favor a matriarchal family structure, transforming the traditional private sphere of the home into their center of business, while maintaining heteronormative assumptions about the objectification of women. Considering the popularity of the Kardashians, what can this tell us about dominant American social ideologies? \n\n \t\n  3. Studying the popular is political. Audiences often make and remake their identities, in terms of gender, sexuality, race, ethnicity, class, and any other mode of intersectionality, in response to the popular culture they consume. Those identities, in turn, inform their personal political choices. If we want to understand the political culture of any historical moment, we should understand its popular culture, too. And, love them or hate them, the Kardashians are popular. They constitute a large portion of the ubiquitous feed of mediated information that the populace consumes through social media, tabloid journalism, music, and reality television. While the definition of popular culture as the cultural activities of “the people” has been scrutinized by scholars such as Stuart Hall for its essentialist view of the binary between “the people” and “the elite,” the  old Arnoldian meanings of the terms “culture” and “art” developed in response to early industrialization, mutually reinforcing the aesthetic, intellectual, and social values of an anti-bourgeois elite class. By studying popular culture, by giving it space alongside more traditional subjects of study, we push towards eradicating this long established distinction between high and low.\n\n \t\n  4. As noted in the previous bullet, Stuart Hall rejects this essentialist definition of popular culture in favor of a definition which stresses a constant tension between high and low (and between the hegemonic and counterhegemonic). Popular culture is dynamic, constantly shifting from marginalized to widespread acceptance, at times assimilating into “high culture” or vice versa. This understanding of a dynamic relationship of give and take between high and low culture is especially important in the 21st century, as the internet and mass media have converged these themes, see for example, [Lady Gaga’s Collaboration with Jeff Koons](http://www.slate.com/articles/arts/culturebox/2013/11/lady_gaga_jeff_koons_and_marina_abramovi_the_bad_romance_between_gaga_and.html). In fact, the Kardashians are no stranger to the contemporary art world. [Kim Kardashian’s book of Selfies was widely praised](http://www.vulture.com/2015/05/saltz-how-kim-kardashian-became-important.html) as a pop art exploration of selfhood and identity construction.\n\n\n"},{"id":"2017-02-15-over-the-moon-and-down-to-earth-scholars-lab-versions-of-space","title":"Over the Moon and Down to Earth: Scholars' Lab versions of space","author":"alison-booth","date":"2017-02-15 18:53:48 -0500","categories":["Announcements","Digital Humanities","Geospatial and Temporal","Makerspace"],"url":"over-the-moon-and-down-to-earth-scholars-lab-versions-of-space","content":"![](http://scholarslab.org/wp-content/uploads/2015/08/IMG_03831-300x225.jpg)![GIS data](http://scholarslab.org/wp-content/uploads/2017/01/gis_data-300x236.jpeg)\n\nSomething about space has gone to our heads here in the Scholars' Lab.  Any day, I expect to come out of a trance to find I've been wearing a motion capture suit while I geo-referenced an historic map, programmed an arduino, flew a drone, and printed a replica of an artifact from an ancient burial site. Of course, I only imagine doing all this myself. But the interdisciplinary collaborations of Scholars' Lab staff perform this sort of spatial magic in and around the Library every day.\n\nThe rich array of spatial research has been a forte of the Scholars' Lab since before I joined the group (my own project had its early days here).  [GIS](https://egsc.usgs.gov/isb//pubs/gis_poster/) specialists and the [Neatline](http://neatline.org/) project (now ably led by **Eric Rochester** and **Ronda Grizzle**) have been located in the Scholars' Lab for years.  But we have some more recent additions that make for an exciting mix of geospatial perspectives.\n\nWe recently welcomed **Geospatial Consultant Drew MacQueen,** who has been serving in Facilities Management at the University of Virginia. Drew will work closely with **Chris Gist** on the many GIS services needed in research and teaching at the University; see the [Library's guide](http://guides.lib.virginia.edu/gis).  For example, a longstanding project on the Salem witch trials led by Religious Studies professor Ben Ray recently identified the site of execution, based on Chris's [topographical analysis of the viewshed and historic maps](https://news.virginia.edu/content/uvas-help-salem-finally-discovers-where-its-witches-were-executed).\n\n**Will Rourk and Arin Bennet** joined th![library-research-credit-will-rourk-extrax800](https://i1.wp.com/news.library.virginia.edu/files/2017/01/Library-Research-Credit-Will-Rourk-EXTRAx800.jpg?resize=584%2C417&ssl=1)e Scholars' Lab team in 2016, bringing to bear their expertise in spatial research and the high-end equipment of the former DML.  They collaborate with faculty and staff in IATH, SHANTI, the Scholars' Lab, and elsewhere on cultural heritage, much of it on or near Grounds, but some as remote (or, now, accessible) as [the Tibetan capital, Lhasa.  ](https://magazine.arts.virginia.edu/stories/lhasa-vr-visualizing-the-historic-tibetan-capital)It turns out, the extended team of Scholars' Lab people participate in interrelated projects, from mapping to 3-D printing.\n\n**The Makerspace** was thriving when I arrived a year ago, organized by **Jeremy Boggs** and **Laura Miller**, and increasingly guided by **Ammon Shepherd**, with a team that includes DH dissertation fellow Shane Lin and graduate and undergraduate staff and drop-in experimenters.  (See the [Makerspace](http://scholarslab.org/makerspace/) technologists.) The tentacles of this experimentation reach far.![The Makerspace sign](http://scholarslab.org/wp-content/uploads/2014/05/makerspace5-300x168.jpg)\n\n![](http://scholarslab.org/wp-content/uploads/2017/02/DSC9814-300x200.jpg)\n\n\n\n**And there were cookies.  **On February 15, 2017 we held the first of a new series of **brown-bag lunches**, this one to discuss Daniel Punday's [\"Space Across Narrative Media: Towards an Archeology of Narratology,](http://muse.jhu.edu/article/644085)\" _Narrative_ 25 (Jan 2017): 92-112.  Some argued that architecture is a better metaphor than archeology for Punday's narratological approach, in the context of building and inhabiting things human beings make, such as the storyworlds of games or novels.\n\n![](http://scholarslab.org/wp-content/uploads/2017/02/PundayIllustration-300x253.png)\n\nIt was delightful to discuss this thought-provoking piece with 19 people, graduate students and faculty from Spanish, German, history, Slavic, computer science, and English, and library staff in archeology and Scholars' Lab as well as IATH.  One of Punday's examples, Jason Nelson's _I made this. You play this.  We are enemies_. playfully makes a point about the different spaces of the game's action and the orienting background (captured from commercial sites).  We can say, _we made this. We play this. We are energetic participants._  We like to playfully make things that have points--the coordinates of spatial data.\n"},{"id":"2017-02-23-amanda-visconti-is-our-new-managing-director","title":"Amanda Visconti Is Our New Managing Director","author":"alison-booth","date":"2017-02-23 08:40:13 -0500","categories":null,"url":"amanda-visconti-is-our-new-managing-director","content":"I am thrilled to announce yet more delightful news: Amanda Visconti will join the Library as Managing Director of the Scholars’ Lab.  We had terrific contenders for this position, and Amanda won us over!  She brings a high level of qualities seldom found in one person: intellectual commitments and depth, knowledge, and training in humanities; versatile and robust technical skills and experience collaborating on advanced digital research; immersion in the library world of information, teaching, service, and research; gifts in creative community building, especially through social media; imagination, integrity, and effective management.  She already practices the ethics and aims of our Charter, and will be a delightful co-director as DH@ UVA is developed into more coordinated collaboration across schools and entities, and into a curriculum with a certificate.\n\nAmanda wrote the first entirely-digital dissertation in literary humanities: [Infinite Ulysses](http://mith.umd.edu/research/infinite-ulysses/), which could be described as crowd-sourced annotation-as-usability-testing.  The process of building it, and its ongoing participatory development, have made it a model both in Modernist and Joyce studies and in DH: award-winning, widely reviewed, with tens of thousands of users.  She has been assistant professor (tenure track) and digital humanities specialist librarian in the Libraries and Information Science Department, Purdue University.  She is much in demand as a speaker and contributor to DH publications.  Recently, she launched the influential Digital Humanities Slack ([TinyUrl.com/DHSlack](http://TinyUrl.com/DHSlack)).  Having earned the M.S. In Information, DH, and HCI at the University of Michigan, she earned her doctorate working with John Unsworth’s former advisee, Professor Matt Kirschenbaum, and others at the University of Maryland.  Just as our collaboration with Washington and Lee is ongoing, we look forward to formalizing regular interactions with MITH and other regional groups such as Roy Rosenzweig CHNM at George Mason, University of Richmond, and elsewhere, with Amanda’s leadership. Happily, Amanda is prepared to guide the Scholars’ Lab’s agenda well beyond literary DH (see my recent post on the SLAB blog about how spatial we are).  She is looking forward, as well, to advancing our programs and projects related to social justice, diversity, and accessibility.\n\nPlease help us welcome Amanda to the Scholars’ Lab and Library!  She will certainly be on Grounds by May 1 (we’re hoping to see her sooner).  Communication is one of her fortes, as we're already in good contact on Slack and other ways.\n"},{"id":"2017-02-23-brandon-walsh-is-our-new-head-of-graduate-programs-starting-april-24-2017","title":"Brandon Walsh is our New Head of Graduate Programs, starting April 24, 2017","author":"alison-booth","date":"2017-02-23 04:10:46 -0500","categories":null,"url":"brandon-walsh-is-our-new-head-of-graduate-programs-starting-april-24-2017","content":"I’m thrilled to announce that Brandon Walsh will join the Library as Head of Graduate Programs in the Scholars’ Lab. This competitive national search revealed the talent that is growing out there: people dedicated to libraries and the pedagogy/service/advanced research model, with the combination of versatile and active technological skills and deep training in humanities or other disciplines. Brandon rose to the top. Currently the Mellon Digital Humanities Fellow, Visiting Assistant Professor of English at Washington and Lee University, he has recently collaborated with the Praxis Program from Lexington, and helped to build a DH curriculum and center in W&L’s library. Brandon holds the PhD in English, UVA; his dissertation, “AudioTextual: Modernism, Sound Recordings, and Networks of Reception,” provides a model of interdisciplinary work for our current graduate students, uniting sound studies, Modernist literary studies, and DH. We’re excited by the work he has continued to publish or present on open educational resources, DH pedagogy, and sound-language studies in audio data sets. Brandon brings further skills needed in the Head of Graduate Programs position: gifts as a teacher (he won a teaching award here); ability to teach digital methods and tools, as in his participation HILT; proven ability to manage grant funds and host public events; integrity and geniality in all kinds of personal interaction. Knowledge of an R1 public university, and this particular one, will help him quickly take a co-leadership role in the Lab. The cohorts of DH and Praxis fellows, and an increasing group of student interns (undergraduate and graduate) will benefit from his inspirational presence.\n\nAfter his teaching duties at Washington and Lee are completed in April, he will be settling in on April 24 to Purdom Lindblad’s former desk in the Scholars' Lab. Please help us welcome him. Hint: you may see him around C'ville, or he will be virtually present in helping to select next years’ fellows.\n"},{"id":"2017-02-28-disrupt-the-humanities-managing-director-job-talk","title":"Disrupt the Humanities? (Managing Director job talk)","author":"amanda-visconti","date":"2017-02-28 01:00:12 -0500","categories":["Digital Humanities"],"url":"disrupt-the-humanities-managing-director-job-talk","content":"When scholars share their job talks after being hired, DH and libraries interviewing processes become a little less mysterious. Academia has many genres of presentation. It can be difficult to draft your first job talk without seeing how others in your field translate a job ad's requirements into a job talk's expression of their unique expertise. [Lee Skallerup Bessette](http://readywriting.org/uncategorized/faculty-development-for-the-21st-century-and-beyond/), [Celeste Tuong Vy Sharpe](http://celestesharpe.com/onward/), [Chris Bourg](https://chrisbourg.wordpress.com/2015/07/12/infrastructure-and-culture-a-job-talk/), and [Brandon Walsh](http://scholarslab.org/digital-humanities/in-out-across-with-collaborative-education-and-digital-humanities-job-talk-for-head-of-graduate-programs/) all share successful job talks on their websites (I shared the [talk](http://literaturegeek.com/2016/02/28/DHjobtalk) for my previous role, too).\n\nI've long been a fan of the digital humanities at the University of Virginia, and I’m excited to join the Scholars' Lab and University of Virginia Libraries teams as [Managing Director of the Scholars’ Lab](http://scholarslab.org/uncategorized/amanda-visconti-is-our-new-managing-director/). In this post, I’ll share the talk I presented as part of my campus interview for this role.\n\nThe presentation prompt asked for a discussion of my past projects:\n\n\n<blockquote>The digital has the potential to be a disruptive force in humanities scholarship, giving scholars the means to critique, reimagine, and transform ideas, theories, material artifacts, and even interpersonal relationships. Using examples from your own collaborations with faculty, graduate students, and staff, please discuss how this disruption can be successfully embodied in scholarly inquiry, as well as in the cultivation of people and organizations.</blockquote>\n\n\n\n\n## The presentation\n\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.001.png)\n\nMy digital humanities is not disruptive.\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.002.png)\n\nDisruption can be a force for good, especially against systemic problems. But to my ear, disruptive is not what I want DH to be.\n\nDisruption is inherently non-introspective; you disrupt others, rather than looking hard at yourself.\n\nDisruption can ignore a history of similar work, and the humanities can’t afford that mistake. For example, a movement to make the humanities more public that ignored the fan fiction communities’ existing public humanities—people using the web to improve one another’s close readings and counterfactual interventions—would be deeply flawed. DH has diverse roots in pre-digital humanities scholarship, including highly recognizable non-digital, algorithmic criticism of the kinds Stephen Ramsay explores in his book _Reading Machines_. We have much to gain by connecting our efforts today to what has worked in the humanities of the past, and to the humanities that also thrives outside academia.\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.004.png)\n\nSo when I share some of my recent collaborative DH work with you today, I’d like us to consider this work not in terms of how it _disrupts_ the humanities, but through a variety of more positive and generative lenses. DH as…\n\n\n\n \t\n  * An applied humanities\n\n \t\n  * Experimental\n\n \t\n  * Public and participatory\n\n \t\n  * A radically interdisciplinary humanities, embracing social science, science, and art\n\n \t\n  * Open to any challenges to which we can bring our skills of humanities thinking\n\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.005.png)\n\nI’m hopeful for the humanities Bethany Nowviskie has imagined in her work “[on capacity and care](http://nowviskie.org/2015/on-capacity-and-care/)”. Nowviskie cautions: “Please do not mistake [capacity and care work] for something idealistic and motherly and sweet. I offer care as a hard-nosed survival strategy… to increase the reach and grasp (which is at the root of the word “capacity”—the “capture”) of the humanities.”\n\nSeeing both the positive and negative sides of disruption, and turning to other concepts with different values, aren’t only reactions fueled by thinking these are good ways for a person to be in the world. There are significant gains for pushing our scholarship to be more intellectually generous, more locally interconnected, and better at crediting key emotional labor such as team-building and mentorship.\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.004.png)\n\nSo in the next twenty minutes today, I want to tell you a bit about two of my recent DH projects, one where the bulk of the work is my own, and one where I’m only nominally the creator. This work articulates some of my hopes for what the digital humanities could be, a DH deeply part of the humanities rather than opposed to it, a DH that’s more interested in the right column up on the screen [_in the image above this paragraph, for blog readers_]—in seeing innovation in maintainance, the quality rather than the quantity of impact, on “people over projects” (as the Scholars’ Lab puts it).\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.007.png)\n\nThe first project I’ll discuss, _[Infinite Ulysses](http://www.infiniteulysses.com)_, shows humanities scholarly inquiry that’s been reimagined and opened beyond academia, rather than disrupted.\n\nI’m a textual scholar. I decided that instead of focusing on the scholarly editing aspect of digital editions, my skills and interest were more in line with work like that of Alan Galey, whose _Visualizing Variation_ project created code that lets editors of digital editions intervene in their texts in unique ways. Rather than creating a scholarly edition, I focused more on interface design aimed at opening a literary edition to a public audience.\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.009.png)\n\nI asked: What if we build a digital edition and invite everyone? What if millions of scholars, first-time readers, book clubs, teachers and their students, show up and annotate a text with their “infinite” annotations (that is, digital marginalia of interpretations, questions, contextualizations, and other comments)? How would we do that, and what would it do to the text and our understandings of it?\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.010.png)\n\nI worked through this speculative experiment, with its admittedly unlikely hypothetical of a massive public desire to read _Ulysses_ online. But I also built a prototype that was immediately useful in exploring these questions, through the generous annotations and feedback of an actual, modestly sized community of readers.\n\n_Infinite Ulysses_ (at [InfiniteUlysses.com](http://www.infiniteulysses.com)), is a participatory digital edition of James Joyce’s challenging novel _Ulysses_. Its goal was to support a public conversation about the novel, bringing readers of different backgrounds into participating in the same discussion space. The design, code, usertesting, and thinking that went into this project doubled as my literature dissertation.\n\nThis is what reading a page on _Infinite Ulysses_ looks like. There are three main activities you can pursue:\n\n\n\n \t\n  1. While reading the novel, you can highlight parts of the text and annotate these with your questions, interpretations, reactions, translations, and other comments.\n\n \t\n  2. You can also read the annotations created by other readers.\n\n \t\n  3. And I tried, at a basic level, to personalize your reading experience by filtering the displayed annotations so that you could just be shown the ones that fit your needs, interests, and background. For example, a reader can choose to see the top-rated annotations first, or the newest annotations; or to not see any spoilers; or to only see annotations tagged as clues to the book’s mysteries, references to Irish politics, or jokes you might miss.\n\n\nThere is some solid interest in this approach. My work was [cited in _The New York Times_](https://www.nytimes.com/2016/07/17/books/review/cant-get-through-ulysses-digital-help-is-on-the-way.html) this past summer, drew around 13,000 unique visitors in its first month of beta testing, and drew over 25,000 unique visitors during its beta year (April 2015-2016), despite little intentional publicity after the first month. I share these numbers to show that there’s interest in this scholarship, but in doing so I feel uncomfortably close to getting caught up in the allure of large-scale impact.\n\nMuch more important to me, and to the humanities, are the far smaller number of people who actually repeatedly visited and annotated _Ulysses_ on the site. What matters is the students who hear about my dissertation and are encouraged to also pursue the most appropriate methods for their research questions. I care about my impact at the scale of individual readers, like the South Korean reader who found my edition more accessible for her translation and understanding. Or the 90-year-old retired man who wrote that he’d always wanted to read _Ulysses_, and now he felt like he could take his laptop to a bar, pull up a stool, and maybe actually read the thing this time using my site.\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.013.png)\n\nThere are other facets to the scholarship of _Infinite Ulysses_, clear areas of research inquiry:\n\nMy 1st research question was an interdisciplinary DH one:\n\n_How can we design digital editions that are not just theoretically accessible to the public, but invite and assist public participation? And are there ways to design for meaningful public participation in literature, that don’t require the public to learn scholarly rhetoric?_\n\nMy 2nd question was from the field of information science:\n\n_How can we design public DH websites to handle a huge influx of readers and annotations, so that individual user experience doesn’t suffer and diverse user needs are met? How would each user find the “signal” of the best annotations for them to read, from the noise of “infinite” public annotations?_\n\nMy 3rd question was from the field of literature:\n\n_Textual scholars often create editions as their scholarship. By separating out historical textual scholarship values from how the editions realizing those values looked, can we imagine other forms still holding true to the values of our field? What can we learn by accepting scholarly editions as just one of many ways of embodying textual scholarship values?_\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.016.png)\n\nNow, I’m here to speak about collaborative DH today, and you might be wondering how a literature dissertation can be a collaborative rather than a largely solo activity. I did perform the full scholarly labor you’d expect from a literature dissertation—most people would agree successfully defending the dissertation and becoming a tenure-track assistant professor shows that. But beyond my dissertation-length work, the larger project around my dissertation relied strongly on collaboration and the intellectual generosity of others.\n\nA huge number of people made _Infinite Ulysses_ possible, so much so that I can’t name everyone right now, but I’ve put as many as possible up on the screen [_image above previous paragraph, and [this credits webpage](http://www.infiniteulysses.com/credits) goes into more detail_].\n\nMy advisor [Matt Kirschenbaum](http://twitter.com/mkirschenbaum) and committee members [Neil Fraistat](https://twitter.com/fraistat), [Kari Kraus](https://twitter.com/karikraus), [Melanie Kill](https://twitter.com/melaniekill), and Brian Richardson are amazingly intellectually generous scholars. They met with me as a team regularly throughout the entire dissertation, which was hugely helpful in keeping everyone informed about and happy with the developing look of the dissertation. I also benefitted from the feedback of colleagues at the [MITH](http://mith.umd.edu) DH center where I worked 2009-2015, such as [Stephanie Sapienza](https://twitter.com/sapienza77) and [Ed Summers](https://twitter.com/edsu). Their questions, guidance, and encouragement were critical in realizing my project.\n\nThrough social media, DHer [Michael Widner](https://twitter.com/mwidner) at Stanford and I discovered we were working on similar tech (the Drupal annotator module). Despite being on opposite coasts, we Skyped to discuss our work, and he ended up generously allowing me to build on top of some of his [_Lacuna Stories_](https://www.lacunastories.com/) project code that hadn’t yet been publicly released, allowing me to spend more time on a different piece of coding more closely tied to my research questions.\n\nThe people who visited _Infinite Ulysses_ and shared annotations and/or feedback on the site [are collaborators, too](http://www.infiniteulysses.com/credits). The majority of the site’s annotations were authored by people other than me. Ten types of user testing, data gathering, and participatory design, with a variety of people, over the course of the site’s development, directly shaped the growth of the site.\n\n_Infinite Ulysses_ was a collaborative project with a large portion created by one person. Now, I want to tell you about a project that’s far more of a distributed and balanced collaborative, the [Digital Humanities Slack](http://tinyurl.com/DHSlack).\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.019.png)\n\nSlack is a social media platform that functions like a set of interconnected, themed chat rooms. Using the Slack platform, I created and manage the scholarly forum of the Digital Humanities Slack (or DH Slack). Started a little over a year ago, it now supports over 1,200 digital humanists through around 70 “channels” (thematic chat rooms on topics like libraries and the digital humanities, open access, data sharing, and various DH methods and theories).\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.020.png)\n\nThe DH Slack is open to anyone with a curiosity about DH and/or related interests (e.g. digital libraries, museums, and archives)—those interested just visit [tinyurl.com/DHslack](http://tinyurl.com/DHslack) to join. Absolutely no DH expertise is required, and we have several specific channels devoted to DH beginners, students, job seekers, and asking all kinds of DHy questions.\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.021.png)\n\nThe DH Slack is ““mine”” [_all the air quotes_] in an extremely limited sense, if any. I set up the Slack, publicize its existence, moderate it, periodically started discussions (more in its early days than now), answer questions about using the Slack, and lead policy-making such as adapting a DH-specific code of conduct. That code of conduct covers things like not publicly posting screenshots of Slack conversations without first getting the consent of everyone shown in the shot, which is why I’m largely showing tweets rather than screengrabs.\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.022.png)\n\nFive other DHers generously collaborate with me as additional Slack moderators: [Alan G. Pike](https://twitter.com/agilchristpike), [Sam Abrams](https://twitter.com/sabramse), [Alex Gil](https://twitter.com/elotroalex), [Ed Summers](https://twitter.com/edsu), and [Paige C. Morgan](https://twitter.com/paigecmorgan). We take our code of conduct seriously, and have taken firm action when it’s been called for, to protect our community members.\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.023.png)\n\nThe DH Slack exists because of its members, who build a humanities community through their conversations and sharing. Even the most collaborative and democratic of teams needs one person to really be in charge of project management, and I feel like my current role with the DH Slack is similar—helping things along, being available and responsible should problems arise, and making the final call after group discussion.\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.024.png)\n\nWe’re still figuring out how Slack can be useful to our scholarship: Can it allow different kinds of conversations than Twitter? On the screen, you can see examples of DHers moving between the Slack and Twitter to have different kinds of conversations—in that middle tweet, a technical discussion that was limited by Twitter’s format switched to the Slack to continue more freely.\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.025.png)\n\nSome of our members work at non-profits or in K-12 teaching, or aren’t professionally involved in academics; some are undergrads or grad students. It’s clear that the Slack, like many other platforms, can support geographically dispersed collaborators. But can we use the Slack to teach and support people interested in DH who don’t know of any potential collaborators, don’t have mentors geographically near them, or who aren’t inside academia? For example, I’m hoping to do a Slack hangout this spring where I’ll be available on the Slack to help anyone who wants to work through [my _Programming Historian_ lesson on creating their own first research website](http://programminghistorian.org/lessons/building-static-sites-with-jekyll-github-pages). If DH mentors can periodically be virtually available to support questions about specific tech tutorials, maybe we can ease new DHers experimentation with digital methods.\n\nInteresting uses of the DH Slack I’ve seen so far include allowing remote participants at MITH’s recent “[Night Against Hate](http://mith.umd.edu/research/night-against-hate/)” scholar-activist hackathon, mentorship for new digital humanists, a place to connect regional DH networks such as in Tennessee, Baltimore, and Australia and languages such as Spanish, and as a space to expand Twitter conversations. We’ve been used as a model by other scholars for their own Slack instances, and I authored an invited guest post about the DH Slack [for the London School of Economics blog](http://blogs.lse.ac.uk/impactofsocialsciences/2016/07/13/using-slack-to-support-a-geographically-dispersed-community/) this past summer. If you’d like a place to discuss and learn about the digital humanities, or a friendly place to ask questions, please join us via [TinyUrl.com/DHSlack](http://TinyUrl.com/DHSlack).\n\nBoth the DH Slack and _Infinite Ulysses_ model a digital humanities that’s not disruptive, but different. Both take an experimental approach grounded on past humanities successes, whether that’s DH Twitter or speculative textual experiments like the [Ivanhoe](http://ivanhoe.scholarslab.org/) game or [Prism](http://prism.scholarslab.org/). The experiments of both projects recognizably demonstrate humanities scholarship and community.\n\nBoth experiments also demonstrate my key interests. I’m deeply invested in DH design and building as a route to new knowledge, especially through interfaces for public, communal learning. And I’m keenly interested in the larger questions of how we make DH go: infrastructure and process, project management, and community-building. If you visit my [LiteratureGeek.com](http://www.LiteratureGeek.com) blog, you’ll see that I share everything from theory and analysis, to my software and design decisions, how I get started planning a new project, and how my work space is set up. This is all as a way to think through not just the strategies, but also the daily tactics of successful DH.\n\n_Infinite Ulysses_ and the DH Slack are at two ends of the collaboration spectrum, but the bulk of my interdisciplinary collaboration experience actually happens quite differently, in project teams that regularly meet face-to-face (as with a group of graduate students I taught to work with the [_Shelley-Godwin Archive_](http://shelleygodwinarchive.org)) or over Skype (as with the [BitCurator](http://literaturegeek.com/2013/08/26/bitcuratordigitalforensics) team).\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.028.png)\n\nFor example, in my current role as a DH librarian and faculty at Purdue Libraries, I enjoy a close working relationship with my colleagues in both Archives and Special Collections, and in our university press. Together, we’re using DH as the connector to build a workflow for campus DH research and teaching that runs from archival description and discovery, through scholarly communication and publication.\n\nI didn’t cover this kind of collaboration in detail in this talk, as I suspect this kind of face-to-face collaboration is the most common and familiar. But, I’ll wrap up by listing on the screen a few of my more typically collaborative DH projects on the screen, and I’m happy to discuss these further in the Q&A or over email.\n\n![](http://www.literaturegeek.com/assets/UVaJobTalkSlides/UVaManagingDirector.031.png)\n\n_Originally posted at [LiteratureGeek.com](http://literaturegeek.com/2017/02/23/scholars-lab-dh-center-managing-director-job-talk). Edited 3/6/2017 to link to Brandon's job talk._\n"},{"id":"2017-03-06-in-out-across-with-collaborative-education-and-digital-humanities-job-talk-for-head-of-graduate-programs","title":"In, Out, Across, With: Collaborative Education and Digital Humanities (Job Talk for Head of Graduate Programs)","author":"brandon-walsh","date":"2017-03-06 06:15:23 -0500","categories":["Digital Humanities","Grad Student Research","Technical Training"],"url":"in-out-across-with-collaborative-education-and-digital-humanities-job-talk-for-head-of-graduate-programs","content":"_[Crossposted on my [personal blog](http://walshbr.com/blog/2017/03/02/in-out-across-with/) and the [WLUDH blog](https://digitalhumanities.wlu.edu/blog/2017/03/06/in-out-across-with-collaborative-education-and-digital-humanities-job-talk-for-scholars-lab/).]_\n\nI’ve accepted a new position as the [Head of Graduate Programs in the Scholars’ Lab](http://scholarslab.org/uncategorized/brandon-walsh-is-our-new-head-of-graduate-programs-starting-april-24-2017/), and I’ll be transitioning into that role over the next few weeks! As a part of the interview process, we had to give a job talk. While putting together this presentation, I was lucky enough to have past examples to work from (as you’ll be able to tell, if you check out this past [job talk](http://literaturegeek.com/2016/02/28/DHjobtalk) by Amanda Visconti). Since my new position will involve helping graduate students through the process of applying for positions like these, it only feels right that I should post my own job talk as well as a few words on the thinking that went into it. Blemishes, jokes, and all, hopefully these materials will help someone in the future find a way in, just as the example of others did for me. And if you’re looking for more, Visconti has a great list of other examples linked from her [more recent job talk](http://scholarslab.org/digital-humanities/disrupt-the-humanities-managing-director-job-talk/) for the Scholars’ Lab.\n\nFor the presentation, I was asked to respond to this prompt:\n\n\n<blockquote>What does a student (from undergraduate to doctoral levels) need to learn or experience in order to add “DH” to his or her skill set? Is that an end or a means of graduate education? Can short-term digital assignments in discipline-specific courses go beyond “teaching with technology”? Why not refer everyone to online tutorials? Are there risks for doctoral students or the untenured in undertaking digital projects? Drawing on your own experience, and offering examples or demonstrations of digital research projects, pedagogical approaches, or initiatives or organizations that you admire, make a case for a vision of collaborative education in advanced digital scholarship in the arts and humanities.</blockquote>\n\n\nI felt that each question could be a presentation all its own, and I had strong opinions about each one. Dealing with all of them seemed like a tall order. I decided to spend the presentation close reading and deconstructing that first sentence, taking apart the idea that education and/or digital humanities could be thought of in terms of lists of skills at all. Along the way, my plan was to dip into the other questions as able, but I also assumed that I would have plenty of time during the interview day to give my thoughts on them. I also wanted to try to give as honest a sense as possible of the way I approach teaching and mentoring. For me, it’s all about people and giving them the care that they need. In conveying that, I hoped, I would give the sort of vision the prompt was asking for. I also tried to sprinkle references to the past and present of the Scholars’ Lab programs to ground the content of the talk. When I mention potential career options in the body of the talk, I am talking about specific alumni who came through the fellowship programs. And when I mention graduate fellows potentially publishing on their work with the Twitter API, well, [that’s not hypothetical either](http://scholarslab.org/uncategorized/working-with-an-archive-of-the-now/).\n\nSo below find the lightly edited text of the talk I gave at the Scholars’ Lab - “In, Out, Across, With: Collaborative Education and Digital Humanities.” I’ve only substantively modified one piece - swapping out one example for another.\n\nAnd a final note on delivery: I have heard plenty of people argue over whether it is better to read a written talk or deliver one from notes. My own sense is that the latter is far more common for digital humanities talks. I have seen both fantastic read talks and amazing extemporaneous performances, just as I have seen terrible versions of each. My own approach is, increasingly, to write a talk but deliver that talk more or less from memory. In this case, I had a pretty long commute to work, so I recorded myself reading the talk and listened to it a lot to get the ideas in my head. When I gave the presentation, I had the written version in front of me for reference, but I was mostly moving through my own sense of how it all fit together in real time (and trying to avoid looking at the paper). My hope is that this gave me the best of both worlds and resulted in a structured but engaging performance. Your mileage may vary!\n\n\n## In, Out, Across, With: Collaborative Education and Digital Humanities\n\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/slide01.jpg) It’s always a treat to be able to talk with the members of the UVA Library community, and I am very grateful to be here. For those of you that don’t know me, I am Brandon Walsh, Mellon Digital Humanities Fellow and Visiting Assistant Professor of English at Washington and Lee University. The last time I was here, I gave a talk that had almost exclusively animal memes for slides. I can’t promise the same robust Internet culture in this talk, but talk to me after and I can hook you up. I swear I’ve still got it.\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/slide02.jpg) In the spirit of Amanda Visconti, the resources that went into this talk (and a number of foundational materials on the subject) can all be found in a Zotero collection at the [above link](https://goo.gl/r6MCwD). I’ll name check any that are especially relevant, but hopefully this set of materials will allow the thoughts in the talk to flower outwards for any who are interested in seeing its origins and echoes in the work of others.\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/slide03.jpg) And a final prefatory note: no person works, thinks or learns alone, so here are the names of the people in my talk whose thinking I touch upon as well as just some – but not all – of my colleagues at W&L who collaborate on the projects I mention. Top tier consists of people I cite or mention, second tier is for institutions or publications important to discussion, and final tier is for direct collaborators on this work.\n\nToday I want to talk to you about how best to champion the people involved in collaborative education in digital research. I especially want to talk about students. And when I mention “students” throughout this talk, I will mostly be speaking in the context of graduate students. But most of what I discuss will be broadly applicable to all newcomers to digital research. My talk is an exhortation to find ways to elevate the voices of people in positions like these to be contributors to professional and institutional conversations from day one and to empower them to define the methods and the outcomes of the digital humanities that we teach. This means taking seriously the messy, fraught, and emotional process of guiding students through digital humanities methods, research, and careers. It means advocating for the legibility of this digital work as a key component of their professional development. And it means enmeshing these voices in the broader network around them, the local context that they draw upon for support and that they can enrich in turn. I believe it is the mission of the Head of Graduate Programs to build up this community and facilitate these networks, to incorporate those who might feel like outsiders to the work that we do. Doing so enriches and enlivens our communities and builds a better and more diverse research and teaching agenda. ![](http://scholarslab.org/wp-content/uploads/2017/03/slide04.jpg)This talk is titled “In, Out, Across, With: Collaborative Education and Digital Humanities,” and I’ll really be focusing on the prepositions of my title as a metaphor for the nature of this sort of position. I see this role as one of connection and relation. The talk runs about 24 minutes, so we should have plenty of time to talk.\n\nWhen discussing digital humanities education, it is tempting to first and foremost discuss what, exactly, it is that you will be teaching. What should the students walk away knowing? To some extent, just as there is more than one way to make breakfast, you could devise numerous baseline curricula. ![](http://scholarslab.org/wp-content/uploads/2017/03/slide05.jpg)\n\nThis is what we came up with at Washington and Lee for students in our [undergraduate digital humanities fellowship program](http://digitalhumanities.wlu.edu/initiatives/undergraduate-fellowship/). We tried to hit a number of kinds of skills that a practicing digital humanist might need. It’s by no means exhaustive, but the list is a way to start. We don’t expect one person to come away knowing everything, so instead we aim for students to have an introduction to a wide variety of technologies by the end of a semester or year. They’ll encounter some technologies applicable to project management, some to front-end design, as well as a variety of programming concepts broadly applicable to a variety of situations. Lists like this give some targets to hit. But still, even as someone who helped put this list together, it makes me worry a bit. I can imagine younger me being afraid of it! It’s easy for us to forget what it was like to be new, to be a beginner, to be learning for the first time, but I’d like to return us to that frame of thinking. I think we should approach lists like these with care, because they can be intimidating for the newcomer. So in my talk today I want to argue against lists of skills as ways of thinking.\n\nI don’t mean to suggest that programs need no curriculum, nor do I mean to suggest that no skills are necessary to be a digital humanist. But I would caution against focusing too much on the skills that one should have at the end of a program, particularly when talking about people who haven’t yet begun to learn. I would wager that many people on the outside looking in think of DH in the same way: it’s a big list of unknowns. I’d like to get away from that.\n\nTemplates like this are important for developing courses, fellowship, and degree-granting programs, but I worry that the goodwill in them might all too easily seem like a form of gatekeeping to a new student. It is easy to imagine telling a student that “you have to learn GitHub before you can work on this project.” It’s just a short jump from this to a likely student response - “ah sorry - I don’t know that yet.\" And from there I can all too easily imagine the common refrain that you hear from students of all levels - “If I can’t get that, then it’s because I’m not a technology person.” From there - \"Digital humanities must not be for me.”\n\nInstead of building our curricula out of as-yet-unknown tool chains, I want to float, today, a vision of DH education as an introduction to a series of professional practices. Lists of skills might be ends but I fear they might foreclose beginnings. ![](http://scholarslab.org/wp-content/uploads/2017/03/slide06.jpg)Instead, I will float something more in line with that of the Scholarly Communication Institute (held here at UVA for a time), which outlined what they saw as the needs of graduate and professional students in the digital age. I’ll particularly draw upon their first point here (last of my slides with tons of text, I swear): graduate students need training in “collaborative modes of knowledge production and sharing.”\n\nI want to think about teaching DH as introducing a process of discovery that collapses hierarchies between expert and newcomer: that’s a way to start. This sort of framing offers digital humanities not as a series of methods one does or does not know, but, rather, as a process that a group can engage in together. Do they learn methods and skills in the process? Of course! Anyone who has taken part in the sort of collaborative group projects undertaken by the Scholars’ Lab comes away knowing more than they came in with. But I want to continue thinking about process and, in particular, how that process can be more inclusive and more engaging. By empowering students to choose what they want to learn and how they want to learn it, we can help to expand the reach of our work and better serve our students as mentors and collaborators. There are a few different in ways in which I see this as taking place, and they’ll form the roadmap for the rest of the talk. ![](http://scholarslab.org/wp-content/uploads/2017/03/slide07.jpg) Apologies - this looks like the sort of slide you would get at a business retreat. All the same - we need to adapt and develop new professional opportunities for our students at the same time that we plan flexible outcomes for our educational programs. These approaches are meant to serve increasingly diverse professional needs in a changing job market, and they need to be matched by deepening support at the institutional level.\n\nSo to begin. One of our jobs as mentors is to encourage students to seek out professionally legible opportunities early on in their careers, and as shapers of educational programs we can go further and create new possibilities for them. At W&L, we have been [collaborating with the Scholars’ Lab](https://github.com/wludh/research-one-collab) to bring UVA graduate students to teach short-form workshops on digital research in W&L classrooms. Funded opportunities like this one can help students professionalize in new ways and in new contexts while paying it forward to the nearby community. A similar initiative at W&L that I’ve been working on has our own library faculty and undergraduate fellows visiting local high schools to speak with advanced AP computer science students about how their own programming work can apply to humanities disciplines. I’m happy to talk more about these in Q&A.\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/slide08.jpg)We also have our student collaborators present at conferences, both on their own work and on work they have done with faculty members, both independently and as co-presenters. Here is Abdur, one of our undergraduate Mellon DH fellows, talking about the writing he does for his thesis and how it is enriched by and different from the writing he does in digital humanities contexts at the Bucknell Digital Scholarship Conference last fall. While this sort of thing is standard for graduate students, it’s pretty powerful for an undergraduate to present on research in this way. Learning that it’s OK to fail in public can be deeply empowering, and opportunities like these encourage our students to think about themselves as valuable contributors to ongoing conversations long before they might otherwise feel comfortable doing so.\n\nBut teaching opportunities and conferences are not the only ways to get student voices out there. I think there are ways of engaging student voices earlier, at home, in ways that can fit more situations. We can encourage students to engage in professional conversations by developing flexible outcomes in which we are equal participants. One approach to this with which I have been experimenting is group writing, which I think is undervalued as a taught skill and possible approach to DH pedagogy. An example: when a history faculty member at W&L approached the library (and by extension, me) for support in supplementing an extant history course with a component about digital text analysis, we could have agreed to offer a series of one-off workshops and be done with it. ![](http://scholarslab.org/wp-content/uploads/2017/03/slide09.jpg) Instead, this faculty member – Professor Sarah Horowitz – and I decided to collaborate on a more extensive project together, producing [Introduction to Text Analysis: A Coursebook](https://www.walshbr.com/textanalysiscoursebook). The idea was to put the materials for the workshops together ahead of time, in collaboration, and to narrativize them into a set of lessons that would persist beyond a single semester as a kind of publication. The pedagogical labor that we put into reshaping her course could become, in some sense, professionally legible as a series of course modules that others could use beyond the term. So for the book, we co-authored a series of units on text analysis and gave feedback on each other’s work, editing and reviewing as well as reconfiguring them for the context of the course. Professor Horowitz provided more of the discipline-specific material that I could not, and I provided the materials more specific to the theories and methods of text analysis. Neither one of us could have written the book without the other.\n\nProfessor Horowitz was, in effect, a student in this moment. She was also a teacher and researcher. She was learning at the same time that she produced original scholarly contributions. Even as we worked together, for me this collaborative writing project was also a pedagogical experiment that drew upon the examples of [Robin DeRosa](https://openamlit.pressbooks.com/), [Shawn Graham](http://workbook.craftingdigitalhistory.ca/), and [Cathy Davidson](http://www.hastac.org/collections/field-notes-21st-century-literacies), in particular. ![](http://scholarslab.org/wp-content/uploads/2017/03/slide10.jpg) Davidson taught a graduate course on “21st Century Literacies” where each of her students wrote a chapter that was then collected and published as an open-access book. For us as for Davidson, the process of knowing, the process of uncovering is something that happens together. In public. And it’s documented so that others can benefit. Our teaching labor could become visible and professionally legible, as could the labor that Professor Horowitz put into learning new research skills. As she adapts and tries out ideas, and as we coalesce them into a whole, the writing product is both the means and the end of an introduction to digital humanities.\n\nProfessor Horowitz also wanted to learn technical skills herself, and she learned quite a lot through the writing process. Rather than sitting through lectures or being directed to online tutorials by me, I thought she would learn better by engaging with and shaping the material directly. Her course and my materials would be better for it, as she would be helping to bind my lectures and workshops to her course material. The process would also require her to engage with a list of technologies for digital publishing. ![](http://scholarslab.org/wp-content/uploads/2017/03/slide11.jpg) Beyond the text analysis materials and concepts, the process exposed her to a lot of technologies: command line, Markdown, Git for version control, GitHub for project management. In the process of writing this document, in fact, she covered most of the same curriculum as our undergraduate DH fellows. ![](http://scholarslab.org/wp-content/uploads/2017/03/slide12.jpg) She’s learning these things as we work together to produce course materials, but, importantly, the technical skills aren’t the focus of the work together. It’s a writing project! Rather than presenting the skills as ends in themselves, they were the means by which we were publishing a thing. They were immediately useful. And I think displacing the technology is helpful: it means that the outcomes and parameters for success are not based in the technology itself but, rather, in the thinking about and use of those methods. We also used a [particular platform](https://www.gitbook.com) that allowed Professor Horowitz to engage with these technologies in a light way so that they would not overwhelm our work – I’m happy to discuss more in the time after if you’re interested.\n\nThis to say: the outcomes of such collaborative educations can be shaped to a variety of different settings and types of students. Take another model, [CUNY’s Graduate Center Digital Fellows program](https://digitalfellows.commons.gc.cuny.edu/), whose students develop open tutorials on digital tools. Learning from this example, rather than simply direct students or colleagues towards online tutorials like these, why not have them write their own documents, legible for their own positions, that synthesize and remix the materials that they already have found? ![](http://scholarslab.org/wp-content/uploads/2017/03/slide13.jpg) The learning process becomes something productive in this framing. I can imagine, for example, directing collaboratively authored materials by students like these towards something like [The Programming Historian](http://programminghistorian.org/). If you’re not familiar, The Programming Historian offers a variety of lessons on digital humanities methods, and they only require an outline as a pitch to their editorial team, not a whole written publication ready to go. Your graduate students could, say, work with the Twitter API over the course of a semester, blog about the research outcomes, and then pitch a tutorial to The Programming Historian on the API as a result of their work. It’s much easier to motivate yourselves to write something if you know that the publication has already been accepted. Obviously such acceptance is not a given, but working towards a goal like this can offer student researchers something to aim for. Their instructors could co-author these materials, even, so that everyone has skin in the game.\n\nThis model changes the shape of what collaborative education can look like: it’s duration and its results. You don’t need a whole fellowship year. You could, in a reasonably short amount of time, tinker and play, and produce a substantial blog post, an article pitch, or a Library Research Guide (more on that in a moment).\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/slide14.jpg)As Jeff Jarvis has said, “we need to move students up the education chain.” And trust me - the irony of quoting a piece titled “Lectures are Bullshit” during a lecture to you is not lost on me. But stay with me.\n\nCollaborative writing projects on DH topics are flexible enough to fit the many contexts for the kind of educational work that we do. After all, no one needs or values the same outcomes, and these shared and individual goals need to be worked out in conversation with the students themselves early on. Articulating these desires in a frank, written, and collaborative mode early on (in the genre of [the project charter](http://praxis.scholarslab.org/charter/)), can help the program directors to better shape the work to fit the needs of the students. But I also want to suggest that collaborative writing projects can be useful end products as well as launching pads, as they can fit the shape of many careers. After all, students come to digital humanities for a variety of different reasons. Some might be aiming to bolster a research portfolio on the path to a traditional academic career. Others might be deeply concerned about the likelihood of attaining such a position and be looking for other career options. Others still might instead be colleagues interested in expanding their research portfolio or skillset but unable to commit to a whole year of work on top of their current obligations. Writing projects could speak to all these situations.\n\nI see someone in charge of shaping graduate programs as needing to speak to these diverse needs. This person is both a steward of where students currently are – the goals and objectives they might currently have – as well as of where they might go – the potential lives they might (or might not!) lead. After all, graduate school, like undergraduate, is an enormously stressful time of personal and professional exploration. If we think simply about a student’s professional development as a process of finding a job, we overlook the real spaces in which help might be most desired. Frequently, those needs are the anxieties, stresses, and pressures of refashioning yourself as a professional. We should not be in the business of creating CV lines or providing lists of qualifications alone. We should focus on creating strong, well-adjusted professionals by developing ethical programs that guide them into the professional world by caring for them as people.\n\nIn the graduate context, this involves helping students deal with the academic job market in particular. ![](http://scholarslab.org/wp-content/uploads/2017/03/slide15.jpg) To me in its best form, this means helping students to look at their academic futures and see proliferating possibilities instead of a narrow and uncertain route to a single job, to paraphrase the work of Katina Rogers. A sprinkler rather than a pipeline, in her metaphor. As Rogers’s work, in particular, has shown, recent graduate students increasingly feel that, while they experienced strong expectations that they would continue in the professoriate, they received inadequate preparation for the many different careers they might actually go on to have. [The Praxis Program](http://praxis.scholarslab.org) and the [Praxis Network](http://praxis-network.org/) are good examples of how to position digital humanities education as answers to these issues. Fellowship opportunities like these must be robust enough that they can offer experiences and outcomes beyond the purely technical, so that a project manager from one fellowship year can graduate with an MA and go into industry in a similar role just as well-prepared as a PhD student aiming to be a developer might go on to something entirely different. And the people working these programs must be prepared for the messy labor of helping students to realize that these are satisfactory, laudable professional goals.\n\nIt should be clear that this sort of personal and professional support is the work of more than just one person. One of the strengths of a digital humanities center embedded in a library like this one at UVA is that fellows have the readymade potential to brush up against a variety of career options that become revealed when peaking outside of their disciplinary silos: digital humanities developers and project manager positions, sure, but also metadata specialists, archivists, and more. I think this kind of cross-pollination should be encouraged: library faculty and staff have a lot to offer student fellows and vice versa. Developing these relationships brings the fellows further into the kinds of the work done in the library and introduces them to careers that, while they might require further study to obtain, could be real options.\n\nTo my mind the best fellowship programs are those fully aware of their institutional context and those that both leverage and augment the resources around them as they are able. We have been working hard on this at W&L. We are starting to institute a series of workshops led by the undergraduate fellows in consultation with the administrators of the fellowship program. The idea is that past fellows lead workshops for later cohorts on the technology they have learned, some of which we selectively open to the broader library faculty and staff. The process helps to solidify the student’s training – no better way to learn than to teach – but it also helps to expand the student community by retaining fellows as committed members. It also helps to fill out a student’s portfolio with a cv-ready line of teaching experience. This process also aims to build our own capacity within the library by distributing skills among a wider array of students, faculty, and staff. After all, student fellows and librarians have much they could learn from one another. I see the Head of Graduate Programs as facilitating such collaborations, as connecting the interested student with the engaged faculty/staff/librarian collaborator, inside their institution or beyond.\n\nBut we must not forget that we are asking students and junior faculty to do risky things by developing these new interests, by spending time and energy on digital projects, let alone presenting and writing on them in professional contexts. The biggest risk is that we ask them to do so without supporting them adequately. All the technical training in the world means little if that work is illegible and irrelevant to your colleagues or committee. ![](http://scholarslab.org/wp-content/uploads/2017/03/slide16.jpg) In the words of Kathleen Fitzpatrick, we ask these students to “do the risky thing,” but we must “make sure that someone’s got their back.” I see the Head of Graduate Programs as the key in coordinating, fostering, and providing such care.\n\nStudents and junior faculty need support – for technical implementation, sure – but they also need advocates – people who can vouch for the quality of their work and campaign on their behalf in the face of committees and faculty who might be otherwise unable to see the value of their work. Some of this can come from the library, from people able to put this work in the context of guidelines for the evaluation of digital scholarship. But some of this support and advocacy has to come from within their home departments. The question is really how to build up that support from the outside in. And that’s a long, slow process that occurs by making meaningful connections and through outreach programs. At W&L, we have worked to develop an [incentive grant program](http://digitalhumanities.wlu.edu/initiatives/incentive-grants/), where we incentivize faculty members who might be new to digital humanities or otherwise skeptical to experiment with incorporating a digital project into their course. The result is a slow burn – we get maybe one or two new faculty each term trying something out. That might seem small, but it’s something, particularly at a small liberal arts college. This kind of slow evangelizing is key in helping the work done by digital humanists to be legible to everyone. Students and junior faculty need advocates for their work in and out of the library and their home departments, and the person in this position is tasked with overseeing such outreach.\n\nSo, to return to the opening motif, lists of skillsets certainly have their place as we bring new people into the ever-expanding field: they’re necessary. They reflect a philosophy and a vision, and they’re the basis of growing real initiatives. But it’s the job of the Head of Graduate Programs to make sure that we never lose sight of the people and relationships behind them.\n\nForemost, then, I see the Head of Graduate Programs as someone who takes the lists, documents, and curricula that I have discussed and connects them to the people that serve them and that they are meant to speak to. This person is one who builds relationships, who navigates the prepositions of my title. ![](http://scholarslab.org/wp-content/uploads/2017/03/slide17.jpg) It’s the job of such a person to blast the boundary between “you’re in” and “you’re out” so that the tech-adverse or shy student can find a seat at the table. This is someone who makes sure that the work of the fellows is represented across institutions and in their own departments. This person makes sure the fellows are well positioned professionally. This person builds up people and embeds them to networks where they can flourish. Their job is never to forget what it’s like to be the person trying to learn. Their job is to hear “I’m not a tech person” and answer “not yet, but you could be! and I know just the people to help. Let’s learn together.”\n"},{"id":"2017-03-13-gothic-dh-at-washington-and-lee","title":"“Gothic DH” at Washington and Lee","author":"christian-howard","date":"2017-03-13 05:00:54 -0400","categories":["Digital Humanities","Experimental Humanities","Grad Student Research"],"url":"gothic-dh-at-washington-and-lee","content":"*Cross-posted on the [Washington and Lee DH Blog](http://digitalhumanities.wlu.edu/blog/2017/03/13/gothic-dh-at-washington-and-lee/)*\n\nToward the beginning of the semester, I was contacted by Brandon Walsh, a UVA alumnus and the current Mellon Digital Humanities Fellow at Washington and Lee. As part of his fellowship, Brandon has been pairing UVA DH folks with professors at Washington and Lee; the initiative is aimed at integrating new developments in technology with more traditional pedagogical methods practiced at a liberal arts university (read more about it here: [http://augustafreepress.com/mellon-foundation-grants-washington-and-lee-funds-for-digital-humanities-study-of-histor/](http://augustafreepress.com/mellon-foundation-grants-washington-and-lee-funds-for-digital-humanities-study-of-histor/)). Brandon paired me with Professor Taylor Walle, who is teaching Washington and Lee’s English 232: “Frantic and Sickly, Idle and Extravagant: The Gothic Novel, 1764-2002.”\n\nNow I’m no expert in the gothic novel, but after talking with Taylor, it became clear that our interests overlapped in unexpected yet productive ways. That is, one of Taylor’s aims in her course is to trouble the distinction between “highbrow” and “lowbrow” literature. A few of the questions that Taylor set to her students include: “Do you think the gothic deserves its long history of being associated with ‘popular’ (read: trash) fiction? Do you think the distinction between ‘highbrow’ and ‘lowbrow’ is legitimate or useful? How do we determine what counts as ‘high’ and what counts as ‘low’? Are these categories meaningful or arbitrary?” These questions resonated with my own research into what Nick Levey has termed “post-press” literature ([http://post45.research.yale.edu/2016/02/post-press-literature-self-published-authors-in-the-literary-field-3/](http://post45.research.yale.edu/2016/02/post-press-literature-self-published-authors-in-the-literary-field-3/)) and other technologically experimental publication platforms, including Twitter and digital comics. Because such self-published literature is not approved by the “gatekeepers” of the publishing market, many people – especially authors who have successfully navigated the publishing market – have denigrated self-published literature. Award-winning author Laurie Gough, for instance, states: “I’d rather share a cabin on a Disney cruise with Donald Trump than self-publish. To get a book published in the traditional way, and for people to actually respect it and want to read it — you have to go through the gatekeepers of agents, publishers, editors, national and international reviewers. These gatekeepers are assessing whether or not your work is any good. …The problem with self-publishing is that it requires zero gatekeepers.” So we can see a current-day example of the tension between so-called “highbrow” and “lowbrow” literature in the form of traditionally-published vs. independent and experimental publication methods.\n\nI started looking for post-press and technologically-experimental literature that echoes gothic themes and characteristics. One work that Taylor and her class were discussing particularly caught my interest, namely, Oscar Wilde’s _Picture of Dorian Gray_. So I began my search for the “digital gothic” using this work as my focal point, and the results were intriguing. Not only has _Dorian Gray _been a favorite subject of Hollywood, but there have also been multiple graphic novels starring Dorian. Bluewater Comics even published an interactive, digital comic titled _Dorian Gray_ that incorporates sound and movement.\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/2017-02-18-17.17.59-300x169.png)\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/2017-03-07-16.01.38-300x169.png)\n\n***Screen shot of _Dorian Gray_ the digital comic, created by Darren G. Davis, Scott Davis, and Federico De Luca.**\n\nI continued brainstorming and gathering materials, until it was finally time to visit Taylor’s class on Wednesday, March 1, 2017. Despite the relatively early hour (8:30 am), Taylor’s class was enthusiastic and eager to participate. We reviewed the categories of highbrow and lowbrow literature before jumping into a discussion about Wilde’s _Dorian Gray_, which the students had just finished reading. Wilde’s descriptions and imagery gave us a pivoting point to examine other interpretations of Wilde’s work, particularly these graphic versions. But before we looked at these versions, I had the students watch part of Scott McCloud’s TED Talk, “The Visual Magic of Comics” ([https://www.ted.com/talks/scott_mccloud_on_comics](https://www.ted.com/talks/scott_mccloud_on_comics)). McCloud clearly and persuasively lays out the visual resources available to comics, and he further discusses the future of comics in a digital age. Armed with the vocabulary outlined by McCloud, we examined first a (printed) graphic novel and then the digital comic version of Wilde’s novel.\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/2017-03-01-08.54.43-300x169.jpg)\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/2017-03-01-08.56.09-300x169.jpg)\n\n***Our examination of the visual affordances of graphic novels and digital comics**\n\nWe looked at several other instances of contemporary gothic digital literature, including webcomics (particularly “Bongcheon-Dong Ghost”) and hypertext fiction (Shelley Jackson’s _Patchwork Girl_, a retelling of Mary Shelley’s _Frankenstein _featuring the creation of a female monster).\n\n\n###  ![](http://scholarslab.org/wp-content/uploads/2017/03/2017-03-01-09.42.21-300x169.jpg)\n\n\n***Students examine Shelley Jackson’s hypertext fiction, _Patchwork Girl_**\n\nThe students were particularly interested in the interactivity and changing role of the reader/user demanded by digital literature. One student observed that the physical reactions to print literature and interactive digital literature are different in that interactivity heightens the physical response, especially during moments of shock or horror (if you want to experience this yourself, check out the webcomic “Bongcheon-Dong Ghost” - [http://comic.naver.com/webtoon/detail.nhn?titleId=350217&no=31&weekday=tue](http://comic.naver.com/webtoon/detail.nhn?titleId=350217&no=31&weekday=tue)). Another student speculated that it might be precisely because of these visceral reactions that such digital literature is relegated to “horror” or “popular” genres rather than being considered “serious” literature.\n\nAfter the class was over, Taylor expressed her enthusiasm for the material and medial considerations that this foray into the digital had on her class. I know I certainly enjoyed the lively and insightful conversation that emerged from this unlikely pairing of the gothic and the digital, and I am excited to think more about the developing and emerging genres inspired by these new literary forms!\n"},{"id":"2017-03-15-topic-modeling-twitter","title":"Topic Modeling Twitter","author":"alicia-caticha","date":"2017-03-15 10:10:30 -0400","categories":null,"url":"topic-modeling-twitter","content":"**What is topic modeling?**\n\n\n\n\n\n\n\nTopic modeling is type of statistical model that sorts through a large corpus of writing through language processing algorithms with the purpose of discovering the broad topics under discussion by grouping together words frequently used in tandem.  This method has been used in the past by scholars working on distant reading, a method of studying literature that aggregates and analyzes massive amounts of text with the goal of uncovering the fundamentals of literature on a vast, universal level (the opposite of close reading, which focuses on singular and often exceptional canonized texts). \n\n\n\n\n\n\n\n**How did we implement it?**\n\n\n\n\n\n\n\nUsing the program [Mallet](http://mallet.cs.umass.edu/index.php) (Machine learning for language toolkit), I was able to import the text files of tweets and run the program.  The program resulted in two documents: a list of \"topics\" consisting of keywords, and meta data surrounding these topics.  This meta data included the percentage of tweets that focused one each \"topic\" and a list of the relevant tweets.\n\n\n\n\n\n\n\n\n\n\n\n**What did we learn?**\n\n\n\n\n\n\n\n\n\n\n\n\n\nTopic modeling is often assumed to be a primary text analysis tool for digital humanists. However, the utility of topic modeling is more ambiguous.  Where topic modeling may be an integral tools for studying an enormous body of law documents, for example, does it have the same utility for a corpus of tweets?\n\n\n\n\n\n\n\n\n\nKey \"topics\" revealed by the Kim Kardashian tweet corpus included clusters similar to the ones following: \n\n\n\n\n\n\n\n\"kardashian kollection sears dress spring\"\n\n\n\n\n\"excited line tomorrow launch coming sisters\"\n\n\n\n\n\"love guys wow million omg twitter followers congrats\"\n\n\n\n\n\"only fragrance buy free special people\"\n\n\n\n\n\"kendalljenner kyliejenner kylie sister kendall fun proud sis\"\n\n\n\n\n\"back nyc home missed time sleep\"\n\n\n\n\n\"shoes shoedazzle obsessed love\"\n\n\n\n\n\"keeping kardashians tonight season watch watching episode coast tune\"\n\n\n\n\n\"book kardashian today selfish beauty\"\n\n\n\n\n\"mom bruce dad cute kris khloe proud\"\n\n\n\n\n\"fashion love show week paris watching givenchy icon\"\n\n\n\n\n\"city selfie years sex double yeezus saint magic pablo\"\n\n\n\n\n\n\n\n\n\n\n\nNone of these \"topics\" are truly revealing.  We _know_ Kim tweets about her family, about fashion, about her personal life (Kanye West frequently appears in multiple \"topics\").  We _know_ that twitter is a key place for the Kardashian to promote their show _Keeping up with the Kardashians_ and this is blatantly clear in the multiple word clusters encouraging people to \"tune in.\" \n\n\n\n\n\n\n\n\n\nThe majority of the topics were simply variations of the aforementioned topics.  One notable exception was **\"people armenian power join stand april truth bro real genocide armo pray proud.\" ** We had known that Kim tweeted about her Armenian heritage and that the Kardashian family has been vocal about the Armenian genocide.  What the topic modeling allowed me to do was find _every_ tweet in which Kim mentioned anything relating to her Armenian heritage.  It further allowed me to place these tweets within broader categories.  Half of what I will term the Armenian tweets were also related to charity and activism tweets, using the same language and subject matter as anti-bullying campaigns.  The other half of Armenian tweets were of a social nature, discussing cooking Armenian food, going out to eat, and discussing notable Armenians.\n\n\n\n\n<blockquote>\n\n> \n> Everyone: Please call Speaker Pelosi TODAY at 202-225-0100 and URGE her to schedule a vote on H.Res.252, the Armenian Genocide Resolution\n> \n> \n— Kim Kardashian West (@KimKardashian) [December 9, 2010](https://twitter.com/KimKardashian/status/12999938942181376)</blockquote>\n\n\n\n\n\n\n\n\n\n\n\n\n**Conclusion**\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile topic modeling did not offer much in terms of sorting through the corpus of Kardashian tweets at our disposal, it did allow me to collect and analyze one specific thematic topic.  After this preliminary step, however, the onus to continue to study these tweets and how they might inform our study of the Kardashians public use of their Armenian heritage is on us.\n\n\n"},{"id":"2017-03-16-if-you-want-to-master-something-teach-it-digital-humanities-and-the-aha-moment","title":"If You Want to Master Something, Teach it: Digital Humanities and the ‘Aha!’ Moment","author":"nora-benedict","date":"2017-03-16 06:00:17 -0400","categories":["Digital Humanities","Grad Student Research","Technical Training"],"url":"if-you-want-to-master-something-teach-it-digital-humanities-and-the-aha-moment","content":"_[Cross-posted to the [Washington and Lee Digital Humanities Blog](http://digitalhumanities.wlu.edu/blog/2017/03/16/if-you-want-to-master-something-teach-it-digital-humanities-and-the-aha-moment/)]_\n\nI was recently invited to give a guest lecture in Caleb Dance’s Classics Course (“Blasts from the Classical Past: In Consideration of the Ancient Canon”) at Washington and Lee University as part of their [Mellon Foundation Grant for Digital Humanities](http://news.blogs.wlu.edu/2015/07/13/mellon-foundation-grants-wl-funds-for-digital-humanities-study-of-history/), which provides support for research, workshops, guest lectures, and the general development of Digital Humanities initiatives. As a contemporary Latin American scholar, who works on Jorge Luis Borges and publishing history, I was, at first, quite daunted by the task of teaching about classical canons and antiquity. Lucky for me (and the students), I was asked to work through possible avenues of investigation for their final project, “Book Biographies,” that required students to add their own text to the class’s “canon” and justify their selection with quantitative and visual data. More specifically, Caleb asked me to present students with a series of approaches for the project, including any necessary platforms and databases, and also touch on some potential problems that might arise in the process.\n\nBefore diving into a discussion of the digital tools for the day, I wanted to pause and parse out what exactly a “bibliographical biography” might be. Or rather, what students understood as “bibliographical” or a “bibliography” more generally. The entire class immediately defined the term to mean a list of works cited and used for research (i.e. enumerative bibliography). I then added to their definition by introducing the concepts of descriptive, analytical, and textual bibliography. We spent the most time walking through descriptive bibliography, or the physical descriptive of books as objects, because I felt that an understanding of this branch of bibliography would best serve students in thinking about the physical data necessary for their projects.\n\nI devoted the remainder of the class to presenting students with two avenues of investigation for their digital humanities projects. For fear of selecting a classical work that another student might have already chosen for his/her own project, I used Jorge Luis Borges’s Ficciones (1944) as my example text to add to their class “canon.”\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/Screen-Shot-2017-03-13-at-8.48.41-PM-231x300.png)\n\nFirst, drawing on my own current digital project, I introduced students to different modes of visualizing data with digital tools. For starters, I asked students what types of questions they might ask their chosen books to gather the necessary data to populate a map or visualizing tool. Together, we formed a list that included the work’s printing history, cost, copies produced, places of publication, languages of publication, and circulation, which would all help students to answer the central question of why their book should be added to the class’s “canon.” Moreover, I continually emphasized the need to accumulate as much physical data as possible about their work, and keep this information in an easily readable format (such as an excel spreadsheet).\n\nNext, I showed them a demo project I created with an annotated Google map, which plotted the locations of archival materials, such as manuscripts and correspondence, in green and translations of Ficciones in different languages in yellow. As a class, we added new plot points to track the circulation of Ficciones in libraries in the United States with data we quickly acquired from [WorldCat](http://www.worldcat.org) in purple:\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/Screen-Shot-2017-03-13-at-5.19.10-PM-300x138.png)\n\nAfter we mapped out several locations and entered detailed metadata for each point, I wanted to show students several examples of more advanced data visualization projects. My hope was that as these students explored and experimented with their first digital humanities projects, they would be inspired to work with more complex platforms and programs for future projects. Given my own training in the [UVA Scholars' Lab](http://scholarslab.org/), their unique program [Neatline](http://neatline.org/) was the most logical place to turn. In particular, I walked the students through a demo of the project, “Mapping the Catalogue of Ships,” which relies on data gathered from the second book of the Iliad to map the physical route discussed based on the locations named, which seemed most appropriate for a Classics course:\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/Screen-Shot-2017-03-13-at-9.28.37-PM-300x146.png)\n\nWhile platforms and programs for data visualization allowed the students to see the immediate impact of their selected text in terms of its production and circulation, I wanted to also push them to think about ways to represent the links, connections, and relations between certain authors and works across time and space. For starters, I asked students to think about how many works have been written about their selected texts (in terms of literary references, allusions, critical studies, or even parodies). I then showed them [DBPEDIA](http://wiki.dbpedia.org/), which extracts data and datasets from Wikipedia pages for further analysis. Looking to the [page dedicated to Jorge Luis Borges](http://dbpedia.org/page/Jorge_Luis_Borges), I scrolled to the section of writers that are influenced by him:\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/Screen-Shot-2017-03-13-at-5.23.15-PM-300x161.png)\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/Screen-Shot-2017-03-13-at-5.23.51-PM-212x300.png)\n\nThinking about the various names on this list, and the potential writers that might populate the lists for their own selected writers, allowed students to see the possible outcomes of analyzing social networks of impact. I told students that this type of data was not limited to people and could be expanded to think about various historical, social, or even political movements.\nAfter discussing several possible ways to gather data about the social networks related to their own texts, I showed the students a few examples of how their data might visually manifest itself, drawing on sample screenshots from [Cytoscape](http://www.cytoscape.org/), a platform which helps create complex network visualizations:\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/Screen-Shot-2017-03-13-at-5.25.29-PM-300x163.png)\n\nWalking through a few visual examples of network analysis with digital platforms got students really excited for their own projects and their potential outcomes. I then introduced students to [Palladio](http://hdlab.stanford.edu/palladio/), which is one specific tool engineered by [Stanford Digital Humanities](http://shc.stanford.edu/digital-humanities) for their [\"Mapping the Republic of Letters\"](http://republicofletters.stanford.edu/) that they might consider using for their own projects. One of the most intriguing aspects of this tool is the ways in which you can manipulate your data. More specifically, as we saw with the sample dataset, you are able to visualize your information in the form of a map, a graph, a table, or through a photo gallery of the players involved:\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/Screen-Shot-2017-03-13-at-5.22.28-PM-300x135.png)\n\nThis variety in format was particularly promising for students that hoped to present their projects in diverse ways and draw on both visualization and social network tools.\n\nEven though we experienced some connectivity issues due to a campus-wide network outage, students were able to see the benefits of using digital humanities approaches for their own projects while also getting a feel for a few of these tools with hands-on tutorials. Moreover, instead of panicking about sites and videos that wouldn’t load for the students, I stepped back and saw these connection problems as a teaching moment. In particular, I embraced the slow internet speeds as a catalyst for reflecting on [minimal computing](http://go-dh.github.io/mincomp/) and questions of access in certain parts of the world, such as Latin America. In turn, I encouraged the students to think critically about their projected audiences and how they hoped to not only present their ideas digitally, but also how they hoped to preserve them and make them accessible to a wide range of people.\n\nAs a whole I am eternally grateful to Washington and Lee and Caleb Dance for this opportunity to share some of my favorite digital humanities tools, tips, and tricks with undergraduate students and introduce them to software and platforms that can make many of their imagined projects a reality. With each new tool we discussed, I was overjoyed to see students feverishly writing notes and having “Aha!” moments about their unique projects. Much of my DH fellowship year in the Scholars’ Lab has been about exploration and experimentation that tends to end in failure and a return to the drawing board, but, in the process, I’ve learned an incredible amount and had my own personal “Aha!” moments. Successfully being able to teach these students about data visualization and social network analysis was, quite possibly, the biggest “Aha!” moment of my DH fellowship thus far and a real turning point in my career as a digital humanities teacher-scholar.\n"},{"id":"2017-03-17-fair-use-dh-and-the-kardashians","title":"Fair Use, DH, and the Kardashians","author":"joseph-thompson","date":"2017-03-17 04:21:46 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"fair-use-dh-and-the-kardashians","content":"This week, the Praxis cohort heard from [Brandon Butler](https://twitter.com/bc_butler), UVA’s Director of Information Policy, who gave a fascinating talk on the evolution of copyright law and the meaning of intellectual property. He covered the U.S. Constitution’s intellectual property clause, the rolling boundaries of public domain, and the shift from monetary value to cultural value as a metric in the decision of Fair Use cases.\n\nUnderstanding Fair Use holds important implications for all academics, and the Praxis Program is no exception, especially given the subject of this year’s project: the Kardashians. Considering the Kardashians know a thing or two about how to make money off of selling their likenesses and names, it’s crucial that we understand Fair Use, as the Praxis cohort jumps into studying images and texts created by this family.\n\nAfter listening to Brandon, we learned some key tenets of Fair Use like articulating the scholarly purpose of your work that uses original copyrighted material, proving your work has transformed the original in some capacity, and/or showing that the reproduction of copyrighted material does not inhibit the creator’s ability to profit from that original work. He also hipped us to the [Center for Media and Social Impact](http://cmsimpact.org/), which offers great guidelines of best practices regarding Fair Use and the reproduction of copyrighted materials.\n\nOver the past few months, we have collected Tweets, Twitpics, transcripts of _Keeping Up with the Kardashians_, _US Weekly_ articles, fan fiction stories, and Instagram images to create our archive of the Kardashian media ecology. In an effort to maintain transparency and accessibility, we had kicked around the idea of making this archive accessible through the website we’re building. Now, we’re rethinking that goal, but forging ahead with a project that will rely heavily on copyrighted materials.\n\nBut what’s really at stake in engaging with their copyrighted or trademarked material? Let’s take a quick look at a recent example of Kardashian jurisprudence.\n\nLast month, Kylie Jenner lost a legal battle with pop singer Kylie Minogue to trademark the name “Kylie.” For those who don’t know, the nineteen-year-old Jenner rose to fame as one of the stars of _Keeping Up with the Kardashians_, and is the youngest child of the Kardashian/Jenner matriarch, Kris Jenner. She’s been on a reality television show since she was nine. Who can blame her for thinking that she should trademark her name?\n\nAnd in the mode of the Kardashian family business model, Jenner wanted to turn her name into a licensed brand that she uses to sell her line of [Kylie Cosmetics](https://www.kyliecosmetics.com/). But Minogue already owns [www.kylie.com](http://www.kylie.com), and her millions of fans all over the globe know her simply as Kylie. According to a recent [_Billboard_](http://www.billboard.com/articles/news/7678242/kylie-minogue-vs-kylie-jenner-trademark-battle-has-a-winner) article about the case, Minogue has sold over 65 million albums in a career that dates back to 1988. Clearly, Minogue held something of a precedent regarding the use of the name. In other words, Jenner is influential and carries a certain amount of media power, but she’s no Kylie.\n\nAlthough copyright and trademark laws are certainly different animals, it’s safe to assume that someone who tries to trademark a name might be interested in a project that is digging into and trying to map their family’s media ecology - the very essence of their presence in the world. They are, after all, a family who makes a substantial living monetizing their likenesses and names via copyright and trademark, establishing themselves as the embodiment of celebrity power in the process.\n\nAnd if they are truly the current archetypes of “famous for being famous” based on curated images of themselves, then how do we think critically about the Kardashians without gathering these copyrighted and trademarked materials and, in some cases, reproducing them to further our analysis? Well, we don’t. So we will be reproducing some credited but copyrighted images on our site. Luckily, Fair Use goes before us.\n"},{"id":"2017-03-24-congratulations-to-the-praxis-2016-2017-cohort","title":"Congratulations to the Praxis 2016-2017 Cohort","author":"alicia-caticha","date":"2017-03-24 09:33:14 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"congratulations-to-the-praxis-2016-2017-cohort","content":"![](http://scholarslab.org/wp-content/uploads/2017/03/FullSizeRender-300x168.jpg)\n\nOn Tuesday, March 20th the Praxis 2016-2017 Cohort was awarded first place for their project \"Dash-Amerikan: Keeping up with Kardashian Media Ecologies\" at the 2017 Huskey Research Exhibition, hosted by the University of Virgina Graduate School of Arts and Sciences.\n\nThey will be presenting their findings again in early May.  Until then, here is the abstract of their presentation to wet your appetite!\n\n\n**Dash-Amerikan: Keeping Up with Kardashian Media Ecologies **\n\n\nIn 2015, the season premiere of Keeping Up with the Kardashians was the most viewed Sunday cable program, out performing the series finale of the critically acclaimed television drama, Mad Men. Kim and company have never received the critical adulation that Don Draper elicited. Yet the Kardashians’ sheer popularity demands further inspection by scholars who claim interest in the cultural productions that reflect and shape our current historical moment. The Kardashians remain particularly important for their ability to normalize a matriarchal family structure and transform the traditionally private sphere of the home into their center of business, all while maintaining heteronormative assumptions about the objectification of women. Furthermore, their use of Twitter, Instagram, mobile apps, online blogs, and even tabloid coverage work together to engage fans in an unending advertisement for their show. Whether commenting on news events, promoting awareness of the Armenian genocide, or producing extravagant television specials, the Kardashian family has harnessed the 24/7 media landscape in new and unprecedented ways. The 2016-2017 Praxis Cohort examines the Kardashian media empire to reveal the discursive power of celebrities, fans, and their critics. To do so, we created a data set of every tweet written by a member of the Kardashian family, as well as every relevant US Weekly article, by using web scraping, text analysis, and topic modeling techniques. Using these digital humanities methods, we aim to interrogate constructions of race, class, and sexuality in contemporary popular culture and the pervasiveness of social media on our daily lives.\n\nPlease join me in congratulating Jordan Buysse, Alicia Caticha, Alyssa Collins, Justin Greelee, Sarah Mceleney, and Joseph Thompson.\n\n![](http://scholarslab.org/wp-content/uploads/2017/03/FullSizeRender-1-300x276.jpg)\n"},{"id":"2017-03-29-the-long-and-messy-history-of-privacy","title":"The Long and Messy History of Privacy","author":"shane-lin","date":"2017-03-29 04:14:27 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"the-long-and-messy-history-of-privacy","content":"_[Cross-posted to the [Washington and Lee Digital Humanities Blog](http://digitalhumanities.wlu.edu/blog/2017/03/29/the-long-and-messy-history-of-privacy/). He came to W&L to give a workshop through a Mellon-funded collaboration with the [Scholars’ Lab](http://scholarslab.org/). More information about this initiative can be found [here](https://github.com/wludh/research-one-collab).]_\n\nI was invited by Brandon Walsh, the Mellon Digital Humanities Fellow at Washington and Lee and a former Scholars' Lab Praxis fellow of my cohort, to come to W&L as part of an ongoing collaboration between W&L and the Scholars' Lab. The program pairs UVA graduate students with W&L undergraduate students studying a relevant subject and gives the former the opportunity to expose the latter to new modes of digital scholarship.\n\nI was fortunate to be matched with Brandon's own English composition class: \"Writing in the Age of Digital Surveillance\". My dissertation research is on the history of cryptography and the construction of digital privacy rights from the mid-1970s through the 1990s and involves a text analysis project examining influential privacy-related Usenet newsgroups and mailing list messages, so this seemed like a perfect match.\n\nMy first instinct was to give a quick rehash of my work, an introduction to modern cryptography, and a few hands-on exercises in code-making and code-breaking. I could also reuse a demonstration I used from the last time I taught undergraduate students: how to anonymously buy cocaine on the darkweb with cryptocurrency.\n\nYet none of these approaches seemed quite appropriate. Brandon had done a good job. The syllabus showed that the students had engaged very closely with the ramifications of modern technology on surveillance, drawing on thinkers like Siva Vaidhyanathan and Clay Shirky. My research, though focused narrowly on cryptography, interfaced with so many of the big ideas that the students had already broached. I didn't want to bore them with historical detail (\"scholarly rigor\"!) or to just rehash the broader themes they were already well familiar with. Nor did I think that it was entirely appropriate to give a workshop on cryptography tools or on digital research techniques in a writing class. It didn't seem very useful to throw prime factorization or Python web scraping libraries at these unsuspecting students for a single class.\n\nI was supposed to speak about DH and digital technology, but Brandon assured me that I had wide latitude to choose the topic. So I decided to hardly mention computers at all and talk instead about privacy in the context of the fundamental idea of history: things change. Privacy is not a fixed principle. The abstract notion of privacy rights is a very modern construction and even the practical, everyday conception of physical privacy has radically shifted through history, owing much to the affordances of technologies we may not think of as having much to do at all with privacy.\n\nI started by examining evidence of privacy in ancient Greece and how quantitative research on ruins has shown that prioritization of privacy was built into the architecture of Mediterranean homes. We discussed the rise of public bathing and its shifting practices and cultural significance under the Roman empire. And we spoke of the etymology of the word \"eavesdrop\" and its political connotations during the rein of Henry VIII. This was followed by a tour through the communications infrastructure of the early American republic and the role of the revolutionary and partisan presses and the post office in democratizing privacy by broadening access to both subversive ideas and the means to convey them. Finally, we discussed the dynamic legal conception of privacy, from the Fourth Amendment's originally weak protections against searches to the landmark 1967 _Katz v. United States_ decision that codified an expectation of privacy based solely in the realm of ideas.\n\nDigital technology was the end-point of this crooked journey. Though they have dramatically altered our understandings of privacy and the topography of power that supports such conceptions, the rights that these technologies challenged or championed were forged through centuries of history. Focusing on just the most recent debate wrongly implies that digital technologies are uniquely potent mechanisms and that the shifting landscape of privacy in our tremulous times represents a singular historical moment. I thought it was important to put our modern, contested notion of privacy in broader context, a context that includes the changes wrought by aqueducts, fire pits, chimneys, printing presses, bureaucratic organization, and other earlier technologies of decidedly analog mode.\n\nImage: Fyodor Bronnikov, _The Roman Baths_. A 19th-century conception of 3rd-century privacy.\n"},{"id":"2017-04-04-No_Content_Found","title":"Working at Scholars’ Lab Makerspace, pt. 1","author":"spyridon-simotas","date":"2017-04-04 14:19:25 -0400","categories":["Makerspace"],"url":"No Content Found","content":"Last fall, having no prior experience in 3D printing, I joined the team of Makerspace technologists. Maybe that’s something nobody’s supposed to know, but it’s true. I began assisting at the Makerspace with only [Shane](http://scholarslab.org/people/shane-lin/)’s super-comprehensive presentation on 3D printing, which is the activity that attracts the most students to the SLab. Shane’s knowledge and competency, both historical and technical, had me wondering what I had gotten myself into. But as he kept speaking, using words I had never heard before, a whole new dimension had started to rise. Words like slicer, .stl file and GCode, layer height and infill, PLA and ABS, extruder, nozzle size, and bed leveling, had started to layer up in the inevitable Z axis!\n\nLuckily, the first day on the job was painless. No incidents. No mishaps. No failures. No adhesion or warping problems. No blocked nozzles requiring cold pulling. As a matter of fact, given the uncomplicated nature of my first print, there was no reason for things to go wrong. The command was for a small and uncomplicated cylinder tube. I sliced the .stl file with Cura, then loaded the GCode to the Ultimaker2 and I had basically done my job! It was now up to the machine to translate the GCode into movements in the X, Y, and Z axes by pouring hot plastic in the process. Indeed, after the nozzle was hot enough (over 200ºC) to let the filament come through in liquid form, a motor started to push it into the extruder as it started piling up circle after circle every .2mm. The process may sound tedious, but it is also mesmerizing. There is something exciting and soothing at the same time, to let yourself get lost in the paths of the printer’s head. And its music!\n\nSo, that first print didn’t last long, as most prints usually do. The actual size of the cylinder tube was a few centimeters long. In less than 30 minutes, the tube was erected on the heated bed, ready to be peeled off. In less than 30 minutes, a small physical object had come into being and was now departing in the hands of my first happy patron.\n\nSince that first print, I can’t say the same seamless process reflects Makerspace’s reality. Most commonly, success comes as the result of many aborted attempts, tweaking, and recommendations from more experienced makers. Due to all kinds of unexpected technical failures, with adhesion probably being the #1, I have learned to tame my enthusiasm until the final layer is applied, and to use glue! More importantly, I have learned to observe more experienced makers and absorb their know-how and admirable problem-solving skills. Certainly, 3D printing is not rocket science (although it helps you build rockets, right [Duy](http://scholarslab.org/people/duy-nguyen/)?), but it can be quite frustrating if a significant number of parameters don’t line up. There is an artisanal quality to 3D printing that I didn't expect existed. The attitude is to preserve the right amount of detachment from the final results and keep experimenting. And in your darkest hour of troubleshooting, it is useful to put things in perspective through poetry: \"[Hope the voyage is a long one](http://www.cavafy.com/poems/content.asp?cat=1&id=74).\" For that reason, Makerspace offers exposure, good mentorship, an amazingly friendly and playful learning environment, as long as unlimited stocks of filament!\n\n\n"},{"id":"2017-04-12-are-you-our-new-senior-developer","title":"Are You Our New Senior Developer?","author":"alison-booth","date":"2017-04-12 10:55:03 -0400","categories":["Announcements","Job Announcements"],"url":"are-you-our-new-senior-developer","content":"We're very excited that Managing Director Amanda Visconti and Head of Graduate Programs Brandon Walsh will be on board by April 24. And we are rebuilding our research team at a time when DH@UVA and a new Certificate in DH will be coming into clearer focus! Please spread the word of [this newest job posting for a Senior Developer at the Scholars' Lab](http://jobs.virginia.edu/applicants/Central?quickFind=81600). (_In case that link doesn't work, you can visit [hr.virginia.edu](http://www.hr.virginia.edu/), click \"Jobs\", and search for staff posting 0620730_). A description of the role follows:\n\nThe University of Virginia Library is the hub of a lively and growing community of practice in technology and the humanities, arts, and social sciences. As part of that community, the Scholars' Lab has risen to international pre-eminence as a library-based center for digital humanities.\n\nThe Scholars' Lab collaborates with faculty, librarians, and students on a range of projects and tools, including spatial humanities, data visualization, text analysis, digital archiving, 3D modeling, and experimental humanities. Our [Praxis Program](http://praxis.scholarslab.org/) and Digital Humanities Fellowships are both models for graduate education in Digital Humanities. Further, the Library and the Scholars' Lab are committed to diversity and safe spaces, and we particularly focus our speaker series and practice on accessibility and social justice in all senses.\n\nWe welcome curious, critical, and compassionate professionals with integrity and a strong work ethic, and who possess a keen and deep understanding of what it takes to continuously improve and maintain research projects within a major academic research library. We particularly welcome applications from women, people of color, LGBTQ, and others who are traditionally underrepresented among software developers.\n\n**Responsibilities:** The Senior Developer will consult with faculty and students to advance research projects and training; evaluate scholarly needs and define project goals for research projects; provide input on appropriate deliverables and reasonable schedules for completion; write, test, and debug original software code for applications that enable scholars and library users to collect, manage, produce, manipulate, or analyze digital information resources. The Senior Developer will modify existing applications to improve their functioning or achieve broader and more effective use and engage with new technologies to help researchers find their use and interest for research.\n\n**Qualifications:**\n\n_Required:_\n\n\n\n \t\n  * Graduate degree or equivalent experience\n\n \t\n  * Up to 4 years of experience with software development, application development, or systems administration\n\n \t\n  * Experience with relational database systems, including Postgresql and MySQL\n\n \t\n  * Experience with a number of programming languages, including PHP, Ruby, Python, Java, SQL, JavaScript, shell\n\n \t\n  * Experience with version control, including Git and Subversion Systems: *nix\n\n \t\n  * Ability to scope and implement software in diverse environments\n\n \t\n  * Ability to communicate effectively with researchers and fellow developers\n\n \t\n  * Ability to encourage and develop a community of users and developers.\n\n\n_Preferred: _\n\n\n\n \t\n  * Graduate degree or equivalent experience in the humanities or social sciences\n\n \t\n  * Experience with data collections, analysis, visualization, and interpretation\n\n \t\n  * Experience with a variety of text analysis or image analysis methods and tools\n\n \t\n  * Familiarity with a variety of application frameworks, including Rails, Django, and Zend\n\n \t\n  * Experience with TEI, XML, Solr, Cocoon, Tomcat\n\n\nApplications will be accepted until the position is filled. A CV/resume, cover letter, and contact information for three references are required pieces of your application.\n\n_Email a Friend:_ [jobs.virginia.edu/applicants/Central?quickFind=81600](http://jobs.virginia.edu/applicants/Central?quickFind=81600)\n"},{"id":"2017-04-12-raspberry-pi-on-uva-wifi-network","title":"Raspberry Pi on UVa WiFi Network","author":"ammon-shepherd","date":"2017-04-12 08:59:24 -0400","categories":["Makerspace","Technical Training"],"url":"raspberry-pi-on-uva-wifi-network","content":"[caption id=\"attachment_13392\" align=\"aligncenter\" width=\"254\"]![](http://scholarslab.org/wp-content/uploads/2017/04/raspberry-pi-logo-254x300.png) https://www.raspberrypi.org/trademark-rules/[/caption]\n\nThe easiest way to get your Raspberry Pi connected to the Internet at the University of Virginia is to use an Ethernet cable. If you want to use wireless, pretty much the only option is to use the hidden \"wahoo\" network.\n\nThis is a quick tutorial for getting your Raspberry Pi to connect to the \"wahoo\" WiFi network at the University of Virginia.\n\nI'm starting with a clean install of [Raspbian OS](https://www.raspberrypi.org/downloads/raspbian/) (follow the link for a great tutorial on installing Raspbian OS).\n\n\n\n## Change Password\n\n\n\n![](http://scholarslab.org/wp-content/uploads/2017/04/2017-04-11-134736_1824x984_scrot-1024x552.png)\n\nType \"passwd\", enter the current password \"raspberry\" (you won't see any text typed), then hit enter.\n\nNow type in a new password (again, you will see no text), hit enter, and type it a second time.\n\n\n\n\n\n## Change Localization Settings\n\n\nThere are two ways to change these settings: 1. using the graphical interface, or 2. the command line. Pick one option and go with it.\n\n\n\n#### 1. Graphical\n\n\n![](http://scholarslab.org/wp-content/uploads/2017/04/2017-04-11-135248_1824x984_scrot-1024x552.png)\n\nGo to the Raspberry Pi menu (the raspberry logo in the top left corner), then to Preferences, then to Raspberry Pi Configuration.\n\nUnder the Localization tab, you'll want to change the Locale, Timezone, Keyboard and WiFi Country settings. (As a side note, you can also change the password under the \"System\" tab.)\n\n![](http://scholarslab.org/wp-content/uploads/2017/04/2017-04-11-142809_1824x984_scrot-1024x552.png)\n\nFor our purposes, you need to set the WiFi to \"US\".\n\nDon't restart the system, yet. We have more to do.\n\n\n#### 2. Command Line\n\n\n![](http://scholarslab.org/wp-content/uploads/2017/04/2017-04-11-143004_1824x984_scrot-1024x552.png)\n\nIn the terminal, type \"sudo raspi-config\". The screen will show a textual display of options. Select Localization Options, and from there change the Locale, Timezone and WiFi Country settings. (Note, you can also change the password using this tool.)\n\nDon't restart the system yet. We have more to do.\n\n\n\n\n## Edit System Config\n\n\nChange the\n\n[code]/etc/network/interfaces[/code]\n\nfile. You'll need to open the file with root privileges, so we'll use the terminal to open a program for editing the file.\n\nIn the terminal, type in:\n\n[code]sudo leafpad /etc/network/interfaces[/code]\n\n![](http://scholarslab.org/wp-content/uploads/2017/04/2017-05-09-101958_1824x984_scrot-1024x552.png)\n\nTowards the bottom of the file are two sections for a \"wlan0\" and \"wlan1\". Delete those lines, and in place of them add in three new lines for \"wlan0\" (indent the third line with four spaces or a tab).\n\n[code]\nauto wlan0\niface wlan0 inet dhcp\n    wireless-essid wahoo\n[/code]\n\nWhat this code does:\n\nauto wlan0 = this will automatically turn on the wireless card\n\niface wlan0 inet dhcp = this declares an interface card called wlan0 allowed to connect to the TCP/IP network (the Internet) using DHCP (basically, the router will give your Pi an IP address).\n\nwireless-essid wahoo = look for the wireless Extended Service Set Identification named wahoo.\n\nThe entire file should look like this:\n\n[code]\n# interfaces(5) file used by ifup(8) and ifdown(8)\n\n# Please note that this file is written to be used with dhcpcd\n# For static IP, consult /etc/dhcpcd.conf and 'man dhcpcd.conf'\n\n# Include files from /etc/network/interfaces.d:\nsource-directory /etc/network/interfaces.d\n\nauto lo\niface lo inet loopback\n\niface eth0 inet manual\n\nauto wlan0\niface wlan0 inet dhcp\n  wireless-essid wahoo\n[/code]\n\n\n\n## \n\n\n\n\n## Get WiFi MAC Address\n\n\nWhile you have the Raspberry Pi on and the terminal open. Grab the MAC address for the wireless card.\n\nIn the terminal type\n\n[code]ifconfig[/code]\n\n![](http://scholarslab.org/wp-content/uploads/2017/04/2017-05-09-103046_1824x984_scrot-1024x552.png)\n\nNow look for the section for the \"wlan0\" and on the first line you'll see information for a \"HWaddr\" or Hardware Address. It will be in the format\n\n[code]aa:11:bb:22:cc:33[/code]\n\nThe HWaddr is the MAC address. Write the MAC address down on a piece of paper, you'll need for the next step.\n\n\n\n\n## Register the MAC Address\n\n\nYou will need a computer or mobile device that already has an Internet connection in order to complete this step.\n\nIn order to get your Raspberry Pi connected to the 'wahoo' network, you will first need to register the MAC address with ITS.\n\nFollow this link to the ITS Registration page: [https://netreg.itc.virginia.edu/cgi-bin/mac_registration.cgi?alien=1](https://netreg.itc.virginia.edu/cgi-bin/mac_registration.cgi?alien=1)\n\nITS suggests getting a static IP on the more secure network. You can make that request through [ITS help desk](http://its.virginia.edu/helpdesk/).\n\nIf on the more secure network, your computer must be on that network as well if you want to SSH into the Raspberry Pi. You can connect to that network through the ITS provided [CISCO VPN client](http://its.virginia.edu/vpn/).\n\n\n\n\n## Restart the Pi\n\n\nNow you can restart the Raspberry Pi. When it boots up, it will automatically detect and connect to the \"wahoo\" network.\n"},{"id":"2017-04-17-endangered-data-week","title":"Endangered Data Week","author":"alison-booth","date":"2017-04-17 06:22:37 -0400","categories":null,"url":"endangered-data-week","content":"A range of Scholars' Lab and Praxis folks are collaborating with Research Data Services and others in the Library for a series of workshops under the umbrella, [Endangered Data,](http://data.library.virginia.edu/endangered-data-week/) this week.\n\nOutline of events:\n\n\n\n \t\n  * Introduction to Libra Data (Dataverse at UVa) – Monday, April 17th 11am-noon, in [Brown Library, Room 133](http://www.library.virginia.edu/map/#%21/map/science/1).\n\n \t\n  * Introduction to Git/Github – Tuesday, April 18th, noon-1:30pm, in [Brown Library, Room 133](http://www.library.virginia.edu/map/#%21/map/science/1).\n\n \t\n  * Introduction to DocNow – Tuesday, April 18th, 2pm-4pm, in [Alderman Library, Room 421](http://www.library.virginia.edu/map/#%21/directions/alderman/4).\n\n \t\n  * Web Scraping with R – Wednesday, April 19th, 10:30am-noon, in [Brown Library, Room 133](http://www.library.virginia.edu/map/#%21/map/science/1).\n\n \t\n  * Preserving Artifact and Architecture with Cultural Heritage Informatics – Friday, April 21st, 10:30am-11:30am, in [Clemons 3rd Floor VizLounge](http://www.library.virginia.edu/map/#%21/study/clemons/3/third-floor).\n\n \t\n  * Endangered Data Week webinar – Friday, April 21st, 1pm-2:30pm, in [Brown Library, Room 133](http://www.library.virginia.edu/map/#%21/map/science/1).\n\n\nWithin the UVa Library, these events are being hosted by [Research Data Services](http://data.library.virginia.edu) and the [Scholars’ Lab](http://scholarslab.org).\n\nKudos to Jeremy, Will, and Arin, and our colleagues in Brown Library, as well as the DLF Webinar.\n\nMeanwhile Ronda is at Texas A&M, sharing her expertise in [Neatline](http://docs.neatline.org/) Training!\n"},{"id":"2017-05-03-announcing-2017-2018-fellows","title":"Announcing 2017-2018 Fellows!","author":"brandon-walsh","date":"2017-05-03 06:11:38 -0400","categories":["Announcements","Digital Humanities","Grad Student Research"],"url":"announcing-2017-2018-fellows","content":"We are thrilled to announce the 2017-2018 Scholar’s Lab fellows for the [Praxis Program](http://praxis.scholarslab.org/), the new Digital Humanities Project Incubator Fellowship, and the [Graduate Fellowship in the Digital Humanities](http://scholarslab.org/graduate-fellowships/). We are welcoming 14 fellows from 6 disciplines from the [arts, humanities, and social sciences](http://gsas.virginia.edu/). Our graduate fellows are joining a robust and vibrant community of [past fellows](http://scholarslab.org/people/)!\n\n\n## Praxis Program\n\n\nWe are delighted to welcome 6 diverse disciplinary team members to the 7th year of the Praxis Program:\n\n\n\n \t\n  * Monica Blair (History)\n\n \t\n  * Ankita Chakrabarti (English)\n\n \t\n  * Victoria Clark (Music, Critical and Comparative Studies)\n\n \t\n  * Tanner Greene (Music, Critical and Comparative Studies)\n\n \t\n  * Christian Howard (English)\n\n \t\n  * Spyros Simotas (French)\n\n\nLook forward to more details about the Praxis Program’s new project in the fall!\n\n\n## Digital Humanities Project Incubator Fellowship\n\n\nThis year we will be piloting a set of smaller, short-term fellowships dedicated to boosting nascent digital humanities research in a flexible, collaborative environment.\n\n\n\n \t\n  * James Ascher and Sarah Berkowitz (English, Summer 2017)\n\n \t\n  * Benjamin Gorham (Art and Architectural History, Fall 2017)\n\n \t\n  * Ryan Maguire (Music, Composition and Computer Technologies; Fall 2017)\n\n \t\n  * Joseph Thompson (History, Fall 2017)\n\n\nThese students will work with our R&D team for a single semester to rapidly prototype their project ideas and position themselves for future work in digital humanities. Look for more from them this summer and fall!\n\n\n## Graduate Fellows in the Digital Humanities\n\n\nFinally, we are looking forward to working with Julia Haines and Ethan Reed, our 2017-2018 Graduate Fellows in the Digital Humanities.\n\n\n\n \t\n  * Julia Haines’ (Anthropology) dissertation is titled, _Archaeology at 19th-century Bras d’Eau, Mauritius: Intimate Spaces and Industrial Landscapes of Indentured Laborers_.\n\n \t\n  * Ethan Reed’s (English) dissertation is titled, _Forms of Frustration: Unrest and Unfulfillment in American Literature after 1934._\n\n\nThese fellows will work with our team throughout the year and over the summer on substantial research projects related to their dissertations.\n\nWe are looking forward to working with all these fantastic students in the coming year!\n"},{"id":"2017-05-04-1st-annual-makerspace-hackontest","title":"1st Annual Makerspace Hackontest","author":"ammon-shepherd","date":"2017-05-04 06:25:22 -0400","categories":["Makerspace"],"url":"1st-annual-makerspace-hackontest","content":"# Egg Drop Hackontest\n\n\nThe Scholars' Lab staff and Makerspace Technologists completed the first ever annual/biannual/semesterly SLab Makerspace Hackontest. The brain-child of Shane Lin, this contest, in short, was to get SLab staff and Makerspace techs together to create something that would keep an egg from cracking; a spin on the traditional egg drop competitions you may have had in middle or high school.\n\n\n# What's a Hackontest?\n\n\nA hackontest is a contest where you hack together something. (clever, right?)\n\n\n# The Rules\n\n\n\n\nWe (I?) cordially invite you to participate in the 1st biannual (or possibly biennial) Makerspace Employee Hackathon (MEH)! **This event is open to all Scholars' Lab makerspace technologists and full-time employees.**\n\n\n\n\nAs an arguably fun activity, each of you is challenged to complete a design and production challenge using tools and materials found in the makerspace.\n\n\n\n\nFor this inaugural event, the challenge is a spin on that staple of American primary education science classes: the egg drop. Design a mechanism to allow an American-standard Large size unboiled egg to survive two sequential drops from 2 meters onto a hard floor without cracking the egg. Our twist on this project is that in between the drops, the entire design must bear the compressive force of 1 metric Shane mass at Earth gravity applied sequentially on three perpendicular axes at the longest point (i.e. I must be able to stand on it).\n\n\n\n\nIf multiple designs allow an egg to survive both drops, the team with the design with the lowest pre-drop weight (in case fuel or reaction mass is expended in the course of the drop) will win the Grand Technical Prize and be temporarily presented with the prestigious SLabby Trophy.\n\n\n\n\nAfter we perform the drops, participants will also vote on a Special Ingenuity Award for designs that incorporate exceptional trickery or superior aesthetics.\n\n\n\n\nTeams will be announced by **Monday 3/6** (so please fill out the form by the Sunday before) and have until **Monday 3/27** to submit their design and model (we'll figure out the best time to perform the drop after we assign teams).\n\n\n\n\nAdditional rules:\n\n\n\n\nA. Each participant will be assigned by the organizers to a team to be decided in accordance with our secret agenda (chiefly, fraternization).\n\n\n\n\nB. Use of personal computers are allowable, as is any freely-available and Lab-provided software, including 3D designs, but any part that touches the egg must be an original design.\n\n\n\n\nC. You may incorporate any materials found in and of the Makerspace in your designs, with the following exceptions:\n\n\n\n\n1. No paper and paper-like materials, including cardboard and card stock.\n\n\n\n\n2. No liquid adhesives or hot glue (glue sticks are okay)\n\n\n\n\n3. No adhesive tape\n\n\n\n\n4. No bubble wrap\n\n\n\n\nIn the case of any questions or questionable edge-cases, please send me (Shane: [ssl2ab@virginia.edu](mailto:ssl2ab@virginia.edu)) an email and I will arbitrate. As a rule of thumb, I really like unorthodox ideas.\n\n\n\n\nGood luck!\n\n\n\n\n\n# The Judge\n\n\nShane was the brains and the judge behind the contest.\n\n![](http://scholarslab.org/wp-content/uploads/2017/05/IMG_20170503_154042.jpg)\n\n\n# The Competition\n\n\nThree entries made it to the competition:\n\n\n## Team Bomb!\n\n\n![](http://scholarslab.org/wp-content/uploads/2017/05/IMG_20170503_154036.jpg)\n\n[video width=\"720\" height=\"1280\" mp4=\"http://scholarslab.org/wp-content/uploads/2017/05/VID_20170503_155400.mp4\"][/video]\n\n\n## ![](http://scholarslab.org/wp-content/uploads/2017/05/IMG_20170503_155520.jpg)\n\n\n\n\n## Team Urchin\n\n\n[caption id=\"\" align=\"aligncenter\" width=\"3480\"]![](http://scholarslab.org/wp-content/uploads/2017/05/IMG_20170503_154617.jpg) Shane was just helping out. Team Urchin was Spyros and Duy[/caption]\n\n[video width=\"720\" height=\"1280\" mp4=\"http://scholarslab.org/wp-content/uploads/2017/05/VID_20170503_155152.mp4\"][/video]\n\n![](http://scholarslab.org/wp-content/uploads/2017/05/IMG_20170503_155326.jpg)\n\n\n## Team DragonSpawn\n\n\n[caption id=\"\" align=\"aligncenter\" width=\"3456\"]![](http://scholarslab.org/wp-content/uploads/2017/05/IMG_20170503_154024.jpg) Team DragonSpawn: Lauren and Ammon[/caption]\n\n[video width=\"720\" height=\"1280\" mp4=\"http://scholarslab.org/wp-content/uploads/2017/05/VID_20170503_154937.mp4\"][/video]\n\n\n# ![](http://scholarslab.org/wp-content/uploads/2017/05/IMG_20170503_155616.jpg)\n\n\n\n\n# The Winner is...\n\n\nOnly one competitor survived all the tests: drop from 2 meters, the full weight of one metric Shane unit on **three** axis, and a second drop from 2 meters...\n\n**Team DragonSpawn!**\n![](http://scholarslab.org/wp-content/uploads/2017/05/20170503_160322-1-e1493907510317-905x1024.jpg)\n\nCongratulations, you get the first SLabby award!\n\n\n\n"},{"id":"2017-05-22-introductions-meet-charm-and-wit-or-wit-and-charm","title":"Introductions: Meet Charm and Wit, or Wit and Charm","author":"sarah-berkowitz","date":"2017-05-22 07:17:50 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"introductions-meet-charm-and-wit-or-wit-and-charm","content":"Sarah E. Berkowitz\n\n![](http://scholarslab.org/wp-content/uploads/2017/05/Sarah-and-Layla1-300x300.jpg)\n\nI will be working with James Ascher this summer on a Scholar’s Lab Digital Humanities Project Incubator Fellowship. Our project consists of creating a digital edition of “Characters” from Samuel Butler’s posthumous _Genuine Remains _(1759). James and I are both rising fifth year Ph.D. candidates in the English Department, and we both study the eighteenth century, but there the similarities end. I study primarily novels, and I’m primarily interested in post-structuralist literary criticism. My dissertation focuses on male characters in the late eighteenth-century novel, and I’m wildly interested in Butler’s text because of what I think it can tell us about characters outside of novels in the eighteenth century. Oh, and did I mention? I have absolutely no digital humanities experience. Suffice it to say James’s interests and experiences are very different from mine, but I’ll let him introduce himself.\n\nBecause we have an Incubator fellowship, our goal is to create a fast and furious prototype by the end of the summer. My job is primarily to provide a critical framework for thinking about character. In the dream version of this project, I would be writing the footnotes that explain and contextualize Butler’s individual characters. But, I still need to learn some basic computer skills and, and because of the truncated time frame I have to learn them fast! I’ve start using a free [website](https://learnpythonthehardway.org/book/appendixa.html)  to teach myself basic command line, and have started to go through the built-in [Emacs tutorial](https://www.gnu.org/software/emacs/). Next on the list is learning [Git](https://try.github.io/levels/1/challenges/1). The hope is that I can master enough basic skills by Memorial Day that I can meaningfully contribute to our conversations about design. This seems like a tall order right now, but I’m working on it a little every day. Then the real fun will begin.\n\n\n\nJames P. Ascher\n\n![](http://scholarslab.org/wp-content/uploads/2017/05/Ascher-300x200.jpg)\n\nSarah Berkowitz has kindly provided a gracious introduction to our project and our collaboration, but I’d be remiss if I didn’t write of my reciprocal enthusiasm.  Characteristic of our collaboration, we debated if we were in fact different in our interest in literary criticism—we feel different, but are we?  As we talk, the names we engage with are different: David Brewer, Deidre Lynch, and Catherine Gallagher provide mortar the bricks of Sarah’s thinking and G. T. Tanselle, David Foxon, and James Raven are smeared like honey on goblet of medicine that I try to present the world.  Of course, I think that I’m critiquing literature—as the eighteenth century would have defined it: written language presented to the public in printed form.  Of course, Sarah is right that the burden of understanding how those marks on paper came to be is so large that I seldom get a chance to talk about phenomena like character;--it’s too far to induce from printing history to ideology.  Hence, my excitement in this project.  I feel as though we’re reaching across a gap, trying to find a bridge between the physical facts of items and the theoretical preoccupations of post-structuralist literary criticism.  I think of William Whewell’s inductive tables as a sort of emblem for us !\n\n![](http://scholarslab.org/wp-content/uploads/2017/05/Inductive-Table-300x181.png)\n\nWhewell proposes the table as a way of visualizing the method behind the slow progress of induction building on the ideas of the past—two separate explanations become one and then that one is combined with another explanation to make an even grander theory.  Repeat forever, as first causes—the ultimate end of such a chain of induction, are unreachable.  In this way, he imagines enacting Francis Bacon’s great instauration: renewing the accumulation of private natural-history knowledge as theories that explain the world.  You collect data, find patterns, write a theory, and make a theory out of those theories.  I think this accurately describes what both of us are doing, but on different parts of the chart that would represent literature.  Sarah builds on the long history of the study of character; I build on the history of printed forms of meaning.  What this project can helpfully show us is the bracket that would correspond to the induced theory that contains both of these chains of induction.  Surely, we wouldn’t bother reading printed works if it weren’t for characters?  Surely, we couldn’t read about as many characters if it weren’t for printed works?  Okay, that means they relate, but how?\n"},{"id":"2017-05-24-accessibility-online-take-aways-from-the-luis-perez-workshop","title":"Accessibility Online--Take aways from the Luis Perez workshop","author":"sarah-berkowitz","date":"2017-05-24 05:14:49 -0400","categories":["Digital Humanities","Grad Student Research"],"url":"accessibility-online-take-aways-from-the-luis-perez-workshop","content":"One of our aims for our summer project is to build a product that is accessible for all users, but I realized today that James and I have a different idea of what it means for digital humanities to be “accessible.” When I think of making a digital product accessible, I imagine a home bound elderly user who still has an AOL email account. For these users, something that is straightforward and unintimidating is crucial. I think about stressing plain language and a simple interface. When James thinks about accessibility, he imagines a self-taught Linux user in her basement who has no access to software through institutions. For James, open source products and a process that leaves a digital blueprint so that it can be easily recreated by users outside the academy is essential.\n\nI’ve been thinking about accessibility because I recently attended a workshop sponsored by [The Center for Teaching Excellence](http://cte.virginia.edu/) and the Office of Accessibility that brought [Dr. Luis Perez ](https://luisperezonline.com/)on grounds to discuss technology and academic accessibility. The workshop stressed the benefits of technology in the classroom, and particularly emphasized the concept of “Universal Design,” the idea that technology should be designed with _all_ users in mind, not just able bodied ones. Dr. Perez’s point was that technology that was helpful for some was necessary for others. For instance, the speech to text feature on a cellphone may be useful for a sighted user, but necessary for a visually impaired one.\n\nThe academic accessibility workshop got me thinking about how we might apply Universal Design to our summer project. Print can be especially disabling for people with visual impairments and processing disorders. How, then, can we make a mostly text and image based project accessible for all users?\n\nThe workshop introduced the acronym “SLIDE” to think about designing accessible projects. You can find a more full explanation at Dr. Perez’s website [here](https://luisperezonline.com/2016/07/11/slide-into-accessibility-5-tips-for-making-learning-materials-work-for-everyone/), but I think they bear repeating.\n\n**SLIDES** stand for Styles, Links, Images, Design and Empathy.\n\n\n\n \t\n  * **Styles** refers to the structure of a website. This model stresses consistent uses of headings and subheadings to make websites easier to navigate with text to voice software. Without consistent headings and subheadings, a reading app would have to read an entire page before getting the user to the relevant parts.\n\n \t\n  * **Links** means that links should never be “naked,” that is, should never be presented without a descriptive shortcut. This prevents a voice to text program from reading out the entire (sometimes long and unwieldy) url, and provides succinct information that can be delivered verbally.\n\n \t\n  * **Images** asks that all images have an alternate text description, so visually impaired users can have their computers describe the images rather than missing out on them altogether.\n\n \t\n  * **Design** implies that design of a given webpage should be consistent. “If your web design is consistent,” Dr. Perez said, “it’s like I can see it a little better every time I use it.” If it’s idiosyncratic, it can be hard to get a feel for the page.\n\n \t\n  * And lastly, **Empathy** refers to the idea that websites are designed with all users in mind. Everyone will be disabled eventually, empathy asks that you design a project for the user you will one day become rather than the user you necessarily are.\n\n\nI want to keep these principles in mind as we move forward building our edition. We’re planning to use a lot of visual information, and I would like to make sure our site is accessible to all users. I don’t know if this will possible given the time frame allotted, but I think these are good guidelines to keep in mind!\n"},{"id":"2017-05-25-visualizing-paper-evidence-using-digital-reproductions","title":"Visualizing Paper Evidence Using Digital Reproductions","author":"james-ascher","date":"2017-05-25 04:41:48 -0400","categories":["Digital Humanities","Experimental Humanities","Grad Student Research","Research and Development","Visualization and Data Mining"],"url":"visualizing-paper-evidence-using-digital-reproductions","content":"Digital images both lie to us and tell us truths that exist outside of our normal perception. The lie comes about through both deliberate distortions and distortions produced by limitations in digital and in other reproduction methods. The limitations of reproductions are easy to see for anyone who considers the situation carefully, but understanding the problems that they create in our understanding of the past requires some study. The most coherent summary of the problems of the late 1980s and the problems with reproductions in general was written by G. T. Tanselle in 1989. In this essay, [\"Reproductions and Scholarship\"](http://xtf.lib.virginia.edu/xtf/view?docId=StudiesInBiblio/uvaBook/tei/sibv042.xml;chunk.id=vol042.02;toc.depth=1;toc.id=vol042.02;brand=default), Tanselle reminds us that \"everyone knows (though many people act as if they do not know) that every form of reproduction can lie, by providing a range of possibilities for interpretation that is different from the one offered by the original.\"1 These reproductions can be useful supplements to the originals, but serious research ought to be checked against the originals to determine where the reproduction may mislead. He summarizes the obligation of those who study the verbal texts of the past,\n\n\n<blockquote>The words that come to us from the past, transmitted by paper and ink, cannot be assumed to reflect accurately what their authors intended; in order to assess how the words that are present in documents came to be there, and indeed to try to make sure that we know what words are in fact there, we must avail ourselves of all the evidence that comes with them.2</blockquote>\n\n\nWhile photocopies, photographs, and microphotographs can place texts before us that we cannot always get to ourselves, they are all several steps removed from the original and each step provides opportunities to alter---intentionally and not---the final product of the reproduction process. Taking up this line of thinking, Bonnie Mak recently wrote about the problematic assumptions in doing literary research from the digitized versions of high-contrast microfilms, created under war-time expediencies.3 Her [\"Archaeology of digitization\"](http://onlinelibrary.wiley.com/doi/10.1002/asi.23061/abstract;jsessionid=B6DCFD7DB8E924CE0DEBF2679116FDC9.f03t04) describes the context in which the familiar database of _Early English Books Online_ was produced and how it has come to stand in for access to the original. Echoing Tanselle's objection, she notes that the expediencies of war-time microfilming and batch digitization lead to unpredictable, but often interpretable, errors that themselves are worthy of study. Rather than bemoaning the poor state of---what is for many students and scholars distant from major libraries---the \"archive of record\" for English books, she claims that we can study the errors in these reproductions to understand a historical moment that sees verbal texts in a certain way. Digitization, microfilming, and OCR fail us, but we can use \"paratextual\" clues to determine how a particular digital reproduction came to be and so compensate for actual and potential errors, albeit incompletely.\n\nBoth of these arguments anticipate specialized tools that reproduce information that would not normally be part of the experience of looking at a book, which is right, but something that I think deserves further investigation. While a high-contrast microphotograph may obscure faint ink, blue pencil, or small dots, it also shows us something that we would not normally be attentive to. Consider this photograph of page 382 from a copy of Samuel Butler's _Genuine Remains_ (ViU PR3338 .A17 1759 c.2 v.2),\n\n\n    ![](http://scholarslab.org/wp-content/uploads/2017/05/page-208x300.jpg)\n\n\n\n\n\n\n\nP. 382\n\n\nOur eyes immediately read the heading \"A Knave\" and, if you are literately minded, you---perhaps---start reading the text, wondering what a knave might be. But, let me point out the black background, the brown leather of the cover on the left, the transparent fingers holding the page down, and the show-through of the ink from the other side of the leaf in the image. If we thought that those things did not matter, we might have a good reason to alter the digital image by tweaking the colors so that the colors that do not interest us are not visible. But first, we would need to identify which colors interest us. One way of visualizing the range of colors in an image is whats called a [color histogram](https://en.wikipedia.org/wiki/Color_histogram),\n\n\n\n\n![](http://scholarslab.org/wp-content/uploads/2017/05/histogram.png)\n\n\nHistogram for p. 382\n\n\n\n\n\n\nThe peaks to the left are the dark colors of the background. The peaks to the right are the brown colors of the paper. In between those peaks, the smaller peaks represent ink, discolored paper, the restraining fingers, and other features. If we alter the image by emphasizing those colors that interest us, we destroy the paratextual evidence valued by Mak and obscure the evidence noted by Tanselle, but emphasize what interests us. If we take on the obligation of returning to the original to check our assumptions, this alteration can focus to our investigation on the facts that interest us. For example,\n\n\n\n\n![](http://scholarslab.org/wp-content/uploads/2017/05/ink-page-208x300.jpg)\n\n\nInk emphasized\n\n\n\n\n\n\nstretching the colors in the middle of the histogram to cover the whole range of reproducible colors eliminates information in the outer ends of the histogram, which turns out to be mostly show-through and portions of the digitization apparatus, other pages, and the binding. Quantitatively, the first image has 10,778 unique colors and the stretched image that emphasizes ink has 26,344. This means, as both Mak and Tanselle have rightly pointed out, that we have fabricated evidence by altering the image. The second image has over twice as many colors which do not occur in the original and it would be wrong to suggest that we have somehow discovered new information with the second image. In fact, we have destroyed evidence to render a digital image more useful for a particular line of investigation: examining only the fully inked bits of the page. A careful observer may also note that the image looks a bit Fauvist with artificial colors. Were I to believe that I was _improving_ the image by altering its color range, I would have to \"correct\" that error by tweaking the colors further, but as this image is an _artificial visualization_ the unnatural coloring helps to remind me and anyone else who sees this image that it is a deliberate alteration of what one would see if they looked at the book. The stumbling block in understanding the limitations of visualizations like this one is that they foreground what most people look at when they look at a book: the letters that have been deliberately and sufficiently inked and impressed on the page. If we consider a physical codex as a noisy container for works of art made of text, one can imagine---à la [Claude Shannon](https://en.wikipedia.org/wiki/Information_theory)---that this alteration restores the original text by eliminating noise. The problem with this approach is that it eliminates the opportunity to apply our judgment to the full range of evidence available, even in a limited reproduction. A book, and even the digital image of a page of a book, contains far more information than just the sequence of letter-forms that appear sufficiently on its surface. Consider the digitization process itself and what it can capture.\n\n\n## 8-Bit Per Channel Versus 16-Bit Per Channel\n\n\nHuman vision can normally distinguish a bit more than sixteen-million colors, so modern digital images provide a color range that encompasses that in three channels---typically, red, green, and blue. Each pixel---that is, evenly-colored dot---in the image has three numbers associated with it that correspond with the amount of red light, green light, and blue light that must be mixed together to produce the right color on an illuminated screen. (There are many variations of this in different [color spaces](https://en.wikipedia.org/wiki/Color_space), CMYK is more useful for printing, Munsell is better for colorimetry; but this particular explanation is sufficient for this post.) A quick bit of math shows that eight bits gives 256 intensities to each channel (red, blue, or green) and that these three channels together give 256 times 256 times 256 colors, that is 16,777,216 different colors. Thus, it is simply a matter of calibration and choosing the right range of intensities for an eight-bit per channel image to reproduce the entire range of colors that can be seen. However, image detectors can distinguish more than this and digital photographers can use this extended range to mimic the large range of colors that can be perceived in the world. Let me explain how we can perceive more colors than we can see.\n\nWhen we look at a dark alley in the sunlight, our eyes adjust the intensity of the light as our gaze passes over the scene. When we look at the clouds around the sun, our irises constrict, reducing the amount of light that enters our eye so that we can see clouds as white on blue backgrounds rather than as just bright white light. When we stare into the alleyway, our irises expand to take in more light, so that we can see the red bricks on the side rather than just a dark alley. Normally, our eyes move over a scene quickly and take in different intensities of light and our brain combines them together into one image in our mind. Based on contextual clues, we can perceive a wider range of colors than our limited sixteen-million-color vision can see because of how our eyes adjust and our intuition about how light functions in the world. (Josef Albers explains how this visual intuition is not truly a deeper knowledge about color, but an intuition that can be manipulated into seeing colors that are not really there in his pioneering _Interaction of Color_4) Digital detectors, as of yet, do not operate quite in this way, but can mimic the process. One way is to take several digital photographs at different brightness levels and stitch them together. In our example, we could take a too-bright photograph to capture the red brick of the alley and a too-dark photograph to capture the white and blue of the clouds. Photographers call this [high-dynamic-range imaging,](https://en.wikipedia.org/wiki/High-dynamic-range_imaging) and it essentially extends the detection of color beyond the sixteen-million limit. As this method has existed for awhile, detectors have been built that can detect colors outside of the normal range to save the trouble of taking multiple exposures. (Thanks to [Shane Lin](http://scholarslab.org/people/shane-lin/) for helping me to understand this.) The commercial purpose of these detectors is to help in high-dynamic-range photography, but the result is that they can detect sixteen-bits of information per red, blue, and green channel (or equivalent) giving 65,563 colors per channel and giving trillions (281,823,012,408,547) of possible colors. One way of considering how this extended range might improve digital reproductions, is that sixteen-million times _more evidence_ captured. If the color of a page means something---and it certainly does since color is the way we distinguish the letter-forms that make up the text---then we have much, much more information in a sixteen-bit per channel image.\n\nThe [Digital Production Team](http://static.lib.virginia.edu/directory/teams/library-digital-production-team.html) at the University of Virginia graciously provided these higher-bit-per-channel images. (A special thanks belong to [Christina Deane](http://static.lib.virginia.edu/directory/staff/cmm2t.html) who approved what must have seemed like a plan hatched by the Mad Hatter and [Sam Pierceall](http://static.lib.virginia.edu/directory/staff/sbp2k.html) who joined the tea party and helped us to understand his workflow---and possibilities for digitization using his equipment.) Looking back at the histogram earlier in the article, you can see that the broadest peaks fall on the right. These correspond to the brownish color of the paper and, in the sixteen-bit per channel images, cover a range of around eleven-million colors. These eleven-million colors fall into around ten-thousand distinguishable colors under normal circumstances. However, using a similar Fauvist-like visualization to the one above, we can uncover more information. For this experiment, we focused on a traditionally bibliographical object of study, paper.\n\n\n## Finding the Color Range for a Typical Image of a Piece of Paper in a Book\n\n\nThe first problem was to uncover the typical color of the paper in the images of the book. If you look closely at any sheet of paper, or any image of a sheet of paper, you see a range of colors. For example,\n\n\n\n\n![](http://scholarslab.org/wp-content/uploads/2017/05/page-close-300x295.png)\n\n\nClose-up of p. 382\n\n\n\n\n\n\nyou can see the fragments of dark-brown wood and whitish fibers along with yellow-brown linen fibers that have a range of shades. Furthermore, since we are exploring shades of brown beyond normal human vision, we cannot assume that what appears---using human vision---to be a uniform brown paper throughout is actually uniform. We needed to find the average shade across all five-hundred and thirty images of the pages of the body of the book without relying on our eyes. Luckily, we have [ImageMagick](https://www.imagemagick.org/script/index.php), a tool aggressively developed by a range of experimenters who work not only with sixteen-bit per channel color, but even higher and stranger bit depths. After recompiling the tool to work with higher bit-per-channel images, we could create a running average of every ten pages, which looks like semi-transparent pages on top of each other,\n\n\n\n\n![](http://scholarslab.org/wp-content/uploads/2017/05/average-mosaic-287x300.jpg)\n\n\nMosaic of averages (converted to 8-bit per channel here)\n\n\n\n\n\n\nThis visualization---at this point no one would think that this tries to accurately reproduce the look of a book---has some interest in itself. Were we doing distant-reading, we could note that the book seems to be mostly in prose, with some footnotes, and a table of contents at the front. But, if we average these fifty-three images together (we leave, as a trivial exercise to the reader, the fact that averaging ten images at a time and then averaging all those together is equivalent to averaging all five-hundred and thirty images together, but is far less computationally intensive) we get what we are aiming at---the average of all the images of pages of our text, including the average color of the paper,\n\n\n\n\n![](http://scholarslab.org/wp-content/uploads/2017/05/overall-average-smaller-197x300.jpg)\n\n\nAverage page\n\n\n\n\n\n\nUsing this, we can identify the range of colors in the images of paper from the book as a whole and stretch those eleven-million or so colors, that would normally look like only ten-thousand, across the whole range of human vision. We should be able to visualize subtle shades of brown that we could not normally see.\n\n\n## Arranging the Images\n\n\nAltering each image with ImageMagick is a relatively simple exercise, but leaves five-hundred and thirty Fauvist-style, blue-tinted images. Some detail can be made out, such as wirelines, but these visualizations still focuses on the page as a unit of inquiry while the paper in a book is not printed by page, but by gathering. Summarizing the principles of collation is beyond the scope of this post, but for those who understand the fundamentals of descriptive bibliography, the book collates thus,\n\n8o: A4 B-2K8 [$1-4 (-A1,3,4) signed] 260 leaves, pp. [_8_] [1-2] 3-512 (4=iv, 14-15=15-14, 474=74).\n\nIn other words, it is printed in octavo---that is eight pages to each side of a large sheet of paper---and folded down into gatherings of eight leaves and sixteen pages, although gathering A is slightly different. This means that we can use imposition schemes from Gaskell to recreate the layout of how the sheets of paper looked as they came off the printing press.5 Using ImageMagick to flip and arrange the digitized images (slightly cropped, so that there is not too much of the digitizing apparatus visible and leaving out the irregular A), we get,\n\n\n![](http://scholarslab.org/wp-content/uploads/2017/05/border-scheme-8-mosaic-smaller-2-300x236.jpg)\n\n\n\n\n\n\n\nSheets of the Book\n\n\nIn this visualization---it cannot be a reproduction since the original sheet form of the book no-longer exists, though it might be called a \"simulation\" of the original form---the white lines represent the boundaries of each sheet of paper and the yellow lines the front and back of each sheet. That is, the eight pages to the left of each yellow line represent the outer forme---as identified by Gaskell---and the eight pages to the right of the yellow line represent the inner forme of the sheet. Looking at the image, things do not look promising. Each sheet of paper should be a slightly different shade of brown because many should come from separate batches: the printer would have bought paper for the whole book, but printed off each forme in sequence so that the sheet of paper produced immediately before or after any particular sheet of paper in this book is probably not in this copy. Furthermore, the finished paper coming from the paper mill may have been shuffled along the way, or at the printing shop, so even if we knew the exact sheet that was printed after a given sheet (in another copy of this book, or perhaps another book altogether, or some broadside ballad), we still would not know if it was produced in the same batch. It is likely that many of these sheets came from different batches, but there is no way of knowing because there are only subtle differences of the shade of brown to distinguish them. Yet, by stretching the color range of the paper to the whole range of human vision, we can perceive another pattern.\n\n\n[![](http://scholarslab.org/wp-content/uploads/2017/05/border-paper-scheme-8-mosaic-smaller-3-300x236.jpg)](https://github.com/cacology/characters/blob/master/paper%20evidence/border-paper-scheme-8-mosaic.jpg)\n\n\n\n\n\n\n\nVisually Altered Sheets of the Book ([Download the large version](https://github.com/cacology/characters/blob/master/paper%20evidence/border-paper-scheme-8-mosaic.jpg))\n\n\nEach sheet of eight pages per side, seems to have a different shade of blue. In the upper left, you see a darker, speckled sheet. In the middle, pale blue sheets that might be the same batch. In other places, you can see brown smears across the surface of the sheet itself or brown fingerprints on the individual pages. This visualization shows some of the evidence contained in the physical copy of the book that can be captured with a higher bit-depth image. The deviation of color from the average happens over whole sheets, portions of one side of a sheet, on facing pages, or on single pages. Each of these discolorations---relative to the average color---is evidence of different sorts things that happened at different times in the history of this book.\n\n\n### Whole Sheet Deviation\n\n\nPaper is made in vats of floating fibers that are periodically refilled with more fibers, so no two sheets are quite alike. At the end of a day of work, a vat would be left and refilled the next day, perhaps with new fibers. So, that two sheets do not have quite the same color makes sense: they have different combinations of fibers. Two sheets made one right after the other should be somewhat similar, like the sheets in the middle of this book, but sheets made by different producers, with different fibers, or on different days would likely look totally different. Thus, when we see both sides of a sheet of paper colored the same as each other, but differently colored than their neighbors, it provides evidence of the paper-making process from the vat where discoloration would be evenly distributed. It is possible that while the sheets were stored, dampness or other factors may have discolored the whole sheet, but as a physical object in the world, the source of the discoloration is more likely to come from one side or the other, and is likely to be localized to only one part of the sheet.\n\n\n### Portions of One Side of a Sheet Discoloration\n\n\nSingle-sided discolorations can be seen throughout, in some places with an apparently matching discoloration on the adjacent sheet. These suggest evidence that comes about during the sheet-phase of the book's life. Either when it was paper being stored and transported, or newly printed sheets drying (or being stored or transported) before they were rough folded. As such, the matching brown smears on the two sheets in the middle suggest that they might have been stored one on top of the other with some moisture or other discoloring substance between them. In other cases, the discoloration may come from the unfolded or partially folded sheets being stored one on top of the other with felts between them. Another source of discoloration could be fingerprints from the papermakers, printers, or warehouse employees handling the pages before they were folded. These can be seen on the outer edges of the sheets of paper, where you would grab these if you were picking them up. Fingerprints and small marks in the middle of the sheet suggest something else.\n\n\n### Facing Page Discoloration\n\n\nDuring the page-phase of the book's life, after it was bound or rough-folded into a codex form, dirty fingers could still leave marks on pages. Marks on the edges of page units, particularly those that would have fallen in the middle of a sheet, rather than marks on the edges of sheets suggest that handling soiled them. If a substance that discolored the page was left on it, when the book was closed the substance could also transfer to the adjacent page. It follows that a mirror-image of the discoloration might be found on the facing page. Look in the lower right, a normally-visible water-stain becomes black on _both_ facing sheets. Some of the faint smudges in the middle have even fainter smudges on the page facing them. These marks suggest things that happened to the book as it was read or handled in codex form. We can note the more heavily soiled pages as a way of suggesting---perhaps bolstered with evidence from other copies---the pages that were most read, at least by people with dirty fingers.\n\n\n### Single Page Discoloration\n\n\nA few pages stand out as different from their neighbors, and differing from the page opposite them on the same leaf. These discolorations must have come about during the codex phase of the book's life, but how it was isolated to one page is unclear to me. In some cases, it may merely be a smudge, or ink mark, that had time to dry and so was not transferred to a neighbor, but was not wet enough to penetrate the thin page to the opposite side. In other cases, it could be the result of an inconsistency in the digitization process (a light started to flicker, the software had a bug that altered the color differently) or image manipulation process. A more remote possibility is that the one page is somehow different---say a cancellans, a term coined by R. W. Chapman to name the replacement sheet inserted in a book to correct an error or add to a text6---but that it is glued to an original page. Or, the page could be a full cancellans from a stock of paper that was stored differently and was discolored then. Lastly, it could be soiling from a time when the book was stored as rough-folded gatherings. In any of these cases, I think of Tanselle's suggestion to check the reproduction \"against the original\"7 when proof-reading. This check becomes particularly crucial here because our visualization is artificially colored and does not even attempt to try to realistically reproduce the characteristics of the original object. The pages are not blue, so these images do not reproduce these pages and the artifact itself may provide more insight. These single page discolorations suggest that further research into particular pages needs to be done, something that this whole exercise has already suggested.\n\n\n## The Importance of the Original Object\n\n\nThis post, I hope, demonstrates two things. First, that information about the paper in books, and how it was handled, can be partially uncovered using digital visualization techniques. While the Fauvist-like, blue-book imposition scheme is totally artificial and does not reproduce anything in the real world, it corresponds with the features of a real artifact. We can always learn more about the book as a mute witness to history by collecting and arranging detectable data. Second, I hope this post demonstrates that a digital reproduction is not only an insufficient substitute for an artifact, but that one digital reproduction is not equivalent to another. Only a few years ago, we would not have been able to produce the sixteen-bit per channel image needed for this work, yet the information was still contained in the artifact, waiting to be uncovered. Who is to say that more subtle and powerful digitization methods will not be discovered? Too often, requests for digitization are met with \"it's already in Google Books\", but I think this project shows that particular digital reproductions reproduce particular characteristics that do not encompass all the characteristics that can be captured using other digitization techniques. Multi-spectral imaging and higher-quality spectroscopy promise to generate even better visualizations, but if the pace of technical progress continues, these will just be another waypoint along the way of continuing technological improvement. Next decade's imaging may enable visualization of information contained in original artifacts that we have not even considered, another reason to preserve the original artifact after it has been digitized and to avoid damaging materials while studying them through digitization.\n\nWhile our books bear silent and eloquent witness to their own production history, technology keeps changing and tomorrow's technology might let us know a little bit more of the history that books embody. For this project, this visualization suggests the range of evidence to be found in particular copies. Returning to the author's manuscript, where available, or attempting to infer what the author's manuscript must have looked like, has a great deal of value as a coherent aim for historical investigation, but---so too---has the recovery of the history of printed artifacts which first exposed readers to a composite of production processes that take an author's manuscript, edit it, revise it, order it, format it, and impress it on paper. Studying a particular edition, while not replacing editing for authorial intention, recovers the intention of compositors and correctors who also had a say in the texts that the past left to us, and so should have a voice it recovering their history.\n\n\n\n\n\n\n* * *\n\n\n\n\n\n\n \t\n  1. G. Thomas Tanselle, “Reproductions and Scholarship,” in _Literature and Artifacts_ (Charlottesville: The Bibliographical Society of the University of Virginia, 1998), 33.↩\n\n \t\n  2. Ibid., 38--39.↩\n\n \t\n  3. Bonnie Mak, “Archaeology of a Digitization,” _Journal of the Association for Information Science and Technology_ 65, no. 8 (2014): 151–1526, doi:[10.1002/asi.23061](https://doi.org/10.1002/asi.23061).↩\n\n \t\n  4. Josef Albers, _Interactions of Color_, Rev. ed. (New Haven: Yale University Press, 1975).↩\n\n \t\n  5. Philip Gaskell, _A New Introduction to Bibliography_ (Oxford: Oxford University Press, 1972), p. [92], fig. 50.↩\n\n \t\n  6. R. W. Chapman, _Cancels_, Bibliographia (London: Constable & Co. Ltd., 1930), 6.↩\n\n \t\n  7. Tanselle, “Reproductions and Scholarship,” 38.↩\n\n\n\n"},{"id":"2017-05-31-why-to-teach-students-to-not-read-novels","title":"Why To Teach Students to Not-Read Novels","author":"james-ascher","date":"2017-05-31 04:57:35 -0400","categories":["Digital Humanities","Experimental Humanities","Grad Student Research","Visualization and Data Mining"],"url":"why-to-teach-students-to-not-read-novels","content":"_Scholars' Lab Fellow James Ascher went to Washington and Lee University to give a workshop in [Prof. Taylor Walle’s](https://www.wlu.edu/english-department/faculty-and-staff/profile?ID=x17053) ENGL 335 course through a Mellon-funded collaboration with the [Scholars’ Lab](http://scholarslab.org/) in the UVA Library. More information about this initiative can be found [here](https://github.com/wludh/research-one-collab). His post is [cross-listed on the W&L blog](http://digitalhumanities.wlu.edu/blog/2017/05/31/why-to-teach-students-to-not-read-novels/)._\n\nThis post has a simple argument: if you teach novels, you should teach students to not-read novels. Now, before you get concerned, I'm not arguing against teaching literature or avoiding novels altogether; the hyphen in \"not-read\" means a method, not a rejection of reading. Indeed, my whole argument is based on the idea that students need help returning careful attention to texts, but faced with a deluge of texts, we teachers ought to show them how some professional literary critics not-read, or as others call it \"distant read.\"\n\nWhat is a novel anyhow? A reasonable question to ask your students as a semester goes along, since it seems to be a long form of prose that comes to dominate what we now consider literature. If one is to believe [Amazon Rankings](https://www.amazon.com/charts/), then the most read forms of electronic books and the most purchased books remain novels (at least when I wrote this, but I'd be surprised if it changes). This wasn't always the case and part of the history of the novel is its commercial success. Franco Moretti makes the case clear in his [_Graphs, Maps, and Trees_](http://www.the-tls.co.uk/articles/private/forms-of-life/) where he argues that the rise of the novel can be studied using charts, as he does in his previous work [_Atlas of the European Novel_](http://www.the-tls.co.uk/articles/private/the-country-of-the-mind/) ([Moretti's response, which is more useful.](http://www.the-tls.co.uk/articles/private/literary-mappings/)) Building on the idea of charting large-scale phenomena over time, he notices that novels rise (fig. 1).\n\n\n\n\n![](http://scholarslab.org/wp-content/uploads/2017/05/RisesofNovels-1.png)\n\n\nFig. 1 rises of novels from _Graphs, Maps, and Trees_\n\n\n\n\n\n\nSomething about the form, the markets, the people, or something else means that this particular literary form comes to dominate, so whatever time period you consider, it's worth considering what was going on.\n\nIts well established that the English novel rose first---the form seems to have been invented there---so even in a non-English classroom, it's worth considering how those novels were imported into a broader literary discourse. Luckily, the text of most eighteenth-century English novels are freely available online. We have a fortuitous confluence of the intellectually important materials that have become technologically available (a careful reader will note that this is one possible explanation of the novel's rise as well). But, how can you study the features of a whole genre? An easy way is to read them by putting them on your syllabus, which I encourage, but after reading them you can look back at the whole syllabus and chart the topics that come up.\n\n\n\n\n![Fig. 2 Novel topics](http://scholarslab.org/wp-content/uploads/2017/05/Syllabus.png)\n\n\nFig. 2 Novel topics\n\n\n\n\n\n\nTo test this idea, I presented for Taylor Walle in her English 335 \"Radical Jane: the Politics of Class, Gender, and Race in Austen’s 'Polite' Fiction.\" Her course asked students to think about how English novels formed identities and related to the growing issues of British society. It seemed like a great chance to try some topic-modeling across her entire syllabus, the chart was produced with ten topics across the whole class (fig. 2). You can see the lesson [here](https://github.com/cacology/topic-modeling-18c-lit) if you want to reproduce the work.\n\nAfter demonstrating how the method works, we turned to this chart and looked for topics that crossed the texts. The works read for the class and on the chart---from left to right---are _Emma_, _Northanger Abbey_, _Pride and Prejudice_, _Sense and Sensibility_, _A Sentimental Journey_, _A Sicilian Romance_, and _A Vindication of the Rights of Woman_. You can see, and the class immediately saw, that the topics broke at the boundaries of books. You can see that many of the topics to detect specific books, but a few cross boundaries. Now, the topic used in topic modeling isn't quite the normal sense of the word \"topic.\" It means a list of words with probabilities that, when they occur, signal the topic that is that list of words. The topics by their top words are,\n\n    \n    <code>time made heart letter moment feelings mind spirits happiness\n    present long felt thought affection left hope return day love\n    situation ...\n    \n    elizabeth darcy bennet jane bingley wickham collins mrs sister\n    lydia catherine lady lizzy longbourn gardiner father family\n    netherfield kitty charlotte ...\n    \n    miss mrs good great dear young make time room house give day\n    thought friend heard man home replied pleasure hear ...\n    \n    julia marquis door ferdinand madame hippolitus castle heart duke\n    heard marchioness length appeared light night discovered time part\n    count scene ...\n    \n    man life love woman character mind world society sense opinion\n    great beauty good present taste nature understanding husband\n    degree subject ...\n    \n    emma harriet weston mrs knightley elton thing jane woodhouse miss\n    fairfax frank churchill body hartfield bates highbury father sort\n    harriet's ...\n    \n    fleur paris monsieur poor hand count thou man set told madame good\n    french thy heart lady tis put made nature ...\n    \n    elinor marianne mrs dashwood edward jennings sister willoughby\n    colonel lucy john mother thing brandon ferrars barton middleton\n    marianne's lady town ...\n    \n    catherine tilney isabella thorpe morland allen general henry bath\n    eleanor catherine's brother james father street hour northanger\n    abbey john captain ...\n    \n    women men reason virtue sex respect mind duties affection make\n    heart children power render human virtues true allowed till duty\n    ...</code>\n\n\nAs far as the course goes, the first topic seems to cover what all these texts have in common, but notice everything isn't perfectly lined up by novel. The topic beginning \"julia marquis door\" clearly comes from _Sicilian Romance_, but also hits on some later chapters of _Northanger Abbey_. Why would that be? Well, if you know the texts, you realize that some of the same Gothic themes occur in both texts and they use the same words.---\"Dear students, can anyone bring us to where in the text this happens?\" And, we enter the realm of the normal literature classroom.\n\nBy presenting a broad view of the texts, built by a computer algorithm, but out of the words of the text, we invite students to re-read works. Not-reading becomes re-reading and presenting words across the entire corpus puts students into partnership with technology to ask what it is about the form of novelistic prose that makes it popular and speak to social issues. Furthermore, we also encourage students to be critical of the results of computerized analysis. Several students noted that these topics were obvious, having read the works, and that they could have come up with them by hand, which is---of course---true. It's easy to scale these up beyond what you could do by hand, but seeing how they reflect what is accurately in the text shows that they provide some purchase on truth and also suggests what might be going on with other computerized analyses. One way we imagined it was that the computer applied an obvious rule at a fine level of detail. If we follow the same method, but only for _Emma_ by paragraph, we get a much messier chart (fig. 3), but seeing that chart, students can begin to engage with both literary texts and computers that help them to not read---to ask what it means and what can be done with it.\n\n\n\n\n![Fig. 3 Emma by paragraph](http://scholarslab.org/wp-content/uploads/2017/05/EmmaByChap.png)\n\n\nFig. 3 _Emma_ by paragraph\n\n\n\n\n"},{"id":"2017-06-05-what-should-you-do-in-a-week","title":"What Should You Do in a Week?","author":"brandon-walsh","date":"2017-06-05 07:00:10 -0400","categories":["Digital Humanities","Technical Training"],"url":"what-should-you-do-in-a-week","content":"_[Cross-posted to [my personal blog](http://walshbr.com/blog/2017/06/03/what-should-you-do-in-a-week/).]_\n\nFor the past several years, I've taught a [Humanities Programming](http://humanitiesprogramming.github.io) course at [HILT](http://www.dhtraining.org/). The course was piloted by Wayne Graham and Jeremy Boggs, but, these days, I co-teach the course with [Ethan Reed](http://scholarslab.org/people/ethan-reed/), one of our DH fellows in the Scholars' Lab. The course is a soup-to-nuts introduction to the kinds of methods and technologies that are useful for humanities programming. We're changing the course a fair amount this year, so I thought I'd offer a few notes on what we're doing and the pedagogical motivations for doing so. You can find our syllabus, slides, resources, and more on [the site](https://humanitiesprogramming.github.io/syllabus/).\n\nWe broke the course down into two halves:\n\n\n\n \t\n  * Basics: command line, Git, GitHub, HTML/CSS\n\n \t\n    * Project: personal website\n\n\n\n\n \t\n  * Programming concepts: Ruby\n\n \t\n    * Project: Rails application deployed through Heroku and up on GitHub\n\n\n\n\n\nIn the first half, people learned the basic stack necessary to work towards a personal website, then deploying that site through GitHub pages. In the second half, students took in a series of lessons about Ruby syntax, but the underlying goal was to teach them the programming concepts common to a number of programming languages. Then, we shifted gears and had them work through a series of Rails tutorials that pushed them towards a real-life situation where they're working through and on a thing (in this case a sort of platform for crowdsourcing transcriptions of images).\n\nI really enjoyed teaching the Rails course, and I think there was a lot of good in it. But over the past few years it has raised a number of pedagogical questions for me:\n\n \t\n  * What can you reasonably hope to teach in a week-long workshop?\n\n \t\n  * Is it better to do more with less or less with more?\n\n \t\n  * What is the upper-limit on the amount of new information students can take in during the week?\n\n \t\n  * What will students actually use/remember from the course once the week is over?\n\n\nTo be fair, week-long workshops like this one often raise similar concerns for me. I had two main concerns about our course in particular.\n\nThe first was a question of audience. We got people of all different skill levels in the course. Some people were there to get going with programming for the first time. These newcomers often seemed really comfortable with the course during the first half, while the second half of the course could result in a lot of frustration when the difficulty of the material suddenly seemed to skyrocket. Other students were experienced developers with several languages under their belt who were there specifically to learn Rails. The first half of the course seemed to be largely review for this experienced group, while the second half was really what they were there to take on.  It's great that we were able to pull in students with such diverse experiences, but I was especially concerned for the people new to programming who felt lost during the second half of the course. Those experienced folks looking to learn Rails? I think they can probably find their way into the framework some other way. But I didn't want our course to turn people off from programming because the presentation of the material felt frustrating. We can fix that. I always feel as though we should be able to explain these methods to anyone, and I wanted our alumni to feel that they were empowered by their new experiences, not frustrated. I wanted our course to reflect that principle by focusing on this audience of people looking for an introduction, not an advanced tutorial.\n\nI also wondered a lot about the outcomes of the course. I wondered how many of the students really did anything with web applications after the course was over. Those advanced students there specifically for Rails probably did, and I'm glad that they had tangible skills to walk away with. But, for the average person just getting into digital humanities programming, I imagine that Rails wasn't something they were going to use right away. After all, you use what you need to do what you need. And, while Rails gives you a lot of options, it's not necessarily the tool you need for the thing in front of you - specially when you're starting out.\n\nSo we set about redesigning the course with some of these thoughts in mind and with a few principles:\n\n \t\n  * Less is more.\n\n \t\n  * A single audience is better than many.\n\n \t\n  * If you won't use it, you'll lose it.\n\n\nI wondered how we might redesign the course to better reflect the kinds of work that are most common to humanists using programming for their work. I sat down and thought about common tasks that I use programming for beyond building apps/web services. I made a list of some common tasks that, when they confront me, I go, \"I can write a script for that!\" The resulting syllabus is [on the site](https://humanitiesprogramming.github.io/syllabus/), but I'll reiterate it here. The main changes took place in the second half of the course:\n\n\n\n \t\n  * Basics: command line, git, GitHub, HTML/CSS\n\n \t\n    * Project: personal website\n\n\n\n\n \t\n  * Programming concepts: Python\n\n \t\n    * Project(s): Applied Python for acquiring, processing, and analyzing humanities data\n\n\n\n\n\nThe switch from Python to Ruby reflects, in part, my own changing practices, but I also find that the Pythonic syntax enforces good stylistic practices in learners. In place of working on a large Rails app, we keep the second half of the course focused on daily tasks that programming is good for. After learning the basic concepts from Python, we introduce a few case studies for applied Python. Like all our materials, these are available [on our site](http://humanitiesprogramming.github.io/resources/). But I'd encourage interested folks to check out the Jupyter notebooks for these units if you're interested. These are the new units on applications of Python to typical situations:\n\n\n\n \t\n  * [Working with CSV files](https://nbviewer.jupyter.org/github/humanitiesprogramming/humanitiesprogramming.github.io/blob/master/python/notebooks/working-with-csv.ipynb)\n\n \t\n  * [Getting data from API's](https://nbviewer.jupyter.org/github/humanitiesprogramming/humanitiesprogramming.github.io/blob/master/python/notebooks/working-with-apis.ipynb)\n\n \t\n  * [Introduction to Web Scraping](https://nbviewer.jupyter.org/github/humanitiesprogramming/humanitiesprogramming.github.io/blob/master/python/notebooks/intro-to-scraping.ipynb)\n\n \t\n  * [Basic Text Analysis](https://nbviewer.jupyter.org/github/humanitiesprogramming/humanitiesprogramming.github.io/blob/master/python/notebooks/text-analysis.ipynb)\n\n\nIn the process of working through these materials, the students work with real, live humanities data drawn from [Project Gutenberg](https://www.gutenberg.org/), the [DPLA](https://dp.la/), and the [Jack the Ripper Casebook](http://www.casebook.org/press_reports/). We walk the students through a few different options for building a corpus of data and working with it. After gathering data, we talk about problems with it and how to use it. Of course, you could run an entire course on such things. Our goal here is not to cover everything. In fact, I erred on the side of keeping the lessons relatively lightweight, with the assumption that the jump in difficulty level would require us to move pretty slowly. The main goal is to show how situations that appear to be much more complicated still boil down to the same basic concepts the students have just learned. We want to shrink the perceived gap between those beginning exercises and the kinds of scripts that are actually useful for your own day-to-day work. We introduce some slightly more advanced concepts along the way, but hopefully enough of the material will remain familiar that the students can excel. Ideally, the concepts we work through in these case studies will be more immediately useful to someone trying to introduce programming into their workflow for the first time. And, in being more immediately useful, the exercises might be more likely to give a lasting foundation for them to keep building on into the future.\n\nWe've also rebranded the course slightly. The course description has changed, as we've attempted to soften jargon and make it clear that students are meant to come to the course not knowing the terms or technologies in the description (they're going to learn them with us!). The course name has changed as well, first as a joke but then in a serious way. Instead of simply being called \"Humanities Programming,\" the course is now \"Help! I'm a Humanist! - Programming for Humanists with Python.\" The goal there is to expose the human aspect of the course - no one is born knowing this stuff, and learning it means dealing with a load of tough feelings: anxiety, frustration, imposter syndrome, etc. I wanted to foreground all of this right away by making my own internal monologue part of the course title. The course can't alleviate all those feelings, but I hoped to make it clear that we're taking them into account and thinking about the human side of what it means to teach and learn this material. We're in it together.\n\nSo. What can you do in a week? Quite a lot. What should you do - that's a much tougher question. I've timed this post to go out right around when HILT starts. If I figure it out in the next week I'll let you know.\n"},{"id":"2017-06-21-lami-summer-fellows-2017","title":"LAMI Summer Fellows 2017","author":"brandon-walsh","date":"2017-06-21 07:30:46 -0400","categories":["Digital Humanities","Technical Training"],"url":"lami-summer-fellows-2017","content":"For the third year in a row, the Scholars' Lab and the University of Virginia Library are helping host summer fellows from the Leadership Alliance Mellon Initiative (LAMI) at UVA. The students will pursue original research this summer at UVA in consultation with a faculty mentor. For our part, the Scholars' Lab and the Library have worked with Keisha John, Director of Diversity of Programs in the [Office of Graduate and Postdoctoral Affairs](http://gradstudies.virginia.edu/about), to organize a weekly series of workshops introducing the students to digital humanities and library research methods. They'll be getting a broad introduction to digital research and the resources of the library as they think towards graduate school, and we've also coordinated weekly board game sessions over lunch (for SLab-style bonding).\n\nIn addition to introducing these students to the resources available at UVA and in the library system, the program aims to increase the number of demographically underrepresented students pursuing graduate work and careers in the academy. You can find more information about the program in a 2015 [press release](https://news.virginia.edu/content/summer-program-opens-door-graduate-studies-minority-students) put out by UVA Today when our first cohort was in residence. Two of our own graduate fellows, Jordan Buysse and Sarah McEleney, are serving as dh mentors.\n\nThese are the students that you might meet if you happen to be around the Scholars' Lab this summer. Look for more information about them and their projects by clicking through to their bios!\n\n\n\n \t\n  * [Je’lon Alexander](http://scholarslab.org/people/jelon-alexander/)\n\n \t\n  * [Valeria Arce](http://scholarslab.org/people/valeria-arce/)\n\n \t\n  * [Sara Castro](http://scholarslab.org/people/sara-castro/)\n\n \t\n  * [Madison Choi](http://scholarslab.org/people/madison-choi/)\n\n \t\n  * [Matt Ford](http://scholarslab.org/people/matt-ford/)\n\n \t\n  * [Victoria Juarez](http://scholarslab.org/people/victoria-juarez/)\n\n \t\n  * [Kaitlin Mitchell](http://scholarslab.org/people/kaitlin-mitchell/)\n\n \t\n  * [Ryan Russell](http://scholarslab.org/people/ryan-russell/)\n\n \t\n  * [Gabriela Trinidad-Perez](http://scholarslab.org/people/gabriele-trinidad-perez/)\n\n\nThey're a fantastic group, and we're excited to work with them this summer. Thanks to all of our colleagues at UVA, the Library, and the Scholars' Lab for their participation in the program.\n\n![](http://scholarslab.org/wp-content/uploads/2017/06/20170609-_DSC9874-Edit.jpg)\n\n\n"},{"id":"2017-06-27-job-opening-curious-about-focusing-on-dh-development","title":"Job Opening: Curious about focusing on DH development?","author":"amanda-visconti","date":"2017-06-27 10:24:54 -0400","categories":["Announcements","Job Announcements"],"url":"job-opening-curious-about-focusing-on-dh-development","content":"You might have seen [our opening for a Senior Developer](/2017/04/12/are-you-our-new-senior-developer/)—we're now seeking an additional colleague for our R&D team: DH Developer! [Apply here](http://jobs.virginia.edu/applicants/Central?quickFind=82179) (posting number #0621212), or read on for more information.\n\nWe welcome applications from women, people of color, LGBTQ, and others who are traditionally underrepresented among software developers. **In particular, we invite you to contact us even if you do not currently consider yourself to be a software developer.** We seek someone with the ability to collaborate and to expand their technical skill set in creative ways.\n\nThis is a full-time position, with flexibility to help you achieve a healthy work-life balance. Like all our team members, this role includes 20% of your time devoted to your own research initiatives and professional development. This \"personal R&D\" time includes  access to our experimental humanities makerspace, other high-end facilities, and expert collaborators and mentors.\n\n**About us**\n\nThe University of Virginia Library is the hub of a lively and growing community of practice in technology and the humanities, arts, and social sciences. As part of that community, the Scholars’ Lab has risen to international pre-eminence as a library-based center for digital humanities. The Scholars’ Lab collaborates with faculty, librarians, and students on a range of projects and tools, including spatial humanities, interface design, innovative pedagogy, data visualization, text analysis, digital archiving, 3D modeling, virtual reality and gaming, and other experimental humanities approaches.\n\nThe Library and the Scholars’ Lab are committed to diversity, inclusion, and safe spaces, and we have focused recent speaker series and practice on accessibility and social justice ([check out our team-authored charter for more on our values](http://scholarslab.org/about/charter/)). We welcome curious, critical, and compassionate professionals who are keenly interested in the overlaps between technology and the humanities (literature, history, art, cultural heritage, and related fields).\n\nThe Scholars' Lab currently consists of 11 staffers (plus a senior developer role), as well as an amazing cohort of graduate fellows and makerspace technicians.\n\n**Anticipated salary range: **$65,000-75,000, plus benefits such as professional development/conference travel funding, health insurance, and retirement savings plan\n\n**Responsibilities**\n\nUnder the direction of the Head of R&D for the Scholars' Lab in the UVA Library, the DH Developer\n\n\n\n\n  * works with scholars from the humanities and social sciences to understand their needs and define project goals\n\n\n  * provides professional opinions on appropriate project deliverables and reasonable schedules for completion of projects\n\n\n  * collaborates on building applications that enable scholars and library users to collect, manage, produce, manipulate, or analyze digital information resources in interesting ways\n\n\n  * writes original code, and tests and improves on existing code\n\n\n  * learns about and engages with new technologies toward widening and deepening the Scholars' Lab's pool of staff expertise\n\n\n  * creates documentation for both internal Lab and external non-technical audience use\n\n\n**Qualifications**** **\n\n**Minimum requirements:**\n\n\n\n\n  1. Experience equivalent to one full-time year with **either** a programming language (such as—but not limited to—PHP, Ruby, Python, Java), or HTML, CSS, and JavaScript\n\n\n  2. 2 years of web development experience, with tech skills demonstrated in an accessible portfolio of work.\n\n\n  3. Familiarity with a code version control system such as Git.\n\n\n  4. Ability to work and communicate with technical and non-technical collaborators.\n\n\n  5. Either education through the master's level **or** equivalent experience through your job, hobbies, or other activities, preferably in the humanities or library/information science.\n\n\n  6. Interest in the humanities (literature, history, art, cultural heritage, etc.)\n\n\n**Preferred:**\n\n\n\n\n  1. Graduate degree **or** equivalent professional or other experience in the humanities or social sciences.\n\n\n  2. Knowledge of and interest in the digital humanities.\n\n\n  3. Experience with collaborative project work.\n\n\n  4. Experience with any of the following: data collections, analysis, visualization, and interpretation; front-end web development and design; back-end web development; systems and database management; text analysis or image analysis methods and tools; frameworks such as Rails, Django, and Zend; TEI, XML, Solr, Cocoon, Tomcat.\n\n\n  5. Experience taking initiative to suggest or begin new projects, and to carry out projects with little supervision.\n\n\n**Interested?**\n\nYou can [apply here](http://jobs.virginia.edu/applicants/Central?quickFind=82179) (posting number #0621212), but please feel free to reach out with any questions—for yourself or a friend—by emailing [visconti@virginia.edu](mailto:visconti@virginia.edu) or tweeting [@scholarslab](http://www.twitter.com/scholarslab) or [@literature_geek](http://www.twitter.com/literature_geek). In particular, we're very happy to talk with anyone who's interested, but not sure whether they have the required technical background. All job discussions will be treated as confidential.\n"},{"id":"2017-07-05-neatline-2-5-2","title":"Neatline 2.5.2","author":"ronda-grizzle","date":"2017-07-05 09:15:00 -0400","categories":["Announcements"],"url":"neatline-2-5-2","content":"New release!\n\nFirst, a huge thank you to Jamie Folsom and Andy Stuhl from [Perfomant Software Solutions LLC](http://www.performantsoftware.com/), who did the heavy lifting on the coding for this release. We couldn't have done it without them. We're grateful, as well, to Neatline community member Adam Doan ([@doana](https://github.com/doana) on Github) from the University of Guelph, whose code contributions made Neatline's first accessibility functionality possible.\n\n\n#### What's Fixed:\n\n\n**Google Maps API issues.** We originally embedded the API key for Google Maps directly in the Neatline code, but Google changes the way apps should connect to their codebase fairly regularly, and with little or no warning. It's just easier for everyone if you can directly configure an API key for your specific installation of Neatline, so that's what we've done. Updated installation and configuration instructions (with screencaps!) are available on our [documentation site](http://docs.neatline.org/installing-neatline.html) .\n\n**WMS map layer issues.** We thought we had this one squished, but it came back again because of issues with our implementation of OpenLayers 2.0 and conflicts with the way that MapWarper passes data via URL. MapWarper WMS layers will now render properly as exhibit items and as base layers for an exhibit.\n\n\n#### What's New:\n\n\n**Accessibility.** Thanks to Neatline community member [@doana](https://github.com/doana), you can now specify a URL to an accessible version of your Neatline exhibit in the exhibit's settings. If the accessible URL exists, a hidden link will be rendered at the top of the public exhibit page directing users of assistive technology to the alternative page so that their screen reader can render the page for them. This feature relates specifically to [Guideline 1.1 of WCAG 2.0](https://www.w3.org/WAI/WCAG20/quickref/#text-equiv). Our documentation of this new feature will be available on [docs.neatline.org](http://docs.neatline.org) by July 5, 2017.\n\nFor more detail on this update, check out the [Changelog](https://github.com/scholarslab/Neatline/blob/master/CHANGELOG.md).\n\nReady to download? Get the latest release from the [Omeka Add-Ons Repository](http://omeka.org/add-ons/plugins/neatline/).\n\nEncounter an issue? ask a question on the [Omeka Forums](https://forum.omeka.org/) or submit an issue, or feature request, directly to us on our [issue tracker](https://github.com/scholarslab/Neatline/issues).\n"},{"id":"2017-07-05-transcribing-typography-with-markdown","title":"Transcribing Typography with Markdown","author":"james-p-ascher","date":"2017-07-05 14:28:00 -0400","categories":["Digital humanities","Experimetnal humanities","Grad Student Research","Research and Development"],"url":"transcribing-typography-with-markdown","content":"Digital technologies are not new solutions to our old problems, but are\nnew problems asking for us to return to old solutions. People have been\ntranscribing texts for as long as there have been texts. So it is no\nsurprise that some of the earliest applications for computers were\nconcerned with transcribing texts. These applications built on ideas\nbased on previous ideas which were themselves based on yet earlier\nones---the genealogical chains of some going back centuries. These\ngenealogies have their own fascinating histories, but the problem that\nconcerns this post is finding ways to use those centuries of accumulated\nideas to reproduce texts in a computerized platform that has developed\nits own sense of how texts should function. That is, how do we digitally\ntranscribe using the techniques that already exist? Techniques developed\nin prior decades can be difficult to use with computer-based methods\nbecause these computer-based methods have only recently developed and\nhave not yet been applied to as wide a range of sources as traditional\ntranscription methods. The solutions that computer applications first\nstumbled across for one problem initially seem to be right for every\nproblem, but on careful study it turns out they often obscure important\ndetails. We do not yet have an extensive repertoire of computer\ntranscription to consider, so it naturally seems like the solution that\nexists, however idiosyncratic, is the solution for all situations. For\nexample, `<em>` is widely used to indicate both italics and emphasis, which seems fine, but conflates two different sorts of text forms: The\nformer could indicate poetry, summaries, Latin or citations, while the\nlater suggests something that is an exception---a key glossary term or\nforeign phrase. The conventional computerized scheme is to treat the\nstylistic choice of italic as separate from the semantic choice of\nemphasis. For a non-emphasis use of italic, a style could be applied to\ndistinguish that particular semantic meaning, but these semantic\nmeanings would have to be carefully integrated into the existing scheme\nwithout conflicting with any other components. This integration becomes\nincreasingly complicated as the schemes become more complete because\nthere are more opportunities for conflict. New standards for HTML and\nTEI make the situation more flexible, but fundamentally transcriptions\non computers must either force their intellectual goals into an existing\nframework---i.e. marking a distinct convention of italics as though it\nwas the same as emphasis---or extend the framework itself---i.e.\ninventing a tag that distinguishes all the semantic types of italic used\nin the work.\n\nWhen I consider what digital tools could bring to the\nhumanities, I envision new forms of writing which challenge us to\nreimagine traditional questions in systems with new capabilities but\nwhich accept their own limited application and eventual obsolescence in\na long history of writing technologies. If a writer recognizes that\neventually all technologies become obsolete, they cannot reasonably\nignore the wisdom recorded by the technologies of the past. Furthermore,\nwriting serves a wider range of uses when it collaborates with as many\nconventions as possible, encoding the wisdom of old and new systems and\nclearly seeing the goals of past writers as well-worn traditional tools,\nand not creating new tools when a better old one remains to be\nunderstood. Timothy Morton's idea of \"ecology\" serves as well as any\nother metaphor here: he invites a move from placing nature on a pedestal\nto thinking of ecology as a sort of *ambient poetics*. Rather than\nasking how nature writing is done, we ask about the contexts in places,\nspace and material that surround both nature and the writing about\nnature. I propose that his idea of ecology can be extended to the way we\nthink of computer tools and their relationship to the wider range of\nwriting tools---the more widely we share tools and link to other ideas,\nthe more comprehensible will be our approach which also sets us up for\nthe next innovation. We ought not put a particular scheme on a pedestal\nbut instead recognize how it interacts with all other\nschemes.[[^1^](#fn1){#fnref1 .footnoteRef}]{.citation} \n\nTranslating the physical and intellectual features of a text into a system of\ntranscription requires judgment and I think adapting the most\nestablished technologies to the task does more good than taking on the\nunproven baggage of complicated new systems. A note on the scope of\ntechnology I'm talking about here: By established, I don't mean HTML or\neven ASCII, I mean conventions that span hundreds or even thousands of\nyears: punctuation, glossing, words in sequence, and that sort of thing.\nThe stability of the technology of punctuation means that it has been\nadopted in more recent technologies and could be used without careful\nconsideration, but I think that people concerned with the past need to\nask what questions the past has already answered well enough. Questions\nof how to transcribe have two components: the intellectual work of\nselection and description that needs to be done and the conventions\navailable for encoding. The former requires developing judgment, the\nlater only familiarity with a particular system. \n\nAdapting the ethos of the 1999 essay by David L. Vander Meulen and G. Thomas Tansell, \"A System of Manuscript Transcription,\" which broke new ground by adhering as closely to common sense as possible rather than breaking new ground\nwith a byzantine new system, we can write to foreground the historical\ntext and our judgments about it.[[^2^](#fn2){#fnref2\n.footnoteRef}]{.citation} Their approach admits that a transcription\nre-forms a text, requiring decisions and highlighting where those\ndecisions are made. Consider a hypothetical example in Markdown using\nsomething like the Vander Meulen-Tanselle approach:\n\n    The [?funny *conceled*] cat ran after the [*next two words\n    inserted*] big brown dog.\n\nWhich renders as: \"The \\[?funny *canceled*\\] cat ran after the \\[*next\ntwo words inserted*\\] big brown dog.\" The convention of using square\nbrackets to mark later insertions is so well established that it\nrequires little explanation to understand that these are editorial\ncomments and that the text that the editor believes is correct is\noutside of the brackets.[^3^](#fn3){#fnref3 .footnoteRef} Enclosing\nitalic text in asterisks is a Markdown convention, but is relatively\nfamiliar to most readers of digital texts. In contrast, one way to mark\nthe same text in TEI would be:\n\n     <p>The <del> <unclear> funny </unclear> </del> cat ran after the\n     <add> big brown </add> dog </p>\n\nBut those more familiar with TEI might be tempted to write:\n\n      <p>The <del rend=\"strikethrough\" cert=\"medium\"> <unclear\n      reason=\"smudged\"> funny </unclear> </del> cat ran after the <add\n      place=\"above\"> big brown </add> dog </p>\n\nThese tags instantiate collaboratively delineated concepts intended to\nbe broadly applicable to a wide range of texts, which aspire to encode\nthe document as it is. Various style sheets can cause these semantic\nmeanings to render as floating notes, in brackets, as plain text or in\nnearly any other way imagined by the designer of the particular style\nsheet. However, there is no default rendering and no rule about which\ndetails to record. TEI users are tempted to imagine that they merely\nrecord what they see rather than applying their judgment. Does knowing\nthat the certainty of the word \"funny\" is medium, that it was canceled\nwith a strike, and that \"big brown\" was above the line add to the\nmeaning of this text? I think, typically, it would not. But, if in a\nproject the method of cancellation mattered, it could easily be included\nin the editorial comments in the Vander Meulen-Tanselle system in plain\nEnglish. Indeed, if the method of cancellation mattered, writing it in\nEnglish would emphasize it rather than hide it in the appearance of the\ndisplay. Another difference between a TEI-like approach and an approach\nlike Vander Meulen and Tanselle's is whether or not the system is\nexpanded using the resources of plain English or by using the coding\nexpertise of a specific community, and, by implication, whether either\nsystem encourages developing judgment appropriate to the task at hand\nand demonstrating where that judgment is applied---or if it encourages\noutsourcing judgment to experts unfamiliar with your particular problem\nand hiding that application of judgment under the aegis of an official\nstandard. In one case, capable readers of English can understand the\nmarkup, while in the later case, you would need access to the TEI\nstandards and discussions to determine how to express and understand a\nfeature correctly according to people probably not involved with your\nparticular project. The plain English system can be read by competent\nand informed readers; the TEI system requires specialist expertise. \n\nNow, I do not mean to pick on the members of the TEI Consortium, who do good\nwork, but to outline the sort of thinking that influences the use of\ndigital technology in transcription. If we care to write about a text in\nhistory, we must think about which of the innumerable facts about that\ntext apply to the project at hand. This work can be shortcut by adhering\nto any present or future standard that displaces judgment onto someone\nelse, but at the expense of the system working for your project. A\nsystem needs to grow from the problems in the project. \n\nFor this project,\nwe aim to *recreate, digitally, the experience of reading\neighteenth-century texts for commonplace entries.* Having looked at\nseveral eighteenth-century books that survive and thinking about manuals\non preparing commonplace books (Locke has [a particularly good\none](https://archive.org/details/gu_newmethodmaki00lock)), we realized\nthat the physical layout of the text affected reading. Since texts occur\nin some layout which presents their meaning, it is obvious that the\nlayout links to meaning, but you would not think so based on text\nencoding standards that ignore white space and line breaks. Furthermore,\ncommonplacing readers may read with a pen in hand to mark the text\nitself in the margin. Conventional computer document formats are not\nwell suited to those approaches that remain attentive to the layout of\nthe page of the text they comment on. Recently, [Ted Nelson began a new\nlecture series](https://www.youtube.com/watch?v=KdnGPQaICjk) describing\nhow current computer systems reflect arbitrary compromises that stuck.\nOne example he mentions is that most modern document formats include no\nway to indicate marginal notes. For centuries, people commonly commented\non texts with marginal notes, but because the early designers did not\nuse those sorts of notes in their own day-to-day work, they were not\nincluded in most modern standards for texts. \n\nWe aim to capture features\nlike white space and line breaks that are not currently recorded in\ntranscription systems, but which would have been part of the\neighteenth-century text's features. But, how can we wrestle the\nstrangeness of old texts to the modern systems we know? One approach\ndraws on improv theater---just start doing it and then look over the\nwork to see which parts are useful. For this portion, three of\nus---Sarah Berkowitz, Elizabeth Ferguson and James P. Ascher---each\ntranscribed a few pages of the printed text we have been\nstudying---Samuel Butler's *Characters*---after talking about the aims\nof the project. We compared our approaches to find what did and did not\nwork. These solutions, like other good transcription solutions, were\ndeveloped collaboratively but are quite specific to this project. Our\nhope is that the reasoning here, while it should not be used to answer\nyour specific questions, should demonstrate an approach to thinking\nabout your own project.\n\nHeadings\n--------\n\nWithin Butler's *Characters*, each character has its own distinct\nheading, so they are important to record because they structure the text\nand name the theme for each part. On the printed page, these headings\nhave a different appearance from the main text: They are centered, in a\nlarger face, use capitals and small caps, and have rules above and below\nthem. One approach might be to transcribe thus:\n\n    {Double Rule}\n\n    A {Small Caps}\n\n    HUFFING COURTIER\n\nOr, with kerning between the letters\n\n    {Double Rule}\n\n       A\n\n    S M A L L  P O E T\n\nBoth of these approaches recreate the layout of the text in the document\nand preserve the capitalization. Another approach could ignore the\nlayout and kerning:\n\n    [*two rules*]\n\n    PREFACE.\n\nOr, following the Markdown convention for signaling a heading, we could\nwrite:\n\n    [*two rules*]\n\n    # AN\n    # ALDERMAN\n\nAll four approaches, although they may seem somewhat different, are\nshaped by two consistent categories of decision making: first, the\nselection of the elements worthy of notice and, second, the system of\nrecording those elements that have been noted. In each example, the\nheading is marked as different: in two cases by giving the typography\nand in two other cases by interpreting the typography as bearing\nsemantically equivalent meaning to a heading. Going back to the aim of\nthe project, recreating the reading practices of the past, since the\ntypography of headings is fairly consistent, it seems right to merely\napply semantic judgment to the text and save the reader of the\ntranscription time---as long as we're confident of our interpretation.\nSure, they could figure out that centered small caps words are\nheadlines, but instead we could tell them every headline is like that in\nan introduction and just note any variation. All four transcriptions,\nhowever, regard the presence of the rules as significant. The rule seems\nlike an element to notice since a reader of the text would come to\nexpect the pattern and the rules interrupt the purely alphabetic text\nbefore and after. The matter of how to transcribe is more about\nconvenience for converting the file and will be treated later.\n\nInitial Letters\n---------------\n\nEach section also begins with what is called a \"drop cap,\" a capital\nletter that is bigger than the adjacent letters, and which drops down\nseveral lines. We have a few examples of how to record this as well\n(unrelated portions of the transcription are omitted):\n\n     [*two line initial*]HAS taken his Degree in Cheating [...]\n     [*two line initial*] the higheſt of his Faculty [...]\n\nAnother:\n\n     [I]s one, that would fain make [...]\n     [ ]which Nature never meant him; [...]\n\nAnother:\n\n     [Two Line initial*]Is a Cypher, that has no Value himsself, but\n                        from the Place he stands in.  All his Hap-\n\nAnother:\n\n     T^2^HE writing of Characters [...]\n     much in Faſhion [...]\n\nThe first three mark the lines that a the drop cap initial spreads to.\nThis spread may be significant as drop caps in many French books of this\nsame time period go upwards rather than downwards, extending above the\nline. Furthermore, the word on the next line might be confused as\nrelating to the drop capital; resolving this confusion could be part of\nthe reading practice. Note also, that the first and third example\npreserve the space between the capital and the text of the second line,\nthe second does not (but that seems fine because the extension of the\ncapital is already blank). The last transcription, which is based on the\nsuggestion of Fredson Bowers in his *Principles*, uses a superscript in\nMarkdown to signal the initial, but does not record whether or not it\ngoes up or down.[[^4^](#fn4){#fnref4 .footnoteRef}]{.citation} This\nchoice may be fine if we are restricted to English books of a certain\ntype where these always seem to go down, but might be a problem if\ndeveloping a transcription standard that covers all types of books.\nSince in our case the practice is uniform across the text and merely\nserves to signal the reader that they have arrived at the beginning of a\nnew section, Bower's approach seems to best preserve the readability of\nthe text. It would render \"T^2^HE writing of Characters.\" While the\nsuperscript 2 is somewhat confusing, a note could explain and the\nsuperscript follows a convention developed from early twentieth-century\nwork on incunabula that has been used in certain serious scholarship for\nover a century.\n\nLong 's' characters\n-------------------\n\nEighteenth-century printers used both the long s---i.e. 'ſ'---and the\nshort s---i.e. 's.' Typically, the long s is used in the middle of a\nword and the short one at the beginnings and endings, but the practice\nis hardly consistent. It is not until 1785 that the short s really\ndominates mainstream printing as it does now. For this reason, most\ntranscriptions note the presence of a long s in some way. We have three\nexamples in our transcriptions: as \"ſ,\" as \"ss\" or as \"s\\*.\" Each\ntranscriber seems to want to include the letter as distinct from 's,'\nwhich seems like the right choice since long s at the very least could\npotentially be confused with an 'f' by current, or past, readers.\nHowever, each mode of transcription emphasizes the long s in different\nways. On the one hand, one marks it with an asterisk, suggesting that\nsomething unusual is going on; the asterisk is the convention for notae\nand footnotes, so this could work to mark and make visible all the\nplaces that this unusual letter occurs. Consider this passage,\n\n        He has found out a Way to s*ave the Expence\n     of much Wit and Sens*e: For he will make\n     les*s than s*ome have prodigally laid out upon\n     five or s*ix Words s*erve forty or fifty Lines.\n     This is a thrifty Invention, and very eas*y; and,\n     if it were commonly known, would much in-\n     creas*e the Trade of Wit, and maintain a Mul-\n\nIt makes it extremely clear that the long s occurs throughout in the\nbeginning and middle of words, pointing out that it deviates from the\nconvention I proposed above. On the other hand, using \"ss\" blends in and\nseems to be another spelling of a word. Since spelling was mostly\nnormalized in this time period, a deliberate misspelling can signal\nspecial typography, but we can never be totally certain that that\nsomething might not be an error or a word we do not know. Consider\n\"paſs\" and \"pasſ,\" which would both be transcribed in this system as\n\"passs,\" which seems to conflate the common situation with the odder\none. Using the [Unicode character, 'ſ,'](https://codepoints.net/U+017F)\nseems to both mark the letter as exceptional and to leave a readable\ntext. Those familiar with the conventions of eighteenth-century printing\nwill be not be surprised by it, but for those who are not familiar with\nthe conventions, it would recommend itself for further study.\n\nFootnotes, Quotations and Certainty\n-----------------------------------\n\nBrief, conventional quotations provide no special problems since the\nquote mark in ASCII and Unicode serves well enough, but two kinds of\ntypographic style associated with references require special attention:\nfootnotes in smaller type at the foot of the page and running quotes. An\nexample that combines both (slightly altered so that only those two\nconventions are apparent),\n\n    if it were commonly known, would much in-\n    crease the Trade of Wit, and maintain a Mul-\n    \n    We read that Virgil used to make, &c ] This alludes to a Passage\n    in the Life of Virgil ascribed to Donatus. \" Cum Georgica scrie-\n    \" traditur quotidio meditatos mane plurianos versus dic_tare so-\n    \" litus, [---Illegible need to check original copy (sarah)]\"\n\nWhile the footnote text in the original is smaller, this transcription\ndoes not document that fact, reasonably since the bracket and the verbal\ntext itself alerts us that we are in a footnote. Additionally, skipping\none line and continuing on the same page preserves the ability to make\nline references to both the main text and the footnotes in the same\nsystem. Yet, in comparison to our treatment of headings, it seems like\nwe could provide some signal to the reader that the text before them has\nspecial semantic value. Following the convention of square brackets, we\ncan adopt the Markdown notation, \"`[^1]`\" and \"`[^1]:`,\" to signal,\nrespectively, the location in the source text that is marked and the\nlines of the text that comment on it. The problem we run into using this\nnotation with this particular footnote is that it comments not on the\npage it occurs on, but on the facing page. If several footnotes occur,\nthe number can be incremented, but---as far as I know---footnotes\ncommenting on a text in another file are uncommon enough as to not have\nbeen dealt with. It seems that such a linkage needs two pieces of\ninformation: a linking symbol (i.e. the footnote mark or passage\nfootnoted) and the file that contains the proper footnote. Markdown\nprovides such a mechanism in the \"reference link\"; i.e. our source\npassage could be `[words][^1]` and the note could read\n`[^1]: otherfile.md`. The problem is that to Markdown, this reference\nnotation means the word \"words\" becomes a hyperlink to file\n\"otherfile.md,\" which isn't quite the right linkage. A simple extension\nof this scheme would be to include a statement of the location at both\nends of the footnote viz., for page 22,\n\n    [...]\n    The words were found in the notes. \\*[^1]\n    \n    [^1]: [*the footnote occurs on page 23*]\n\nPage 23 has an additional footnote but still refers back:\n\n    [...]\n    Someunrelated text with its own note, \\*[^1]\n    that doesn't relate to the wrong note.\n    \n    [^1]: This is a note for this page, so no comment.\n    [^2]: [*referring to the note on page 22*] I here note.\n\nNotice a few aspects of this approach. Since the numbering for footnotes\nin the brackets is not part of the transcription, but merely an aid for\nthe abstract structure, it only matters that the numbers are consistent\nwithin one document. Footnote \"1\" on one page could very well be\nfootnote \"2\" on another page while preserving the enumeration or symbols\nprovided by the printer. In this imaginary example, we prefixed the\nasterisk with a backslash so that any computerized parser would see that\nit is a symbol not a special character indicating an italic font. In the\noriginal example, note also that the running quotation marks are simply\ntranscribed in the margin with their apparent spacing. This seems right\nas the aim is to preserve the reading experience of the page which would\nhave these marked out by quotes. Lastly, note the final editorial\ncomment that expresses the uncertainty of the transcription. While a\nuniform language for expressing uncertainty is desirable, the nature of\nuncertainty is so various that providing free-reign to the editor to\nexplain what's going on seems the most prudent.\n\nItalics, Small Caps and Other Type Changes\n------------------------------------------\n\nTranscribing italics is both conventional and is part of the procedure\nused by all three of the transcribers in this project. Options include\ntagging the text with `[i]` or `{i}` or `<i>` as well as using `*` to\nenclose the passage. In each case, the transcription identifies the\nmoment in setting type where the compositor would have switched from\npulling letters out of the physical case of roman type to pulling\nletters out of the physical case of italic type and vice versa.\nWhichever sign is used, the aim is to note the presence of another style\nof typeface with the same body size. As\n[Markdown](https://daringfireball.net/projects/markdown/) understands\nasterisks to mean italic, something like `*this*` seems to be the right\napproach only because it makes the text easier to parse with\nconventional tools and does not misstate the situation. Following the\nsame technique, small capitals often signal different sorts of\ninformation in the text. Markdown does not provide a solution that is\nquite as elegant as the one for italic, but these type changes are a bit\nless common. The convention is to write\n`<span style=\"font-variant:small-caps;\">In Small Caps</span>` for a\npassage with *I*, *S* and *C* in full caps with the remaining letters in\nsmall caps. Aside from being unwieldy, this captures exactly what is\nhappening and provides a pattern for other changes between type cases\nthat warrant note in the transcribed text: Simply alter \"small-caps\" to\nthe appropriate font-variant of interest for a particular project. It is\nworth remembering, however, that the goal is to digitally recreate a\nreading experience, so for cases of different type sizes indicating a\nheading, it seems sufficient to use the mark for heading. For cases of\ndifferent type sizes indicating footnotes, the semantic marking for\nfootnotes seems sufficient. Lastly, we may come across a broken or\ndamaged letter. One example transcribes a *t* that is damaged as `<t>`,\nwhich follows the old tradition of using angle-brackets to indicate\nportions of the text that have been mutilated but which the editor can\nrecover. However, given the inconsistent use of different kinds of\nbrackets in different kinds of editions, this might be confusing.\nAnother option is to use editorial notes, i.e.\n`t [*previous letter shifted*]` which interrupts the text to announce\nthe damage. A further option would be to note the *t* plainly and\ninclude a note at the bottom of the page, such as this one:\n\n    [*letter t on line 8 shifted*]\n\nThis last approach emphasizes that the situation with the letter *t* is\ntotally comprehensible---it is just shifting type---and would not\ninterrupt the reading experience. It seems that the choice between these\nlast two approaches has to do with how prominent the mistake seems to\nbe.\n\nMaterial in the Skeleton: Headlines, Page Numbers and Signatures\n----------------------------------------------------------------\n\nA page of text includes not just the text of the work, but also what\nGenette would call paratexts which indicate the subject of chapters,\nlocation in the book or instructions to a binder. These occur in type\nimposed into a forme, but have a different sort of relationship to the\ntext set in a continuous operation by the compositors. Only in unusual\ncircumstances would an author expect a printer to follow their page\nnumbers, so compositors normally just provide the body text and\nfootnotes; when these are imposed, a skeleton forme of page numbers,\nsignatures and headlines from the previous imposition can be moved over\nand the details corrected for the new set of pages. A full description\nof a book should account for all the textual elements of a page, but it\nmakes perfect sense to segregate the information that was inserted as a\nguide surrounding the main text from the text itself. Since this project\nincludes a conventional bibliographical description, this information\ncan be put there while the text transcriptions can focus on the text\nthat forms the work of the compositors before the material was imposed\nin a skeleton forme.\n\nSpacing Between Letters and Around Punctuation\n----------------------------------------------\n\nEighteenth-century punctuation often used spacing differently than we do\nnow. A semicolon might have a thin space before it and a thick space\nafter it. To fit some punctuation into a line, there might be no spaces\nafter a period but before the next sentence begins. In another line, the\nspace after a period might be exceptionally large, or the spaces between\nwords exceptionally large. A compositor may put extra spaces between the\nletters of a word to give it emphasis as a heading, which seems like a\nsemantic choice rather than one that aims to preserve the justification\nof the line. That is, there seem to be two possible reasons to have a\nnoticeable variation in space: either the need to provide a line of type\nof a certain width or to indicate a type of semantic information.\nExperience must be the guide in distinguishing between these two, but\nthe situation should generally be clear after some study. The problem\nbecomes one of how to represent variations in spacing widths if it is\ndecided that they represent semantic meaning. Since each line is\njustified separately, the unit of analysis must be the line. Different\nwidths of spaces between two different lines almost certainly represent\nthe need to justify that particular line, but different widths within a\nline may---if the editor judges it to have meaning---be transcribed\nwithin that line. To encode this, Unicode provides a range of spaces of\ndifferent widths. The characters [\"HAIR SPACE,\" \"PUNCTUATION SPACE,\" \"EM\nSPACE,\" \"THREE-PER-EM SPACE,\" \"FOUR-PER-EM SPACE,\" \"SIX-PER-EM\nSPACE\"](https://www.cs.tut.fi/~jkorpela/chars/spaces.html) cover a wide\nvariety of types of spaces and widths of spaces (the [Unicode Standard\nitself](http://www.unicode.org/versions/Unicode10.0.0/ch06.pdf) covers\nthese in far more detail). The most sensible treatment of space, since a\ncompositor would not really be distinguishing a space used for\npunctuation from a similarly narrow space, would be to follow Peter\nBlayney's approach in the Appendix to his *Texts of King\nLear.*[^5^](#fn5){#fnref5 .footnoteRef} When different sizes of space\nappear in the same line, simply use different sizes of space in the\ntranscription to indicate that. The only modification for our project is\nthat those spacing elements ought---in the judgment of the editor---to\nbear some sort of semantic meaning.\n\nAn Example Page\n---------------\n\nThis post has discussed a wide range of choices in transcribing a text\nto preserve the reading experience from a printed book that can be\nsummarized simply: use Markdown and Unicode and make judgments clear in\nsquare brackets when alterations are needed to use Unicode and Markdown.\nYet, it can be useful to have an example---however fabricated---that\nbrings these elements together,\n\n    [*two rules*]\n\n    #AN\\\n    EXAMPLE PAGE\n    \n    T^2^he *text* on this page isn't in any book, but <span\n    style=\"font-variant:small-caps;\">Demonstrates</span> some\n    techniques you might use to tranſ-\\\n    scribe texts as you see them.  Note that each line breaks with a\\\n    backſlash before the newline. [^1]  This signals the difference\n    between\\\n    a newline needed to fit the text on one screen and one which rep-\\\n    resents an actual line break.  \"We find that the quotes run\\\n    \" along the side for extended quotes.  Just as they do in\\\n    \" eighteenth century texts.\" And , that punctuation spaces can be\\\n    coded as such .  What a text! [*last word poorly inked, could be \"hex\"*]\n    \n    [^1]: before the newline ] a newline is a special sort of chara-\\\n    cter that means you begin a new line of text.\n    \n    [*the letter t in \"techniques\" on the first line of the text is\n    shifted upward*]\n\nOne way this would render by default would be: \\[*two rules*\\]\n\\#E X A M P L E P A G E T^2^he *text* on this page isn't in any book,\nbut <span style=\"font-variant:small-caps;\">Demonstrates</span> some\ntechniques you might use to tranſ- scribe texts as you see them. Note\nthat each line breaks with a backſlash before the\nnewline.[^6^](#fn6){#fnref6 .footnoteRef} This signals the difference\nbetween a newline needed to fit the text on one screen and one which\nrep- resents an actual line break. \"We find that the quotes run \" along\nthe side for extended quotes. Just as they do in \" eighteenth century\ntexts.\" And , that punctuation spaces can be coded as such . What a\ntext! \\[*last word poorly inked, could be \"hex\"*\\] \\[*the letter t in\n\"techniques\" on the first line of the text is shifted upward*\\]\n\n<div class=\"footnotes\">\n\n<hr>\n\n1. <div id=\"fn1\">\n\n1.  </div>\n\n    Timothy Morton, *Ecology Without Nature: Rethinking Environmental\n    Aesthetics* (Cambridge, Mass.: Harvard University Press,\n    2009).[↩](#fnref1)\n2. <div id=\"fn2\">\n\n    </div>\n\n    David L. Vander Meulen and G. Thomas Tanselle, “A System of\n    Manuscript Transcription,” *Studies in Bibliography* 52 (1999):\n    201–12.[↩](#fnref2)\n3. <div id=\"fn3\">\n\n    </div>\n\n    For those unfamiliar, brackets are a standard symbol used to\n    indicate editorial additions and italics distinguish descriptions\n    and explanations of the roman text: since \"big brown\" is conjectured\n    to belong to the final text, the words are placed outside of the\n    brackets; since \"funny\" is conjectured to belong to an earlier\n    version, the word is place inside the brackets.[↩](#fnref3)\n4. <div id=\"fn4\">\n\n    </div>\n\n    Fredson Bowers, *Principles of Bibliographical Description* (New\n    York: Russell & Russell, 1962).[↩](#fnref4)\n5. <div id=\"fn5\">\n\n    </div>\n\n    [Peter W.M. Blayney, *The Texts of King Lear and Their Origins*\n    (Cambridge: Cambridge University Press, 1982)]{.citation}; Blayney\n    is studying the recurrence of types so chooses to transcribe both\n    semantic meaning and what evidence he finds of the typographical\n    habits of the compositors.[↩](#fnref5)\n6. <div id=\"fn6\">\n\n    </div>\n\n    before the newline \\] a newline is a special sort of chara- cter\n    that means you begin a new line of text.[↩](#fnref6)\n\n</div>\n"},{"id":"2017-07-25-what-can-digital-humanities-tell-us-about-character","title":"What can digital humanities tell us about Character?","author":"sarah-berkowitz","date":"2017-07-25 12:31:00 -0400","categories":["Grad Student Research","Research and Development"],"url":"what-can-digital-humanities-tell-us-about-character","content":"My part of the collaboration with James has been thinking through what this text has to tell us about \"Character\" as a literary category and to consider how digital tools can help modern users interact with eighteenth-century characters.\n\nThere’s been a learning curve for me as I find out more and more about what digital formats can and can’t do. I think my biggest challenge has been learning to think about digital material spatially—in order for something to exist in our final product we have to think about where it goes and how to attach it. Our original plan was to preserve every page in three separate files—one with the image of the text, one with a transcription of the text, and a third that contained commentary for that page. The hope was that we could sync every file by line and thus create a no frills edition that could be accessible and transparent for all users.\n\nWe’ve been forging full steam ahead with the transcriptions, and I’ve learned a great deal about how to preserve physical features on page in a digital translation. I began to realize that I think of character conceptually, not spatially, and thus finding a way to break down what this text can tell us about Character by page began to seem less and less feasible—let alone breaking it down by line! A line by line commentary is useful to explicate specific things in the text—allusions that would escape a twenty first century reader, say, or translating Latin phrases into English. Each of these things occur at a specific place in the text, and are thus well suited to line by line annotation. We’ve shied away from doing that kind of annotation—not because it’s not useful, but because it’s already been done, and done well, first by Robert Thyer for the 1759 edition and for modern audiences by Charles Daves in 1970.\n\nButler’s work is a collection of Theophrastan Characters—a genre of writing that enjoyed a revival when Butler was writing in the late seventeenth century, but which had fallen out of fashion by the time the collection was published posthumously in 1759. Theophrastan Characters are an odd genre. They break down characters into general “types” and give a description that ostensibly describes every person that falls under that category. For instance, when Butler writes about “An Amorist” that “His Passion is as easily set on Fire as a Fart, and as soon out again.” We are meant to assume that 1) this is true of all Amorists, and 2) if we ever meet somebody whose passion is, err, easily stirred and just as quickly extinguished, that person is an Amorist.\n\nWe’re used to breaking down literary characters into round and flat characters, or individuals and types. Theophrastan Characters dwell completely on the side of types, which, when you think about it is kind of nuts. We tend to think of people specifically, not generally. If I were to ask you to imagine a lawyer, you would probably think of a lawyer you know, or a famous lawyer you’ve seen in the news or in pop culture—Elle Woods, say, or Johnny Cochran. But Butler asks us to imagine a generic lawyer, someone whose “Opinion is one thing while it’s his own, and another when it is paid for,” a figure who represents all lawyers everywhere. This is familiar to us when we think about type—who doesn’t love a good lawyer joke? But it’s strange when we consider this figure as a “Character.” In literature, even type characters require a modicum of specificity, which is dictated by their literary surroundings. When a lawyer appears in <em>Bleak House</em>, even though that lawyer is just a flat, type character, we still imagine a single figure in Chancery during 1852 litigating Jarndyce vs Jarndyce; it could <em>not</em> be Elle Woods, or Johnny Cochran or your college friend who went to law school. But Butler’s characters are devoid of context—his lawyer is at once every lawyer and no lawyer at all.\n\nI’m hoping this project will be able to tell us two things. First, what tools do you use to create a general character? Just a surface read through shows us that Butler seldom uses traits or characteristics to describe his characters—they’re too individualizing. Instead he writes largely with metaphors. An Amorist is “like an Officer in a corporation” and a Lawyer is “like a French duelist”—which of course begs the question, what are the officers of corporations and French duelists like? Are there other devices that Butler uses? Does he use the same devices for every character? My plan is to run the text through Stylo to see if we can learn anything about how Butler creates his types.\n\nSecond, what will it take to find examples of Butler’s characters? What does it take to fit a specific person into a general description? Could we argue that perhaps Butler is describing Johnny Cochran, even if he is not describing Elle Woods? How would we show that Cochran fits into Butler’s category? By looking at what he’s done? How he acts? Who he is? Leaving aside lawyers, would we be able to find examples of Henpect Men or Fifth Monarchy Men in today’s world—or are types too dependent on their political and cultural context to translate?\n\nNow that we have a good number of transcriptions we can begin to create a corpus, which I hope will be able to answer some of these questions."},{"id":"2017-08-03-fall-2017-gis-workshops","title":"Fall 2017 UVa Library GIS Workshop Series","author":"chris-gist","date":"2017-08-03 10:10:00 -0400","categories":["Announcements","Events","Geospatial and temporal"],"url":"fall-2017-gis-workshops","content":"All sessions are one hour and assume participants have no previous experience using GIS.  Sessions will be hands-on with step-by-step tutorials and expert assistance.  All sessions will be held on <strong>Tuesdays from 3PM to 4PM in the Alderman Electronic Classroom, ALD 421</strong> (adjacent to the Scholars’ Lab) and are free and open to the UVa and larger Charlottesville community.  No registration, just show up!\n\nSeptember 12<sup>th</sup>\n\n<strong>Making Your First Map with ArcGIS</strong>\n\nHere’s your chance to get started with geographic information systems software in a friendly, jargon-free environment.  This workshop introduces the skills you need to make your own maps.  Along the way you’ll get a taste of Earth’s most popular GIS software (ArcGIS) and a gentle introduction to cartography. You’ll leave with your own cartographic masterpieces and tips for learning more in your pursuit of mappiness at UVa.\n\nSeptember 19<sup>th</sup>\n\n<strong>Georeferencing a Map - Putting Old maps and Aerial Photos on Your Map</strong>\n\nWould you like to see historic map overlaid on modern aerial photography?  Do you need to extract features of a map for use in GIS?  Georeferencing is the first step.  We will show you how to take a scan of a paper map and align in it in ArcGIS.\n\nSeptember 26<sup>th</sup>\n\n<strong>Getting Your Data on a Map</strong>\n\nDo you have a list of Lat/Lon coordinates or addresses you would like to see on a map?  We will show you how to do just that.  Through ArcGIS’s Add XY data tool and Geocoding (address matching), it is easy to take your tabular lists and generate points on a map.\n\nOctober 10<sup>th</sup>\n\n<strong>Points on Your Map: Street Addresses and More Spatial Things</strong>\n\nDo you have a list of street addresses crying out to be mapped?  Have a list of zip codes or census tracts you wish to associate with other data?  We’ll start with addresses and other things spatial and end with points on a map, ready for visualization and analysis.\n\nOctober 17<sup>th</sup>\n\n<strong>Taking Control of Your Spatial Data: Editing in ArcGIS</strong>\n\nUntil we perfect that magic “extract all those lines from this paper map” button we’re stuck using editor tools to get that job done.  If you’re lucky, someone else has done the work to create your points, lines, and polygons but maybe they need your magic touch to make them better.  This session shows you how to create and modify vector features in ArcMap, the world’s most popular geographic information systems software.  We’ll explore tools to create new points, lines, and polygons and to edit existing datasets.  At version 10, ArcMap’s editor was revamped introducing new templates, but we’ll keep calm and carry on.\n\nOctober 24<sup>th</sup>\n\n<strong>Easy Demographics</strong>\n\nNeed to make a quick demographic map or religious adherence?  This workshop will show you how easily navigate Social Explorer.  This powerful online application makes it easy to create maps with contemporary and historic census data and religious information.\n\nOctober 31<sup>st</sup>\n\n<strong>Introduction to ArcGIS Online</strong>\n\nWith ArcGIS Online, you can use and create maps and scenes, access ready-to-use maps, layers and analytics, publish data as web layers, collaborate and share, access maps from any device, make maps with your Microsoft Excel data, customize the ArcGIS Online website, and view status reports.\n\nNovember 7<sup>th</sup>\n\n<strong>Expanding Content in ArcGIS Online</strong>\n\nYou can also use ArcGIS Online as a platform to build custom location-based apps.  You can create stories and context around online maps for things like storytelling, tours or map comparisons.   Many of these applications have templates that make for easy viewing on mobile devices."},{"id":"2017-08-15-walt-whitmans-jack-engle-and-lola-montez-new-from-collective-biographies-of-women","title":"Walt Whitman's Jack Engle and Lola Montez: New from Collective Biographies of Women","author":"alison-booth","date":"2017-08-15 12:39:00 -0400","categories":["digital humanities","research and development"],"url":"walt-whitmans-jack-engle-and-lola-montez-new-from-collective-biographies-of-women","content":"*(This is the first part of a short essay I posted on the blog of Collective Biographies of Women and elsewhere on August 15, 2017.  See [the CBW blog](https://pages.shanti.virginia.edu/CBW_Blog/2017/08/14/the-clerk-and-the-spanish-dancer-walt-whitmans-jack-engle-and-lola-montez) for the entirety, with additional notes and references.)*\n\nIn February, 2017, there was some [exciting news](https://www.theguardian.com/books/2017/feb/21/walt-whitmans-lost-novel-the-life-and-adventures-of-jack-engle-found) of the kind that gratifies literary scholars everywhere.  Graduate student Zachary Turpin had discovered a lost short novel that Walt Whitman serialized anonymously in New York’s *Sunday Dispatch* in 1852.  *The Life and Adventures of Jack Engle*, as narrated by a young clerk of that name, gives impressions of New York life as Whitman experienced it before he became revered as the Good Gray Poet.  I am no Whitman scholar and have little to add to the discussion of US periodicals in the 1850s.  But as I quickly devoured the news and the novel itself, I was taken with a minor character closely related to my own research: the Spanish dancer, Inez.  Could this be a version of Lola Montez?\n\n![Photo by Robbie Hott, July 3, 2017, *Lola Montez* by Joseph Stieler, 1847, for Ludwig I’s Gallery of Beauties at Schloss Nymphenburg, Munich](https://pages.shanti.virginia.edu/CBW_Blog/files/2017/08/LolaMontezPortraitNymphenburg-e1502721352970-1024x1365.jpeg)\n\n*Photo by Robbie Hott, July 3, 2017,* Lola Montez *by Joseph Stieler, 1847, for Ludwig I’s Gallery of Beauties at Schloss Nymphenburg, Munich.*\n\nThe improbable “auto-biography” of Jack Engle now attributed to Whitman claims in the preface to be a “true story” about “familiar” people; “the main incidents were of actual occurrence,” giving “the performers in this real drama, unreal names” (Whitman, *Engle* 3). Clearly, the “life and adventures” of the quasi-Dickensian hero differ from Whitman’s (Walt was no orphan, for example). But Whitman might have given an unreal name to the real [Lola Montez, Spanish dancer](http://cbw.iath.virginia.edu/women_display.php?id=14501), whom I have long featured in my digital project on women’s biographies, Collective Biographies of Women or CBW (Booth).  The Irish-born adventuress who became the Countess of Landsfeld, who was buried in New York as Eliza Gilbert in 1861, has received many full-length and brief biographies. Whitman’s connection to this celebrity is not unknown, though little remarked.  She was in New York during the production of Whitman’s novella, *Jack Engle*. On January 5, 1852, weeks into her first star turn in New York, she danced in *Un Jour de Carneval à Seville* in the role of Donna Inez (Morton 205). [[1\\]](https://pages.shanti.virginia.edu/CBW_Blog/?p=441&preview=true#_ftn1) Then, after controversial appearances in Boston, Hartford, and elsewhere, she appeared at the Broadway Theatre in *Lola Montez in Bavaria*, a play in five acts recounting her famous alliance with King Ludwig I and the rebellions and backlash that led to the king’s abdication (“The Danseuse, the Politician, The Countess, the Revolutionist and finally the Fugitive”; Morton 218). Whitman could easily have seen her reprise of this play at the Bowery Theatre on 28 June, or could have attended one of her benefit performances that spring, as Jack attends Inez’s benefit performance in the novella. Certainly Whitman and Montez coincided when she was back in New York six years later and they frequented [Pfaff’s, Whitman’s bohemian hangout](http://pfaffs.web.lehigh.edu/node/54272) after first publication of *Leaves of Grass* (Lehigh University).\n\nThese enterprising mid-century figures have more interesting qualities in common than coinciding in New York in certain years.  Her defiant self-making is not out of keeping with his celebration of the body.  Notably, during their shared New York-bohemian years, both published highly gendered self-help.  [*Manly Health and Training*](https://www.theguardian.com/books/2016/apr/30/walt-whitman-revealed-as-author-of-manly-health-guide)*,* an advice book by “Mose Velsor,” was serialized in the *New York Atlas* in 1858, and Zachary Turpin recently discovered Whitman’s authorship (Velsor).  [*The Arts of Beauty: or Secrets of a Lady’s Toilet*](https://books.google.com/books?id=1DQEAAAAYAAJ&printsec=frontcover#v=onepage&q&f=false) (New York: Dick & Fitzgerald, 1858) capitalized on Lola Montez as the famous author, drawing upon her series of popular lectures in New York, London, and elsewhere (Montez).\n\nWhitman left unacknowledged his authorship of the episodic entertainment, *Jack Engle*.  We might then allow a canonical poet to steer clear of a notorious entertainer whose vocational tag in the Oxford *Dictionary of National Biography* is “adventuress.”  To follow through on my first impulse to post that “Whitman’s Inez is Lola Montez,” it would take more than the known connections in 1858; the novel, again, was churned out topically and serially in 1852.  The Whitman scholars I contacted were less than convinced that Inez resembles Montez.  I share their opinion that Inez can be a composite of Spanish dancers Whitman might have known in New Orleans (she was there in 1853, he in 1848) or New York, as well as some features of George Sand and others whom Whitman admired.  The fictional Spanish dancer has no exalted political past and, like other characters in the novel, she derives a great deal from the conventions of romance and melodrama. But it is certain that Lola Montez was big news in New York in the early months of 1852, and there are interesting connections with Whitman’s plotline of the hero’s growing intimacy with a belle of the town. [2](https://pages.shanti.virginia.edu/CBW_Blog/?p=441&preview=true#_ftn2) Though “Spanish” connotes hot-blooded, it also connotes veiled and hard to get. The portrayal of the novel’s Spanish dancer points to significant features of the well-educated, entrepreneurial celebrity. Whitman’s version also renders the performer more bourgeois and less interesting than the real thing, downplaying Montez’s kinky suggestiveness. The differences are a measure of the fictional purpose of this minor character.  The hero rises from street life to office work and a brief escapade outside the law that ends happily, all the more because he was never in real danger of falling in love with Inez.\n\n![Lola Montez in a daguerreotype (color added), 1851, by Southworth & Hawes](https://pages.shanti.virginia.edu/CBW_Blog/files/2017/08/LolaMontez4337210833_78668a4725.jpg)\n\n*Lola Montez in a daguerreotype (color added), 1851, by Southworth & Hawes*\n\nYou know the type: “Spanish,” “dancer”; theaters would be places to find all sorts of accessible women.  But Whitman’s Inez and the real Lola Montez might be called, in hard-boiled speak, **classy dames*. I intentionally hit on the sore point of typecasting, because it is almost inescapable, even in fact-based historical biography.  The surprise is not the higher quality of love object implicit in the reputations of Inez and Lola, and not even that they evince manners and education, but that they are businesswomen, capitalists.  In *Jack Engle*, the narrator is a reluctant young apprentice in Covert’s law office, where he notices a young lady client.  Covert is advising her on a doubtful purchase of shares (happily, it turns out she never buys into the fake scheme).  “She had the stylish, self-possessed look, which sometimes marks those who follow a theatrical life. Her face, though not beautiful, was open and pleasing, with bright black eyes, and a brown complexion. Her figure, of good height and graceful movement, was dressed in a costly pale colored silk” (27).  She calls out to the pet dog, also named Jack, who jumps up and muddies her dress.  Inez is annoyed, and then laughs it off—a preview of her responses to drooling men and to Jack himself.  In chapter six of *Jack Engle*, Inez appears “really fascinating” on stage in the “short gauzy costume of a dancing girl. Her legs and feet were beautiful, and her gestures and attitudes easy and graceful” (29). These characterizing details correspond somewhat with the historical Montez.  Montez was fair, with striking blue eyes, unlike Inez.  She was frequently depicted in association with animals.  Contemporaries range between calling Montez altogether beautiful, or merely fascinating with a face that was not beautiful.  But then of course there was her figure.  Accounts usually disparage Montez’s performing ability, but those who were not too scandalized avidly praised the legs and the costume.  Images in newspapers always emphasize the tiny waist, ballooning bosom, and short skirts….\n\n## Notes\n\n[[1\\]](https://pages.shanti.virginia.edu/CBW_Blog/?p=441&preview=true#_ftnref1) Kirsten Greusz suggests Inez was a common name for the Spanish-beauty type, as in antebellum novels “Inez the Beautiful, or, Love on the Rio Grande” (Harry Hazel, 1846) or Augusta Evans Wilson’s “Inez, A Tale of the Alamo” (1850).  I also consulted with Ed Whitley, Ken Price, and Ed Folsom.\n\n[[2\\]](https://pages.shanti.virginia.edu/CBW_Blog/?p=441&preview=true#_ftnref2) Ed Folsom and Ken Price, in their article on Whitman for *The Walt Whitman Archive,* indicate Whitman’s affiliations with women activists Abby Price, Paulina Wright Davis, Sarah Tyndale, and Sara Payson Willis (Fanny Fern), as well as the “queen of Bohemia” Ada Clare.  CBW includes only [Fanny Fern](http://cbw.iath.virginia.edu/cbw_db/persons.php?id=9247) of these women, though abolitionists and activists for women’s rights do appear in some collections listed in our bibliography.\n\nRead more via [the full essay on the CBW blog](https://pages.shanti.virginia.edu/CBW_Blog/2017/08/14/the-clerk-and-the-spanish-dancer-walt-whitmans-jack-engle-and-lola-montez/)."},{"id":"2017-08-15-welcome-senior-developer-shane-lin","title":"Welcome Senior Developer Shane Lin!","author":"jeremy-boggs","date":"2017-08-14 21:00:00 -0400","categories":["Announcements"],"url":"welcome-senior-developer-shane-lin","content":"The Scholars' Lab team is thrilled to welcome [Shane Lin](/people/shane-lin) as our new Senior Developer! \n\nShane first joined the Scholars' Lab as a [Praxis Program](http://praxis.scholarslab.org/) graduate fellow in 2012. Since then, he's served as a Technologist in our [Makerspace](http://scholarslab.org/makerspace/), where he's provided invaluable guidance on research and pedagogy related to desktop fabrication and physical computing. This past academic year, Shane was a [Digital Humanities graduate fellow](http://scholarslab.org/graduate-fellowships/) and worked on software to study networks of information exchange related to cryptography on Usenet lists. That fellowship work contributes to his doctoral work in History at UVA, and his dissertation on the history of cryptography and evolving notions of privacy since 1975. \n\nIn addition to being an incredible developer and scholar, Shane is a talented photographer, and has taken nearly all the photos of our staff and students. Come by the Lab to say hi to Shane, or welcome him via email at ssl2ab at virginia.edu."},{"id":"2017-08-24-cfp-pmla-special-issue-varieties-of-digital-humanities","title":"CFP: PMLA Special Issue, Varieties of Digital Humanities","author":"alison-booth","date":"2017-08-24 09:08:00 -0400","categories":["uncategorized"],"url":"cfp-pmla-special-issue-varieties-of-digital-humanities","content":"I want to call attention to the opportunity to publish your work in the leading journal in literary studies.  Miriam Posner and I will be co-editing a special issue on digital humanities, and we very much welcome varieties of approaches as well as topics.  *PMLA* has a very strenuous and blind peer review process that gives ample feedback--usually, it's well worth this feedback even if excellent work, in the end, doesn't make the difficult cut.  But that also means, in other words, that it's not just up to the two of us to decide what will actually appear in the journal.  We would be happy to advise on the kinds of submissions you might send in.  Feel free to reach out at booth@virginia.edu.  Here is the CFP wording that appears at the above site, where you may find instructions on how to prepare and submit the 9000-word-maximum document file. \n\nDeadline for submissions: 12 March 2018 \n\nCoordinators: Alison Booth (Univ. of Virginia) and Miriam Posner (Univ. of California, Los Angeles) \n\nDigital humanities (DH) may not be a full-fledged discipline, but it has advanced beyond “the next big thing” to become a reality on many campuses. Like many fields that have received a great deal of attention, DH derives energy from internal combustion and external friction—dissenters, supporters, and detractors see different sides of what may after all be too large a variety of practice to cohere as a field in the future. This moment, then, seems a good time to ask, What is next for DH? And what can we learn from what has come before? _PMLA_ invites essays that will help assess the past of DH, outline its current state, and point to its future directions among diverse participants, allies, and critics. The special issue welcomes well-informed critical essays that articulate varieties of digital experience with DH as it is commonly understood and as it is practiced in a more expansive, even contested, way, including but not limited to the following topics: game studies; digital narrative and poetry; social media and blogging; digital arts, including music and theater; digital pedagogy in languages, literatures, and writing (teaching with technology, e-portfolios, immersive technology, mapping assignments); textual editing; edited digital archives of manuscript or print materials; natural language processing and textual analysis of large corpora such as historical newspapers or a genre or a literary era; prosopographies, from ancient to modern; 3-D printing or modeling; virtual reality and photogrammetry documenting cultural heritage sites or artifacts; mapping and time lines to visualize trends in cultural or literary history; issues of copyright and commercial databases; theories and histories of digital technologies and their industrial and cultural impact; the growing field of criticism on digital scholarship and institutional change; advocacy or cultural criticism oriented toward new media and transformative practice. \n\nThe _PMLA_ Editorial Board welcomes collaborative or single-author essays that take note of digital humanities of these or other varieties, whether centered on education or other spheres, whether ephemeral or long-standing. Submissions that consider a specific project should go beyond reporting on its methods and findings and emphasize its implications for digital literature and language scholarship. Of particular interest are reflections on DH as practiced beyond North America and Europe. Issues and themes might include accessibility, sustainability, standards of evidence, transforming the academic career, changing or pursuing further the abiding questions in the discipline. Histories, predictions, and manifestos may be welcome, but all essays should be accessible and of interest to the broad _PMLA_ readership.   https://www.mla.org/Publications/Journals/PMLA/Submitting-Manuscripts-to-PMLA"},{"id":"2017-08-30-2017-virginia-higher-ed-gis","title":"2017 Virginia Higher Ed GIS Meeting","author":"chris-gist","date":"2017-08-30 08:13:00 -0400","categories":["Announcements","geospatial and temporal"],"url":"2017-virginia-higher-ed-gis","content":"November 2, 2017 – 10am to 3pm (check in begins at 9:30am)\n\nScholars’ Lab, Alderman Library - University of Virginia – Charlottesville, VA \n\nA meeting of all Virginia higher education Esri/GIS representatives and other GIS support people This meeting is for Esri designates and other GIS support staff to come together to discuss common needs and solutions.  We will kick off with a plenary talk from an Esri representative.  Then in an “unconference” format, the group will decided the topics for the remainder of the day.  Depending on interest and need, we will break into groups for further discussions. \n\nRegistration (required): [https://tinyurl.com/VAgis2107](https://tinyurl.com/VAgis2107)\n\n<span style=\"text-decoration: underline;\">Schedule</span>\n\n9:30am – Check in Begins \n\n10am – Plenary Session w/ Esri Education Account Manager - Ridge Waddell (tentative) \n\n11am – Group Topic Discussion Decision Making \n\n11:30am – Lunch \n\nNoon – Topic Discussions – break-outs if necessary \n\n2:45pm – Group Next Steps\n\n3pm – Adjourn \n\n**NOTE:**  Lunch is being provided by the UVa Library’s Scholars’ Lab.  Because of this, we ask that everyone register in advance.  It is assumed that everyone will drive in for the day and not stay in Charlottesville.  However, we are happy to provide hotel information.  More details on parking, etc. to follow to registered participants.  If you have questions about anything, please feel free to contact Chris Gist at [cgist@virginia.edu](mailto:cgist@virginia.edu)."},{"id":"2017-09-01-digital-humanities-fellows-applications-2018-2019","title":"Digital Humanities Fellows Applications - 2018-2019","author":"brandon-walsh","date":"2017-09-01 10:36:00 -0400","categories":["Announcements","digital humanities","grad student research"],"url":"digital-humanities-fellows-applications-2018-2019","content":"_[Read closely: our menu options have changed. Note especially the changes to the application timeline, eligibility, and funding structure of the fellowship. Questions should be directed to [Brandon Walsh](mailto:bmw9t@virginia.edu), Head of Graduate Programs for the Scholars' Lab.]_ \n\nWe are now accepting applications for the 2018-2019 DH Fellows Cohort! Applications are due **Wednesday, November 1st**. The Digital Humanities Fellowship supports advanced doctoral students doing innovative work in the digital humanities at the University of Virginia. The Scholars’ Lab offers Grad Fellows advice and assistance with the creation and analysis of digital content, as well as consultation on intellectual property issues and best practices in digital scholarship and DH software development. \n\nFellows join our vibrant community, have a voice in intellectual programming for the Scholars’ Lab, make use of our dedicated grad office, and participate in one formal colloquium at the Library per fellowship year. As such, students are expected to be in residence on Grounds for the duration of the fellowship. \n\nSupported by the Jeffrey C. Walker Library Fund for Technology in the Humanities, the Matthew & Nancy Walker Library Fund, and a challenge grant from the National Endowment for the Humanities, the highly competitive Graduate Fellowship in Digital Humanities is designed to advance the humanities and provide emerging digital scholars with an opportunity for growth. The award provides living support in the amount of $20,000 for the academic year, as well as full remission of tuition and University fees and the student health insurance premium for single-person coverage. Living support includes wages for a half-time graduate teaching assistantship in each semester.  A graduate instructorship, particularly one with a digital humanities inflection, may be substituted for the GTA appointment based on availability within the fellow’s department. Applicants interested in such an option should indicate as such in their application and discuss the possibility in advance with [Brandon Walsh](mailto:bmw9t@virginia.edu). See past fellowship winners [on our People page](http://scholarslab.org/people/). The call for applicants is issued annually in August. \n\n**Eligibility, Conditions, and Requirements**\n\n*   Applicants must be ABD, having completed all course requirements and been admitted to candidacy for the doctorate in the humanities, social sciences or the arts at the University of Virginia.\n*   The fellowship is provided to students who have exhausted the financial support offered to them upon admission. As such, students will typically apply during their fifth year of study or beyond for a sixth year of support.*\n*   Applicants are expected to have digital humanities experience, though this background could take a variety of forms. Experience can include formal fellowships like the [Praxis Program,](http://praxis.scholarslab.org/) but it could also include work on a collaborative digital project, comfort with programing and code management, public scholarship, or critical engagement with digital tools.\n*   Applicants must be enrolled full time in the year for which they are applying.\n*   A faculty advisor must review and approve the scholarly content of the proposal.\n\n**How to Apply**\n\nA complete application package will include the following materials:\n\n*   a cover letter, addressed to the selection committee, containing:\n    *   a summary of the applicant’s plan for use of digital technologies in his or her dissertation research;\n    *   a summary of the applicant’s experience with digital projects;\n    *   and a description of UVa library digital resources (content or expertise) that are relevant to the proposed project;\n*   a [Graduate Fellowship Application Form;](http://scholarslab.org/wp-content/uploads/2017/09/dhfellowsappform.pdf)\n*   a dissertation abstract;\n*   and 2-3 letters of nomination and support, at least one being from the applicant’s dissertation director who can attest to the project’s scholarly rigor and integration within the dissertation.\n\nQuestions about Grad Fellowships and the application process should be directed to [Brandon Walsh](mailto:bmw9t@virginia.edu). Applicants concerned about their eligibility, for whatever reason, are strongly encouraged to write as well. \n\n\\* Please note that, per University policy, a student who has undertaken affiliate status and ceased to enroll full time is not eligible to resume full-time enrollment or hold a graduate teaching assistantship.  Because GTA appointments are a component of the DH Fellowship, students who have already undertaken affiliate status are not eligible to be considered for this award."},{"id":"2017-09-06-hello-world-4","title":"Hello World!","author":"spyridon-simotas","date":"2017-09-06 08:47:00 -0400","categories":["digital humanities","experimental humanities"],"url":"hello-world-4","content":"My name is Spyros Simotas and I am a PhD candidate at the French Department at UVa. This year, I am also a Praxis fellow at the Scholars’ Lab. In this first blog post I would like to briefly introduce myself honoring [Brandon’s](http://scholarslab.org/people/brandon-walsh/%20 \"Brandon\") ice-breakers. Brandon always comes to our meetings with an ice-breaker. Here are the three we have had so far:\n\n1.  Which is your favorite animal?\n2.  Which is your favorite plant?\n3.  Who would you like to have dinner with, dead or alive?\n\nMy favorite animals are elephants, my favorite plants are palm trees and if I could have a meal with anyone dead or alive, I would like to have coffee with David Lynch. I like elephants because they are big, they make the sound of a trumpet and they care about each other. Despite their size, elephants do not pose a threat to other beings. They are also smart and [they can paint](https://www.youtube.com/watch?v=7meBvOEyuzQ \"Elephants painting\"). Has anyone ever calculated the size ratio between an elephant and an average-sized bug? Bugs are the most common wild life form we are stuck with in the industrialized and post-industrialized world. Domesticated farm animals that we use for food or pets don’t count. We are stuck with bugs both literally and metaphorically. Unfortunately, I have never seen an elephant hanging from the wall, or lurking inside a piece of software. I have seen palm trees! The reason I like them is because of their simple shape. Their trunk doesn’t branch out, it only ends with a crown of leaves, like a messy toupee. Palm trees are easy to draw. When I lived in California, I remember that sometimes, their tops would disappear in the early morning mist. Also, three cut out palm trees figure on the cover of The Cure's [Boys Don’t Cry](https://en.wikipedia.org/wiki/Boys_Don't_Cry_(The_Cure_album) \"Boys Don't Cry by The Cure\") as a fine representation of their iconic hair style. Which brings me to David Lynch and his own impeccably messy hairdo. Having begun his career with Eraserhead, it is hard to tell whose, his character’s or his own hair, is the source of inspiration for this electrified spiky hair style. Since then, he has created a lot of strange and heartbreaking characters. Joseph Merrick’s story, better known as [The Elephant Man](https://www.youtube.com/watch?v=2ToC4vh_itg \"The Elephant Man Trailer\"), [The Staight Story](https://en.wikipedia.org/wiki/The_Straight_Story \"The Staight Story Wikipedia Page\"), not to mention all the characters from his early 90’s TV series Twin Peaks revived recently, 25 years later, for a third and final season. Thanks to his book on meditation, consciousness and creativity, I was also introduced to [TM](https://youtu.be/BH4qD5Fzyjk?t=12m1s \"David Lynch explaining TM\"). It is a small book, called [Catching the big fish](https://www.davidlynchfoundation.org/catching-the-big-fish-meditation-consciousness-and-creativity.html \"David Lynch's book\"), very easy to read and highly recommended. As an ending to this post, I chose the following excerpt from the chapter \"The Circle\" where Lynch refers to the feedback loop between an art work and its audience.\n\n> “I like the saying: “The world is as you are.” And I think films are as you are. That’s why, although the frames of a film are always the same—the same number, in the same sequence, with the same sounds—every screening is different. The difference is sometimes subtle but it’s there. It depends on the audience. There is a circle that goes from the audience to the film and back. ... So you don’t know how it’s going to hit people. But if you thought about how it’s going to hit people, or if it’s going to hurt someone, or if it’s going to do this or do that, then you would have to stop making films.\" [^1]\n\nI think the same can be said about digital humanities. Our public scholarship, experiments, code, teaching, and service, also reflect who we are and reverberate with our audience. In our first Praxis meeting, we talked about impact, trying to pinpoint the idea of success. But ultimately, we don't know \"how it’s going to hit people.\" In which case, it is always useful to remember the well-known Marshall McLuhan scheme of technology as an extension of certain urges or desires. It is important to understand what is the urge that we are trying to extend because technology, according to [Jonathan Harris](https://youtu.be/yS1a5TBya14?t=34m10s \"Jonathan Harris, Ethics of Code\") (who also came up in our first discussion), can have \"dramatic effects\" on people. That's why, he calls for \"a self-regulated ethics that comes from the mind and the heart of the creator.\" Finding our own common interests and desires as a team will help us define the direction we want our project to go. At this early stage, we only know that we want to work with data from the Library, using technology to create new interactions with the archive. But it is with the principles of love, care and good intentions that we embark on this year's Praxis adventure.\n\n[^1]:  David Lynch, Catching the Big Fish: Meditation, Consciousness, and Creativity, 2016\n"},{"id":"2017-09-10-2-about-my-research-computers-and-digital-humanities","title":"About my research, computers and Digital Humanities","author":"spyridon-simotas","date":"2017-09-10 17:24:00 -0400","categories":["uncategorized"],"url":"2-about-my-research-computers-and-digital-humanities","content":"In my inaugural post a few days ago, I introduced myself to the world in kind of an oblique way. Some people may wonder what I am studying or what my research interests are. This post is here to mend this omission. In large brush strokes, I will talk about my dissertation and then about some general research interests that connect me to digital humanities. Coincidentally, a brief mention of a computer prototype from the late 60’s will echo for the Praxis folks our last meeting (Sept. 5, 2017) and the lesson on the history of computers.\n\nMy current project focuses on three French contemporary authors who are using new technologies to create and disseminate their work, as well as connect with their audience. More specifically, I am looking at the ways in which new technologies expand the boundaries of literature to include practices often reserved to other artistic disciplines. I am also interested in the new online literary communities clustering around the websites of my corpus and in the margins of the print and prize-driven French literature.\n\nHaving escaped the pages of the book, literature meets with visual arts, with sound and performance, in new poetic hybrids. The book is always a place where textual content can return to, but it is not the only option. Moreover, various acts of transcoding, made possible through digital technologies, have liberated writing from its exclusive attachment to text. Our contemporary “associated technical milieu” has made the creative gesture a practice available to anyone with a computer connected to the Internet.\n<blockquote>“Rather than dissociating consumption from production, as did broadcast mass media (from phonography to global real-time television), today’s microtechnologies and the social networking practices they facilitate connect them: if you can use these technologies to consume, you can also use them to produce.”<sup><a id=\"ffn1\" class=\"footnote\" href=\"#fn1\">1</a></sup></blockquote>\nInterestingly enough, the gap between amateurs and professionals is narrowing , which revives Jean Dubuffet’s concept of “art brut” (i.e. art made by people without formal training). Under these circumstances where <em>everything is created by everybody</em>, how does a contemporary author find her place? How does she define her space and the value of her work?\n\nKenneth Goldsmith dubbed these practices “uncreative writing” and traced their origin to some French avant-garde techniques such as those invented by the Situationists (<em>détournement</em>, psychogeographical drifts) and Oulipo.\n<blockquote>“Oulipo, short for Ouvroir de littérature potentielle, or ‘Workshop for Potential Literature’ was founded fifty years ago, in 1960, by the writer Raymond Queneau and the mathematician François Le Lionnais with the purpose of exploring the possible uses of mathematics and formal modes of thought in the production of new literature. Oulipo sought to invent new kinds of rules for literary composition, and also to explore the use of now-forgotten forms in the literatures of the past. ”<sup><a id=\"ffn2\" class=\"footnote\" href=\"#fn2\">2</a></sup></blockquote>\nGeorges Perec, one of the most popular authors among the Oulipo group (<a title=\"(2817) Perec\" href=\"http://www.minorplanetcenter.net/db_search/show_object?utf8=%E2%9C%93&amp;object_id=2817\">the star!</a>), has experimented with algorithmic writing, imitating the inner workings of a computer program, in <em>The art and craft of approaching your head of department to submit a request for a raise</em>,<sup><a id=\"ffn3\" class=\"footnote\" href=\"#fn3\">3</a></sup>or with extreme self-imposing lipogrammatic constraints in <em>A Void</em><sup><a id=\"ffn4\" class=\"footnote\" href=\"#fn4\">4</a></sup> (exclusively composed of words that don’t contain the letter “e”), has also written a <a title=\"Le computeur pour tous\" href=\"http://escarbille.free.fr/vme/?txt=lcpt\">a very brief enthusiastic text</a> about computers. Published at a time where computers were still the size of a room, Perec anticipated their everyday personal and social use. “Why not us?” he asks, claiming a programmable machine for creative purposes at home, a place already targeted by a horde of appliances: washing machines and toasters, coffee makers and vacuum cleaners, TV sets and food processors.\n<h3>A dynamic medium for creative thought: the Dynabook</h3>\nAround the same time, at the Palo Alto Xerox PARC Alan Kay and Adele Goldberg were working on a prototype computer strikingly similar to a today’s tablet. They called it <a title=\"Wikipedia page for Dynabook\" href=\"https://en.wikipedia.org/wiki/Dynabook\"><em>Dynabook</em></a> (portmanteau for dynamic book) and they imagined it as\n<blockquote>“a self-contained knowledge manipulator in a portable package the size and shape of an ordinary notebook. Suppose it had enough power to outrace your senses of sight and hearing, enough capacity to store for later retrieval thousands of page-equivalents of reference materials, poems, letters, recipes, records, drawings, animations, musical scores, waveforms, dynamic simulations and anything else you would like to remember and change.” <sup><a id=\"ffn5\" class=\"footnote\" href=\"#fn5\">5</a></sup></blockquote>\n<em>Dynabook</em>, unlike any other computer of its generation, was not targeting the military or corporate business. It was designed “for kids of all ages”, people who would use it to enhance their learning and creativity. I want to emphasize the last words here: “to remember and change”. If the computer was to become personal, it was not only because of its capacity to store information, archiving one’s files, and consequently exteriorizing and extending one’s memory but also by offering new techniques to process the information stored and eventually to create new. Technology has always been about extending human capabilities.\n<blockquote>“The human evolves by exteriorizing itself in tools, artifacts, language, and technical memory banks. Technology on this account is not something external and contingent, but rather an essential—indeed, the essential—dimension of the human.” <sup><a id=\"ffn6\" class=\"footnote\" href=\"#fn6\">6</a></sup></blockquote>\nAs a matter of fact, the idea of a mechanical memory storage was not new. Vannevar Bush in his well-known article <a title=\"As We May Think\" href=\"http://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/\">“As we may think”</a> published in 1945 had already introduced a mechanical memory (memex) for individual use <sup><a id=\"ffn7\" class=\"footnote\" href=\"#fn7\">7</a></sup>. Beyond the scope of the <a title=\"Wikipedia Universal Turing Machine\" href=\"https://en.wikipedia.org/wiki/Universal_Turing_machine\">Universal Turing Machine</a> –a machine that could simulate other machines– Alan Kay and Adele Goldberg’s ambition was to create a <em>Universal Media Machine</em>, a machine that could simulate all other media forms, from books to images to films.\n<blockquote>“For educators, the <em>Dynabook</em> could be a new world limited only by their imagination and ingenuity. They could use it to show complex historical inter-relationships in ways not possible with static linear books. Mathematics could become a living language in which children could cause exciting things to happen. Laboratory experiments and simulations too expensive or difficult to prepare could easily be demonstrated. The production of stylish prose and poetry could be greatly aided by being able to easily edit and file one's own compositions.” <sup><a id=\"ffn8\" class=\"footnote\" href=\"#fn8\">8</a></sup></blockquote>\nBut in order to achieve this goal of becoming a ”platform for <em>all</em> existing expressive artistic media”, <em>Dynabook</em> had to exceed its function as a storing machine, by adding a new structural level on top of the hardware allowing an easy interaction with the machine. Hence, GUI was born with tools and icons that could help the user perform the same actions across applications, without needing to know the underlying programmatic commands.\n<blockquote>“Putting all mediums within a single computer environment does not necessarily erase all differences in what various mediums can represent and how they are perceived—but it does bring them closer to each other in a number of ways. Some of these new connections were already apparent to Kay and his colleagues; others became visible only decades later when the new logic of media set in place at PARC unfolded more fully; some may still not be visible to us today because they have not been given practical realization. One obvious example of such connections is the <strong>emergence of multimedia</strong> as a standard form of communication: web pages, PowerPoint presentations, multimedia artwork, mobile multimedia messages, media blogs, and other communication forms which combine multiple mediums. Another is the adoption of <strong>common interface conventions and tools which we use in working with different types of media regardless of their origin:</strong> for instance, a virtual camera, a magnifying lens, and of course the omnipresent copy, cut and paste commands. Yet another is <strong>the ability to map one media into another</strong> using appropriate software—images into sound, sound into images, quantitative data into a 3D shape or sound, etc.—used widely today in such areas as DJ/VJ/live cinema performances and information visualization. All in all, it is as though different media are actively trying to reach towards each other, exchanging properties and letting each other borrow their unique features. ” <sup><a id=\"ffn9\" class=\"footnote\" href=\"#fn9\">9</a></sup></blockquote>\nThe success of the personal computer was therefore due to its structural coupling with software that led –so far– to three major shifts in the way we interact with media. Word processors to movie editors, allowed the user to mix, juxtapose, cut and paste, alter, and eventually produce new media. Using the same machine to perform changes in the stored contents was an empowering new form of grammatization.\n<h3>Return to kindergarten</h3>\nI borrow the concept of grammatization from Bernard Stiegler. Derrida's former student, Stiegler calls grammatization every flow that becomes a process through a series of discrete marks, <em>grammés</em>, that can form a code (grammar) and can be endlessly reproduced in all sorts of combinations. Writing, for example, is the grammatization of speech and it is made possible by the invention of the letters (<em>grammata</em> ) of the alphabet. Alphanumeric linear writing, up until personal computers came along, was the dominant form of recording, from facts (history) to thoughts and ideas (literature). So much so that the activities of learning to read and write were the main literacy focus of a certain humanistic tradition, from grade school to the academy.\n\nIn his seminal book <em>Does Writing Have A Future?</em>, Vilém Flusser speculates on the disruption of this tradition brought forth by the computers and their new ways of writing through digital recording and digitization. Without discarding the value of the alphanumeric writing he embraces the possibility of new forms of writing that could lead to a progressive replacement of “the alphabet or Arabic numerals”.\n<blockquote>What was once written can now be conveyed more effectively on tapes, records, films, videotapes, videodisks, or computer disks, and a great deal that could not be written until now can be noted down in these new codes. … Many people deny this … They have already learned to write, and they are too old to learn the new codes. We surround this … with an aura of grandeur and nobility.</blockquote>\nFlusser foresees with a great clarity what is yet to come when he publishes his book in 1987. What may seem as a radical stance, results from his position not to resist or reject the new technologies, but to discover their creative and pedagogical potential altering and adding new avenues to the the millennia old practices of reading and writing. But the newness of these tools, their sometimes <a href=\"https://github.com/alex/what-happens-when\">complex inner workings</a> call for a return to kindergarten.\n<blockquote>We have to go back to kindergarten. We have to get back to the level of those who have not yet learned to read and write. In this kindergarten, we will have to play infantile games with computers, plotters, and similar gadgets. We must use complex and refined apparatuses, the fruit of a thousand years of intellectual development, for childish purposes. It is a degradation to which we must submit. Young children who share the nursery with us will surpass us in the ease with which they handle the dumb and refined stuff. We try to conceal this reversal of the generation hierarchy terminological gymnastics. While we’re about this boorish non-sense, we don’t call ourselves Luddite idiots but rather progressive computer artists. <sup><a id=\"ffn10\" class=\"footnote\" href=\"#fn10\">10</a></sup></blockquote>\nIsn’t it the “digital turn” that Flusser anticipated with his “infantile games with computers”? And isn’t it Flusser’s kindergarten spirit that lives in labs and DH centers across the academy? Similarly, most recent “making turn” also happens in the same centers and labs.\n<blockquote>”As the historian David Staley explains, the “maker turn” introduces “an approach to the humanities that moves our performances off the page and the screen and onto the material world, a hermeneutic performance whereby humanists create non-textual physical objects.” <sup><a id=\"ffn11\" class=\"footnote\" href=\"#fn11\">11</a></sup></blockquote>\nInspired by Patrick Jagoda’s recent article on “Critique and Critical Making”, this year’s Praxis cohort is set to explore the intersection of DH and the bricolage of physical computing. Taking the cue from Pierre Bayard’s <em>How to talk about books you haven’t read</em><sup><a id=\"ffn12\" class=\"footnote\" href=\"#fn12\">12</a></sup> , we have been wondering \"how to make books you haven’t read talk!\" But more about it in the next post. Stay tuned!\n<ol id=\"footnotes\">\n \t<li id=\"fn1\">Excerpt from Mark B. N. Hansen’s introduction to Bernand Stiegler’s chapter on Memory published in W. J. T Mitchell et Mark B. N Hansen, <em>Critical Terms for Media Studies</em> (Chicago; London: The University of Chicago Press, 2010). <a href=\"#ffn1\">↩</a></li>\n \t<li id=\"fn2\">David Bellos in his introduction to Georges Perec’s The art and craft of approaching your head of department to submit a request for a raise, (London; New York: Verso Books, 2011). <a href=\"#ffn2\">↩</a></li>\n \t<li id=\"fn3\">Georges Perec, The art and craft of approaching your head of department to submit a request for a raise, trad. par David Bellos (London; New York: Verso Books, 2011). <a href=\"#ffn3\">↩</a></li>\n \t<li id=\"fn4\">Georges Perec, A Void (London: Harvill, 1994). <a href=\"#ffn4\">↩</a></li>\n \t<li id=\"fn5\">Lev Manovich, Software Takes Command: Extending the Language of New Media, International Texts in Critical Media Aesthetics 5 (New York, NY: Bloomsbury, 2013). <a href=\"#ffn5\">↩</a></li>\n \t<li id=\"fn6\">Mark Hansen’s Introduction to Bernard Stiegler’s article on Memory, in W. J. T Mitchell et Mark B. N Hansen, Critical Terms for Media Studies (Chicago; London: The University of Chicago Press, 2010). <a href=\"#ffn6\">↩</a></li>\n \t<li id=\"fn7\">Consider a future device for individual use, which is a sort of mechanized private file and library. It needs a name, and, to coin one at random, \"memex\" will do. A memex is a device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory. <a href=\"#ffn7\">↩</a></li>\n \t<li id=\"fn8\">Personal Dynamic MediaAlan Kay, Adele Goldberg<a href=\"http://www.vpri.org/pdf/m1977001_dynamedia.pdf\">http://www.vpri.org/pdf/m1977001_dynamedia.pdf</a> <a href=\"#ffn8\">↩</a></li>\n \t<li id=\"fn9\">Lev Manovich, Software Takes Command: <em>Extending the Language of New Media</em>, International Texts in Critical Media Aesthetics 5 (New York, NY: Bloomsbury, 2013). Emphasis mine. <a href=\"#ffn9\">↩</a></li>\n \t<li id=\"fn10\">Vilém Flusser et Mark Poster, Does Writing Have a Future?, Electronic Mediations, v. 33 (Minneapolis: University of Minnesota Press, 2011). <a href=\"#ffn10\">↩</a></li>\n \t<li id=\"fn11\">Patrick Jagoda, « Critique and Critical Making », PMLA 132, no 2 (1 mars 2017): 356‑63, doi:10.1632/pmla.2017.132.2.356. <a href=\"#ffn11\">↩</a></li>\n \t<li id=\"fn12\">Pierre Bayard, How to talk about books you haven’t read (New York, NY: Bloomsbury USA : Distributed to the trade by Holtzbrinck Publishers, 2007). <a href=\"#ffn12\">↩</a></li>\n</ol>\n"},{"id":"2017-09-12-etcrc-local","title":"/etc/rc.local","author":"christian-howard","date":"2017-09-12 17:24:00 -0400","categories":["uncategorized","digital humanities","grad student research"],"url":"etcrc-local","content":"<img class=\"alignnone wp-image-13760 aligncenter\" src=\"http://scholarslab.org/wp-content/uploads/2017/09/ch0-300x95.png\" alt=\"\" width=\"565\" height=\"179\" />\n\nHello again, my fine digital-humanist friends! It’s a delight to be back in the Scholars’ Lab this year!\n\nFor those who don’t know me, my name is Christian Howard, and I am a PhD Candidate at UVA in English literature and one of the 2017-2018 Praxis Fellows. If you do happen to know me, you might also know that I was fortunate to work in the Makerspace of the Scholars’ Lab last year. In any case, I’m excited to combine the knowledge that I gained there working on hands-on, material projects with finer computer skills and the even greater conceptualizations into which I expect our Praxis team will delve.\n\nI’ve recently been rereading Johanna Drucker’s <em>Graphesis: Visual Forms of Knowledge Production</em>, and I want to reflect briefly on one of Drucker’s points, which I think is especially central to our Praxis team this year. Drucker brilliantly exposes “data” as constructs, constructs that cannot “pre-exist their parameterization.” As such, Drucker opts for the alternative term, “capta,” stating: “<em>Data are capta</em>, taken not given, constructed as an interpretation of the phenomenal world, not inherent in it” (128). <em>Capta </em>comes from the Latin verb <em>capio</em>, <em>capere</em>, which, translated literally, means “to capture, take, seize.” Yet in a more figurative sense, <em>capere </em>could also mean “to take in, understand.” It is partly because of this pun that I find Drucker’s redefinition particularly apt, for it is precisely the act of “capturing” information that facilitates our understanding of that information. In other words, every decision to define the parameters under which “data” will be taken is itself an interpretive strategy.\n\nSo what does this mean for humanists, and digital humanists in particular? I’ll quote Drucker again, this time at length:\n\n“To expose the constructedness of data as capta a number of systematic changes have to be applied to the creation of graphical displays. That is the foundation and purpose of a <em>humanistic approach </em>to the qualitative display of graphical information. That last formulation should be read carefully, <em>humanistic approach</em> means that the premises are rooted in the recognition of the <em>interpretive </em>nature of knowledge, that this <em>display </em>itself is conceived to <em>embody qualitative expressions</em>, and that the <em>information </em>is understood as <em>graphically constituted</em>” (128-129).\n\nIt is this recognition – namely, in the fundamentally interpretive nature of data-as-capta – that distinguishes the humanities as a discipline.\n\nAs a Praxis cohort, we are still working to define the shape that our project will take; nonetheless, in developing our charter or mission statement, we have unanimously agreed that transparency is of the utmost importance to us. As such, we are committed not only to sharing the result of our collaboration with the public, but also to showing the processes through which our project develops, thereby enabling anyone to trace the interpretations and assumptions underlying our own work.\n\nWell, that’s all the heavy-lifting for today. For those of you who found this introductory post too lengthy, I’ve provided a handy summary for you below:\n\n<strong>TL;DR: </strong>Born at a young age, I have pursued my education in order to justify my caffeine-dependency. Most recent greatest achievement? I’ve just beaten my all-time personal record of most consecutive days lived! Time to celebrate with some coffee and chocolate.\n\n&nbsp;\n\nDrucker, Johanna. <em>Graphesis: Visual Forms of Knowledge Production</em>. Cambridge: Harvard University Press, 2014."},{"id":"2017-09-18-welcome-new-dh-developer-zoe-leblanc","title":"Welcome new DH Developer Zoe LeBlanc!","author":"amanda-visconti","date":"2017/09/18 08:51:03+00:00","categories":null,"url":"welcome-new-dh-developer-zoe-leblanc","content":"# Welcome new DH Developer Zoe LeBlanc!\n\nWe are delighted to announce that Zoe LeBlanc has accepted our [DH Developer position](/2017/06/27/job-opening-curious-about-focusing-on-dh-development/)! Zoe rose to the top of an extremely strong pool of over 60 applicants. A History ABD at Vanderbilt University, she focuses on post-colonialist movements and media in Cairo and other capitals. She brings solid technical experience in the areas of front-end web design, text and image analysis, and mapping and data visualization, with skills including React, Redux, Elixir, and Postgres, and fluency in French and Arabic. Zoe is a rising junior DH scholar, presenting on [network analysis at a well-attended panel](https://dh2017.adho.org/abstracts/428/428.pdf) at DH2017 in Montreal, as well as through [a DH2017 poster on an archival research app](https://dh2017.adho.org/abstracts/548/548.pdf) she learned to build in response to archival research challenges. Her particular expertise and passion for making technically difficult DH methods accessible and enjoyable to all complements the SLab's emphasis on pedagogy and mentorship. She balances the SLab's literature scholars and complements our history scholars, both diversifying our areas of work to the Middle East and adding new expertise in archival research in countries with different archival practices and challenges from the U.S. Come by the Lab once Zoe joins us in mid-October to say hi!\n"},{"id":"2017-09-19-crafting-our-charter-praxis-program-2017-2018","title":"Crafting Our Charter - Praxis Program 2017-2018","author":"monica-blair","date":"2017-09-19 07:28:11 -0400","categories":null,"url":"crafting-our-charter-praxis-program-2017-2018","content":"# Crafting Our Charter - Praxis Program 2017-2018\n\nAs a historian, when I think of charters, the first things I think of are royal charters.\n\n![](http://scholarslab.org/wp-content/uploads/2017/09/Virginia-Company-charter-16061-300x208.jpg)![](http://scholarslab.org/wp-content/uploads/2017/09/pocahontas-21-300x174.png)\n\nThe first result when you Google charter, on the other hand, is Charter Telecommunications Company because _of course_.\n\nBut as members of the new Praxis Fellowship cohort, my fellow fellows and I tried to chart (I’m sorry) a very different path. The result of our work, The Praxis Charter, 2017-2018, is the first thing we ever created together. <http://praxis.scholarslab.org/charter/charter-2017-2018/> Transparency is one of our core values, so I am going to use this post to reveal the process by which we made this document. Our charter's first draft was written in a jam session in a Scholars' Lab meeting room, and the fact that we are all teachers was readily evident. We privately brainstormed, we paired and shared those ideas, and then we had a class discussion with Christian at the technological helm. I often think of grad school as a lesson in liminality. ![](http://scholarslab.org/wp-content/uploads/2017/09/0bc2317892d97c002c86f61fd5fa3aba-300x130.jpg)\n\n\"Piled Higher and Deeper\" by Jorge Cham www.phdcomics.com\n\nThat was on full display as we drew on the techniques we use to facilitate classroom discussions to jumpstart our own collaborative work. The liminality of grad school isn't always to its credit, but in this case, the results were lovely. As a teacher and a historian of education, I spend a lot of time thinking about pedagogy. The pedagogy modeled here made my heart happy! We melded the skill sets of both teachers and students pretty seamlessly to create a productive partnership. Our conversation always seemed to come back to values. Values are, I think, the core of this document. Of course, for every positive value, there is an equal and opposite disvalue. The opposite of humility is egotism. The opposite of flexibility is rigidity. The opposite of transparency is obfuscation. I think this connects to a comment my fellow Torie made, that writing this charter was almost cathartic, because we could list every problem we had encountered with group work and essentially say: _not that_. This, of course, points to the idea of conflict. As our joyful leader Brandon Walsh noted, past Praxis cohorts have tended to avoid naming conflict in their charters in the hopes that their silence would prevent it from ever rearing its ugly head. Think of conflict as the he-who-must-not-be-named of group work, if you will. ![](http://scholarslab.org/wp-content/uploads/2017/09/AAEAAQAAAAAAAAiEAAAAJDk1Yjg4OTEzLTg0MzQtNDYwYi1iZjM4LWE5ZjFjZGNhNzkwYw-300x161.jpg) Ignoring conflict didn't really work out for the Ministry of Magic though, and I doubt that the academy fairs much better. My hope is that by setting out clear goals, values, and strategies for coping with conflict we will enable our future selves to handle disagreements with aplomb and grow from them, rather than shrink from them. Perhaps the most radical value embodied in our charter is our commitment to \"the creation of a participatory democracy.\" Participatory democracy is an idea coined by one of my favorite historical figures, civil rights and feminist icon Ella Baker. Participatory Democracy embraces two ideas, \"[a decentralization of authoritative decision-making and a direct involvement of amateurs or non-elites in the political decision-making process.](https://www.american.edu/spa/publicpurpose/upload/Partcipatory-Democracy-The-Bridge-from-Civil-Rights-to-Women-s-Liberation.pdf)\" Participatory democracy seems like the perfect fit for the Praxis Program as we are all relative amateurs in the digital humanities, and we have been given the task of working and learning together. It also just seems to fit our collective personality. When we talked about past Praxis strategies, we decided we didn't want to divide and conquer the tasks ahead like many previous years had. We wanted to work on individual elements of our project together so that we could get the most out of our training. This would also allow us to commit to a truly shared vision. In so many ways, a charter is a reminder of our deeply held values. We all carry around ideals of honesty and creativity, kindness and diversity, but writing out a charter makes you actually reflect on those values and why you hold them dear. Writing a charter allows you to reflect on what it is you like about collaborative work - and what it is you don't, and then make a promise to yourself and to others to try and embody the best of what collaboration has to offer. As for our radical experiment in participatory democracy, I can already hear people asking, is that practical? The true answer is: I don't know. But Praxis seemed like just the place to try it out.\n"},{"id":"2017-09-21-how-to-make-books-you-havent-read-talk","title":"How to make books you haven’t read, talk.","author":"spyridon-simotas","date":"2017/09/21 11:02:18+00:00","categories":null,"url":"how-to-make-books-you-havent-read-talk","content":"# How to make books you haven’t read, talk.\n\nAs promised in [my previous post](/2017/09/10/2-about-my-research-computers-and-digital-humanities/), here is an idea for this year’s Praxis Program. It is uncertain at this early stage of brainstorming whether it will be retained as the one uniting everybody’s creative forces and ingenuity, but I believe it has a lot of potential of unfolding into a project where everybody’s common interests meet: library’s holdings, global culture and world languages, power and inequality, literature and sound. At its core, it is as humanistic as it can be, and its execution requires the use of common digital humanities and critical making techniques, that we are here to train for. But before I get to the idea, let me take you to a journey where books are no longer written, nor pressed into rectangular objets made out of ink and paper and they are by no means meant to be read.\n\n## The end of books\n\nMore than a hundred years ago, at the turn of the 19th century, Octave Uzanne, a French bibliophile and journalist, conceived The End of Books1 ([audio file](http://ia800604.us.archive.org/1/items/nonfiction025_librivox/snf025_endofbooks_uzanne_cs.mp3)) in one of his most cited short nonfictional works. His prediction, mid way between pure speculation and prophecy was that the new media of his time, the rise of electricity and phonography, would soon replace the old Gutenberg’s invention.\n\n> “I do not believe (and the progress of electricity and modern mechanism forbids me to believe) that Gutenberg’s invention can do otherwise than sooner or later fall into desuetude as a means of current interpretation of our mental products.” “our grand-children will no longer trust their works to this somewhat antiquated process, now become very easy to replace by phonography”.\n\nThe leap was enormous. Uzanne’s reverie, not only depicted books as a dying medium with no future, but shifted their inherent mutism to the vivacity of the audio recording. It is important to notice, and Uzanne himself insists on the matter, that books don’t have to put a strain on our eyes and bodies anymore, keeping us immobile, squint and hunched over the small print of the page.\n\n> \"You will surely agree with me that reading, as we practice it today, soon brings on great weariness; for not only does it require of the brain a sustained attention which consumes a large proportion of the cerebral phosphates, but it also forces our bodies into various fatiguing attitudes.\" \"Our eyes (…) have been too long abused, and I like to fancy that some one will soon discover the need there is that they should be relieved by laying a greater burden upon our ears.”\n\nWhat is in the book that can not live in another recorded medium? Ideas, scientific knowledge and scholarship, literary work, can all exist in an audible format. For Uzanne, phonography not only can afford the contents of the book, but this change of reception through another sensory organ, the ear instead of the eye, has clear benefits for the overall mental and physical health of the listener.\n\n> “Hearers will not regret the time when they were readers; with eyes unwearied, with countenances refreshed, their air of careless freedom will witness to the benefits of the contemplative life.\" “At home, walking, sightseeing, these fortunate hearers will experience the ineffable delight of reconciling hygiene with instruction; of nourishing their minds while exercising their muscles for there will be pocket phono-operagraphs, for use during excursions among Alpine mountains or in the canyons of the Colorado.”\n\nIt is obvious that Uzanne not only imagined the audiobook but also a prototype portable device that would play it back. ![](http://www.scholarslab.org/wp-content/uploads/2017/09/Screen-Shot-2017-09-18-at-1.35.23-PM.png) It is worth noticing then, that before Sony’s Walkman, or Apple’s iPod “a pocket apparatus (…) suspended by a strap from the shoulder” was not designed to accommodate “a thousand songs in your pocket” (Steve Jobs) but a portable device to liberate the bibliophile’s body from the immobility of the study room. Uzanne’s intuitions, albeit prophetic for the most part, failed to envision a future where both printed and audiobooks exist without posing a threat to each other. New technologies first thought as replacement to the old ones end up coexist offering alternative options of engagement. Audiobooks didn’t replace print books and certainly listening didn’t replace reading.\n\n## The impossible task of reading\n\nHowever, reading, despite being an unhealthy activity as we just saw, heavily taxing one’s eyes and body, forcing its muscles to atrophy, is an overall impossible task. Too much to read, too little time.\n\n> \"When Brandon was entering graduate school, an older student once summed up one of life’s problems as a sort of equation: There is an infinite of material that one could read. There is a finite amount of time that you can spend reading. The lesson was that there are limits to the amount of material that even the most voracious reader can take in. One’s eyes can only move so quickly, one’s mind only process so much. This might sound depressing, as if you’re playing a losing game. But it can also be freeing: if you cannot read everything, why feel the need to try to do so? Instead, read what you can with care.\" 2\n\nThe sentiment is not new. Today’s readers may feel completely crushed under the weight and the abundance of reading material, but so did the erudite from the early modern era. Compiling methods (common place books, anthologies, florilegia) were thus put in place to compress books within books and save the reader from the folly of having to read everything _in extenso_. Pierre Bayard in his first chapter of his now classic _How to Talk About Books You Haven’t Read_ 3 addresses the issue by suggesting a few methods of non-reading.\n\n> “Reading is first and foremost non-reading. Even in the case of the most passionate lifelong readers, the act of picking up and opening a book masks the countergesture that occurs at the same time: the involuntary act of not picking up and not opening all the other books in the universe.” 4\n\nThe paradoxical nature of reading as non-reading, leads Bayard to an important insight: the contents of the book don’t really matter. They can be interchangeable even.5 After all, one’s memory of the books read, will inevitably boil its intricate details to a mush.\n\n> “ The interior of the book is less important than its exterior, or, if you prefer, the interior of the book _is_ its exterior, since what counts in a book is the books alongside it.\" 6\n\nDon’t lose the forest for the trees is what Bayard basically saying. A library is a whole ecosystem that invites the “truly cultured to tend toward exhaustiveness rather than the accumulation of isolated bits of knowledge.” 7 There is a whole network of connections between one book and the totality of books which is undermined when the attention is only given to each book’s singularities.\n\n> “It is, then, hardly important if a cultivated person hasn’t read a given book, for though he has no exact knowledge of its _content_, he may still know its _location_, or in other words how it is situated in relation to other books.” 8\n"},{"id":"2017-10-16-isam-2017-libraries-are-for-making","title":"ISAM 2017 - Libraries are for making","author":"ammon-shepherd","date":"2017/10/16 14:47:00+00:00","categories":null,"url":"isam-2017-libraries-are-for-making","content":"# ISAM 2017 - Libraries are for making\n\nI recently participated in the International Symposium of Academic Makerspaces. I presented a paper, co-authored by [Jennifer Grayburn](https://jennifergrayburn.com/) (formerly a Makerspace Technologist, and now at Temple University's [Digital Scholarship Center](https://sites.temple.edu/tudsc/)). I present here the slides and talking notes of the 7 minute presentation, and a link to the full paper [[Link to PDF](http://scholarslab.org/wp-content/uploads/2017/10/Grayburn-Shepherd-Final.pdf)]. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_000-1024x576.png) Good morning, and thank you for coming. My name is Ammon Shepherd. My paper, co-authored by Jennifer Grayburn, looks at how libraries are uniquely suited to provide makerspaces for traditionally book-bound disciplines. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_001-1024x576.png) Jen Grayburn works at the Digital Scholarship Center, located in Paley Library at Temple University in Philadelphia. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_002-1024x576.png) I am located at the Scholars' Lab in the Alderman Library at the University of Virginia in Charlottesville, Virginia. We both come from humanities backgrounds, so this paper is light on empirical research and heavy on anectodal evidence, but we are both working on tracking data and analyzing that with research questions in mind. To wit, our main question we sought to address with this paper is, How can we get more humanities researchers into our library makerspaces? In the paper we posit that libraries fulfill a unique roll at universities because they are typically departmentally agnostic. Libraries, in general, cater to all faculty, staff, students, and even members of the community. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_003-1024x576.png) With that in mind, Jen and I looked at both of our spaces (Yet Another Cross-space Comparison) and found four comparable attributes of how we attract and support research from humanities researchers. In this paper we look at four attributes:\n\n  1. accessibility,\n  2. contextualization,\n  3. collaboration,\n  4. outreach\n![](http://scholarslab.org//wp-content/uploads/2017/10/isam-presentation_004-1024x576.png) Both our spaces seek to piggy back on the aforementioned phenomenon of Libraries as an academically neutral space. But adding technology normally only seen in the STEM fields proves to be a mental barrier to humanities researchers. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_005-1024x576.png) To address this, both spaces first sought to break down any physical barriers to entry. We are both located in open spaces in the main library on campus. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_006-1024x576.png) The Scholars' Lab space is in a prime study and group-work area with great natural lighting. Physically open access is relatively easy to address, but mental barriers take more detailed planning. The remainder of the comparison points, and some take aways at the end, help to address the issue of breaking down mental barriers. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_007-1024x576.png) The major issue facing humanities research is the mental frustration with technology; usually the reason they give for picking the humanities in the first place. How then to ease that burden? ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_008-1024x576.png) Both the DSC and SLab are staffed with individuals from very diverse backgrounds and skill levels. The DSC has full-time library staff, post-docs and graduate students from departments ranging from science, architectural history, and engineering to business. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_009-1024x576.png) The SLab has 3 full-time staff with library and history degrees, and graduate and undergraduate paid, part-time student employees from language, engineering and chemistry backgrounds. This broad academic background encourages students from all fields to use our spaces. One anecdotal account comes from a bio-med student who felt more comfortable prototyping in our space because she didn't feel an inferiority complex. She probably thought, they're just historians, what do they know? :) ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_010-1024x576.png) Encouraging collaboration enriches both staff and users, and both spaces encourage staff to work on personal research and collaborate with others. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_011-1024x576.png) The DSC partnered with the Ginsburg Library to offer free 3D printing for research, educational or clinical purposes. The 3D print of a pelvis from a CT scan is such a result. They also partnered with the Center for Advancement of Teaching to provide grants to faculty ranging from $500-$3500. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_012-1024x576.png) The SLab provides short term fellowships to humanities grad students for prototyping ideas. We have provided support for students to use 3D prints for presentations, and are helping a cardiovascular medical researcher print exercise equipment for mice. More examples are in the paper. Collaboration with all departments expands the usefulness of the space beyond the physical location and engages the entire university, even humanities scholars. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_013-1024x576.png) Finally, outreach plays a major role in attracting any makers, especially interested humanities scholars. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_014-1024x576.png) The DSC provides workshops and training for all their equipment. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_015-1024x576.png) They also encourage staff and users to blog about successes and failures, and to publish results in journals. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_016-1024x576.png) The SLab holds workshops, has a prominent display case, and is a major stop on all the mandatory freshman library tours. ![](http://scholarslab.org/wp-content/uploads/2017/10/isam-presentation_017-1024x576.png)\n"},{"id":"2017-10-26-gis-day-wednesday-november-15","title":"GIS Day - Wednesday, November 15","author":"chris-gist","date":"2017/10/26 13:41:34+00:00","categories":null,"url":"gis-day-wednesday-november-15","content":"# GIS Day - Wednesday, November 15\n\nMark your calendars, Wednesday, Nov. 15 is GIS Day (http://www.gisday.com/).  To celebrate, all are invited to the University of Virginia Library’s Scholars’ Lab for an afternoon of events. 1PM – Presentation: ArcGIS Pro for ArcMap Users 2PM – Lightning Round Talks 3PM – GIS Day Cake Location: Alderman Library Electronic Classroom (ALD 421, just off Scholars’ Lab) **Presentation: ArcGIS Pro for ArcMap Users** At 1PM, join us for a session on making the switch to ArcGIS Pro by our own Drew Macqueen.  This is a quick and dirty overview of the major differences between ArcMap and ArcGIS Pro. Accept your fate as we delve into the future of desktop GIS. Spoiler alert, it’s totally worth it! **Lighting Talks** Starting at 2PM, our annual tradition of lightning talks continues.  If you have never seen lightning round talks, they can be pretty entertaining: a rapid fire succession of speakers given a set, short amount of time and PowerPoint slides.  In previous years, many great presenters have shown the incredible breadth of disciplines and fields in which GIS is used in meaningful ways. We encourage everyone, including students (UVa, PVCC and high school), researchers and practitioners in the greater Charlottesville community to contribute. In this year’s round, each speaker will be given five to ten minutes (depending on number of presenters) with a maximum of ten slides.  It is a fairly easy task to create and give a lighting round talk.  Help make this year’s event special by participating in the talks.  You can present on anything spatially related you like.  It could be about a project you have worked on, things going on at your office or just something of personal interest. If you have any interest in participating in the lightning round talks, please email us at [uvagis@virginia.edu](mailto:uvagis@virginia.edu) as soon as possible. **GIS Day Cake** Another great tradition continues.  Please join us for the GIS Day cake unveiling and partake in the feeding frenzy."},{"id":"2017-11-14-3d-printed-enclosures-with-openscad","title":"3D Printed Enclosures with OpenSCAD","author":"ryan-maguire","date":"2017/11/14 12:35:03+00:00","categories":null,"url":"3d-printed-enclosures-with-openscad","content":"# 3D Printed Enclosures with OpenSCAD\n\nThis is a tutorial on how to use OpenSCAD to design a 3D object via code instead of using a WYSIWYG editor like Tinkercad, Fusion360, etc. We are currently creating a customized media player to allow people to interact with MP3 artifacts. We've been working in Python to prepare the audio and wanted to generate the enclosure programmatically as well, ideally using open source software. OpenSCAD is a great open source solution for CAD and 3D printing projects. **Modules** In OpenSCAD, you can quickly build duplicates of small parts into more complex designs using \"modules\". By assigning variables to parameters, you can vary the size and location of these objects easily. Modules also help break a larger job into more manageable parts and keep the code nice and clean. The four modules below construct the main body of the enclosure, arrange the holes in the enclosure for our electronic components, add a texture to the enclosure, and assemble all the pieces together. After calling those four modules, all that is left to do is split the enclosure in two and render the halves as separate STL files for printing.   **Main Enclosure Body**\n    \n    \n    /* This module constructs the main body of the enclosure. First, we name the module: */\n    \n    module enclosure() {\n    \n    /* Next, we call the difference function. This specifies that we will be subtracting the second object we call from the first. We will use this to make our cube hollow. */\n    \n    difference() {\n    \n    /* The first object will be our main cube. to give the cube rounded edges, we call minkowski, which will trace the shape we specify around the edges. We will use a sphere, so that the hard edges of the cube will take on the shape of the sphere. */\n    \n    minkowski()\n    {\n    \n    /* Lastly, I am calling difference again here because I wanted to add a small indentation to the bottom of the cube so that it would be more comfortable to hold. Again, difference subtracts the second object from the first, so here, we see a cube; and then an offset (translated), smaller cube(); */\n    \n    difference()\n    {\n    cube([60,40,15], center=true);\n    translate([-15,-10,-8])\n    cube([30,20,1.5]);\n    };\n    \n    /* Having constructed the main box, we can now specify the size of the sphere that we will use to round the edges. */\n    \n    sphere(2);\n    };\n    \n    /* Having specified our main enclosure body with rounded edges and an indentation on the bottom, we finally hollow it out. */\n    \n    cube([61.5,41.5,16], center=true);\n    }\n    }\n    \n\n    **Making Holes for Electronics Components** The second module creates all of the holes that we will place in the enclosure for our electronics components. \n    \n    \n    module enclosureHoles() {\n    \n    /* This section of the code constructs all of the independent holes and joins them into a uniform object. */\n    \n    union() {\n    \n    // Screen\n    translate([-13.75,-11,5.5])\n    cube([27.5, 19.375, 5]);\n    \n    // LED Backlight\n    translate([-14.6875,10,5.5])\n    cube([29.375, 8.75, 5]);\n    \n    // Volume Pot\n    translate([0,-15.75,5.5])\n    rotate([0,0,0])\n    cylinder(r=1.25, h=5);\n    \n    // Pushbutton #1\n    translate([21.5,0,5.5])\n    rotate([0,0,0])\n    cylinder(r=4.75, h=5);\n    \n    // Pushbutton #2\n    translate([23.5,-12,5.5])\n    rotate([0,0,0])\n    cylinder(r=4.75, h=5);\n    \n    // Pushbutton #3\n    translate([-21.5,0,5.5])\n    rotate([0,0,0])\n    cylinder(r=4.75, h=5);\n    \n    // Pushbutton #4\n    translate([-23.5,-12,5.5])\n    rotate([0,0,0])\n    cylinder(r=4.75, h=5);\n    }\n    }\n    \n\n    **Adding a surface texture** The next module creates a texture on the surface of our enclosure from an image file. We wanted to use an image of JPEG artifacts for our project, but you could use anything you'd like, or skip this step entirely. Be sure to keep your PNG files very simple here, otherwise you will run into problems when trying to render. When our PNG file was 31kb it took many hours to render and resulted in a huge STL file that was impossible to print. We needed to get our PNG down to 6kb to make it render in a reasonable amount of time. This resulted in a 5mb STL file. Still kind of big, but reasonable. Below, we call the translate() function so that it sits right on the surface of our enclosure. \n    \n    \n    module texture() {\n    translate([0,0,9])\n    scale([.41,.36,.006]) surface(file=\"/Users/YourUsername/Path/To/Your/File/fileName.png\",\n    center=true);\n    }\n    \n\n    **Bringing it all together** The final module assembles the previous three modules together. \n    \n    \n    module concat() {\n    \n    /* Difference subtracts the second object from the first */\n    \n    difference() {\n    \n    /* Our first object is the Union of two objects. Here, union attaches the texture to the enclosure. */\n    \n    union() {\n    texture();\n    enclosure();\n    };\n    \n    /* the semicolon signals that that is a complete object. Now the second object is the one we made from the various holes. */\n    \n    enclosureHoles();\n    }\n    }\n    \n\n    **Rendering and Printing** Now all we have to do is render using concat() and save as an STL! \n    \n    \n    /* To render the entire design, run: */ \n    \n    concat();\n    \n    /* To actually print, we’ll need to render it in two separate halves which we will attach later. So, comment out the above concat() command and instead run the below code to render the top only */\n    \n    difference() {\n    concat();\n    translate([0,0,-8.5])\n    cube([65,44,2], center=true);\n    }\n    \n    /* then, comment the above out and run the following code to render the bottom only */\n    \n    difference() {\n    concat();\n    translate([0,0,2])\n    cube([65,44,16], center=true);\n    }\n    \n\n  That's all there is to it! With the two halves rendered, all you have to do is save them as STL Files and then use your favorite 3D printing prep software to print. If you'd like to learn more about OpenSCAD, here is a link to a great [cheat sheet](http://www.openscad.org/cheatsheet/)."},{"id":"2017-11-15-measured-unrest-in-the-poetry-of-the-black-arts-movement","title":"Measured Unrest in the Poetry of the Black Arts Movement","author":"ethan-reed","date":"2017/11/15 15:07:10+00:00","categories":null,"url":"measured-unrest-in-the-poetry-of-the-black-arts-movement","content":"# Measured Unrest in the Poetry of the Black Arts Movement\n\nAs one of the graduate fellows at the Scholars’ Lab this year, I am working on a year-long digital project (that’s also a chapter of my dissertation) in collaboration with the folks at the SLab. To sum it up in a sentence, the project hopes to offer a proof-of-concept for performing sentiment analysis on some of the most politically and affectively charged poetry of the 20th century, that of the Black Arts Movement of the 1960s and 70s. Today I wanted to post a brief overview and introduction to what I’m working on. For some context, my research investigates theories of affect as they relate to race, class, and gender in American literature. I focus in particular upon the provocation and articulation of emotions like frustration, anger, and discontentment within recent US literary history as they relate to systemic injustice. An agitprop play that ends with shouts for workers to unite in class revolution; a poetic broadside that vents frustrations against white supremacy in America; a novel that indulges in a revenge fantasy against America’s colonial history. Unlike plays, poems, or novels that seem to obscure, submerge, or confound their own political dimensions, these works wear their hearts on their sleeves: they are frustrated, pissed off with how things are, and unafraid to speak truth to power in a direct, seemingly “un-literary” way. At a certain level, then, this is a question of how, where, and to what ends aesthetics and politics meet in a work of literature. To offer a tidy narrative of this prickly history, this sensibility that mobilizes aesthetic objects to address political injustice has posed all kinds of unexpected, even contradictory problems for literary study. On the one hand, the cool detachment of aesthetic mediation keeps experimental works like John Dos Passos’s Communist-leaning _U.S.A._ trilogy from being seen as mere propaganda, but runs the risk of appearing elitist or self-indulgent. On the other hand, the red-hot political outrage of a protest poem by Amiri Baraka or Sonia Sanchez grounds itself in the present, but may be attacked for subordinating aesthetic sophistication to political agendas. “Anger is loaded with information and energy,” says Audre Lorde in [a 1981 speech](http://www.blackpast.org/1981-audre-lorde-uses-anger-women-responding-racism) on its political uses—but the nature of this affective information, sparked by a given political present, becomes highly vexed when articulated by different groups through aesthetic objects. Building on recent scholarship (like the work of Lauren Berlant and Sianne Ngai) suggesting that feeling gives structure to cultural formations, I argue that a history of unrest in America reveals a pattern of artistic response, a _sensibility_, precipitated by specific historical moments but translated into aesthetic practice through a stable constellation of affective structures. To this end, I examine continuities between politically-engaged aesthetic projects from three periods of discontent in American history: radical journals like _Partisan Review_ in the 1930s; the revolutionary poetry of the Black Arts Movement in the 60s; and contemporary revenge-driven novels drawing from the Red Power movement. My digital project as a graduate fellow is the second of those three chapters. In it I hope to ask two questions in particular: first, how are the feelings associated with injustice in the 1960s and 1970s coded in terms of race and gender? The Black Arts Movement first took shape at the height of the Black Power Movement with the foundation of the Revolutionary Theatre by Amiri Baraka in 1965. As Larry Neal—one of its principal theorists—says in a 1969 manifesto, the “Black Arts movement seeks to link, in a highly conscious manner, art and politics” toward “the liberation of Black people.” Moreover, the movement’s “black esthetic” is famous for its affective dimensions, often exploring the limits and political uses of anger, frustration, and poetic rage. But while BAM writers sought to link art and politics through explicitly racial terms, many—though by no means all—were marked by a failure to attend to the intersections of gender with racial injustice. This leads to my second question: what can natural language processing techniques like sentiment analysis show us about the relations between different dimensions of poetry—like affect and gender—given that poetry, unlike movie reviews or customer feedback, is highly figurative and notoriously difficult to quantify in terms of sentiment or opinion? How can we combine the powerful scale of sentiment analysis with the granularity of close reading to explore the intersections of feeling, gender, race, and injustice in the radical poetry of this period? Moreover, by employing an interpretive method that is in part suspect from a revolutionary perspective—a distanced, potentially de-contextualized computational analysis—I wonder: what limits might these methods have in reading texts that are themselves shaped by the experience of an intense surveillance culture fearful of radical thought? The already vibrant conversations on sentiment analysis and NLP more generally have been illuminating in forming my questions. The discussion between Matthew Jockers and Annie Swafford on the _Syuzhet_ package and “archetypal plot shapes” has helped me not only to explore the current [possibilities and limitations](https://annieswafford.wordpress.com/category/syuzhet/) of sentiment analysis as applied to literary corpora, but also to think through the kinds of results we expect from digital projects and how we verify those results as an academic community. With regards to poetry and NLP more specifically, Lisa Rhody’s [topic modeling of highly figurative ekphrastic poetry](http://journalofdigitalhumanities.org/2-1/topic-modeling-and-figurative-language-by-lisa-m-rhody/) is a great model for how unexpected failures in textual analysis can also be productive, prompting us towards new questions as well as new understandings of familiar methods like close reading. So far I have been working in collaboration with folks at the Scholars’ Lab to work through [the NLTK handbook](http://www.nltk.org/book/), building and prepping my corpus, and beginning to implement some NLP techniques with [TextBlob ](https://textblob.readthedocs.io/en/dev/)on what I have so far. Another post on those first forays into NLP and sentiment analysis coming soon! In the meantime, if you have any questions about the project, texts or tools I should check out, or just find it interesting and want to talk about it, send me an email! I’ll be posting about my progress over the course of the coming months and aiming to keep my process as open as possible to new ideas, feedback, and inspiration from unexpected places."},{"id":"2017-11-21-learning-to-augment-reality","title":"Learning to Augment Reality","author":"christian-howard","date":"2017/11/21 14:35:21+00:00","categories":null,"url":"learning-to-augment-reality","content":"# Learning to Augment Reality\n\nThe Praxis team is in the midst of defining its project, and for the past few weeks, we’ve been playing around with augmented reality (AR), specifically by using [Vuforia](https://www.vuforia.com/) and [Unity](https://unity3d.com/). Learning about AR has been fascinating and, admittedly, a bit frustrating. I won’t go through the process of getting Vuforia and Unity to work with one another (here’s a great [intro video](https://www.youtube.com/watch?v=mjNAPCFaZ9Y) if you’re interested!), but I will briefly discuss some of the challenges and implications of trying to augment reality. First, the target image. The target image is the image that you augment, such that when you point your phone/camera at said image, the 3D figure that you have virtually “added” to the image appears on your screen. But the target image can be tricky. That is, Vuforia scans the target image for certain key features, by means of which the program can identify when your phone/camera is pointed at the target image. I’ve taken some screen shots of a few of the items that I augmented, which Vuforia ranks in terms of “augmentability.” ![](http://scholarslab.org/wp-content/uploads/2017/11/Screen-Shot-2017-11-21-at-12.34.03-PM-300x166.png) ![](http://scholarslab.org/wp-content/uploads/2017/11/Screen-Shot-2017-11-21-at-12.33.12-PM-300x200.png) ![](http://scholarslab.org/wp-content/uploads/2017/11/Screen-Shot-2017-11-21-at-12.33.47-PM-300x211.png) Images 1, 2, & 3: The Scholars’ Lab sign received an augmentable rating of one star, meaning its identifiable features are minimal. The cover of Vi Khi Nao’s book, _Fish in Exile_, has four stars, and the “cowboy” lunchbox residing in the Scholars’ Lab received an augmentable rating of five stars. The yellow crosses indicate the identifying features and patterns that Vuforia recognizes. Not only does the target image need to have enough unique features to be easily identifiable, but the image should be properly edited so that nothing appears in the background. When the image is uploaded with a background, Vuforia will assume that the background is part of the target image, and it will identify features of the background as part of the patterns it is to look for. This will make it difficult if not impossible for your camera/device to recognize the image unless it appears with the exact same background. ![](http://scholarslab.org/wp-content/uploads/2017/11/Screen-Shot-2017-11-21-at-1.38.36-PM-300x207.png) Image 4: Cover of _Fish in Exile _against a mesh chair. The yellow crosses have primarily identified features of the chair – rather than the cover of the book – as unique features, and the “augmentability” of the image has declined to two stars. Another problem that we ran into has to do with subject matter. We’re currently experimenting with items on or around UVA’s grounds. So we’ve been taking photos of items from the Small Special Collections, buildings, memorials, and even lunchboxes sitting around in office spaces. But this becomes problematic when the photos we take are affected by the environment. For instance, I tried taking a photo of the segment of the Berlin Wall that stands on UVA’s grounds, and here’s how it turned out: ![](http://scholarslab.org/wp-content/uploads/2017/11/2017-11-17-08.59.46-300x169.jpg) Image 5: A photo of the Berlin Wall at UVA. Encased in glass, the Berlin Wall is nearly effaced by the reflection of Small Library opposite it. Even, then, if I use a “clean” shot of the Berlin Wall taken from the Internet as my target image, my augmentation of the image will not be identifiable or reproducible if someone were to point their camera/phone at the _actual_ Wall on grounds. So needless to say, our work with AR is still very much in progress. But as we continue developing our AR ventures, considerations of target image complexity and environmental factors will, it seems, help shape the scope of our project. And on this parting note, I’d like to include a couple fun pictures of the fruits of our augmentation experiments thus far. Enjoy! ![](http://scholarslab.org/wp-content/uploads/2017/11/Screen-Shot-2017-11-07-at-1.57.40-PM-300x170.png) ![](http://scholarslab.org/wp-content/uploads/2017/11/Screen-Shot-2017-11-07-at-1.54.13-PM-300x214.png) ![](http://scholarslab.org/wp-content/uploads/2017/11/Screen-Shot-2017-11-21-at-11.44.18-AM-300x192.png) ![](http://scholarslab.org/wp-content/uploads/2017/11/Screen-Shot-2017-11-21-at-11.45.41-AM-300x176.png) Images 6-9: Augmentations of _Fish in Exile _and the Cowboy lunchbox.\n"},{"id":"2017-11-30-all-of-the-questions-a-recap-of-the-2017-bucknell-university-digital-scholarship-pre-conference","title":"“All of the Questions:” A Recap of the 2017 Bucknell University Digital Scholarship Pre-Conference","author":"kelli-shermeyer","date":"2017/11/30 10:31:07+00:00","categories":null,"url":"all-of-the-questions-a-recap-of-the-2017-bucknell-university-digital-scholarship-pre-conference","content":"# “All of the Questions:” A Recap of the 2017 Bucknell University Digital Scholarship Pre-Conference\n\nIn early October I was sent to represent the Scholars’ Lab at the [Bucknell University Digital Scholarship Conference](http://budsc17.scholar.bucknell.edu/) and the pre-conference meeting. This conference brings together an interdisciplinary group of students, teachers, scholars, librarians, and instructional technologists for a weekend of conversation about many aspects of digital scholarship including pedagogy, community outreach/social justice, and institutional best practices. This year’s conference was called “Looking Forward, Looking Back: The Evolution of Digital Scholarship\" and featured keynotes by Stephen Cartwright, Kalev H. Leetaru, and UVA’s on A.D. Carson. **Pre-conference plan:** How do we engage students in digital scholarship and support instructors as they incorporate DH or DS practices in their traditional classes? The BUDSC pre-conference was initially convened around these concerns and charged with the task of developing a “DS Cookbook” featuring ideas, best practices, and resources for instructors looking to include digital projects within their courses. We were initially asked to reflect on questions about our own experiences: What would have been helpful to know the first time we attempted to use digital scholarship in the classroom? How can we engage students in digital scholarship with limited budget, resources, or support? Participants: • Lee Skallerup Bessette, University of Mary Washington • Joshua Finnell, Colgate University • Sarah Hartman-Caverly, Delaware County Community College • Aaron Mauro, Penn State, Erie • Megan Mitchell, Oberlin College • Courtney Paddick, Bucknell University • Carrie Pirmann, Bucknell University • David Pettegrew, Messiah College • Kelli Shermeyer, University of Virginia • Emily Sherwood, Bucknell University **What actually happened…** After a fortifying breakfast of coffee and donuts, our pre-conference group proceeded to make a list of all of the questions and concerns we were stewing over in our work as scholars, teachers, librarians, and instructional technology specialists. This white board was the result: ![digital scholarship white board](http://scholarslab.org/wp-content/uploads/2017/11/IMG_3495-768x1024.jpg) Some of these issues had to do with the intended purpose of the pre-conference – creating a guide for those interested in engaging students with digital scholarship (early concerns included: how do we scaffold or assess digital projects? What does it mean when administrators want students to have “digital literary” or “digital fluency?”) But it became immediately apparent that the interests of this group had a much wider scope. Our morning session consisted of sorting all of the issues raised on this initial whiteboard into categories that we could work with more easily, as well as discussing and sharing resources that we all had at hand. In our afternoon session, we broke up into small groups to work on articulating major questions, a list of best practices, and a set of helpful resources for approaching these topics in a variety of contexts. The results of our work were presented at the pre-conference recap session of BUDSC (which we re-titled \"All of the Questions\") and will be published online forthcoming, but for now, here are some highlights: Communicating with Stakeholders: This group provided strategies for talking with administrators and other stakeholders about the value of collaborative digital scholarship, how to find funding for cross-disciplinary work, and how to communicate about DH work as part of promotion and tenure. They suggested that [A Short Guide to the Digital_Humanities](http://sites.psu.edu/psudhlab/wp-content/uploads/sites/14420/2016/01/D_H_ShortGuide.pdf) can be used as a helpful introduction to digital scholarship for administrators and faculty who are unsure of what they may be getting themselves into. MLA also has some [guidelines](https://www.mla.org/About-Us/Governance/Committees/Committee-Listings/Professional-Issues/Committee-on-Information-Technology/Guidelines-for-Evaluating-Work-in-Digital-Humanities-and-Digital-Media) for evaluating digital scholarship for P&T purposes. Data Security & Privacy: This group explored a whole set of questions that I, frankly, had never thought about in any great depth. They asked us to consider, “What exactly is data, anyway? What do we consider to be data in the context of digital scholarship? As we delve more into the world of digital scholarship, it’s become evident that so much of what we do is based on some form of data – be that numerical data, textual data, geospatial data, audiovisual data, etc. With that in mind, how do you ensure ethical, responsible creation and maintenance/preservation of datasets?” The [Data Curation Centre](http://www.dcc.ac.uk/) can supply researchers with expert help on this topic. This group also suggested Purdue’s [Digital Retention Policy](https://purr.purdue.edu/legal/digitalpreservation) as a model document for schools or departments wishing to develop their own protocols regarding data. Digital Pedagogy: Our group assembled a slew of resources for teachers wanting to engage with digital projects in their classrooms. We asked: “What are we assessing when we ask our students to complete digital assignments and how do their outcomes interface with the goals of traditional scholarship? How do we encourage them to value the process over the product?” The resource list includes many sample assignments and assessment ideas, as well as a collection of what we called “easy wins” – plug and play tools to work with in the classroom, including: [Timeline js](https://timeline.knightlab.com/) \\- make a simple, multimedia timeline [Voyant](https://voyant-tools.org/) \\- beginning large text analysis [Prism](http://prism.scholarslab.org/) \\- annotate your text [Twine](https://twinery.org/) \\- create interactive fiction [Juxta Commons](http://juxtacommons.org/) \\- Compare texts [IMJ](http://www.zachwhalen.net/pg/imj/) \\- Large image visualization IP/OA/Fair Use: This group explored how to approach fair use and copyright as our students use, remix, and edit online content for their own projects. We can begin by assessing our own/our institution’s tolerance for risk. Very important take-away point: No one is carting you off to jail for remixing something – the worst that will happen is a take-down notice. There’s also an increasing amount of legal precedence for going a little cowboy with fair use, as demonstrated by this video which not even Disney was able to successfully remove: _A Fair(y) Use Tale_ <https://www.youtube.com/watch?v=CJn_jC4FNDo>. Sustainability: The questions of project management, project charters, sunsetting, hosting, institutional repositories, and archiving looked like a separate category for us at first, but discussions of these issues were interwoven throughout the other four categories, rather naturally. Shout outs here went to [Reclaim Hosting](https://reclaimhosting.com/) and Miriam Posner’s [blog post on Project Charters](http://miriamposner.com/classes/dh101f17/assignments/final-project/milestones/charter-guidelines/).     N.B. quotations are from the co-authored pre-conference documents.\n"},{"id":"2017-11-30-first-steps-with-nlp-and-a-collection-of-amiri-barakas-poetry","title":"First Steps with NLP and a Collection of Amiri Baraka's Poetry","author":"ethan-reed","date":"2017/11/30 15:27:04+00:00","categories":null,"url":"first-steps-with-nlp-and-a-collection-of-amiri-barakas-poetry","content":"# First Steps with NLP and a Collection of Amiri Baraka's Poetry\n\n**Amiri Baraka’s _Black Magic_, 1969** In this post I’ll discuss my initial foray into natural language processing (NLP)—cleaning up a corpus and prepping it for some basic text analysis techniques. I want to begin, however, with a note on the small textual corpus that I’m using in these preliminary explorations—_Black Magic_, a 1969 collection of three books of poetry by Amiri Baraka. In a prefatory note to the collection, Baraka offers an “Explanation of the Work” that touches on the three books of poetry contained within. “_Sabotage_,” he writes of the first book, “meant I had come to see the superstructure of filth Americans call their way of life, and wanted to see it fall. To sabotage it,” in a word. The second book, he argues, takes this intensity even further: “But _Target Study_ is trying to really study, like bomber crews do the soon to be destroyed cities. Less passive now, less uselessly ‘literary.’” If these comments are any indication, the poetry of _Black Magic_ has a certain level of emotional and political intensity. These poems articulate rage—they thunder, fulminate, and protest, venting a vindicated anger at racial injustice in America. Others simmer with a more restrained heat, but still tend to employ an often unsettling rhetorical violence. Consider, for example, the conclusion of a poem from Sabotage titled “A POEM SOME PEOPLE WILL HAVE TO UNDERSTAND”: \n\n> We have awaited the coming of a natural phenomenon. Mystics and romantics, knowledgeable workers of the land. But none has come. (repeat) but none has come. Will the machinegunners please step forward?\n\nThough startling, this final image punctuates a familiar narrative: the mounting of frustration, the boiling over of feeling while waiting and waiting for justice. The speaker’s closing remark seems to respond to the question asked in [Langston Hughes’s poem \"Harlem\"](https://www.poetryfoundation.org/poems/46548/harlem)—\"What happens to a dream deferred?”—but raises the ante of the inquiry, and shifts from Hughes’s suggestive but still open-ended conclusion (“_Or does it explode?_”) to an unsettling direct request (“Will the machinegunners please step forward?”). The poem also, however, seems aware of its high dramatic tone: it conveys the gravity of this deferred deliverance with somewhat formal rhetoric like “We have awaited” and “But none has come”, but highlights—and perhaps undercuts—its own theatricality by embedding a stage direction in the poem, “(repeat)”. We’ve waited for long enough, the poem seems to argue, but stages this claim in such a way that the final line’s delivery hangs suspended somewhere between deadpan and dead serious. In short: a heightened revolutionary rhetoric permeates the poems in this collection. Many have noted, however, that a troubling violence permeates them as well. For example, [one scholar describes](https://www.jstor.org/stable/1512315?seq=1#page_scan_tab_contents) “Black Art”—one of the most graphic but also most well-known poems from this collection—as “a difficult poem in its race and gender violence, in its violence against peoples.” In the 1991 _The Leroi Jones/Amiri Baraka Reader_, editor William J. Harris describes _Black Magic_ as a collection in which Baraka “traces his painful exit from the white world and his entry into blackness,” an “exorcism of white consciousness and values [that] included a ten-year period of professed hatred of whites, and most especially jews [sic].” Baraka looks back at this period in [his 1984 autobiography](https://books.google.com/books?id=Z-C4E3zdBxkC&q=snarling#v=onepage&q=%22struggling%20to%20be%20born%22&f=false) at a remove from the red-hot intensity of the poems themselves: “I guess, during this period, I got the reputation for being a snarling, white-hating madman. There was some truth to it, because I was struggling to be born, to break out from the shell I could instinctively sense surrounded my own dash for freedom.” From this perspective, this is the violence of escape, of “struggling to be born” from within a constricting “shell”—a version, perhaps, of the violence of the deferred dream that explodes at the end of Langston Hughes’s poem “Harlem.” **Initial Steps with NLP** As a scholar interested in articulations of anger, resentment, and frustration with injustice—particularly injustice of a systemic and institutional nature—as well as digital methodologies, I thought these texts in particular might be worth looking at more closely with NLP techniques. As a graduate student working in a period that is almost entirely still in copyright, however, _Black Magic_ also interested me because it is a small corpus of works—three books of poetry—to which I currently have access through UVA. Though conceptually unglamorous, basic questions of access have played an enormous role in determining the initial paths in my scholarly decision-making process. In this sense, though assembling workable data is always a challenge, scholars interested in literary texts prior to the early 20th century have more options for readily accessible textual corpora. For 20th- and 21st-century scholars interested in textual analysis, however, questions of copyright have made finding openly available textual data from which a corpus could be built an extremely difficult task: while able to share results of analyses through transformative, non-consumptive use, scholars of these periods cannot share the corpora from which these insights are drawn. This presents additional challenges in terms of reproducibility as well as in the already long, labor-intensive task of assembling, cleaning, and prepping a corpus prior to any actual application of NLP techniques. If texts aren’t already available as text files through a university or institution, they either have to be typed out by hand or scanned page by page, run through optical recognition software that transforms the page image into text, then also ultimately cleaned and corrected by hand. In short: no preexisting corpora means no experiments, prototypes, or conceptual ventures without surmounting certain barriers to entry that often prove time- or cost-prohibitive. In the case of this project, even though UVA has access to the 1969 edition of _Black Magic: Collected Poetry 1961-1967_, the text isn’t ready for NLP out-of-the-box. The page contained a lot of text beyond that of the literary work in question: page numbers, line numbers, bibliographical information, headers and footers, all kinds of weird punctuation, and so on. For example, the title of the first poem in _Sabotage_, “Three Modes of History and Culture,” appeared in this electronic edition as follows: \n\n> Baraka, Imamu Amiri, 1934- : Three Modes of History and Culture [from Black Magic: Collected Poetry 1961-1967 (1969) , The Bobbs-Merrill Company ]\n\nTo perform sentiment analysis on _Sabotage_, then, I first needed to get the raw text. By “raw text” I mean a big bag of all of _Sabotage_’s words. My goal initially was to get this bag of words with no line numbers, no punctuation, no capitalized first letters (otherwise Python would think they were two different words), and no spaces. As someone doing this work for the first time, I felt like I could handle writing a program that would remove capital letters, get the txt file into the correct file-type, maybe even get rid of the line numbers. But what about all this clutter surrounding the title of each poem? I considered how I might remove this with a program, but even something as small as irregular line breaks means the words would be chopped up in slightly different ways each time. Given the size of the corpus, I decided it would be [wiser to remove the clutter by hand](https://xkcd.com/1319/) than to write a one-time program that automated it."},{"id":"2017-11-30-my-experience-leading-a-workshop-on-text-analysis-at-washington-and-lee-university","title":"My Experience Leading a Workshop on Text Analysis at Washington and Lee University","author":"sarah-mceleny","date":"2017/11/30 10:15:22+00:00","categories":null,"url":"my-experience-leading-a-workshop-on-text-analysis-at-washington-and-lee-university","content":"# My Experience Leading a Workshop on Text Analysis at Washington and Lee University\n\n_[Sarah went to Washington and Lee University to give a workshop in Prof. Mackenzie Brooks’s DH 102: Data in the Humanities course through a Mellon-funded collaboration with WLUDH. More information about this initiative can be found [here](https://github.com/wludh/research-one-collab/blob/master/wlu-faculty.md), and this piece is crossposted to the [WLUDH blog](http://digitalhumanities.wlu.edu/blog/2017/11/30/my-experience-leading-a-workshop-on-text-analysis-at-washington-and-lee-university/).]_ As a graduate student participating in the University of Virginia and Washington & Lee University digital humanities collaboration, during the fall 2017 I led a guest workshop on text analysis in Mackenzie Brooks’ course DH 102: Data in the Humanities.  This workshop was an exploration of approaches to text analysis in the digital humanities, which concurrently introduced students to basic programming concepts.  For humanities students and scholars, the question of how to begin to conduct text analysis can be tricky because platforms do exist that allow one to perform basic text analyses without any programming knowledge.  However, the ability to write one’s own scripts for text analysis purposes allows for the fine-tuning and tailoring of one’s work in highly-individualized ways that goes beyond the capabilities of popular tools like Voyant. Additionally, the existence of a multitude of Python libraries allows for numerous approaches for understanding the subtleties of a given text of a corpus of them.  As the possibilities and directions for text analysis that Python enables are countless, the goal of this workshop was to introduce students to basic programming concepts in Python through the completion of simple text analysis tasks. At the start of workshop, we discussed how humanities scholars have used text analysis techniques to create some groundbreaking research, such as Matthew Jockers’ research into the language of bestselling novels, as well as the different ways that text analysis can be approached, briefly looking the online text analysis tool, Voyant. For this workshop students downloaded Python3 and used the simple text editor that is automatically installed with it, IDLE.  This way we didn't have to spend time downloading multiple programs.  While IDLE is rather barebones, its functionality as a text editor is fine for learning the basics of Python, especially if one doesn’t want to install other software.  From here, by using a script provided to the students, we explored the concepts of variables, lists, functions, loops, and conditional statements, and their syntax in Python.  Using these concepts, we were able to track the frequency of chosen words throughout different sections of a story read by the script. The workshop then delved into a discussion of libraries and how work can be enhanced and made to better suit one’s needs by using specific Python libraries.  As the focus of the workshop was on text analysis, the Python library that we looked at was NLTK (Natural Language Toolkit), which has a vast variety of functions that aid in natural language processing work, such as word_tokenize() and sent_tokenize(), which break up a text into individual parts, as words or sentences, respectively.  The NLTK function FreqDist() simplifies the task of getting a count of all the individual words in a text, which we had done with Python alone in the prior script before working with NLTK.  The inclusion of NLTK in the workshop was meant to briefly show students how important and useful libraries can be when working with Python. While only so much can be covered over the course of a single workshop, the premise of the workshop was to show students that you can do some very interesting things with text analysis with basic Python knowledge, and to dive into Python programming headfirst while learning about general concepts fundamental to programming.  As digital humanities methods for humanities research are becoming more and more common, working with Python’s capability for natural language processing is a useful tool for humanists, and in an introductory class, the goal of my workshop was to spark students’ interest and curiosity and provide a stepping stone for learning more, and at the end of the workshop, further resources for students to turn to in learning more about Python and text analysis were discussed."},{"id":"2017-12-13-call-for-spring-2018-makerspace-technologist-applications","title":"Call for Spring 2018 Makerspace Technologist Applications","author":"laura-miller","date":"2017/12/13 17:29:28+00:00","categories":null,"url":"call-for-spring-2018-makerspace-technologist-applications","content":"# Call for Spring 2018 Makerspace Technologist Applications\n\n_Are you a UVA graduate student or upper-level undergraduate in the humanities? Come join our team as a Makerspace technologist!_ Our [Makerspace](/makerspace) is designed to foster experimentation with 3D printing, modeling, and digitization, physical computing (e.g. Arduino, wearables), virtual reality, and more. For humanists, it is a good way to learn more about experimental and digital humanities by exploring new uses for digital technologies in fields that do not traditionally integrate them. No prior experience with electronics or 3D printing is needed. Successful candidates will be trained on these tools and will in turn pass on their training to disciplinarily diverse students, faculty, and staff interested in using them for fun, teaching, and research. We also strongly encourage technologists to work on their own personal projects and to develop expertise based on their own scholarly interests. An important aspect of Maker culture is apprenticeship and supporting makers in their pursuit of professional experience. We are looking for motivated individuals who are capable of working independently and value the opportunity to engage with and support a growing community. Benefits of the job may include: access to expertise and mentoring in your field of interest, opportunities for collaboration and publication, use of equipment and tools, and ability to shape Scholars’ Lab workshops and programming. Candidates should be able to work up to 10 hours per week. Applications should consist of a cover letter discussing their interest in working in the Scholars’ Lab, any experience or interest in participating in a maker space, and any previous experience with public service or assisting others in using technology. Please send inquiries and applications to [scholarslab@virginia.edu](mailto:scholarslab@virginia.edu). Multiple openings are available for Spring semester, and review of applicants is ongoing until filled.\n"},{"id":"2017-12-14-fellowship-calls-and-grad-student-professional-development","title":"Fellowship Calls and Grad Student Professional Development","author":"brandon-walsh","date":"2017/12/14 09:42:42+00:00","categories":null,"url":"fellowship-calls-and-grad-student-professional-development","content":"# Fellowship Calls and Grad Student Professional Development\n\nI want to share several developments from the grad programs side of the Lab this semester. It's been a busy fall, and I'm pleased with all the work the team has put into our programs! For one, the CFPs for two of our fellowship programs are now live. [The Praxis Program](http://scholarslab.org/praxis-program-fellowships/), which will welcome its eighth cohort next year, will have a deadline of February 15th for applications from PhD students at UVA. This flagship program is in many ways the core of our graduate community, and we're very excited that it continues to thrive. I am also very pleased to announce that the [Digital Humanities Prototyping fellowships](http://scholarslab.org/digital-humanities-prototyping-fellowships/), piloted this past year with a cohort of four students, will continue next year with its own application deadline of February 15th. Open to PhD *and* MA students at UVA, these fellowships are meant to shore up our support of students in the intermediate years of their graduate work, to provide collaborative projects a space in our fellowship portfolio, and to give young scholars a chance to craft a spark that might catch further down the line with applications for further funding here or elsewhere. Please tell your students and colleagues! I always strongly encourage students to get in touch with me if they are planning to apply - that way they will be on our radar for other opportunities down the line regardless of how this particular application shakes out. Along with our newly restructured [DH Fellows program](http://scholarslab.org/digital-humanities-fellows/), these three fellowship programs provide support and experience for more stages of the graduate student timeline than was previously possible. In addition to the fellowship announcements, I also wanted to draw attention to a revamping of what was formerly known as the \"graduate fellowships\" page. Our programs have grown a lot since this page was last revised, and the new \"graduate fellowships and opportunities\" page now better represents the wealth of offerings in the Scholars' Lab. This new, catch-all page offers a space where students can see all of our opportunities beyond our annual fellowship programs. We regularly employ graduate students as [Makerspace Technologists](http://scholarslab.org/makerspace-technologists/) to assist in 3D printing and experimental computing in our makerspace (and we just released [a call with multiple openings for spring 2018](http://scholarslab.org/announcements/call-for-spring-2018-makerspace-technologist-applications/)!). [Cultural Heritage Informatics Interns](http://scholarslab.org/cultural-heritage-informatics-internship/) each semester work with [Will](http://scholarslab.org/people/will-rourk/) and [Arin](http://scholarslab.org/people/arin-bennett/) to 3D scan, process, and print artifacts all while getting course credit. Chris and Drew regularly work with student [GIS Technicians](http://scholarslab.org/scholars-lab-gis-technician/) who assist in the uploading of GIS datasets and creating applications on our GIS portal, all while getting valuable experience in spatial humanities. And, finally, a [Mellon-funded collaboration with Washington and Lee University](http://scholarslab.org/visiting-workshops-at-washington-and-lee-university/) allows us to send students to their campus to give workshops on digital humanities to undergraduate courses. The amount of experience required for all these opportunities is quite variable, so be sure to read closely - in many cases we are more than happy to have you learn on the job. We've been doing all these things for quite a while, but hopefully now students can find easier access to information about our programs and how to get involved. Finally, I'm especially pleased to share that we have a new section in this page on [professional development for graduate students](http://scholarslab.org/professional-development/). The Scholars' Lab programs give students valuable experiences and training, but we've also historically gone further than these official offerings. As UVA students apply to alt-ac and DH careers, we regularly give advice on the whole process, from finding a job to producing materials to interviewing. These offerings have long been ad hoc and by request, but I worried over the last several months that some potential students might get left out of such arrangements. A student might not know, for example, that we'd be willing to mock interview them in the happy event that they're invited to campus for that digital humanities developer position. Or a student putting together their first job talk for a post-doc in digital humanities might not realize that we're happy to lend a friendly ear and also share our *own* job talks. This section is not perfect, and it by no means represents the sum of what any program can do to support graduate students. If you see something missing, drop me a line to let me know. But hopefully the statement of services there will serve as nice counterpoint to the values that we lay out in our group [charter](http://scholarslab.org/about/charter/); hopefully the page's presence will help someone find their way to us who might not otherwise have done so. After all, tacit assumptions about how others perceive our services can lead to people falling through the cracks, feeling like they're going through a job search alone. Best that we be explicit, and best that we match our values with public statements of what we will do to back them up. So in short - we're here for you. If you're part of the UVA community and looking for help with your DH or alt-ac job search, swing on by and let me know how we can help!\n"}]